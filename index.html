

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="zh" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Scrapy 1.7.3 文档</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html#document-index" class="icon icon-home"> Scrapy
          

          
          </a>

          
            
            
            
              <div class="version">
                最新
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
            
              <!-- Local TOC -->
              <div class="local-toc"><p class="caption"><span class="caption-text">第一步</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-intro/overview">Scrapy一瞥</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#walk-through-of-an-example-spider">一个spider例子</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#what-just-happened">刚刚发生了什么?</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#what-else">还有什么?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#what-s-next">下一个是什么?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-intro/install">安装指南</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#installing-scrapy">安装Scrapy</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#things-that-are-good-to-know">值得了解的事情</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#using-a-virtual-environment-recommended">使用虚拟环境(推荐)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#platform-specific-installation-notes">各平台安装说明</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#windows">Windows</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#ubuntu-14-04-or-above">Ubuntu 14.04及更高版本</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#mac-os-x">Mac OS X</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#pypy">PyPy</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#troubleshooting">故障排除</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#attributeerror-module-object-has-no-attribute-op-no-tlsv1-1">AttributeError: ‘module’ object has no attribute ‘OP_NO_TLSv1_1’</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-intro/tutorial">Scrapy教程</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#creating-a-project">创建一个项目</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#our-first-spider">我们的第一个Spider</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#how-to-run-our-spider">怎么运行spider</a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#what-just-happened-under-the-hood">在后台发生了什么?</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#a-shortcut-to-the-start-requests-method">一个快捷方法start_requests</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#extracting-data">提取数据</a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#xpath-a-brief-intro">XPath: 一个简短的介绍</a></li>
<li class="toctree-l4"><a class="reference internal" href="index.html#extracting-quotes-and-authors">提取引言和作者</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#extracting-data-in-our-spider">在我们的spider提取数据</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#storing-the-scraped-data">存储数据</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#following-links">Following links</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#a-shortcut-for-creating-requests">A shortcut for creating Requests</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#more-examples-and-patterns">More examples and patterns</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#using-spider-arguments">Using spider arguments</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#next-steps">Next steps</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-intro/examples">Examples</a></li>
</ul>
<p class="caption"><span class="caption-text">Basic concepts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/commands">Command line tool</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#configuration-settings">Configuration settings</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#default-structure-of-scrapy-projects">Default structure of Scrapy projects</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#sharing-the-root-directory-between-projects">Sharing the root directory between projects</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#using-the-scrapy-tool">Using the <code class="docutils literal notranslate"><span class="pre">scrapy</span></code> tool</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#creating-projects">Creating projects</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#controlling-projects">Controlling projects</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#available-tool-commands">Available tool commands</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#startproject">startproject</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#genspider">genspider</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#crawl">crawl</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#check">check</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#list">list</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#edit">edit</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#fetch">fetch</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#view">view</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#shell">shell</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#parse">parse</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#settings">settings</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#runspider">runspider</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#version">version</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#bench">bench</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#custom-project-commands">Custom project commands</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#commands-module">COMMANDS_MODULE</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#register-commands-via-setup-py-entry-points">Register commands via setup.py entry points</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/spiders">Spiders</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-spider">scrapy.Spider</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#spider-arguments">Spider arguments</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#generic-spiders">Generic Spiders</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#crawlspider">CrawlSpider</a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#crawling-rules">Crawling rules</a></li>
<li class="toctree-l4"><a class="reference internal" href="index.html#crawlspider-example">CrawlSpider example</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#xmlfeedspider">XMLFeedSpider</a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#xmlfeedspider-example">XMLFeedSpider example</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#csvfeedspider">CSVFeedSpider</a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#csvfeedspider-example">CSVFeedSpider example</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#sitemapspider">SitemapSpider</a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#sitemapspider-examples">SitemapSpider examples</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/selectors">Selectors</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#using-selectors">Using selectors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#constructing-selectors">Constructing selectors</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#id1">Using selectors</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#extensions-to-css-selectors">Extensions to CSS Selectors</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#nesting-selectors">Nesting selectors</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#selecting-element-attributes">Selecting element attributes</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#using-selectors-with-regular-expressions">Using selectors with regular expressions</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#extract-and-extract-first">extract() and extract_first()</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#working-with-xpaths">Working with XPaths</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#working-with-relative-xpaths">Working with relative XPaths</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#when-querying-by-class-consider-using-css">When querying by class, consider using CSS</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#beware-of-the-difference-between-node-1-and-node-1">Beware of the difference between //node[1] and (//node)[1]</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#using-text-nodes-in-a-condition">Using text nodes in a condition</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#variables-in-xpath-expressions">Variables in XPath expressions</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#removing-namespaces">Removing namespaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#using-exslt-extensions">Using EXSLT extensions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#regular-expressions">Regular expressions</a></li>
<li class="toctree-l4"><a class="reference internal" href="index.html#set-operations">Set operations</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#other-xpath-extensions">Other XPath extensions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#module-scrapy.selector">Built-in Selectors reference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#selector-objects">Selector objects</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#selectorlist-objects">SelectorList objects</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#examples">Examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#selector-examples-on-html-response">Selector examples on HTML response</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#selector-examples-on-xml-response">Selector examples on XML response</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/items">Items</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#declaring-items">Declaring Items</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#item-fields">Item Fields</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#working-with-items">Working with Items</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#creating-items">Creating items</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#getting-field-values">Getting field values</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#setting-field-values">Setting field values</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#accessing-all-populated-values">Accessing all populated values</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#copying-items">Copying items</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#other-common-tasks">Other common tasks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#extending-items">Extending Items</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#item-objects">Item objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#field-objects">Field objects</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/loaders">Item Loaders</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#using-item-loaders-to-populate-items">Using Item Loaders to populate items</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#input-and-output-processors">Input and Output processors</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#declaring-item-loaders">Declaring Item Loaders</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#declaring-input-and-output-processors">Declaring Input and Output Processors</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#item-loader-context">Item Loader Context</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#itemloader-objects">ItemLoader objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#nested-loaders">Nested Loaders</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#reusing-and-extending-item-loaders">Reusing and extending Item Loaders</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#module-scrapy.loader.processors">Available built-in processors</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/shell">Scrapy shell</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#configuring-the-shell">Configuring the shell</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#launch-the-shell">Launch the shell</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#using-the-shell">Using the shell</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#available-shortcuts">Available Shortcuts</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#available-scrapy-objects">Available Scrapy objects</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#example-of-shell-session">Example of shell session</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#invoking-the-shell-from-spiders-to-inspect-responses">Invoking the shell from spiders to inspect responses</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/item-pipeline">Item Pipeline</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#writing-your-own-item-pipeline">Writing your own item pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#item-pipeline-example">Item pipeline example</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#price-validation-and-dropping-items-with-no-prices">Price validation and dropping items with no prices</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#write-items-to-a-json-file">Write items to a JSON file</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#write-items-to-mongodb">Write items to MongoDB</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#take-screenshot-of-item">Take screenshot of item</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#duplicates-filter">Duplicates filter</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#activating-an-item-pipeline-component">Activating an Item Pipeline component</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/feed-exports">Feed exports</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#serialization-formats">Serialization formats</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#json">JSON</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#json-lines">JSON lines</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#csv">CSV</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#xml">XML</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#pickle">Pickle</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#marshal">Marshal</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#storages">Storages</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#storage-uri-parameters">Storage URI parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#storage-backends">Storage backends</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#local-filesystem">Local filesystem</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#ftp">FTP</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#s3">S3</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#standard-output">Standard output</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#settings">Settings</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#feed-uri">FEED_URI</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#feed-format">FEED_FORMAT</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#feed-export-encoding">FEED_EXPORT_ENCODING</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#feed-export-fields">FEED_EXPORT_FIELDS</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#feed-export-indent">FEED_EXPORT_INDENT</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#feed-store-empty">FEED_STORE_EMPTY</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#feed-storages">FEED_STORAGES</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#feed-storage-ftp-active">FEED_STORAGE_FTP_ACTIVE</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#feed-storage-s3-acl">FEED_STORAGE_S3_ACL</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#feed-storages-base">FEED_STORAGES_BASE</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#feed-exporters">FEED_EXPORTERS</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#feed-exporters-base">FEED_EXPORTERS_BASE</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/request-response">Requests and Responses</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#request-objects">Request objects</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#passing-additional-data-to-callback-functions">Passing additional data to callback functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#using-errbacks-to-catch-exceptions-in-request-processing">Using errbacks to catch exceptions in request processing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#request-meta-special-keys">Request.meta special keys</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#bindaddress">bindaddress</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#download-timeout">download_timeout</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#download-latency">download_latency</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#download-fail-on-dataloss">download_fail_on_dataloss</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#max-retry-times">max_retry_times</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#request-subclasses">Request subclasses</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#formrequest-objects">FormRequest objects</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#request-usage-examples">Request usage examples</a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#using-formrequest-to-send-data-via-http-post">Using FormRequest to send data via HTTP POST</a></li>
<li class="toctree-l4"><a class="reference internal" href="index.html#using-formrequest-from-response-to-simulate-a-user-login">Using FormRequest.from_response() to simulate a user login</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#jsonrequest">JSONRequest</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#jsonrequest-usage-example">JSONRequest usage example</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#response-objects">Response objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#response-subclasses">Response subclasses</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#textresponse-objects">TextResponse objects</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#htmlresponse-objects">HtmlResponse objects</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#xmlresponse-objects">XmlResponse objects</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/link-extractors">Link Extractors</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#module-scrapy.linkextractors">Built-in link extractors reference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#module-scrapy.linkextractors.lxmlhtml">LxmlLinkExtractor</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/settings">Settings</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#designating-the-settings">Designating the settings</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#populating-the-settings">Populating the settings</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#command-line-options">1. Command line options</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#settings-per-spider">2. Settings per-spider</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#project-settings-module">3. Project settings module</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#default-settings-per-command">4. Default settings per-command</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#default-global-settings">5. Default global settings</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#how-to-access-settings">How to access settings</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#rationale-for-setting-names">Rationale for setting names</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#built-in-settings-reference">Built-in settings reference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#aws-access-key-id">AWS_ACCESS_KEY_ID</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#aws-secret-access-key">AWS_SECRET_ACCESS_KEY</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#aws-endpoint-url">AWS_ENDPOINT_URL</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#aws-use-ssl">AWS_USE_SSL</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#aws-verify">AWS_VERIFY</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#aws-region-name">AWS_REGION_NAME</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#bot-name">BOT_NAME</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#concurrent-items">CONCURRENT_ITEMS</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#concurrent-requests">CONCURRENT_REQUESTS</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#concurrent-requests-per-domain">CONCURRENT_REQUESTS_PER_DOMAIN</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#concurrent-requests-per-ip">CONCURRENT_REQUESTS_PER_IP</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#default-item-class">DEFAULT_ITEM_CLASS</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#default-request-headers">DEFAULT_REQUEST_HEADERS</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#depth-limit">DEPTH_LIMIT</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#depth-priority">DEPTH_PRIORITY</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#depth-stats-verbose">DEPTH_STATS_VERBOSE</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#dnscache-enabled">DNSCACHE_ENABLED</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#dnscache-size">DNSCACHE_SIZE</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#dns-timeout">DNS_TIMEOUT</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#downloader">DOWNLOADER</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#downloader-httpclientfactory">DOWNLOADER_HTTPCLIENTFACTORY</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#downloader-clientcontextfactory">DOWNLOADER_CLIENTCONTEXTFACTORY</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#downloader-client-tls-method">DOWNLOADER_CLIENT_TLS_METHOD</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#downloader-middlewares">DOWNLOADER_MIDDLEWARES</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#downloader-middlewares-base">DOWNLOADER_MIDDLEWARES_BASE</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#downloader-stats">DOWNLOADER_STATS</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#download-delay">DOWNLOAD_DELAY</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#download-handlers">DOWNLOAD_HANDLERS</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#download-handlers-base">DOWNLOAD_HANDLERS_BASE</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#download-timeout">DOWNLOAD_TIMEOUT</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#download-maxsize">DOWNLOAD_MAXSIZE</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#download-warnsize">DOWNLOAD_WARNSIZE</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#download-fail-on-dataloss">DOWNLOAD_FAIL_ON_DATALOSS</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#dupefilter-class">DUPEFILTER_CLASS</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#dupefilter-debug">DUPEFILTER_DEBUG</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#editor">EDITOR</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#extensions">EXTENSIONS</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#extensions-base">EXTENSIONS_BASE</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#feed-tempdir">FEED_TEMPDIR</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#ftp-passive-mode">FTP_PASSIVE_MODE</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#ftp-password">FTP_PASSWORD</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#ftp-user">FTP_USER</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#item-pipelines">ITEM_PIPELINES</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#item-pipelines-base">ITEM_PIPELINES_BASE</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#log-enabled">LOG_ENABLED</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#log-encoding">LOG_ENCODING</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#log-file">LOG_FILE</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#log-format">LOG_FORMAT</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#log-dateformat">LOG_DATEFORMAT</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#log-level">LOG_LEVEL</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#log-stdout">LOG_STDOUT</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#log-short-names">LOG_SHORT_NAMES</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#logstats-interval">LOGSTATS_INTERVAL</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#memdebug-enabled">MEMDEBUG_ENABLED</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#memdebug-notify">MEMDEBUG_NOTIFY</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#memusage-enabled">MEMUSAGE_ENABLED</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#memusage-limit-mb">MEMUSAGE_LIMIT_MB</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#memusage-check-interval-seconds">MEMUSAGE_CHECK_INTERVAL_SECONDS</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#memusage-notify-mail">MEMUSAGE_NOTIFY_MAIL</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#memusage-warning-mb">MEMUSAGE_WARNING_MB</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#newspider-module">NEWSPIDER_MODULE</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#randomize-download-delay">RANDOMIZE_DOWNLOAD_DELAY</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#reactor-threadpool-maxsize">REACTOR_THREADPOOL_MAXSIZE</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#redirect-max-times">REDIRECT_MAX_TIMES</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#redirect-priority-adjust">REDIRECT_PRIORITY_ADJUST</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#retry-priority-adjust">RETRY_PRIORITY_ADJUST</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#robotstxt-obey">ROBOTSTXT_OBEY</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#scheduler">SCHEDULER</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#scheduler-debug">SCHEDULER_DEBUG</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#scheduler-disk-queue">SCHEDULER_DISK_QUEUE</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#scheduler-memory-queue">SCHEDULER_MEMORY_QUEUE</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#scheduler-priority-queue">SCHEDULER_PRIORITY_QUEUE</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#spider-contracts">SPIDER_CONTRACTS</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#spider-contracts-base">SPIDER_CONTRACTS_BASE</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#spider-loader-class">SPIDER_LOADER_CLASS</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#spider-loader-warn-only">SPIDER_LOADER_WARN_ONLY</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#spider-middlewares">SPIDER_MIDDLEWARES</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#spider-middlewares-base">SPIDER_MIDDLEWARES_BASE</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#spider-modules">SPIDER_MODULES</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#stats-class">STATS_CLASS</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#stats-dump">STATS_DUMP</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#statsmailer-rcpts">STATSMAILER_RCPTS</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#telnetconsole-enabled">TELNETCONSOLE_ENABLED</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#telnetconsole-port">TELNETCONSOLE_PORT</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#templates-dir">TEMPLATES_DIR</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#urllength-limit">URLLENGTH_LIMIT</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#user-agent">USER_AGENT</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#settings-documented-elsewhere">Settings documented elsewhere:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/exceptions">Exceptions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#built-in-exceptions-reference">Built-in Exceptions reference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#dropitem">DropItem</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#closespider">CloseSpider</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#dontclosespider">DontCloseSpider</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#ignorerequest">IgnoreRequest</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#notconfigured">NotConfigured</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#notsupported">NotSupported</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Built-in services</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/logging">Logging</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#log-levels">Log levels</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#how-to-log-messages">How to log messages</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#logging-from-spiders">Logging from Spiders</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#logging-configuration">Logging configuration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#logging-settings">Logging settings</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#command-line-options">Command-line options</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#advanced-customization">Advanced customization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#module-scrapy.utils.log">scrapy.utils.log module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/stats">Stats Collection</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#common-stats-collector-uses">Common Stats Collector uses</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#available-stats-collectors">Available Stats Collectors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#memorystatscollector">MemoryStatsCollector</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#dummystatscollector">DummyStatsCollector</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/email">Sending e-mail</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#quick-example">Quick example</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#mailsender-class-reference">MailSender class reference</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#mail-settings">Mail settings</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#mail-from">MAIL_FROM</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#mail-host">MAIL_HOST</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#mail-port">MAIL_PORT</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#mail-user">MAIL_USER</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#mail-pass">MAIL_PASS</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#mail-tls">MAIL_TLS</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#mail-ssl">MAIL_SSL</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/telnetconsole">Telnet Console</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#how-to-access-the-telnet-console">How to access the telnet console</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#available-variables-in-the-telnet-console">Available variables in the telnet console</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#telnet-console-usage-examples">Telnet console usage examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#view-engine-status">View engine status</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#pause-resume-and-stop-the-scrapy-engine">Pause, resume and stop the Scrapy engine</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#telnet-console-signals">Telnet Console signals</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#telnet-settings">Telnet settings</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#telnetconsole-port">TELNETCONSOLE_PORT</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#telnetconsole-host">TELNETCONSOLE_HOST</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#telnetconsole-username">TELNETCONSOLE_USERNAME</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#telnetconsole-password">TELNETCONSOLE_PASSWORD</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/webservice">Web Service</a></li>
</ul>
<p class="caption"><span class="caption-text">Solving specific problems</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-faq">Frequently Asked Questions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#how-does-scrapy-compare-to-beautifulsoup-or-lxml">How does Scrapy compare to BeautifulSoup or lxml?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#can-i-use-scrapy-with-beautifulsoup">Can I use Scrapy with BeautifulSoup?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#what-python-versions-does-scrapy-support">What Python versions does Scrapy support?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#did-scrapy-steal-x-from-django">Did Scrapy “steal” X from Django?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#does-scrapy-work-with-http-proxies">Does Scrapy work with HTTP proxies?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#how-can-i-scrape-an-item-with-attributes-in-different-pages">How can I scrape an item with attributes in different pages?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-crashes-with-importerror-no-module-named-win32api">Scrapy crashes with: ImportError: No module named win32api</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#how-can-i-simulate-a-user-login-in-my-spider">How can I simulate a user login in my spider?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#does-scrapy-crawl-in-breadth-first-or-depth-first-order">Does Scrapy crawl in breadth-first or depth-first order?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#my-scrapy-crawler-has-memory-leaks-what-can-i-do">My Scrapy crawler has memory leaks. What can I do?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#how-can-i-make-scrapy-consume-less-memory">How can I make Scrapy consume less memory?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#can-i-use-basic-http-authentication-in-my-spiders">Can I use Basic HTTP Authentication in my spiders?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#why-does-scrapy-download-pages-in-english-instead-of-my-native-language">Why does Scrapy download pages in English instead of my native language?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#where-can-i-find-some-example-scrapy-projects">Where can I find some example Scrapy projects?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#can-i-run-a-spider-without-creating-a-project">Can I run a spider without creating a project?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#i-get-filtered-offsite-request-messages-how-can-i-fix-them">I get “Filtered offsite request” messages. How can I fix them?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#what-is-the-recommended-way-to-deploy-a-scrapy-crawler-in-production">What is the recommended way to deploy a Scrapy crawler in production?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#can-i-use-json-for-large-exports">Can I use JSON for large exports?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#can-i-return-twisted-deferreds-from-signal-handlers">Can I return (Twisted) deferreds from signal handlers?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#what-does-the-response-status-code-999-means">What does the response status code 999 means?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#can-i-call-pdb-set-trace-from-my-spiders-to-debug-them">Can I call <code class="docutils literal notranslate"><span class="pre">pdb.set_trace()</span></code> from my spiders to debug them?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#simplest-way-to-dump-all-my-scraped-items-into-a-json-csv-xml-file">Simplest way to dump all my scraped items into a JSON/CSV/XML file?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#what-s-this-huge-cryptic-viewstate-parameter-used-in-some-forms">What’s this huge cryptic <code class="docutils literal notranslate"><span class="pre">__VIEWSTATE</span></code> parameter used in some forms?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#what-s-the-best-way-to-parse-big-xml-csv-data-feeds">What’s the best way to parse big XML/CSV data feeds?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#does-scrapy-manage-cookies-automatically">Does Scrapy manage cookies automatically?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#how-can-i-see-the-cookies-being-sent-and-received-from-scrapy">How can I see the cookies being sent and received from Scrapy?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#how-can-i-instruct-a-spider-to-stop-itself">How can I instruct a spider to stop itself?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#how-can-i-prevent-my-scrapy-bot-from-getting-banned">How can I prevent my Scrapy bot from getting banned?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#should-i-use-spider-arguments-or-settings-to-configure-my-spider">Should I use spider arguments or settings to configure my spider?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#i-m-scraping-a-xml-document-and-my-xpath-selector-doesn-t-return-any-items">I’m scraping a XML document and my XPath selector doesn’t return any items</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#how-to-split-an-item-into-multiple-items-in-an-item-pipeline">How to split an item into multiple items in an item pipeline?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/debug">Debugging Spiders</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#parse-command">Parse Command</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-shell">Scrapy Shell</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#open-in-browser">Open in browser</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#logging">Logging</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/contracts">Spiders Contracts</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#custom-contracts">Custom Contracts</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#detecting-check-runs">Detecting check runs</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/practices">Common Practices</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#run-scrapy-from-a-script">Run Scrapy from a script</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#running-multiple-spiders-in-the-same-process">Running multiple spiders in the same process</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#distributed-crawls">Distributed crawls</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#avoiding-getting-banned">Avoiding getting banned</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/broad-crawls">Broad Crawls</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#use-the-right-scheduler-priority-queue">Use the right <code class="docutils literal notranslate"><span class="pre">SCHEDULER_PRIORITY_QUEUE</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#increase-concurrency">Increase concurrency</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#increase-twisted-io-thread-pool-maximum-size">Increase Twisted IO thread pool maximum size</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#setup-your-own-dns">Setup your own DNS</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#reduce-log-level">Reduce log level</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#disable-cookies">Disable cookies</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#disable-retries">Disable retries</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#reduce-download-timeout">Reduce download timeout</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#disable-redirects">Disable redirects</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#enable-crawling-of-ajax-crawlable-pages">Enable crawling of “Ajax Crawlable Pages”</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#crawl-in-bfo-order">Crawl in BFO order</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#be-mindful-of-memory-leaks">Be mindful of memory leaks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/developer-tools">Using your browser’s Developer Tools for scraping</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#caveats-with-inspecting-the-live-browser-dom">Caveats with inspecting the live browser DOM</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#inspecting-a-website">Inspecting a website</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#the-network-tool">The Network-tool</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/dynamic-content">Selecting dynamically-loaded content</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#finding-the-data-source">Finding the data source</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#inspecting-the-source-code-of-a-webpage">Inspecting the source code of a webpage</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#reproducing-requests">Reproducing requests</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#handling-different-response-formats">Handling different response formats</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#parsing-javascript-code">Parsing JavaScript code</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#pre-rendering-javascript">Pre-rendering JavaScript</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#using-a-headless-browser">Using a headless browser</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/leaks">Debugging memory leaks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#common-causes-of-memory-leaks">Common causes of memory leaks</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#too-many-requests">Too Many Requests?</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#debugging-memory-leaks-with-trackref">Debugging memory leaks with <code class="docutils literal notranslate"><span class="pre">trackref</span></code></a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#which-objects-are-tracked">Which objects are tracked?</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#a-real-example">A real example</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#too-many-spiders">Too many spiders?</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#scrapy-utils-trackref-module">scrapy.utils.trackref module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#debugging-memory-leaks-with-guppy">Debugging memory leaks with Guppy</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#debugging-memory-leaks-with-muppy">Debugging memory leaks with muppy</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#leaks-without-leaks">Leaks without leaks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/media-pipeline">Downloading and processing files and images</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#using-the-files-pipeline">Using the Files Pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#using-the-images-pipeline">Using the Images Pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#enabling-your-media-pipeline">Enabling your Media Pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#supported-storage">Supported Storage</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#file-system-storage">File system storage</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#amazon-s3-storage">Amazon S3 storage</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#id1">Google Cloud Storage</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#usage-example">Usage example</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#additional-features">Additional features</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#file-expiration">File expiration</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#thumbnail-generation-for-images">Thumbnail generation for images</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#filtering-out-small-images">Filtering out small images</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#allowing-redirections">Allowing redirections</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#module-scrapy.pipelines.files">Extending the Media Pipelines</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#custom-images-pipeline-example">Custom Images pipeline example</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/deploy">Deploying Spiders</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#deploying-to-a-scrapyd-server">Deploying to a Scrapyd Server</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#deploying-to-scrapy-cloud">Deploying to Scrapy Cloud</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/autothrottle">AutoThrottle extension</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#design-goals">Design goals</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#how-it-works">How it works</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#throttling-algorithm">Throttling algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#settings">Settings</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#autothrottle-enabled">AUTOTHROTTLE_ENABLED</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#autothrottle-start-delay">AUTOTHROTTLE_START_DELAY</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#autothrottle-max-delay">AUTOTHROTTLE_MAX_DELAY</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#autothrottle-target-concurrency">AUTOTHROTTLE_TARGET_CONCURRENCY</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#autothrottle-debug">AUTOTHROTTLE_DEBUG</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/benchmarking">Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/jobs">Jobs: pausing and resuming crawls</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#job-directory">Job directory</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#how-to-use-it">How to use it</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#keeping-persistent-state-between-batches">Keeping persistent state between batches</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#persistence-gotchas">Persistence gotchas</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#cookies-expiration">Cookies expiration</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#request-serialization">Request serialization</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Extending Scrapy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/architecture">Architecture overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#data-flow">Data flow</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#components">Components</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#scrapy-engine">Scrapy Engine</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#scheduler">Scheduler</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#downloader">Downloader</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#spiders">Spiders</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#item-pipeline">Item Pipeline</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#downloader-middlewares">Downloader middlewares</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#spider-middlewares">Spider middlewares</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#event-driven-networking">Event-driven networking</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/downloader-middleware">Downloader Middleware</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#activating-a-downloader-middleware">Activating a downloader middleware</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#writing-your-own-downloader-middleware">Writing your own downloader middleware</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#built-in-downloader-middleware-reference">Built-in downloader middleware reference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#module-scrapy.downloadermiddlewares.cookies">CookiesMiddleware</a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#multiple-cookie-sessions-per-spider">Multiple cookie sessions per spider</a></li>
<li class="toctree-l4"><a class="reference internal" href="index.html#cookies-enabled">COOKIES_ENABLED</a></li>
<li class="toctree-l4"><a class="reference internal" href="index.html#cookies-debug">COOKIES_DEBUG</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#module-scrapy.downloadermiddlewares.defaultheaders">DefaultHeadersMiddleware</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#module-scrapy.downloadermiddlewares.downloadtimeout">DownloadTimeoutMiddleware</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#module-scrapy.downloadermiddlewares.httpauth">HttpAuthMiddleware</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#module-scrapy.downloadermiddlewares.httpcache">HttpCacheMiddleware</a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#dummy-policy-default">Dummy policy (default)</a></li>
<li class="toctree-l4"><a class="reference internal" href="index.html#rfc2616-policy">RFC2616 policy</a></li>
<li class="toctree-l4"><a class="reference internal" href="index.html#filesystem-storage-backend-default">Filesystem storage backend (default)</a></li>
<li class="toctree-l4"><a class="reference internal" href="index.html#dbm-storage-backend">DBM storage backend</a></li>
<li class="toctree-l4"><a class="reference internal" href="index.html#leveldb-storage-backend">LevelDB storage backend</a></li>
<li class="toctree-l4"><a class="reference internal" href="index.html#writing-your-own-storage-backend">Writing your own storage backend</a></li>
<li class="toctree-l4"><a class="reference internal" href="index.html#httpcache-middleware-settings">HTTPCache middleware settings</a><ul>
<li class="toctree-l5"><a class="reference internal" href="index.html#httpcache-enabled">HTTPCACHE_ENABLED</a></li>
<li class="toctree-l5"><a class="reference internal" href="index.html#httpcache-expiration-secs">HTTPCACHE_EXPIRATION_SECS</a></li>
<li class="toctree-l5"><a class="reference internal" href="index.html#httpcache-dir">HTTPCACHE_DIR</a></li>
<li class="toctree-l5"><a class="reference internal" href="index.html#httpcache-ignore-http-codes">HTTPCACHE_IGNORE_HTTP_CODES</a></li>
<li class="toctree-l5"><a class="reference internal" href="index.html#httpcache-ignore-missing">HTTPCACHE_IGNORE_MISSING</a></li>
<li class="toctree-l5"><a class="reference internal" href="index.html#httpcache-ignore-schemes">HTTPCACHE_IGNORE_SCHEMES</a></li>
<li class="toctree-l5"><a class="reference internal" href="index.html#httpcache-storage">HTTPCACHE_STORAGE</a></li>
<li class="toctree-l5"><a class="reference internal" href="index.html#httpcache-dbm-module">HTTPCACHE_DBM_MODULE</a></li>
<li class="toctree-l5"><a class="reference internal" href="index.html#httpcache-policy">HTTPCACHE_POLICY</a></li>
<li class="toctree-l5"><a class="reference internal" href="index.html#httpcache-gzip">HTTPCACHE_GZIP</a></li>
<li class="toctree-l5"><a class="reference internal" href="index.html#httpcache-always-store">HTTPCACHE_ALWAYS_STORE</a></li>
<li class="toctree-l5"><a class="reference internal" href="index.html#httpcache-ignore-response-cache-controls">HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#module-scrapy.downloadermiddlewares.httpcompression">HttpCompressionMiddleware</a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#httpcompressionmiddleware-settings">HttpCompressionMiddleware Settings</a><ul>
<li class="toctree-l5"><a class="reference internal" href="index.html#compression-enabled">COMPRESSION_ENABLED</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#module-scrapy.downloadermiddlewares.httpproxy">HttpProxyMiddleware</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#module-scrapy.downloadermiddlewares.redirect">RedirectMiddleware</a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#redirectmiddleware-settings">RedirectMiddleware settings</a><ul>
<li class="toctree-l5"><a class="reference internal" href="index.html#redirect-enabled">REDIRECT_ENABLED</a></li>
<li class="toctree-l5"><a class="reference internal" href="index.html#redirect-max-times">REDIRECT_MAX_TIMES</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#metarefreshmiddleware">MetaRefreshMiddleware</a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#metarefreshmiddleware-settings">MetaRefreshMiddleware settings</a><ul>
<li class="toctree-l5"><a class="reference internal" href="index.html#metarefresh-enabled">METAREFRESH_ENABLED</a></li>
<li class="toctree-l5"><a class="reference internal" href="index.html#metarefresh-ignore-tags">METAREFRESH_IGNORE_TAGS</a></li>
<li class="toctree-l5"><a class="reference internal" href="index.html#metarefresh-maxdelay">METAREFRESH_MAXDELAY</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#module-scrapy.downloadermiddlewares.retry">RetryMiddleware</a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#retrymiddleware-settings">RetryMiddleware Settings</a><ul>
<li class="toctree-l5"><a class="reference internal" href="index.html#retry-enabled">RETRY_ENABLED</a></li>
<li class="toctree-l5"><a class="reference internal" href="index.html#retry-times">RETRY_TIMES</a></li>
<li class="toctree-l5"><a class="reference internal" href="index.html#retry-http-codes">RETRY_HTTP_CODES</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#module-scrapy.downloadermiddlewares.robotstxt">RobotsTxtMiddleware</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#module-scrapy.downloadermiddlewares.stats">DownloaderStats</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#module-scrapy.downloadermiddlewares.useragent">UserAgentMiddleware</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#module-scrapy.downloadermiddlewares.ajaxcrawl">AjaxCrawlMiddleware</a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#ajaxcrawlmiddleware-settings">AjaxCrawlMiddleware Settings</a><ul>
<li class="toctree-l5"><a class="reference internal" href="index.html#ajaxcrawl-enabled">AJAXCRAWL_ENABLED</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="index.html#httpproxymiddleware-settings">HttpProxyMiddleware settings</a><ul>
<li class="toctree-l5"><a class="reference internal" href="index.html#httpproxy-enabled">HTTPPROXY_ENABLED</a></li>
<li class="toctree-l5"><a class="reference internal" href="index.html#httpproxy-auth-encoding">HTTPPROXY_AUTH_ENCODING</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/spider-middleware">Spider Middleware</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#activating-a-spider-middleware">Activating a spider middleware</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#writing-your-own-spider-middleware">Writing your own spider middleware</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#built-in-spider-middleware-reference">Built-in spider middleware reference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#module-scrapy.spidermiddlewares.depth">DepthMiddleware</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#module-scrapy.spidermiddlewares.httperror">HttpErrorMiddleware</a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#httperrormiddleware-settings">HttpErrorMiddleware settings</a><ul>
<li class="toctree-l5"><a class="reference internal" href="index.html#httperror-allowed-codes">HTTPERROR_ALLOWED_CODES</a></li>
<li class="toctree-l5"><a class="reference internal" href="index.html#httperror-allow-all">HTTPERROR_ALLOW_ALL</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#module-scrapy.spidermiddlewares.offsite">OffsiteMiddleware</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#module-scrapy.spidermiddlewares.referer">RefererMiddleware</a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#referermiddleware-settings">RefererMiddleware settings</a><ul>
<li class="toctree-l5"><a class="reference internal" href="index.html#referer-enabled">REFERER_ENABLED</a></li>
<li class="toctree-l5"><a class="reference internal" href="index.html#referrer-policy">REFERRER_POLICY</a><ul>
<li class="toctree-l6"><a class="reference internal" href="index.html#acceptable-values-for-referrer-policy">Acceptable values for REFERRER_POLICY</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#module-scrapy.spidermiddlewares.urllength">UrlLengthMiddleware</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/extensions">Extensions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#extension-settings">Extension settings</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#loading-activating-extensions">Loading &amp; activating extensions</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#available-enabled-and-disabled-extensions">Available, enabled and disabled extensions</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#disabling-an-extension">Disabling an extension</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#writing-your-own-extension">Writing your own extension</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#sample-extension">Sample extension</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#built-in-extensions-reference">Built-in extensions reference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#general-purpose-extensions">General purpose extensions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#module-scrapy.extensions.logstats">Log Stats extension</a></li>
<li class="toctree-l4"><a class="reference internal" href="index.html#module-scrapy.extensions.corestats">Core Stats extension</a></li>
<li class="toctree-l4"><a class="reference internal" href="index.html#module-scrapy.extensions.telnet">Telnet console extension</a></li>
<li class="toctree-l4"><a class="reference internal" href="index.html#module-scrapy.extensions.memusage">Memory usage extension</a></li>
<li class="toctree-l4"><a class="reference internal" href="index.html#module-scrapy.extensions.memdebug">Memory debugger extension</a></li>
<li class="toctree-l4"><a class="reference internal" href="index.html#module-scrapy.extensions.closespider">Close spider extension</a><ul>
<li class="toctree-l5"><a class="reference internal" href="index.html#closespider-timeout">CLOSESPIDER_TIMEOUT</a></li>
<li class="toctree-l5"><a class="reference internal" href="index.html#closespider-itemcount">CLOSESPIDER_ITEMCOUNT</a></li>
<li class="toctree-l5"><a class="reference internal" href="index.html#closespider-pagecount">CLOSESPIDER_PAGECOUNT</a></li>
<li class="toctree-l5"><a class="reference internal" href="index.html#closespider-errorcount">CLOSESPIDER_ERRORCOUNT</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="index.html#module-scrapy.extensions.statsmailer">StatsMailer extension</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#debugging-extensions">Debugging extensions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#stack-trace-dump-extension">Stack trace dump extension</a></li>
<li class="toctree-l4"><a class="reference internal" href="index.html#debugger-extension">Debugger extension</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/api">Core API</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#crawler-api">Crawler API</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#module-scrapy.settings">Settings API</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#module-scrapy.spiderloader">SpiderLoader API</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#module-scrapy.signalmanager">Signals API</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#stats-collector-api">Stats Collector API</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/signals">Signals</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#deferred-signal-handlers">Deferred signal handlers</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#module-scrapy.signals">Built-in signals reference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#engine-started">engine_started</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#engine-stopped">engine_stopped</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#item-scraped">item_scraped</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#item-dropped">item_dropped</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#item-error">item_error</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#spider-closed">spider_closed</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#spider-opened">spider_opened</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#spider-idle">spider_idle</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#spider-error">spider_error</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#request-scheduled">request_scheduled</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#request-dropped">request_dropped</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#request-reached-downloader">request_reached_downloader</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#response-received">response_received</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#response-downloaded">response_downloaded</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-topics/exporters">Item Exporters</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#using-item-exporters">Using Item Exporters</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#serialization-of-item-fields">Serialization of item fields</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#declaring-a-serializer-in-the-field">1. Declaring a serializer in the field</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#overriding-the-serialize-field-method">2. Overriding the serialize_field() method</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#built-in-item-exporters-reference">Built-in Item Exporters reference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#baseitemexporter">BaseItemExporter</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#xmlitemexporter">XmlItemExporter</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#csvitemexporter">CsvItemExporter</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#pickleitemexporter">PickleItemExporter</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#pprintitemexporter">PprintItemExporter</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#jsonitemexporter">JsonItemExporter</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#jsonlinesitemexporter">JsonLinesItemExporter</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">All the rest</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-news">Release notes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-1-7-3-2019-08-01">Scrapy 1.7.3 (2019-08-01)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-1-7-2-2019-07-23">Scrapy 1.7.2 (2019-07-23)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-1-7-1-2019-07-18">Scrapy 1.7.1 (2019-07-18)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-1-7-0-2019-07-18">Scrapy 1.7.0 (2019-07-18)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#backward-incompatible-changes">Backward-incompatible changes</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#new-features">New features</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#bug-fixes">Bug fixes</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#documentation">Documentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#deprecation-removals">Deprecation removals</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#deprecations">Deprecations</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#other-changes">Other changes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-1-6-0-2019-01-30">Scrapy 1.6.0 (2019-01-30)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#selector-api-changes">Selector API changes</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#telnet-console">Telnet console</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#new-extensibility-features">New extensibility features</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#new-filepipeline-and-mediapipeline-features">New FilePipeline and MediaPipeline features</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#scrapy-contracts-improvements"><code class="docutils literal notranslate"><span class="pre">scrapy.contracts</span></code> improvements</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#usability-improvements">Usability improvements</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#id2">Bug fixes</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#documentation-improvements">Documentation improvements</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#id3">Deprecation removals</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#other-improvements-cleanups">Other improvements, cleanups</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-1-5-2-2019-01-22">Scrapy 1.5.2 (2019-01-22)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-1-5-1-2018-07-12">Scrapy 1.5.1 (2018-07-12)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-1-5-0-2017-12-29">Scrapy 1.5.0 (2017-12-29)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#id4">Backward Incompatible Changes</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#id5">New features</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#id6">Bug fixes</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#docs">Docs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-1-4-0-2017-05-18">Scrapy 1.4.0 (2017-05-18)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#deprecations-and-backward-incompatible-changes">Deprecations and Backward Incompatible Changes</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#id7">New Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#id8">Bug fixes</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#cleanups-refactoring">Cleanups &amp; Refactoring</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#id9">Documentation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-1-3-3-2017-03-10">Scrapy 1.3.3 (2017-03-10)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#id10">Bug fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-1-3-2-2017-02-13">Scrapy 1.3.2 (2017-02-13)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#id11">Bug fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-1-3-1-2017-02-08">Scrapy 1.3.1 (2017-02-08)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#id12">New features</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#id13">Bug fixes</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#id14">Documentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#cleanups">Cleanups</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-1-3-0-2016-12-21">Scrapy 1.3.0 (2016-12-21)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#id15">New Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#dependencies-cleanups">Dependencies &amp; Cleanups</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-1-2-3-2017-03-03">Scrapy 1.2.3 (2017-03-03)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-1-2-2-2016-12-06">Scrapy 1.2.2 (2016-12-06)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#id16">Bug fixes</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#id17">Documentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#id18">Other changes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-1-2-1-2016-10-21">Scrapy 1.2.1 (2016-10-21)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#id19">Bug fixes</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#id20">Documentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#id21">Other changes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-1-2-0-2016-10-03">Scrapy 1.2.0 (2016-10-03)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#id22">New Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#id23">Bug fixes</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#refactoring">Refactoring</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#tests-requirements">Tests &amp; Requirements</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#id24">Documentation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-1-1-4-2017-03-03">Scrapy 1.1.4 (2017-03-03)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-1-1-3-2016-09-22">Scrapy 1.1.3 (2016-09-22)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#id25">Bug fixes</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#id26">Documentation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-1-1-2-2016-08-18">Scrapy 1.1.2 (2016-08-18)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#id27">Bug fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-1-1-1-2016-07-13">Scrapy 1.1.1 (2016-07-13)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#id28">Bug fixes</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#id29">New features</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#id30">Documentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#tests">Tests</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-1-1-0-2016-05-11">Scrapy 1.1.0 (2016-05-11)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#beta-python-3-support">Beta Python 3 Support</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#additional-new-features-and-enhancements">Additional New Features and Enhancements</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#deprecations-and-removals">Deprecations and Removals</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#relocations">Relocations</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#bugfixes">Bugfixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-1-0-7-2017-03-03">Scrapy 1.0.7 (2017-03-03)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-1-0-6-2016-05-04">Scrapy 1.0.6 (2016-05-04)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-1-0-5-2016-02-04">Scrapy 1.0.5 (2016-02-04)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-1-0-4-2015-12-30">Scrapy 1.0.4 (2015-12-30)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-1-0-3-2015-08-11">Scrapy 1.0.3 (2015-08-11)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-1-0-2-2015-08-06">Scrapy 1.0.2 (2015-08-06)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-1-0-1-2015-07-01">Scrapy 1.0.1 (2015-07-01)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-1-0-0-2015-06-19">Scrapy 1.0.0 (2015-06-19)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#support-for-returning-dictionaries-in-spiders">Support for returning dictionaries in spiders</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#per-spider-settings-gsoc-2014">Per-spider settings (GSoC 2014)</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#python-logging">Python Logging</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#crawler-api-refactoring-gsoc-2014">Crawler API refactoring (GSoC 2014)</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#module-relocations">Module Relocations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="index.html#full-list-of-relocations">Full list of relocations</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#changelog">Changelog</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-0-24-6-2015-04-20">Scrapy 0.24.6 (2015-04-20)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-0-24-5-2015-02-25">Scrapy 0.24.5 (2015-02-25)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-0-24-4-2014-08-09">Scrapy 0.24.4 (2014-08-09)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-0-24-3-2014-08-09">Scrapy 0.24.3 (2014-08-09)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-0-24-2-2014-07-08">Scrapy 0.24.2 (2014-07-08)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-0-24-1-2014-06-27">Scrapy 0.24.1 (2014-06-27)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-0-24-0-2014-06-26">Scrapy 0.24.0 (2014-06-26)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#enhancements">Enhancements</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#id32">Bugfixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-0-22-2-released-2014-02-14">Scrapy 0.22.2 (released 2014-02-14)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-0-22-1-released-2014-02-08">Scrapy 0.22.1 (released 2014-02-08)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-0-22-0-released-2014-01-17">Scrapy 0.22.0 (released 2014-01-17)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#id33">Enhancements</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#fixes">Fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-0-20-2-released-2013-12-09">Scrapy 0.20.2 (released 2013-12-09)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-0-20-1-released-2013-11-28">Scrapy 0.20.1 (released 2013-11-28)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-0-20-0-released-2013-11-08">Scrapy 0.20.0 (released 2013-11-08)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#id34">Enhancements</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#id35">Bugfixes</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#other">Other</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#thanks">Thanks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-0-18-4-released-2013-10-10">Scrapy 0.18.4 (released 2013-10-10)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-0-18-3-released-2013-10-03">Scrapy 0.18.3 (released 2013-10-03)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-0-18-2-released-2013-09-03">Scrapy 0.18.2 (released 2013-09-03)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-0-18-1-released-2013-08-27">Scrapy 0.18.1 (released 2013-08-27)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-0-18-0-released-2013-08-09">Scrapy 0.18.0 (released 2013-08-09)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-0-16-5-released-2013-05-30">Scrapy 0.16.5 (released 2013-05-30)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-0-16-4-released-2013-01-23">Scrapy 0.16.4 (released 2013-01-23)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-0-16-3-released-2012-12-07">Scrapy 0.16.3 (released 2012-12-07)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-0-16-2-released-2012-11-09">Scrapy 0.16.2 (released 2012-11-09)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-0-16-1-released-2012-10-26">Scrapy 0.16.1 (released 2012-10-26)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-0-16-0-released-2012-10-18">Scrapy 0.16.0 (released 2012-10-18)</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-0-14-4">Scrapy 0.14.4</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-0-14-3">Scrapy 0.14.3</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-0-14-2">Scrapy 0.14.2</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-0-14-1">Scrapy 0.14.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-0-14">Scrapy 0.14</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#new-features-and-settings">New features and settings</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#code-rearranged-and-removed">Code rearranged and removed</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-0-12">Scrapy 0.12</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#new-features-and-improvements">New features and improvements</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#scrapyd-changes">Scrapyd changes</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#changes-to-settings">Changes to settings</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#deprecated-obsoleted-functionality">Deprecated/obsoleted functionality</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-0-10">Scrapy 0.10</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#id36">New features and improvements</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#command-line-tool-changes">Command-line tool changes</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#api-changes">API changes</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#id37">Changes to settings</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-0-9">Scrapy 0.9</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#id38">New features and improvements</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#id39">API changes</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#changes-to-default-settings">Changes to default settings</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-0-8">Scrapy 0.8</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#id40">New features</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#id41">Backward-incompatible changes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#scrapy-0-7">Scrapy 0.7</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-contributing">Contributing to Scrapy</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#reporting-bugs">Reporting bugs</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#writing-patches">Writing patches</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#submitting-patches">Submitting patches</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#coding-style">Coding style</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#documentation-policies">Documentation policies</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#tests">Tests</a><ul>
<li class="toctree-l3"><a class="reference internal" href="index.html#running-tests">Running tests</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#writing-tests">Writing tests</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#document-versioning">Versioning and API Stability</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#id1">Versioning</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#api-stability">API Stability</a></li>
</ul>
</li>
</ul>
</div>
            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html#document-index">Scrapy</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html#document-index">文档</a> &raquo;</li>
        
      <li>Scrapy 1.7.3 文档</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/scrapy/scrapy/blob/1.7/docs/index.rst" class="fa fa-github"> 在GitHub编辑</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="scrapy-version-documentation">
<span id="topics-index"></span><h1>Scrapy 1.7 文档<a class="headerlink" href="#scrapy-version-documentation" title="Permalink to this headline">¶</a></h1>
<p>Scrapy 是一个快速高级的<a class="reference external" href="https://en.wikipedia.org/wiki/Web_crawler">web爬虫</a> 和 <a class="reference external" href="https://en.wikipedia.org/wiki/Web_scraping">web抓取</a> 框架, 被用于
抓取网站和网站页面内的结构化数据. 它用途广泛, 从数据挖掘到监控和自动化测试都可用.</p>
<div class="section" id="getting-help">
<h2>寻求帮助<a class="headerlink" href="#getting-help" title="Permalink to this headline">¶</a></h2>
<p>有问题? 我们很乐意帮忙!</p>
<ul class="simple">
<li>尝试 <a class="reference internal" href="index.html#document-faq"><span class="doc">FAQ</span></a> – 可以从这得到一些常见问题的解决办法.</li>
<li>寻找具体信息? 尝试 <a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a> 或者 <a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a>.</li>
<li>询问或查找问题<a class="reference external" href="https://stackoverflow.com/tags/scrapy">使用scrapy标签的StackOverflow</a>.</li>
<li>询问或查找问题在<a class="reference external" href="https://www.reddit.com/r/scrapy/">Scrapy subreddit</a>.</li>
<li>查询问题的档案在<a class="reference external" href="https://groups.google.com/forum/#!forum/scrapy-users">scrapy-users mailing list</a>.</li>
<li>问问题在<a class="reference external" href="irc://irc.freenode.net/scrapy">#scrapy IRC channel</a>,</li>
<li>提交bug给Scrapy在我们的<a class="reference external" href="https://github.com/scrapy/scrapy/issues">问题追踪</a>.</li>
</ul>
</div>
<div class="section" id="first-steps">
<h2>First steps<a class="headerlink" href="#first-steps" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-intro/overview"></span><div class="section" id="scrapy-at-a-glance">
<span id="intro-overview"></span><h3>Scrapy一瞥<a class="headerlink" href="#scrapy-at-a-glance" title="Permalink to this headline">¶</a></h3>
<p>Scrapy应用可以用于抓取网站和结构化数据,它用途广泛，像数据挖掘,数据处理或者历史档案.</p>
<p>尽管Scrapy最初是为<a class="reference external" href="https://en.wikipedia.org/wiki/Web_scraping">web 抓取</a>, 它还可以用于使用APIs提取数据 (像 <a class="reference external" href="https://affiliate-program.amazon.com/gp/advertising/api/detail/main.html">Amazon Associates Web Services</a>) 或者
用做通用爬虫.</p>
<div class="section" id="walk-through-of-an-example-spider">
<h4>一个示例spider(爬虫)<a class="headerlink" href="#walk-through-of-an-example-spider" title="Permalink to this headline">¶</a></h4>
<p>为了展示Scrapy, 我们将带你使用一个最简单的方法运行一个爬虫.</p>
<p>这是一个爬虫(从网站<a class="reference external" href="http://quotes.toscrape.com">http://quotes.toscrape.com</a>抓取名言)的代码:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">QuotesSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;quotes&#39;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s1">&#39;http://quotes.toscrape.com/tag/humor/&#39;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">quote</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;div.quote&#39;</span><span class="p">):</span>
            <span class="k">yield</span> <span class="p">{</span>
                <span class="s1">&#39;text&#39;</span><span class="p">:</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;span.text::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(),</span>
                <span class="s1">&#39;author&#39;</span><span class="p">:</span> <span class="n">quote</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;span/small/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(),</span>
            <span class="p">}</span>

        <span class="n">next_page</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;li.next a::attr(&quot;href&quot;)&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">next_page</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">yield</span> <span class="n">response</span><span class="o">.</span><span class="n">follow</span><span class="p">(</span><span class="n">next_page</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
</pre></div>
</div>
<p>将代码放入一个text文件, 给它命名, 像 <code class="docutils literal notranslate"><span class="pre">quotes_spider.py</span></code>
然后使用<a class="reference internal" href="index.html#std:command-runspider"><code class="xref std std-command docutils literal notranslate"><span class="pre">runspider</span></code></a> 命令运行爬虫:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">runspider</span> <span class="n">quotes_spider</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">o</span> <span class="n">quotes</span><span class="o">.</span><span class="n">json</span>
</pre></div>
</div>
<p>当爬虫结束，你将得到一个 <code class="docutils literal notranslate"><span class="pre">quotes.json</span></code> 文件(一个JSON格式的引言列表), 包含文字和作者, 像下面一样 (为了更好的可读性，这里重新格式化):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[{</span>
    <span class="s2">&quot;author&quot;</span><span class="p">:</span> <span class="s2">&quot;Jane Austen&quot;</span><span class="p">,</span>
    <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;</span><span class="se">\u201c</span><span class="s2">The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.</span><span class="se">\u201d</span><span class="s2">&quot;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s2">&quot;author&quot;</span><span class="p">:</span> <span class="s2">&quot;Groucho Marx&quot;</span><span class="p">,</span>
    <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;</span><span class="se">\u201c</span><span class="s2">Outside of a dog, a book is man&#39;s best friend. Inside of a dog it&#39;s too dark to read.</span><span class="se">\u201d</span><span class="s2">&quot;</span>
<span class="p">},</span>
<span class="p">{</span>
    <span class="s2">&quot;author&quot;</span><span class="p">:</span> <span class="s2">&quot;Steve Martin&quot;</span><span class="p">,</span>
    <span class="s2">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;</span><span class="se">\u201c</span><span class="s2">A day without sunshine is like, you know, night.</span><span class="se">\u201d</span><span class="s2">&quot;</span>
<span class="p">},</span>
<span class="o">...</span><span class="p">]</span>
</pre></div>
</div>
<div class="section" id="what-just-happened">
<h5>发生了什么?<a class="headerlink" href="#what-just-happened" title="Permalink to this headline">¶</a></h5>
<p>当你运行命令 <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">runspider</span> <span class="pre">quotes_spider.py</span></code>, Scrapy looked for a
Spider definition inside it and ran it through its crawler engine.</p>
<p>The crawl started by making requests to the URLs defined in the <code class="docutils literal notranslate"><span class="pre">start_urls</span></code>
attribute (in this case, only the URL for quotes in <em>humor</em> category)
and called the default callback method <code class="docutils literal notranslate"><span class="pre">parse</span></code>, passing the response object as
an argument. In the <code class="docutils literal notranslate"><span class="pre">parse</span></code> callback, we loop through the quote elements
using a CSS Selector, yield a Python dict with the extracted quote text and author,
look for a link to the next page and schedule another request using the same
<code class="docutils literal notranslate"><span class="pre">parse</span></code> method as callback.</p>
<p>Here you notice one of the main advantages about Scrapy: requests are
<a class="reference internal" href="index.html#topics-architecture"><span class="std std-ref">scheduled and processed asynchronously</span></a>.  This
means that Scrapy doesn’t need to wait for a request to be finished and
processed, it can send another request or do other things in the meantime. This
also means that other requests can keep going even if some request fails or an
error happens while handling it.</p>
<p>While this enables you to do very fast crawls (sending multiple concurrent
requests at the same time, in a fault-tolerant way) Scrapy also gives you
control over the politeness of the crawl through <a class="reference internal" href="index.html#topics-settings-ref"><span class="std std-ref">a few settings</span></a>. You can do things like setting a download delay between
each request, limiting amount of concurrent requests per domain or per IP, and
even <a class="reference internal" href="index.html#topics-autothrottle"><span class="std std-ref">using an auto-throttling extension</span></a> that tries
to figure out these automatically.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This is using <a class="reference internal" href="index.html#topics-feed-exports"><span class="std std-ref">feed exports</span></a> to generate the
JSON file, you can easily change the export format (XML or CSV, for example) or the
storage backend (FTP or <a class="reference external" href="https://aws.amazon.com/s3/">Amazon S3</a>, for example).  You can also write an
<a class="reference internal" href="index.html#topics-item-pipeline"><span class="std std-ref">item pipeline</span></a> to store the items in a database.</p>
</div>
</div>
</div>
<div class="section" id="what-else">
<span id="topics-whatelse"></span><h4>What else?<a class="headerlink" href="#what-else" title="Permalink to this headline">¶</a></h4>
<p>You’ve seen how to extract and store items from a website using Scrapy, but
this is just the surface. Scrapy provides a lot of powerful features for making
scraping easy and efficient, such as:</p>
<ul class="simple">
<li>Built-in support for <a class="reference internal" href="index.html#topics-selectors"><span class="std std-ref">selecting and extracting</span></a> data
from HTML/XML sources using extended CSS selectors and XPath expressions,
with helper methods to extract using regular expressions.</li>
<li>An <a class="reference internal" href="index.html#topics-shell"><span class="std std-ref">interactive shell console</span></a> (IPython aware) for trying
out the CSS and XPath expressions to scrape data, very useful when writing or
debugging your spiders.</li>
<li>Built-in support for <a class="reference internal" href="index.html#topics-feed-exports"><span class="std std-ref">generating feed exports</span></a> in
multiple formats (JSON, CSV, XML) and storing them in multiple backends (FTP,
S3, local filesystem)</li>
<li>Robust encoding support and auto-detection, for dealing with foreign,
non-standard and broken encoding declarations.</li>
<li><a class="reference internal" href="index.html#extending-scrapy"><span class="std std-ref">Strong extensibility support</span></a>, allowing you to plug
in your own functionality using <a class="reference internal" href="index.html#topics-signals"><span class="std std-ref">signals</span></a> and a
well-defined API (middlewares, <a class="reference internal" href="index.html#topics-extensions"><span class="std std-ref">extensions</span></a>, and
<a class="reference internal" href="index.html#topics-item-pipeline"><span class="std std-ref">pipelines</span></a>).</li>
<li>Wide range of built-in extensions and middlewares for handling:<ul>
<li>cookies and session handling</li>
<li>HTTP features like compression, authentication, caching</li>
<li>user-agent spoofing</li>
<li>robots.txt</li>
<li>crawl depth restriction</li>
<li>and more</li>
</ul>
</li>
<li>A <a class="reference internal" href="index.html#topics-telnetconsole"><span class="std std-ref">Telnet console</span></a> for hooking into a Python
console running inside your Scrapy process, to introspect and debug your
crawler</li>
<li>Plus other goodies like reusable spiders to crawl sites from <a class="reference external" href="https://www.sitemaps.org/index.html">Sitemaps</a> and
XML/CSV feeds, a media pipeline for <a class="reference internal" href="index.html#topics-media-pipeline"><span class="std std-ref">automatically downloading images</span></a> (or any other media) associated with the scraped
items, a caching DNS resolver, and much more!</li>
</ul>
</div>
<div class="section" id="what-s-next">
<h4>What’s next?<a class="headerlink" href="#what-s-next" title="Permalink to this headline">¶</a></h4>
<p>The next steps for you are to <a class="reference internal" href="index.html#intro-install"><span class="std std-ref">install Scrapy</span></a>,
<a class="reference internal" href="index.html#intro-tutorial"><span class="std std-ref">follow through the tutorial</span></a> to learn how to create
a full-blown Scrapy project and <a class="reference external" href="https://scrapy.org/community/">join the community</a>. Thanks for your
interest!</p>
</div>
</div>
<span id="document-intro/install"></span><div class="section" id="installation-guide">
<span id="intro-install"></span><h3>Installation guide<a class="headerlink" href="#installation-guide" title="Permalink to this headline">¶</a></h3>
<div class="section" id="installing-scrapy">
<h4>Installing Scrapy<a class="headerlink" href="#installing-scrapy" title="Permalink to this headline">¶</a></h4>
<p>Scrapy runs on Python 2.7 and Python 3.4 or above
under CPython (default Python implementation) and PyPy (starting with PyPy 5.9).</p>
<p>If you’re using <a class="reference external" href="https://docs.anaconda.com/anaconda/">Anaconda</a> or <a class="reference external" href="https://conda.io/docs/user-guide/install/index.html">Miniconda</a>, you can install the package from
the <a class="reference external" href="https://conda-forge.org/">conda-forge</a> channel, which has up-to-date packages for Linux, Windows
and OS X.</p>
<p>To install Scrapy using <code class="docutils literal notranslate"><span class="pre">conda</span></code>, run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">conda</span> <span class="n">install</span> <span class="o">-</span><span class="n">c</span> <span class="n">conda</span><span class="o">-</span><span class="n">forge</span> <span class="n">scrapy</span>
</pre></div>
</div>
<p>Alternatively, if you’re already familiar with installation of Python packages,
you can install Scrapy and its dependencies from PyPI with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">Scrapy</span>
</pre></div>
</div>
<p>Note that sometimes this may require solving compilation issues for some Scrapy
dependencies depending on your operating system, so be sure to check the
<a class="reference internal" href="#intro-install-platform-notes"><span class="std std-ref">Platform specific installation notes</span></a>.</p>
<p>We strongly recommend that you install Scrapy in <a class="reference internal" href="#intro-using-virtualenv"><span class="std std-ref">a dedicated virtualenv</span></a>,
to avoid conflicting with your system packages.</p>
<p>For more detailed and platform specifics instructions, as well as
troubleshooting information, read on.</p>
<div class="section" id="things-that-are-good-to-know">
<h5>Things that are good to know<a class="headerlink" href="#things-that-are-good-to-know" title="Permalink to this headline">¶</a></h5>
<p>Scrapy is written in pure Python and depends on a few key Python packages (among others):</p>
<ul class="simple">
<li><a class="reference external" href="http://lxml.de/">lxml</a>, an efficient XML and HTML parser</li>
<li><a class="reference external" href="https://pypi.python.org/pypi/parsel">parsel</a>, an HTML/XML data extraction library written on top of lxml,</li>
<li><a class="reference external" href="https://pypi.python.org/pypi/w3lib">w3lib</a>, a multi-purpose helper for dealing with URLs and web page encodings</li>
<li><a class="reference external" href="https://twistedmatrix.com/">twisted</a>, an asynchronous networking framework</li>
<li><a class="reference external" href="https://cryptography.io/">cryptography</a> and <a class="reference external" href="https://pypi.python.org/pypi/pyOpenSSL">pyOpenSSL</a>, to deal with various network-level security needs</li>
</ul>
<p>The minimal versions which Scrapy is tested against are:</p>
<ul class="simple">
<li>Twisted 14.0</li>
<li>lxml 3.4</li>
<li>pyOpenSSL 0.14</li>
</ul>
<p>Scrapy may work with older versions of these packages
but it is not guaranteed it will continue working
because it’s not being tested against them.</p>
<p>Some of these packages themselves depends on non-Python packages
that might require additional installation steps depending on your platform.
Please check <a class="reference internal" href="#intro-install-platform-notes"><span class="std std-ref">platform-specific guides below</span></a>.</p>
<p>In case of any trouble related to these dependencies,
please refer to their respective installation instructions:</p>
<ul class="simple">
<li><a class="reference external" href="http://lxml.de/installation.html">lxml installation</a></li>
<li><a class="reference external" href="https://cryptography.io/en/latest/installation/">cryptography installation</a></li>
</ul>
</div>
<div class="section" id="using-a-virtual-environment-recommended">
<span id="intro-using-virtualenv"></span><h5>Using a virtual environment (recommended)<a class="headerlink" href="#using-a-virtual-environment-recommended" title="Permalink to this headline">¶</a></h5>
<p>TL;DR: We recommend installing Scrapy inside a virtual environment
on all platforms.</p>
<p>Python packages can be installed either globally (a.k.a system wide),
or in user-space. We do not recommend installing scrapy system wide.</p>
<p>Instead, we recommend that you install scrapy within a so-called
“virtual environment” (<a class="reference external" href="https://virtualenv.pypa.io">virtualenv</a>).
Virtualenvs allow you to not conflict with already-installed Python
system packages (which could break some of your system tools and scripts),
and still install packages normally with <code class="docutils literal notranslate"><span class="pre">pip</span></code> (without <code class="docutils literal notranslate"><span class="pre">sudo</span></code> and the likes).</p>
<p>To get started with virtual environments, see <a class="reference external" href="https://virtualenv.pypa.io/en/stable/installation/">virtualenv installation instructions</a>.
To install it globally (having it globally installed actually helps here),
it should be a matter of running:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ [sudo] pip install virtualenv
</pre></div>
</div>
<p>Check this <a class="reference external" href="https://virtualenv.pypa.io/en/stable/userguide/">user guide</a> on how to create your virtualenv.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If you use Linux or OS X, <a class="reference external" href="https://virtualenvwrapper.readthedocs.io/en/latest/install.html">virtualenvwrapper</a> is a handy tool to create virtualenvs.</p>
</div>
<p>Once you have created a virtualenv, you can install scrapy inside it with <code class="docutils literal notranslate"><span class="pre">pip</span></code>,
just like any other Python package.
(See <a class="reference internal" href="#intro-install-platform-notes"><span class="std std-ref">platform-specific guides</span></a>
below for non-Python dependencies that you may need to install beforehand).</p>
<p>Python virtualenvs can be created to use Python 2 by default, or Python 3 by default.</p>
<ul class="simple">
<li>If you want to install scrapy with Python 3, install scrapy within a Python 3 virtualenv.</li>
<li>And if you want to install scrapy with Python 2, install scrapy within a Python 2 virtualenv.</li>
</ul>
</div>
</div>
<div class="section" id="platform-specific-installation-notes">
<span id="intro-install-platform-notes"></span><h4>Platform specific installation notes<a class="headerlink" href="#platform-specific-installation-notes" title="Permalink to this headline">¶</a></h4>
<div class="section" id="windows">
<span id="intro-install-windows"></span><h5>Windows<a class="headerlink" href="#windows" title="Permalink to this headline">¶</a></h5>
<p>Though it’s possible to install Scrapy on Windows using pip, we recommend you
to install <a class="reference external" href="https://docs.anaconda.com/anaconda/">Anaconda</a> or <a class="reference external" href="https://conda.io/docs/user-guide/install/index.html">Miniconda</a> and use the package from the
<a class="reference external" href="https://conda-forge.org/">conda-forge</a> channel, which will avoid most installation issues.</p>
<p>Once you’ve installed <a class="reference external" href="https://docs.anaconda.com/anaconda/">Anaconda</a> or <a class="reference external" href="https://conda.io/docs/user-guide/install/index.html">Miniconda</a>, install Scrapy with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">conda</span> <span class="n">install</span> <span class="o">-</span><span class="n">c</span> <span class="n">conda</span><span class="o">-</span><span class="n">forge</span> <span class="n">scrapy</span>
</pre></div>
</div>
</div>
<div class="section" id="ubuntu-14-04-or-above">
<span id="intro-install-ubuntu"></span><h5>Ubuntu 14.04 or above<a class="headerlink" href="#ubuntu-14-04-or-above" title="Permalink to this headline">¶</a></h5>
<p>Scrapy is currently tested with recent-enough versions of lxml,
twisted and pyOpenSSL, and is compatible with recent Ubuntu distributions.
But it should support older versions of Ubuntu too, like Ubuntu 14.04,
albeit with potential issues with TLS connections.</p>
<p><strong>Don’t</strong> use the <code class="docutils literal notranslate"><span class="pre">python-scrapy</span></code> package provided by Ubuntu, they are
typically too old and slow to catch up with latest Scrapy.</p>
<p>To install scrapy on Ubuntu (or Ubuntu-based) systems, you need to install
these dependencies:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sudo</span> <span class="n">apt</span><span class="o">-</span><span class="n">get</span> <span class="n">install</span> <span class="n">python</span><span class="o">-</span><span class="n">dev</span> <span class="n">python</span><span class="o">-</span><span class="n">pip</span> <span class="n">libxml2</span><span class="o">-</span><span class="n">dev</span> <span class="n">libxslt1</span><span class="o">-</span><span class="n">dev</span> <span class="n">zlib1g</span><span class="o">-</span><span class="n">dev</span> <span class="n">libffi</span><span class="o">-</span><span class="n">dev</span> <span class="n">libssl</span><span class="o">-</span><span class="n">dev</span>
</pre></div>
</div>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">python-dev</span></code>, <code class="docutils literal notranslate"><span class="pre">zlib1g-dev</span></code>, <code class="docutils literal notranslate"><span class="pre">libxml2-dev</span></code> and <code class="docutils literal notranslate"><span class="pre">libxslt1-dev</span></code>
are required for <code class="docutils literal notranslate"><span class="pre">lxml</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">libssl-dev</span></code> and <code class="docutils literal notranslate"><span class="pre">libffi-dev</span></code> are required for <code class="docutils literal notranslate"><span class="pre">cryptography</span></code></li>
</ul>
<p>If you want to install scrapy on Python 3, you’ll also need Python 3 development headers:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sudo</span> <span class="n">apt</span><span class="o">-</span><span class="n">get</span> <span class="n">install</span> <span class="n">python3</span> <span class="n">python3</span><span class="o">-</span><span class="n">dev</span>
</pre></div>
</div>
<p>Inside a <a class="reference internal" href="#intro-using-virtualenv"><span class="std std-ref">virtualenv</span></a>,
you can install Scrapy with <code class="docutils literal notranslate"><span class="pre">pip</span></code> after that:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">scrapy</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The same non-Python dependencies can be used to install Scrapy in Debian
Jessie (8.0) and above.</p>
</div>
</div>
<div class="section" id="mac-os-x">
<span id="intro-install-macos"></span><h5>Mac OS X<a class="headerlink" href="#mac-os-x" title="Permalink to this headline">¶</a></h5>
<p>Building Scrapy’s dependencies requires the presence of a C compiler and
development headers. On OS X this is typically provided by Apple’s Xcode
development tools. To install the Xcode command line tools open a terminal
window and run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">xcode</span><span class="o">-</span><span class="n">select</span> <span class="o">--</span><span class="n">install</span>
</pre></div>
</div>
<p>There’s a <a class="reference external" href="https://github.com/pypa/pip/issues/2468">known issue</a> that
prevents <code class="docutils literal notranslate"><span class="pre">pip</span></code> from updating system packages. This has to be addressed to
successfully install Scrapy and its dependencies. Here are some proposed
solutions:</p>
<ul>
<li><p class="first"><em>(Recommended)</em> <strong>Don’t</strong> use system python, install a new, updated version
that doesn’t conflict with the rest of your system. Here’s how to do it using
the <a class="reference external" href="https://brew.sh/">homebrew</a> package manager:</p>
<ul>
<li><p class="first">Install <a class="reference external" href="https://brew.sh/">homebrew</a> following the instructions in <a class="reference external" href="https://brew.sh/">https://brew.sh/</a></p>
</li>
<li><p class="first">Update your <code class="docutils literal notranslate"><span class="pre">PATH</span></code> variable to state that homebrew packages should be
used before system packages (Change <code class="docutils literal notranslate"><span class="pre">.bashrc</span></code> to <code class="docutils literal notranslate"><span class="pre">.zshrc</span></code> accordantly
if you’re using <a class="reference external" href="https://www.zsh.org/">zsh</a> as default shell):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">echo</span> <span class="s2">&quot;export PATH=/usr/local/bin:/usr/local/sbin:$PATH&quot;</span> <span class="o">&gt;&gt;</span> <span class="o">~/.</span><span class="n">bashrc</span>
</pre></div>
</div>
</li>
<li><p class="first">Reload <code class="docutils literal notranslate"><span class="pre">.bashrc</span></code> to ensure the changes have taken place:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">source</span> <span class="o">~/.</span><span class="n">bashrc</span>
</pre></div>
</div>
</li>
<li><p class="first">Install python:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">brew</span> <span class="n">install</span> <span class="n">python</span>
</pre></div>
</div>
</li>
<li><p class="first">Latest versions of python have <code class="docutils literal notranslate"><span class="pre">pip</span></code> bundled with them so you won’t need
to install it separately. If this is not the case, upgrade python:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">brew</span> <span class="n">update</span><span class="p">;</span> <span class="n">brew</span> <span class="n">upgrade</span> <span class="n">python</span>
</pre></div>
</div>
</li>
</ul>
</li>
<li><p class="first"><em>(Optional)</em> Install Scrapy inside an isolated python environment.</p>
<p>This method is a workaround for the above OS X issue, but it’s an overall
good practice for managing dependencies and can complement the first method.</p>
<p><a class="reference external" href="https://virtualenv.pypa.io">virtualenv</a> is a tool you can use to create virtual environments in python.
We recommended reading a tutorial like
<a class="reference external" href="http://docs.python-guide.org/en/latest/dev/virtualenvs/">http://docs.python-guide.org/en/latest/dev/virtualenvs/</a> to get started.</p>
</li>
</ul>
<p>After any of these workarounds you should be able to install Scrapy:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">Scrapy</span>
</pre></div>
</div>
</div>
<div class="section" id="pypy">
<h5>PyPy<a class="headerlink" href="#pypy" title="Permalink to this headline">¶</a></h5>
<p>We recommend using the latest PyPy version. The version tested is 5.9.0.
For PyPy3, only Linux installation was tested.</p>
<p>Most scrapy dependencides now have binary wheels for CPython, but not for PyPy.
This means that these dependecies will be built during installation.
On OS X, you are likely to face an issue with building Cryptography dependency,
solution to this problem is described
<a class="reference external" href="https://github.com/pyca/cryptography/issues/2692#issuecomment-272773481">here</a>,
that is to <code class="docutils literal notranslate"><span class="pre">brew</span> <span class="pre">install</span> <span class="pre">openssl</span></code> and then export the flags that this command
recommends (only needed when installing scrapy). Installing on Linux has no special
issues besides installing build dependencies.
Installing scrapy with PyPy on Windows is not tested.</p>
<p>You can check that scrapy is installed correctly by running <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">bench</span></code>.
If this command gives errors such as
<code class="docutils literal notranslate"><span class="pre">TypeError:</span> <span class="pre">...</span> <span class="pre">got</span> <span class="pre">2</span> <span class="pre">unexpected</span> <span class="pre">keyword</span> <span class="pre">arguments</span></code>, this means
that setuptools was unable to pick up one PyPy-specific dependency.
To fix this issue, run <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">'PyPyDispatcher&gt;=2.1.0'</span></code>.</p>
</div>
</div>
<div class="section" id="troubleshooting">
<span id="intro-install-troubleshooting"></span><h4>Troubleshooting<a class="headerlink" href="#troubleshooting" title="Permalink to this headline">¶</a></h4>
<div class="section" id="attributeerror-module-object-has-no-attribute-op-no-tlsv1-1">
<h5>AttributeError: ‘module’ object has no attribute ‘OP_NO_TLSv1_1’<a class="headerlink" href="#attributeerror-module-object-has-no-attribute-op-no-tlsv1-1" title="Permalink to this headline">¶</a></h5>
<p>After you install or upgrade Scrapy, Twisted or pyOpenSSL, you may get an
exception with the following traceback:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>[…]
  File &quot;[…]/site-packages/twisted/protocols/tls.py&quot;, line 63, in &lt;module&gt;
    from twisted.internet._sslverify import _setAcceptableProtocols
  File &quot;[…]/site-packages/twisted/internet/_sslverify.py&quot;, line 38, in &lt;module&gt;
    TLSVersion.TLSv1_1: SSL.OP_NO_TLSv1_1,
AttributeError: &#39;module&#39; object has no attribute &#39;OP_NO_TLSv1_1&#39;
</pre></div>
</div>
<p>The reason you get this exception is that your system or virtual environment
has a version of pyOpenSSL that your version of Twisted does not support.</p>
<p>To install a version of pyOpenSSL that your version of Twisted supports,
reinstall Twisted with the <code class="code docutils literal notranslate"><span class="pre">tls</span></code> extra option:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">twisted</span><span class="p">[</span><span class="n">tls</span><span class="p">]</span>
</pre></div>
</div>
<p>For details, see <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2473">Issue #2473</a>.</p>
</div>
</div>
</div>
<span id="document-intro/tutorial"></span><div class="section" id="scrapy-tutorial">
<span id="intro-tutorial"></span><h3>Scrapy Tutorial<a class="headerlink" href="#scrapy-tutorial" title="Permalink to this headline">¶</a></h3>
<p>In this tutorial, we’ll assume that Scrapy is already installed on your system.
If that’s not the case, see <a class="reference internal" href="index.html#intro-install"><span class="std std-ref">Installation guide</span></a>.</p>
<p>We are going to scrape <a class="reference external" href="http://quotes.toscrape.com/">quotes.toscrape.com</a>, a website
that lists quotes from famous authors.</p>
<p>This tutorial will walk you through these tasks:</p>
<ol class="arabic simple">
<li>Creating a new Scrapy project</li>
<li>Writing a <a class="reference internal" href="index.html#topics-spiders"><span class="std std-ref">spider</span></a> to crawl a site and extract data</li>
<li>Exporting the scraped data using the command line</li>
<li>Changing spider to recursively follow links</li>
<li>Using spider arguments</li>
</ol>
<p>Scrapy is written in <a class="reference external" href="https://www.python.org/">Python</a>. If you’re new to the language you might want to
start by getting an idea of what the language is like, to get the most out of
Scrapy.</p>
<p>If you’re already familiar with other languages, and want to learn Python quickly, the <a class="reference external" href="https://docs.python.org/3/tutorial">Python Tutorial</a> is a good resource.</p>
<p>If you’re new to programming and want to start with Python, the following books
may be useful to you:</p>
<ul class="simple">
<li><a class="reference external" href="https://automatetheboringstuff.com/">Automate the Boring Stuff With Python</a></li>
<li><a class="reference external" href="http://openbookproject.net/thinkcs/python/english3e/">How To Think Like a Computer Scientist</a></li>
<li><a class="reference external" href="https://learnpythonthehardway.org/python3/">Learn Python 3 The Hard Way</a></li>
</ul>
<p>You can also take a look at <a class="reference external" href="https://wiki.python.org/moin/BeginnersGuide/NonProgrammers">this list of Python resources for non-programmers</a>,
as well as the <a class="reference external" href="https://www.reddit.com/r/learnpython/wiki/index#wiki_new_to_python.3F">suggested resources in the learnpython-subreddit</a>.</p>
<div class="section" id="creating-a-project">
<h4>Creating a project<a class="headerlink" href="#creating-a-project" title="Permalink to this headline">¶</a></h4>
<p>Before you start scraping, you will have to set up a new Scrapy project. Enter a
directory where you’d like to store your code and run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">startproject</span> <span class="n">tutorial</span>
</pre></div>
</div>
<p>This will create a <code class="docutils literal notranslate"><span class="pre">tutorial</span></code> directory with the following contents:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tutorial</span><span class="o">/</span>
    <span class="n">scrapy</span><span class="o">.</span><span class="n">cfg</span>            <span class="c1"># deploy configuration file</span>

    <span class="n">tutorial</span><span class="o">/</span>             <span class="c1"># project&#39;s Python module, you&#39;ll import your code from here</span>
        <span class="fm">__init__</span><span class="o">.</span><span class="n">py</span>

        <span class="n">items</span><span class="o">.</span><span class="n">py</span>          <span class="c1"># project items definition file</span>

        <span class="n">middlewares</span><span class="o">.</span><span class="n">py</span>    <span class="c1"># project middlewares file</span>

        <span class="n">pipelines</span><span class="o">.</span><span class="n">py</span>      <span class="c1"># project pipelines file</span>

        <span class="n">settings</span><span class="o">.</span><span class="n">py</span>       <span class="c1"># project settings file</span>

        <span class="n">spiders</span><span class="o">/</span>          <span class="c1"># a directory where you&#39;ll later put your spiders</span>
            <span class="fm">__init__</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
</div>
<div class="section" id="our-first-spider">
<h4>Our first Spider<a class="headerlink" href="#our-first-spider" title="Permalink to this headline">¶</a></h4>
<p>Spiders are classes that you define and that Scrapy uses to scrape information
from a website (or a group of websites). They must subclass
<code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.Spider</span></code> and define the initial requests to make, optionally how
to follow links in the pages, and how to parse the downloaded page content to
extract data.</p>
<p>This is the code for our first Spider. Save it in a file named
<code class="docutils literal notranslate"><span class="pre">quotes_spider.py</span></code> under the <code class="docutils literal notranslate"><span class="pre">tutorial/spiders</span></code> directory in your project:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">QuotesSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;quotes&quot;</span>

    <span class="k">def</span> <span class="nf">start_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">urls</span> <span class="o">=</span> <span class="p">[</span>
            <span class="s1">&#39;http://quotes.toscrape.com/page/1/&#39;</span><span class="p">,</span>
            <span class="s1">&#39;http://quotes.toscrape.com/page/2/&#39;</span><span class="p">,</span>
        <span class="p">]</span>
        <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">urls</span><span class="p">:</span>
            <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="n">url</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">page</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">filename</span> <span class="o">=</span> <span class="s1">&#39;quotes-</span><span class="si">%s</span><span class="s1">.html&#39;</span> <span class="o">%</span> <span class="n">page</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">body</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s1">&#39;Saved file </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">filename</span><span class="p">)</span>
</pre></div>
</div>
<p>As you can see, our Spider subclasses <a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.Spider</span></code></a>
and defines some attributes and methods:</p>
<ul>
<li><p class="first"><a class="reference internal" href="index.html#scrapy.spiders.Spider.name" title="scrapy.spiders.Spider.name"><code class="xref py py-attr docutils literal notranslate"><span class="pre">name</span></code></a>: identifies the Spider. It must be
unique within a project, that is, you can’t set the same name for different
Spiders.</p>
</li>
<li><p class="first"><a class="reference internal" href="index.html#scrapy.spiders.Spider.start_requests" title="scrapy.spiders.Spider.start_requests"><code class="xref py py-meth docutils literal notranslate"><span class="pre">start_requests()</span></code></a>: must return an iterable of
Requests (you can return a list of requests or write a generator function)
which the Spider will begin to crawl from. Subsequent requests will be
generated successively from these initial requests.</p>
</li>
<li><p class="first"><a class="reference internal" href="index.html#scrapy.spiders.Spider.parse" title="scrapy.spiders.Spider.parse"><code class="xref py py-meth docutils literal notranslate"><span class="pre">parse()</span></code></a>: a method that will be called to handle
the response downloaded for each of the requests made. The response parameter
is an instance of <a class="reference internal" href="index.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">TextResponse</span></code></a> that holds
the page content and has further helpful methods to handle it.</p>
<p>The <a class="reference internal" href="index.html#scrapy.spiders.Spider.parse" title="scrapy.spiders.Spider.parse"><code class="xref py py-meth docutils literal notranslate"><span class="pre">parse()</span></code></a> method usually parses the response, extracting
the scraped data as dicts and also finding new URLs to
follow and creating new requests (<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a>) from them.</p>
</li>
</ul>
<div class="section" id="how-to-run-our-spider">
<h5>How to run our spider<a class="headerlink" href="#how-to-run-our-spider" title="Permalink to this headline">¶</a></h5>
<p>To put our spider to work, go to the project’s top level directory and run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">crawl</span> <span class="n">quotes</span>
</pre></div>
</div>
<p>This command runs the spider with name <code class="docutils literal notranslate"><span class="pre">quotes</span></code> that we’ve just added, that
will send some requests for the <code class="docutils literal notranslate"><span class="pre">quotes.toscrape.com</span></code> domain. You will get an output
similar to this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">...</span> <span class="p">(</span><span class="n">omitted</span> <span class="k">for</span> <span class="n">brevity</span><span class="p">)</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">24</span><span class="p">:</span><span class="mi">05</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">engine</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Spider</span> <span class="n">opened</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">24</span><span class="p">:</span><span class="mi">05</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">extensions</span><span class="o">.</span><span class="n">logstats</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Crawled</span> <span class="mi">0</span> <span class="n">pages</span> <span class="p">(</span><span class="n">at</span> <span class="mi">0</span> <span class="n">pages</span><span class="o">/</span><span class="nb">min</span><span class="p">),</span> <span class="n">scraped</span> <span class="mi">0</span> <span class="n">items</span> <span class="p">(</span><span class="n">at</span> <span class="mi">0</span> <span class="n">items</span><span class="o">/</span><span class="nb">min</span><span class="p">)</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">24</span><span class="p">:</span><span class="mi">05</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">extensions</span><span class="o">.</span><span class="n">telnet</span><span class="p">]</span> <span class="n">DEBUG</span><span class="p">:</span> <span class="n">Telnet</span> <span class="n">console</span> <span class="n">listening</span> <span class="n">on</span> <span class="mf">127.0</span><span class="o">.</span><span class="mf">0.1</span><span class="p">:</span><span class="mi">6023</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">24</span><span class="p">:</span><span class="mi">05</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">engine</span><span class="p">]</span> <span class="n">DEBUG</span><span class="p">:</span> <span class="n">Crawled</span> <span class="p">(</span><span class="mi">404</span><span class="p">)</span> <span class="o">&lt;</span><span class="n">GET</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">quotes</span><span class="o">.</span><span class="n">toscrape</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">robots</span><span class="o">.</span><span class="n">txt</span><span class="o">&gt;</span> <span class="p">(</span><span class="n">referer</span><span class="p">:</span> <span class="kc">None</span><span class="p">)</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">24</span><span class="p">:</span><span class="mi">05</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">engine</span><span class="p">]</span> <span class="n">DEBUG</span><span class="p">:</span> <span class="n">Crawled</span> <span class="p">(</span><span class="mi">200</span><span class="p">)</span> <span class="o">&lt;</span><span class="n">GET</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">quotes</span><span class="o">.</span><span class="n">toscrape</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">page</span><span class="o">/</span><span class="mi">1</span><span class="o">/&gt;</span> <span class="p">(</span><span class="n">referer</span><span class="p">:</span> <span class="kc">None</span><span class="p">)</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">24</span><span class="p">:</span><span class="mi">05</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">engine</span><span class="p">]</span> <span class="n">DEBUG</span><span class="p">:</span> <span class="n">Crawled</span> <span class="p">(</span><span class="mi">200</span><span class="p">)</span> <span class="o">&lt;</span><span class="n">GET</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">quotes</span><span class="o">.</span><span class="n">toscrape</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">page</span><span class="o">/</span><span class="mi">2</span><span class="o">/&gt;</span> <span class="p">(</span><span class="n">referer</span><span class="p">:</span> <span class="kc">None</span><span class="p">)</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">24</span><span class="p">:</span><span class="mi">05</span> <span class="p">[</span><span class="n">quotes</span><span class="p">]</span> <span class="n">DEBUG</span><span class="p">:</span> <span class="n">Saved</span> <span class="n">file</span> <span class="n">quotes</span><span class="o">-</span><span class="mf">1.</span><span class="n">html</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">24</span><span class="p">:</span><span class="mi">05</span> <span class="p">[</span><span class="n">quotes</span><span class="p">]</span> <span class="n">DEBUG</span><span class="p">:</span> <span class="n">Saved</span> <span class="n">file</span> <span class="n">quotes</span><span class="o">-</span><span class="mf">2.</span><span class="n">html</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">24</span><span class="p">:</span><span class="mi">05</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">engine</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Closing</span> <span class="n">spider</span> <span class="p">(</span><span class="n">finished</span><span class="p">)</span>
<span class="o">...</span>
</pre></div>
</div>
<p>Now, check the files in the current directory. You should notice that two new
files have been created: <em>quotes-1.html</em> and <em>quotes-2.html</em>, with the content
for the respective URLs, as our <code class="docutils literal notranslate"><span class="pre">parse</span></code> method instructs.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If you are wondering why we haven’t parsed the HTML yet, hold
on, we will cover that soon.</p>
</div>
<div class="section" id="what-just-happened-under-the-hood">
<h6>What just happened under the hood?<a class="headerlink" href="#what-just-happened-under-the-hood" title="Permalink to this headline">¶</a></h6>
<p>Scrapy schedules the <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.Request</span></code></a> objects
returned by the <code class="docutils literal notranslate"><span class="pre">start_requests</span></code> method of the Spider. Upon receiving a
response for each one, it instantiates <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> objects
and calls the callback method associated with the request (in this case, the
<code class="docutils literal notranslate"><span class="pre">parse</span></code> method) passing the response as argument.</p>
</div>
</div>
<div class="section" id="a-shortcut-to-the-start-requests-method">
<h5>A shortcut to the start_requests method<a class="headerlink" href="#a-shortcut-to-the-start-requests-method" title="Permalink to this headline">¶</a></h5>
<p>Instead of implementing a <a class="reference internal" href="index.html#scrapy.spiders.Spider.start_requests" title="scrapy.spiders.Spider.start_requests"><code class="xref py py-meth docutils literal notranslate"><span class="pre">start_requests()</span></code></a> method
that generates <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.Request</span></code></a> objects from URLs,
you can just define a <a class="reference internal" href="index.html#scrapy.spiders.Spider.start_urls" title="scrapy.spiders.Spider.start_urls"><code class="xref py py-attr docutils literal notranslate"><span class="pre">start_urls</span></code></a> class attribute
with a list of URLs. This list will then be used by the default implementation
of <a class="reference internal" href="index.html#scrapy.spiders.Spider.start_requests" title="scrapy.spiders.Spider.start_requests"><code class="xref py py-meth docutils literal notranslate"><span class="pre">start_requests()</span></code></a> to create the initial requests
for your spider:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">QuotesSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;quotes&quot;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s1">&#39;http://quotes.toscrape.com/page/1/&#39;</span><span class="p">,</span>
        <span class="s1">&#39;http://quotes.toscrape.com/page/2/&#39;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">page</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">filename</span> <span class="o">=</span> <span class="s1">&#39;quotes-</span><span class="si">%s</span><span class="s1">.html&#39;</span> <span class="o">%</span> <span class="n">page</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">body</span><span class="p">)</span>
</pre></div>
</div>
<p>The <a class="reference internal" href="index.html#scrapy.spiders.Spider.parse" title="scrapy.spiders.Spider.parse"><code class="xref py py-meth docutils literal notranslate"><span class="pre">parse()</span></code></a> method will be called to handle each
of the requests for those URLs, even though we haven’t explicitly told Scrapy
to do so. This happens because <a class="reference internal" href="index.html#scrapy.spiders.Spider.parse" title="scrapy.spiders.Spider.parse"><code class="xref py py-meth docutils literal notranslate"><span class="pre">parse()</span></code></a> is Scrapy’s
default callback method, which is called for requests without an explicitly
assigned callback.</p>
</div>
<div class="section" id="extracting-data">
<h5>Extracting data<a class="headerlink" href="#extracting-data" title="Permalink to this headline">¶</a></h5>
<p>The best way to learn how to extract data with Scrapy is trying selectors
using the <a class="reference internal" href="index.html#topics-shell"><span class="std std-ref">Scrapy shell</span></a>. Run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">shell</span> <span class="s1">&#39;http://quotes.toscrape.com/page/1/&#39;</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>Remember to always enclose urls in quotes when running Scrapy shell from
command-line, otherwise urls containing arguments (ie. <code class="docutils literal notranslate"><span class="pre">&amp;</span></code> character)
will not work.</p>
<p>On Windows, use double quotes instead:</p>
<div class="last highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">shell</span> <span class="s2">&quot;http://quotes.toscrape.com/page/1/&quot;</span>
</pre></div>
</div>
</div>
<p>You will see something like:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span> <span class="o">...</span> <span class="n">Scrapy</span> <span class="n">log</span> <span class="n">here</span> <span class="o">...</span> <span class="p">]</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">09</span><span class="o">-</span><span class="mi">19</span> <span class="mi">12</span><span class="p">:</span><span class="mi">09</span><span class="p">:</span><span class="mi">27</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">engine</span><span class="p">]</span> <span class="n">DEBUG</span><span class="p">:</span> <span class="n">Crawled</span> <span class="p">(</span><span class="mi">200</span><span class="p">)</span> <span class="o">&lt;</span><span class="n">GET</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">quotes</span><span class="o">.</span><span class="n">toscrape</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">page</span><span class="o">/</span><span class="mi">1</span><span class="o">/&gt;</span> <span class="p">(</span><span class="n">referer</span><span class="p">:</span> <span class="kc">None</span><span class="p">)</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="n">Available</span> <span class="n">Scrapy</span> <span class="n">objects</span><span class="p">:</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">scrapy</span>     <span class="n">scrapy</span> <span class="n">module</span> <span class="p">(</span><span class="n">contains</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">,</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Selector</span><span class="p">,</span> <span class="n">etc</span><span class="p">)</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">crawler</span>    <span class="o">&lt;</span><span class="n">scrapy</span><span class="o">.</span><span class="n">crawler</span><span class="o">.</span><span class="n">Crawler</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x7fa91d888c90</span><span class="o">&gt;</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">item</span>       <span class="p">{}</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">request</span>    <span class="o">&lt;</span><span class="n">GET</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">quotes</span><span class="o">.</span><span class="n">toscrape</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">page</span><span class="o">/</span><span class="mi">1</span><span class="o">/&gt;</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">response</span>   <span class="o">&lt;</span><span class="mi">200</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">quotes</span><span class="o">.</span><span class="n">toscrape</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">page</span><span class="o">/</span><span class="mi">1</span><span class="o">/&gt;</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">settings</span>   <span class="o">&lt;</span><span class="n">scrapy</span><span class="o">.</span><span class="n">settings</span><span class="o">.</span><span class="n">Settings</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x7fa91d888c10</span><span class="o">&gt;</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">spider</span>     <span class="o">&lt;</span><span class="n">DefaultSpider</span> <span class="s1">&#39;default&#39;</span> <span class="n">at</span> <span class="mh">0x7fa91c8af990</span><span class="o">&gt;</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="n">Useful</span> <span class="n">shortcuts</span><span class="p">:</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">shelp</span><span class="p">()</span>           <span class="n">Shell</span> <span class="n">help</span> <span class="p">(</span><span class="nb">print</span> <span class="n">this</span> <span class="n">help</span><span class="p">)</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">fetch</span><span class="p">(</span><span class="n">req_or_url</span><span class="p">)</span> <span class="n">Fetch</span> <span class="n">request</span> <span class="p">(</span><span class="ow">or</span> <span class="n">URL</span><span class="p">)</span> <span class="ow">and</span> <span class="n">update</span> <span class="n">local</span> <span class="n">objects</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">view</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>    <span class="n">View</span> <span class="n">response</span> <span class="ow">in</span> <span class="n">a</span> <span class="n">browser</span>
<span class="o">&gt;&gt;&gt;</span>
</pre></div>
</div>
<p>Using the shell, you can try selecting elements using <a class="reference external" href="https://www.w3.org/TR/selectors">CSS</a> with the response
object:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;title&#39;</span><span class="p">)</span>
<span class="go">[&lt;Selector xpath=&#39;descendant-or-self::title&#39; data=&#39;&lt;title&gt;Quotes to Scrape&lt;/title&gt;&#39;&gt;]</span>
</pre></div>
</div>
<p>The result of running <code class="docutils literal notranslate"><span class="pre">response.css('title')</span></code> is a list-like object called
<a class="reference internal" href="index.html#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal notranslate"><span class="pre">SelectorList</span></code></a>, which represents a list of
<a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a> objects that wrap around XML/HTML elements
and allow you to run further queries to fine-grain the selection or extract the
data.</p>
<p>To extract the text from the title above, you can do:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;title::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;Quotes to Scrape&#39;]</span>
</pre></div>
</div>
<p>There are two things to note here: one is that we’ve added <code class="docutils literal notranslate"><span class="pre">::text</span></code> to the
CSS query, to mean we want to select only the text elements directly inside
<code class="docutils literal notranslate"><span class="pre">&lt;title&gt;</span></code> element.  If we don’t specify <code class="docutils literal notranslate"><span class="pre">::text</span></code>, we’d get the full title
element, including its tags:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;title&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;&lt;title&gt;Quotes to Scrape&lt;/title&gt;&#39;]</span>
</pre></div>
</div>
<p>The other thing is that the result of calling <code class="docutils literal notranslate"><span class="pre">.getall()</span></code> is a list: it is
possible that a selector returns more than one result, so we extract them all.
When you know you just want the first result, as in this case, you can do:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;title::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;Quotes to Scrape&#39;</span>
</pre></div>
</div>
<p>As an alternative, you could’ve written:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;title::text&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;Quotes to Scrape&#39;</span>
</pre></div>
</div>
<p>However, using <code class="docutils literal notranslate"><span class="pre">.get()</span></code> directly on a <a class="reference internal" href="index.html#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal notranslate"><span class="pre">SelectorList</span></code></a>
instance avoids an <code class="docutils literal notranslate"><span class="pre">IndexError</span></code> and returns <code class="docutils literal notranslate"><span class="pre">None</span></code> when it doesn’t
find any element matching the selection.</p>
<p>There’s a lesson here: for most scraping code, you want it to be resilient to
errors due to things not being found on a page, so that even if some parts fail
to be scraped, you can at least get <strong>some</strong> data.</p>
<p>Besides the <a class="reference internal" href="index.html#scrapy.selector.SelectorList.getall" title="scrapy.selector.SelectorList.getall"><code class="xref py py-meth docutils literal notranslate"><span class="pre">getall()</span></code></a> and
<a class="reference internal" href="index.html#scrapy.selector.SelectorList.get" title="scrapy.selector.SelectorList.get"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get()</span></code></a> methods, you can also use
the <a class="reference internal" href="index.html#scrapy.selector.SelectorList.re" title="scrapy.selector.SelectorList.re"><code class="xref py py-meth docutils literal notranslate"><span class="pre">re()</span></code></a> method to extract using <a class="reference external" href="https://docs.python.org/3/library/re.html">regular
expressions</a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;title::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">re</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Quotes.*&#39;</span><span class="p">)</span>
<span class="go">[&#39;Quotes to Scrape&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;title::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">re</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Q\w+&#39;</span><span class="p">)</span>
<span class="go">[&#39;Quotes&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;title::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">re</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;(\w+) to (\w+)&#39;</span><span class="p">)</span>
<span class="go">[&#39;Quotes&#39;, &#39;Scrape&#39;]</span>
</pre></div>
</div>
<p>In order to find the proper CSS selectors to use, you might find useful opening
the response page from the shell in your web browser using <code class="docutils literal notranslate"><span class="pre">view(response)</span></code>.
You can use your browser’s developer tools to inspect the HTML and come up
with a selector (see <a class="reference internal" href="index.html#topics-developer-tools"><span class="std std-ref">Using your browser’s Developer Tools for scraping</span></a>).</p>
<p><a class="reference external" href="http://selectorgadget.com/">Selector Gadget</a> is also a nice tool to quickly find CSS selector for
visually selected elements, which works in many browsers.</p>
<div class="section" id="xpath-a-brief-intro">
<h6>XPath: a brief intro<a class="headerlink" href="#xpath-a-brief-intro" title="Permalink to this headline">¶</a></h6>
<p>Besides <a class="reference external" href="https://www.w3.org/TR/selectors">CSS</a>, Scrapy selectors also support using <a class="reference external" href="https://www.w3.org/TR/xpath">XPath</a> expressions:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//title&#39;</span><span class="p">)</span>
<span class="go">[&lt;Selector xpath=&#39;//title&#39; data=&#39;&lt;title&gt;Quotes to Scrape&lt;/title&gt;&#39;&gt;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//title/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;Quotes to Scrape&#39;</span>
</pre></div>
</div>
<p>XPath expressions are very powerful, and are the foundation of Scrapy
Selectors. In fact, CSS selectors are converted to XPath under-the-hood. You
can see that if you read closely the text representation of the selector
objects in the shell.</p>
<p>While perhaps not as popular as CSS selectors, XPath expressions offer more
power because besides navigating the structure, it can also look at the
content. Using XPath, you’re able to select things like: <em>select the link
that contains the text “Next Page”</em>. This makes XPath very fitting to the task
of scraping, and we encourage you to learn XPath even if you already know how to
construct CSS selectors, it will make scraping much easier.</p>
<p>We won’t cover much of XPath here, but you can read more about <a class="reference internal" href="index.html#topics-selectors"><span class="std std-ref">using XPath
with Scrapy Selectors here</span></a>. To learn more about XPath, we
recommend <a class="reference external" href="http://zvon.org/comp/r/tut-XPath_1.html">this tutorial to learn XPath through examples</a>, and <a class="reference external" href="http://plasmasturm.org/log/xpath101/">this tutorial to learn “how
to think in XPath”</a>.</p>
</div>
<div class="section" id="extracting-quotes-and-authors">
<h6>Extracting quotes and authors<a class="headerlink" href="#extracting-quotes-and-authors" title="Permalink to this headline">¶</a></h6>
<p>Now that you know a bit about selection and extraction, let’s complete our
spider by writing the code to extract the quotes from the web page.</p>
<p>Each quote in <a class="reference external" href="http://quotes.toscrape.com">http://quotes.toscrape.com</a> is represented by HTML elements that look
like this:</p>
<div class="highlight-html notranslate"><div class="highlight"><pre><span></span><span class="p">&lt;</span><span class="nt">div</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;quote&quot;</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">span</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;text&quot;</span><span class="p">&gt;</span>“The world as we have created it is a process of our
    thinking. It cannot be changed without changing our thinking.”<span class="p">&lt;/</span><span class="nt">span</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">span</span><span class="p">&gt;</span>
        by <span class="p">&lt;</span><span class="nt">small</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;author&quot;</span><span class="p">&gt;</span>Albert Einstein<span class="p">&lt;/</span><span class="nt">small</span><span class="p">&gt;</span>
        <span class="p">&lt;</span><span class="nt">a</span> <span class="na">href</span><span class="o">=</span><span class="s">&quot;/author/Albert-Einstein&quot;</span><span class="p">&gt;</span>(about)<span class="p">&lt;/</span><span class="nt">a</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">span</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">div</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;tags&quot;</span><span class="p">&gt;</span>
        Tags:
        <span class="p">&lt;</span><span class="nt">a</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;tag&quot;</span> <span class="na">href</span><span class="o">=</span><span class="s">&quot;/tag/change/page/1/&quot;</span><span class="p">&gt;</span>change<span class="p">&lt;/</span><span class="nt">a</span><span class="p">&gt;</span>
        <span class="p">&lt;</span><span class="nt">a</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;tag&quot;</span> <span class="na">href</span><span class="o">=</span><span class="s">&quot;/tag/deep-thoughts/page/1/&quot;</span><span class="p">&gt;</span>deep-thoughts<span class="p">&lt;/</span><span class="nt">a</span><span class="p">&gt;</span>
        <span class="p">&lt;</span><span class="nt">a</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;tag&quot;</span> <span class="na">href</span><span class="o">=</span><span class="s">&quot;/tag/thinking/page/1/&quot;</span><span class="p">&gt;</span>thinking<span class="p">&lt;/</span><span class="nt">a</span><span class="p">&gt;</span>
        <span class="p">&lt;</span><span class="nt">a</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;tag&quot;</span> <span class="na">href</span><span class="o">=</span><span class="s">&quot;/tag/world/page/1/&quot;</span><span class="p">&gt;</span>world<span class="p">&lt;/</span><span class="nt">a</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
<span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
</pre></div>
</div>
<p>Let’s open up scrapy shell and play a bit to find out how to extract the data
we want:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ scrapy shell &#39;http://quotes.toscrape.com&#39;
</pre></div>
</div>
<p>We get a list of selectors for the quote HTML elements with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;div.quote&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Each of the selectors returned by the query above allows us to run further
queries over their sub-elements. Let’s assign the first selector to a
variable, so that we can run our CSS selectors directly on a particular quote:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">quote</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;div.quote&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>Now, let’s extract <code class="docutils literal notranslate"><span class="pre">text</span></code>, <code class="docutils literal notranslate"><span class="pre">author</span></code> and the <code class="docutils literal notranslate"><span class="pre">tags</span></code> from that quote
using the <code class="docutils literal notranslate"><span class="pre">quote</span></code> object we just created:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">text</span> <span class="o">=</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;span.text::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">text</span>
<span class="go">&#39;“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">author</span> <span class="o">=</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;small.author::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">author</span>
<span class="go">&#39;Albert Einstein&#39;</span>
</pre></div>
</div>
<p>Given that the tags are a list of strings, we can use the <code class="docutils literal notranslate"><span class="pre">.getall()</span></code> method
to get all of them:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tags</span> <span class="o">=</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;div.tags a.tag::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tags</span>
<span class="go">[&#39;change&#39;, &#39;deep-thoughts&#39;, &#39;thinking&#39;, &#39;world&#39;]</span>
</pre></div>
</div>
<p>Having figured out how to extract each bit, we can now iterate over all the
quotes elements and put them together into a Python dictionary:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">quote</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;div.quote&quot;</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">text</span> <span class="o">=</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;span.text::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="gp">... </span>    <span class="n">author</span> <span class="o">=</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;small.author::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="gp">... </span>    <span class="n">tags</span> <span class="o">=</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s2">&quot;div.tags a.tag::text&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">text</span><span class="p">,</span> <span class="n">author</span><span class="o">=</span><span class="n">author</span><span class="p">,</span> <span class="n">tags</span><span class="o">=</span><span class="n">tags</span><span class="p">))</span>
<span class="go">{&#39;tags&#39;: [&#39;change&#39;, &#39;deep-thoughts&#39;, &#39;thinking&#39;, &#39;world&#39;], &#39;author&#39;: &#39;Albert Einstein&#39;, &#39;text&#39;: &#39;“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”&#39;}</span>
<span class="go">{&#39;tags&#39;: [&#39;abilities&#39;, &#39;choices&#39;], &#39;author&#39;: &#39;J.K. Rowling&#39;, &#39;text&#39;: &#39;“It is our choices, Harry, that show what we truly are, far more than our abilities.”&#39;}</span>
<span class="go">    ... a few more of these, omitted for brevity</span>
<span class="go">&gt;&gt;&gt;</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="extracting-data-in-our-spider">
<h5>Extracting data in our spider<a class="headerlink" href="#extracting-data-in-our-spider" title="Permalink to this headline">¶</a></h5>
<p>Let’s get back to our spider. Until now, it doesn’t extract any data in
particular, just saves the whole HTML page to a local file. Let’s integrate the
extraction logic above into our spider.</p>
<p>A Scrapy spider typically generates many dictionaries containing the data
extracted from the page. To do that, we use the <code class="docutils literal notranslate"><span class="pre">yield</span></code> Python keyword
in the callback, as you can see below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">QuotesSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;quotes&quot;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s1">&#39;http://quotes.toscrape.com/page/1/&#39;</span><span class="p">,</span>
        <span class="s1">&#39;http://quotes.toscrape.com/page/2/&#39;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">quote</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;div.quote&#39;</span><span class="p">):</span>
            <span class="k">yield</span> <span class="p">{</span>
                <span class="s1">&#39;text&#39;</span><span class="p">:</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;span.text::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(),</span>
                <span class="s1">&#39;author&#39;</span><span class="p">:</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;small.author::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(),</span>
                <span class="s1">&#39;tags&#39;</span><span class="p">:</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;div.tags a.tag::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">(),</span>
            <span class="p">}</span>
</pre></div>
</div>
<p>If you run this spider, it will output the extracted data with the log:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">2016</span><span class="o">-</span><span class="mi">09</span><span class="o">-</span><span class="mi">19</span> <span class="mi">18</span><span class="p">:</span><span class="mi">57</span><span class="p">:</span><span class="mi">19</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">scraper</span><span class="p">]</span> <span class="n">DEBUG</span><span class="p">:</span> <span class="n">Scraped</span> <span class="kn">from</span> <span class="o">&lt;</span><span class="mi">200</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">quotes</span><span class="o">.</span><span class="n">toscrape</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">page</span><span class="o">/</span><span class="mi">1</span><span class="o">/&gt;</span>
<span class="p">{</span><span class="s1">&#39;tags&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;life&#39;</span><span class="p">,</span> <span class="s1">&#39;love&#39;</span><span class="p">],</span> <span class="s1">&#39;author&#39;</span><span class="p">:</span> <span class="s1">&#39;André Gide&#39;</span><span class="p">,</span> <span class="s1">&#39;text&#39;</span><span class="p">:</span> <span class="s1">&#39;“It is better to be hated for what you are than to be loved for what you are not.”&#39;</span><span class="p">}</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">09</span><span class="o">-</span><span class="mi">19</span> <span class="mi">18</span><span class="p">:</span><span class="mi">57</span><span class="p">:</span><span class="mi">19</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">scraper</span><span class="p">]</span> <span class="n">DEBUG</span><span class="p">:</span> <span class="n">Scraped</span> <span class="kn">from</span> <span class="o">&lt;</span><span class="mi">200</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">quotes</span><span class="o">.</span><span class="n">toscrape</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">page</span><span class="o">/</span><span class="mi">1</span><span class="o">/&gt;</span>
<span class="p">{</span><span class="s1">&#39;tags&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;edison&#39;</span><span class="p">,</span> <span class="s1">&#39;failure&#39;</span><span class="p">,</span> <span class="s1">&#39;inspirational&#39;</span><span class="p">,</span> <span class="s1">&#39;paraphrased&#39;</span><span class="p">],</span> <span class="s1">&#39;author&#39;</span><span class="p">:</span> <span class="s1">&#39;Thomas A. Edison&#39;</span><span class="p">,</span> <span class="s1">&#39;text&#39;</span><span class="p">:</span> <span class="s2">&quot;“I have not failed. I&#39;ve just found 10,000 ways that won&#39;t work.”&quot;</span><span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="storing-the-scraped-data">
<span id="storing-data"></span><h4>Storing the scraped data<a class="headerlink" href="#storing-the-scraped-data" title="Permalink to this headline">¶</a></h4>
<p>The simplest way to store the scraped data is by using <a class="reference internal" href="index.html#topics-feed-exports"><span class="std std-ref">Feed exports</span></a>, with the following command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">crawl</span> <span class="n">quotes</span> <span class="o">-</span><span class="n">o</span> <span class="n">quotes</span><span class="o">.</span><span class="n">json</span>
</pre></div>
</div>
<p>That will generate an <code class="docutils literal notranslate"><span class="pre">quotes.json</span></code> file containing all scraped items,
serialized in <a class="reference external" href="https://en.wikipedia.org/wiki/JSON">JSON</a>.</p>
<p>For historic reasons, Scrapy appends to a given file instead of overwriting
its contents. If you run this command twice without removing the file
before the second time, you’ll end up with a broken JSON file.</p>
<p>You can also use other formats, like <a class="reference external" href="http://jsonlines.org">JSON Lines</a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">crawl</span> <span class="n">quotes</span> <span class="o">-</span><span class="n">o</span> <span class="n">quotes</span><span class="o">.</span><span class="n">jl</span>
</pre></div>
</div>
<p>The <a class="reference external" href="http://jsonlines.org">JSON Lines</a> format is useful because it’s stream-like, you can easily
append new records to it. It doesn’t have the same problem of JSON when you run
twice. Also, as each record is a separate line, you can process big files
without having to fit everything in memory, there are tools like <a class="reference external" href="https://stedolan.github.io/jq">JQ</a> to help
doing that at the command-line.</p>
<p>In small projects (like the one in this tutorial), that should be enough.
However, if you want to perform more complex things with the scraped items, you
can write an <a class="reference internal" href="index.html#topics-item-pipeline"><span class="std std-ref">Item Pipeline</span></a>. A placeholder file
for Item Pipelines has been set up for you when the project is created, in
<code class="docutils literal notranslate"><span class="pre">tutorial/pipelines.py</span></code>. Though you don’t need to implement any item
pipelines if you just want to store the scraped items.</p>
</div>
<div class="section" id="following-links">
<h4>Following links<a class="headerlink" href="#following-links" title="Permalink to this headline">¶</a></h4>
<p>Let’s say, instead of just scraping the stuff from the first two pages
from <a class="reference external" href="http://quotes.toscrape.com">http://quotes.toscrape.com</a>, you want quotes from all the pages in the website.</p>
<p>Now that you know how to extract data from pages, let’s see how to follow links
from them.</p>
<p>First thing is to extract the link to the page we want to follow.  Examining
our page, we can see there is a link to the next page with the following
markup:</p>
<div class="highlight-html notranslate"><div class="highlight"><pre><span></span><span class="p">&lt;</span><span class="nt">ul</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;pager&quot;</span><span class="p">&gt;</span>
    <span class="p">&lt;</span><span class="nt">li</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;next&quot;</span><span class="p">&gt;</span>
        <span class="p">&lt;</span><span class="nt">a</span> <span class="na">href</span><span class="o">=</span><span class="s">&quot;/page/2/&quot;</span><span class="p">&gt;</span>Next <span class="p">&lt;</span><span class="nt">span</span> <span class="na">aria-hidden</span><span class="o">=</span><span class="s">&quot;true&quot;</span><span class="p">&gt;</span><span class="ni">&amp;rarr;</span><span class="p">&lt;/</span><span class="nt">span</span><span class="p">&gt;&lt;/</span><span class="nt">a</span><span class="p">&gt;</span>
    <span class="p">&lt;/</span><span class="nt">li</span><span class="p">&gt;</span>
<span class="p">&lt;/</span><span class="nt">ul</span><span class="p">&gt;</span>
</pre></div>
</div>
<p>We can try extracting it in the shell:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;li.next a&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;&lt;a href=&quot;/page/2/&quot;&gt;Next &lt;span aria-hidden=&quot;true&quot;&gt;→&lt;/span&gt;&lt;/a&gt;&#39;</span>
</pre></div>
</div>
<p>This gets the anchor element, but we want the attribute <code class="docutils literal notranslate"><span class="pre">href</span></code>. For that,
Scrapy supports a CSS extension that lets you select the attribute contents,
like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;li.next a::attr(href)&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;/page/2/&#39;</span>
</pre></div>
</div>
<p>There is also an <code class="docutils literal notranslate"><span class="pre">attrib</span></code> property available
(see <a class="reference internal" href="index.html#selecting-attributes"><span class="std std-ref">Selecting element attributes</span></a> for more):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;li.next a&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">attrib</span><span class="p">[</span><span class="s1">&#39;href&#39;</span><span class="p">]</span>
<span class="go">&#39;/page/2&#39;</span>
</pre></div>
</div>
<p>Let’s see now our spider modified to recursively follow the link to the next
page, extracting data from it:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">QuotesSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;quotes&quot;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s1">&#39;http://quotes.toscrape.com/page/1/&#39;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">quote</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;div.quote&#39;</span><span class="p">):</span>
            <span class="k">yield</span> <span class="p">{</span>
                <span class="s1">&#39;text&#39;</span><span class="p">:</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;span.text::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(),</span>
                <span class="s1">&#39;author&#39;</span><span class="p">:</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;small.author::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(),</span>
                <span class="s1">&#39;tags&#39;</span><span class="p">:</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;div.tags a.tag::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">(),</span>
            <span class="p">}</span>

        <span class="n">next_page</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;li.next a::attr(href)&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">next_page</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">next_page</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">urljoin</span><span class="p">(</span><span class="n">next_page</span><span class="p">)</span>
            <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">next_page</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
</pre></div>
</div>
<p>Now, after extracting the data, the <code class="docutils literal notranslate"><span class="pre">parse()</span></code> method looks for the link to
the next page, builds a full absolute URL using the
<a class="reference internal" href="index.html#scrapy.http.Response.urljoin" title="scrapy.http.Response.urljoin"><code class="xref py py-meth docutils literal notranslate"><span class="pre">urljoin()</span></code></a> method (since the links can be
relative) and yields a new request to the next page, registering itself as
callback to handle the data extraction for the next page and to keep the
crawling going through all the pages.</p>
<p>What you see here is Scrapy’s mechanism of following links: when you yield
a Request in a callback method, Scrapy will schedule that request to be sent
and register a callback method to be executed when that request finishes.</p>
<p>Using this, you can build complex crawlers that follow links according to rules
you define, and extract different kinds of data depending on the page it’s
visiting.</p>
<p>In our example, it creates a sort of loop, following all the links to the next page
until it doesn’t find one – handy for crawling blogs, forums and other sites with
pagination.</p>
<div class="section" id="a-shortcut-for-creating-requests">
<span id="response-follow-example"></span><h5>A shortcut for creating Requests<a class="headerlink" href="#a-shortcut-for-creating-requests" title="Permalink to this headline">¶</a></h5>
<p>As a shortcut for creating Request objects you can use
<a class="reference internal" href="index.html#scrapy.http.TextResponse.follow" title="scrapy.http.TextResponse.follow"><code class="xref py py-meth docutils literal notranslate"><span class="pre">response.follow</span></code></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">QuotesSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;quotes&quot;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s1">&#39;http://quotes.toscrape.com/page/1/&#39;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">quote</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;div.quote&#39;</span><span class="p">):</span>
            <span class="k">yield</span> <span class="p">{</span>
                <span class="s1">&#39;text&#39;</span><span class="p">:</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;span.text::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(),</span>
                <span class="s1">&#39;author&#39;</span><span class="p">:</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;span small::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(),</span>
                <span class="s1">&#39;tags&#39;</span><span class="p">:</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;div.tags a.tag::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">(),</span>
            <span class="p">}</span>

        <span class="n">next_page</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;li.next a::attr(href)&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">next_page</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">yield</span> <span class="n">response</span><span class="o">.</span><span class="n">follow</span><span class="p">(</span><span class="n">next_page</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
</pre></div>
</div>
<p>Unlike scrapy.Request, <code class="docutils literal notranslate"><span class="pre">response.follow</span></code> supports relative URLs directly - no
need to call urljoin. Note that <code class="docutils literal notranslate"><span class="pre">response.follow</span></code> just returns a Request
instance; you still have to yield this Request.</p>
<p>You can also pass a selector to <code class="docutils literal notranslate"><span class="pre">response.follow</span></code> instead of a string;
this selector should extract necessary attributes:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">href</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;li.next a::attr(href)&#39;</span><span class="p">):</span>
    <span class="k">yield</span> <span class="n">response</span><span class="o">.</span><span class="n">follow</span><span class="p">(</span><span class="n">href</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
</pre></div>
</div>
<p>For <code class="docutils literal notranslate"><span class="pre">&lt;a&gt;</span></code> elements there is a shortcut: <code class="docutils literal notranslate"><span class="pre">response.follow</span></code> uses their href
attribute automatically. So the code can be shortened further:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;li.next a&#39;</span><span class="p">):</span>
    <span class="k">yield</span> <span class="n">response</span><span class="o">.</span><span class="n">follow</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><code class="docutils literal notranslate"><span class="pre">response.follow(response.css('li.next</span> <span class="pre">a'))</span></code> is not valid because
<code class="docutils literal notranslate"><span class="pre">response.css</span></code> returns a list-like object with selectors for all results,
not a single selector. A <code class="docutils literal notranslate"><span class="pre">for</span></code> loop like in the example above, or
<code class="docutils literal notranslate"><span class="pre">response.follow(response.css('li.next</span> <span class="pre">a')[0])</span></code> is fine.</p>
</div>
</div>
<div class="section" id="more-examples-and-patterns">
<h5>More examples and patterns<a class="headerlink" href="#more-examples-and-patterns" title="Permalink to this headline">¶</a></h5>
<p>Here is another spider that illustrates callbacks and following links,
this time for scraping author information:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">AuthorSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;author&#39;</span>

    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;http://quotes.toscrape.com/&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="c1"># follow links to author pages</span>
        <span class="k">for</span> <span class="n">href</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;.author + a::attr(href)&#39;</span><span class="p">):</span>
            <span class="k">yield</span> <span class="n">response</span><span class="o">.</span><span class="n">follow</span><span class="p">(</span><span class="n">href</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse_author</span><span class="p">)</span>

        <span class="c1"># follow pagination links</span>
        <span class="k">for</span> <span class="n">href</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;li.next a::attr(href)&#39;</span><span class="p">):</span>
            <span class="k">yield</span> <span class="n">response</span><span class="o">.</span><span class="n">follow</span><span class="p">(</span><span class="n">href</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">parse_author</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">extract_with_css</span><span class="p">(</span><span class="n">query</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="n">query</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>

        <span class="k">yield</span> <span class="p">{</span>
            <span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="n">extract_with_css</span><span class="p">(</span><span class="s1">&#39;h3.author-title::text&#39;</span><span class="p">),</span>
            <span class="s1">&#39;birthdate&#39;</span><span class="p">:</span> <span class="n">extract_with_css</span><span class="p">(</span><span class="s1">&#39;.author-born-date::text&#39;</span><span class="p">),</span>
            <span class="s1">&#39;bio&#39;</span><span class="p">:</span> <span class="n">extract_with_css</span><span class="p">(</span><span class="s1">&#39;.author-description::text&#39;</span><span class="p">),</span>
        <span class="p">}</span>
</pre></div>
</div>
<p>This spider will start from the main page, it will follow all the links to the
authors pages calling the <code class="docutils literal notranslate"><span class="pre">parse_author</span></code> callback for each of them, and also
the pagination links with the <code class="docutils literal notranslate"><span class="pre">parse</span></code> callback as we saw before.</p>
<p>Here we’re passing callbacks to <code class="docutils literal notranslate"><span class="pre">response.follow</span></code> as positional arguments
to make the code shorter; it also works for <code class="docutils literal notranslate"><span class="pre">scrapy.Request</span></code>.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">parse_author</span></code> callback defines a helper function to extract and cleanup the
data from a CSS query and yields the Python dict with the author data.</p>
<p>Another interesting thing this spider demonstrates is that, even if there are
many quotes from the same author, we don’t need to worry about visiting the
same author page multiple times. By default, Scrapy filters out duplicated
requests to URLs already visited, avoiding the problem of hitting servers too
much because of a programming mistake. This can be configured by the setting
<a class="reference internal" href="index.html#std:setting-DUPEFILTER_CLASS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DUPEFILTER_CLASS</span></code></a>.</p>
<p>Hopefully by now you have a good understanding of how to use the mechanism
of following links and callbacks with Scrapy.</p>
<p>As yet another example spider that leverages the mechanism of following links,
check out the <a class="reference internal" href="index.html#scrapy.spiders.CrawlSpider" title="scrapy.spiders.CrawlSpider"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlSpider</span></code></a> class for a generic
spider that implements a small rules engine that you can use to write your
crawlers on top of it.</p>
<p>Also, a common pattern is to build an item with data from more than one page,
using a <a class="reference internal" href="index.html#topics-request-response-ref-request-callback-arguments"><span class="std std-ref">trick to pass additional data to the callbacks</span></a>.</p>
</div>
</div>
<div class="section" id="using-spider-arguments">
<h4>Using spider arguments<a class="headerlink" href="#using-spider-arguments" title="Permalink to this headline">¶</a></h4>
<p>You can provide command line arguments to your spiders by using the <code class="docutils literal notranslate"><span class="pre">-a</span></code>
option when running them:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">crawl</span> <span class="n">quotes</span> <span class="o">-</span><span class="n">o</span> <span class="n">quotes</span><span class="o">-</span><span class="n">humor</span><span class="o">.</span><span class="n">json</span> <span class="o">-</span><span class="n">a</span> <span class="n">tag</span><span class="o">=</span><span class="n">humor</span>
</pre></div>
</div>
<p>These arguments are passed to the Spider’s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method and become
spider attributes by default.</p>
<p>In this example, the value provided for the <code class="docutils literal notranslate"><span class="pre">tag</span></code> argument will be available
via <code class="docutils literal notranslate"><span class="pre">self.tag</span></code>. You can use this to make your spider fetch only quotes
with a specific tag, building the URL based on the argument:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">QuotesSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;quotes&quot;</span>

    <span class="k">def</span> <span class="nf">start_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">url</span> <span class="o">=</span> <span class="s1">&#39;http://quotes.toscrape.com/&#39;</span>
        <span class="n">tag</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;tag&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">tag</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">url</span> <span class="o">=</span> <span class="n">url</span> <span class="o">+</span> <span class="s1">&#39;tag/&#39;</span> <span class="o">+</span> <span class="n">tag</span>
        <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">quote</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;div.quote&#39;</span><span class="p">):</span>
            <span class="k">yield</span> <span class="p">{</span>
                <span class="s1">&#39;text&#39;</span><span class="p">:</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;span.text::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(),</span>
                <span class="s1">&#39;author&#39;</span><span class="p">:</span> <span class="n">quote</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;small.author::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(),</span>
            <span class="p">}</span>

        <span class="n">next_page</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;li.next a::attr(href)&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">next_page</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">yield</span> <span class="n">response</span><span class="o">.</span><span class="n">follow</span><span class="p">(</span><span class="n">next_page</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
</pre></div>
</div>
<p>If you pass the <code class="docutils literal notranslate"><span class="pre">tag=humor</span></code> argument to this spider, you’ll notice that it
will only visit URLs from the <code class="docutils literal notranslate"><span class="pre">humor</span></code> tag, such as
<code class="docutils literal notranslate"><span class="pre">http://quotes.toscrape.com/tag/humor</span></code>.</p>
<p>You can <a class="reference internal" href="index.html#spiderargs"><span class="std std-ref">learn more about handling spider arguments here</span></a>.</p>
</div>
<div class="section" id="next-steps">
<h4>Next steps<a class="headerlink" href="#next-steps" title="Permalink to this headline">¶</a></h4>
<p>This tutorial covered only the basics of Scrapy, but there’s a lot of other
features not mentioned here. Check the <a class="reference internal" href="index.html#topics-whatelse"><span class="std std-ref">What else?</span></a> section in
<a class="reference internal" href="index.html#intro-overview"><span class="std std-ref">Scrapy at a glance</span></a> chapter for a quick overview of the most important ones.</p>
<p>You can continue from the section <a class="reference internal" href="index.html#section-basics"><span class="std std-ref">Basic concepts</span></a> to know more about the
command-line tool, spiders, selectors and other things the tutorial hasn’t covered like
modeling the scraped data. If you prefer to play with an example project, check
the <a class="reference internal" href="index.html#intro-examples"><span class="std std-ref">Examples</span></a> section.</p>
</div>
</div>
<span id="document-intro/examples"></span><div class="section" id="examples">
<span id="intro-examples"></span><h3>Examples<a class="headerlink" href="#examples" title="Permalink to this headline">¶</a></h3>
<p>The best way to learn is with examples, and Scrapy is no exception. For this
reason, there is an example Scrapy project named <a class="reference external" href="https://github.com/scrapy/quotesbot">quotesbot</a>, that you can use to
play and learn more about Scrapy. It contains two spiders for
<a class="reference external" href="http://quotes.toscrape.com">http://quotes.toscrape.com</a>, one using CSS selectors and another one using XPath
expressions.</p>
<p>The <a class="reference external" href="https://github.com/scrapy/quotesbot">quotesbot</a> project is available at: <a class="reference external" href="https://github.com/scrapy/quotesbot">https://github.com/scrapy/quotesbot</a>.
You can find more information about it in the project’s README.</p>
<p>If you’re familiar with git, you can checkout the code. Otherwise you can
download the project as a zip file by clicking
<a class="reference external" href="https://github.com/scrapy/quotesbot/archive/master.zip">here</a>.</p>
</div>
</div>
<dl class="docutils">
<dt><a class="reference internal" href="index.html#document-intro/overview"><span class="doc">Scrapy at a glance</span></a></dt>
<dd>Understand what Scrapy is and how it can help you.</dd>
<dt><a class="reference internal" href="index.html#document-intro/install"><span class="doc">Installation guide</span></a></dt>
<dd>Get Scrapy installed on your computer.</dd>
<dt><a class="reference internal" href="index.html#document-intro/tutorial"><span class="doc">Scrapy Tutorial</span></a></dt>
<dd>Write your first Scrapy project.</dd>
<dt><a class="reference internal" href="index.html#document-intro/examples"><span class="doc">Examples</span></a></dt>
<dd>Learn more by playing with a pre-made Scrapy project.</dd>
</dl>
</div>
<div class="section" id="basic-concepts">
<span id="section-basics"></span><h2>Basic concepts<a class="headerlink" href="#basic-concepts" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-topics/commands"></span><div class="section" id="command-line-tool">
<span id="topics-commands"></span><h3>Command line tool<a class="headerlink" href="#command-line-tool" title="Permalink to this headline">¶</a></h3>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.10.</span></p>
</div>
<p>Scrapy is controlled through the <code class="docutils literal notranslate"><span class="pre">scrapy</span></code> command-line tool, to be referred
here as the “Scrapy tool” to differentiate it from the sub-commands, which we
just call “commands” or “Scrapy commands”.</p>
<p>The Scrapy tool provides several commands, for multiple purposes, and each one
accepts a different set of arguments and options.</p>
<p>(The <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">deploy</span></code> command has been removed in 1.0 in favor of the
standalone <code class="docutils literal notranslate"><span class="pre">scrapyd-deploy</span></code>. See <a class="reference external" href="https://scrapyd.readthedocs.io/en/latest/deploy.html">Deploying your project</a>.)</p>
<div class="section" id="configuration-settings">
<span id="topics-config-settings"></span><h4>Configuration settings<a class="headerlink" href="#configuration-settings" title="Permalink to this headline">¶</a></h4>
<p>Scrapy will look for configuration parameters in ini-style <code class="docutils literal notranslate"><span class="pre">scrapy.cfg</span></code> files
in standard locations:</p>
<ol class="arabic simple">
<li><code class="docutils literal notranslate"><span class="pre">/etc/scrapy.cfg</span></code> or <code class="docutils literal notranslate"><span class="pre">c:\scrapy\scrapy.cfg</span></code> (system-wide),</li>
<li><code class="docutils literal notranslate"><span class="pre">~/.config/scrapy.cfg</span></code> (<code class="docutils literal notranslate"><span class="pre">$XDG_CONFIG_HOME</span></code>) and <code class="docutils literal notranslate"><span class="pre">~/.scrapy.cfg</span></code> (<code class="docutils literal notranslate"><span class="pre">$HOME</span></code>)
for global (user-wide) settings, and</li>
<li><code class="docutils literal notranslate"><span class="pre">scrapy.cfg</span></code> inside a scrapy project’s root (see next section).</li>
</ol>
<p>Settings from these files are merged in the listed order of preference:
user-defined values have higher priority than system-wide defaults
and project-wide settings will override all others, when defined.</p>
<p>Scrapy also understands, and can be configured through, a number of environment
variables. Currently these are:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">SCRAPY_SETTINGS_MODULE</span></code> (see <a class="reference internal" href="index.html#topics-settings-module-envvar"><span class="std std-ref">Designating the settings</span></a>)</li>
<li><code class="docutils literal notranslate"><span class="pre">SCRAPY_PROJECT</span></code> (see <a class="reference internal" href="#topics-project-envvar"><span class="std std-ref">Sharing the root directory between projects</span></a>)</li>
<li><code class="docutils literal notranslate"><span class="pre">SCRAPY_PYTHON_SHELL</span></code> (see <a class="reference internal" href="index.html#topics-shell"><span class="std std-ref">Scrapy shell</span></a>)</li>
</ul>
</div>
<div class="section" id="default-structure-of-scrapy-projects">
<span id="topics-project-structure"></span><h4>Default structure of Scrapy projects<a class="headerlink" href="#default-structure-of-scrapy-projects" title="Permalink to this headline">¶</a></h4>
<p>Before delving into the command-line tool and its sub-commands, let’s first
understand the directory structure of a Scrapy project.</p>
<p>Though it can be modified, all Scrapy projects have the same file
structure by default, similar to this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span><span class="o">.</span><span class="n">cfg</span>
<span class="n">myproject</span><span class="o">/</span>
    <span class="fm">__init__</span><span class="o">.</span><span class="n">py</span>
    <span class="n">items</span><span class="o">.</span><span class="n">py</span>
    <span class="n">middlewares</span><span class="o">.</span><span class="n">py</span>
    <span class="n">pipelines</span><span class="o">.</span><span class="n">py</span>
    <span class="n">settings</span><span class="o">.</span><span class="n">py</span>
    <span class="n">spiders</span><span class="o">/</span>
        <span class="fm">__init__</span><span class="o">.</span><span class="n">py</span>
        <span class="n">spider1</span><span class="o">.</span><span class="n">py</span>
        <span class="n">spider2</span><span class="o">.</span><span class="n">py</span>
        <span class="o">...</span>
</pre></div>
</div>
<p>The directory where the <code class="docutils literal notranslate"><span class="pre">scrapy.cfg</span></code> file resides is known as the <em>project
root directory</em>. That file contains the name of the python module that defines
the project settings. Here is an example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">settings</span><span class="p">]</span>
<span class="n">default</span> <span class="o">=</span> <span class="n">myproject</span><span class="o">.</span><span class="n">settings</span>
</pre></div>
</div>
</div>
<div class="section" id="sharing-the-root-directory-between-projects">
<span id="topics-project-envvar"></span><h4>Sharing the root directory between projects<a class="headerlink" href="#sharing-the-root-directory-between-projects" title="Permalink to this headline">¶</a></h4>
<p>A project root directory, the one that contains the <code class="docutils literal notranslate"><span class="pre">scrapy.cfg</span></code>, may be
shared by multiple Scrapy projects, each with its own settings module.</p>
<p>In that case, you must define one or more aliases for those settings modules
under <code class="docutils literal notranslate"><span class="pre">[settings]</span></code> in your <code class="docutils literal notranslate"><span class="pre">scrapy.cfg</span></code> file:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">settings</span><span class="p">]</span>
<span class="n">default</span> <span class="o">=</span> <span class="n">myproject1</span><span class="o">.</span><span class="n">settings</span>
<span class="n">project1</span> <span class="o">=</span> <span class="n">myproject1</span><span class="o">.</span><span class="n">settings</span>
<span class="n">project2</span> <span class="o">=</span> <span class="n">myproject2</span><span class="o">.</span><span class="n">settings</span>
</pre></div>
</div>
<p>By default, the <code class="docutils literal notranslate"><span class="pre">scrapy</span></code> command-line tool will use the <code class="docutils literal notranslate"><span class="pre">default</span></code> settings.
Use the <code class="docutils literal notranslate"><span class="pre">SCRAPY_PROJECT</span></code> environment variable to specify a different project
for <code class="docutils literal notranslate"><span class="pre">scrapy</span></code> to use:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ scrapy settings --get BOT_NAME
Project 1 Bot
$ export SCRAPY_PROJECT=project2
$ scrapy settings --get BOT_NAME
Project 2 Bot
</pre></div>
</div>
</div>
<div class="section" id="using-the-scrapy-tool">
<h4>Using the <code class="docutils literal notranslate"><span class="pre">scrapy</span></code> tool<a class="headerlink" href="#using-the-scrapy-tool" title="Permalink to this headline">¶</a></h4>
<p>You can start by running the Scrapy tool with no arguments and it will print
some usage help and the available commands:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Scrapy</span> <span class="n">X</span><span class="o">.</span><span class="n">Y</span> <span class="o">-</span> <span class="n">no</span> <span class="n">active</span> <span class="n">project</span>

<span class="n">Usage</span><span class="p">:</span>
  <span class="n">scrapy</span> <span class="o">&lt;</span><span class="n">command</span><span class="o">&gt;</span> <span class="p">[</span><span class="n">options</span><span class="p">]</span> <span class="p">[</span><span class="n">args</span><span class="p">]</span>

<span class="n">Available</span> <span class="n">commands</span><span class="p">:</span>
  <span class="n">crawl</span>         <span class="n">Run</span> <span class="n">a</span> <span class="n">spider</span>
  <span class="n">fetch</span>         <span class="n">Fetch</span> <span class="n">a</span> <span class="n">URL</span> <span class="n">using</span> <span class="n">the</span> <span class="n">Scrapy</span> <span class="n">downloader</span>
<span class="p">[</span><span class="o">...</span><span class="p">]</span>
</pre></div>
</div>
<p>The first line will print the currently active project if you’re inside a
Scrapy project. In this example it was run from outside a project. If run from inside
a project it would have printed something like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Scrapy</span> <span class="n">X</span><span class="o">.</span><span class="n">Y</span> <span class="o">-</span> <span class="n">project</span><span class="p">:</span> <span class="n">myproject</span>

<span class="n">Usage</span><span class="p">:</span>
  <span class="n">scrapy</span> <span class="o">&lt;</span><span class="n">command</span><span class="o">&gt;</span> <span class="p">[</span><span class="n">options</span><span class="p">]</span> <span class="p">[</span><span class="n">args</span><span class="p">]</span>

<span class="p">[</span><span class="o">...</span><span class="p">]</span>
</pre></div>
</div>
<div class="section" id="creating-projects">
<h5>Creating projects<a class="headerlink" href="#creating-projects" title="Permalink to this headline">¶</a></h5>
<p>The first thing you typically do with the <code class="docutils literal notranslate"><span class="pre">scrapy</span></code> tool is create your Scrapy
project:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">startproject</span> <span class="n">myproject</span> <span class="p">[</span><span class="n">project_dir</span><span class="p">]</span>
</pre></div>
</div>
<p>That will create a Scrapy project under the <code class="docutils literal notranslate"><span class="pre">project_dir</span></code> directory.
If <code class="docutils literal notranslate"><span class="pre">project_dir</span></code> wasn’t specified, <code class="docutils literal notranslate"><span class="pre">project_dir</span></code> will be the same as <code class="docutils literal notranslate"><span class="pre">myproject</span></code>.</p>
<p>Next, you go inside the new project directory:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span> <span class="n">project_dir</span>
</pre></div>
</div>
<p>And you’re ready to use the <code class="docutils literal notranslate"><span class="pre">scrapy</span></code> command to manage and control your
project from there.</p>
</div>
<div class="section" id="controlling-projects">
<h5>Controlling projects<a class="headerlink" href="#controlling-projects" title="Permalink to this headline">¶</a></h5>
<p>You use the <code class="docutils literal notranslate"><span class="pre">scrapy</span></code> tool from inside your projects to control and manage
them.</p>
<p>For example, to create a new spider:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">genspider</span> <span class="n">mydomain</span> <span class="n">mydomain</span><span class="o">.</span><span class="n">com</span>
</pre></div>
</div>
<p>Some Scrapy commands (like <a class="reference internal" href="#std:command-crawl"><code class="xref std std-command docutils literal notranslate"><span class="pre">crawl</span></code></a>) must be run from inside a Scrapy
project. See the <a class="reference internal" href="#topics-commands-ref"><span class="std std-ref">commands reference</span></a> below for more
information on which commands must be run from inside projects, and which not.</p>
<p>Also keep in mind that some commands may have slightly different behaviours
when running them from inside projects. For example, the fetch command will use
spider-overridden behaviours (such as the <code class="docutils literal notranslate"><span class="pre">user_agent</span></code> attribute to override
the user-agent) if the url being fetched is associated with some specific
spider. This is intentional, as the <code class="docutils literal notranslate"><span class="pre">fetch</span></code> command is meant to be used to
check how spiders are downloading pages.</p>
</div>
</div>
<div class="section" id="available-tool-commands">
<span id="topics-commands-ref"></span><h4>Available tool commands<a class="headerlink" href="#available-tool-commands" title="Permalink to this headline">¶</a></h4>
<p>This section contains a list of the available built-in commands with a
description and some usage examples. Remember, you can always get more info
about each command by running:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="o">&lt;</span><span class="n">command</span><span class="o">&gt;</span> <span class="o">-</span><span class="n">h</span>
</pre></div>
</div>
<p>And you can see all available commands with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="o">-</span><span class="n">h</span>
</pre></div>
</div>
<p>There are two kinds of commands, those that only work from inside a Scrapy
project (Project-specific commands) and those that also work without an active
Scrapy project (Global commands), though they may behave slightly different
when running from inside a project (as they would use the project overridden
settings).</p>
<p>Global commands:</p>
<ul class="simple">
<li><a class="reference internal" href="#std:command-startproject"><code class="xref std std-command docutils literal notranslate"><span class="pre">startproject</span></code></a></li>
<li><a class="reference internal" href="#std:command-genspider"><code class="xref std std-command docutils literal notranslate"><span class="pre">genspider</span></code></a></li>
<li><a class="reference internal" href="#std:command-settings"><code class="xref std std-command docutils literal notranslate"><span class="pre">settings</span></code></a></li>
<li><a class="reference internal" href="#std:command-runspider"><code class="xref std std-command docutils literal notranslate"><span class="pre">runspider</span></code></a></li>
<li><a class="reference internal" href="#std:command-shell"><code class="xref std std-command docutils literal notranslate"><span class="pre">shell</span></code></a></li>
<li><a class="reference internal" href="#std:command-fetch"><code class="xref std std-command docutils literal notranslate"><span class="pre">fetch</span></code></a></li>
<li><a class="reference internal" href="#std:command-view"><code class="xref std std-command docutils literal notranslate"><span class="pre">view</span></code></a></li>
<li><a class="reference internal" href="#std:command-version"><code class="xref std std-command docutils literal notranslate"><span class="pre">version</span></code></a></li>
</ul>
<p>Project-only commands:</p>
<ul class="simple">
<li><a class="reference internal" href="#std:command-crawl"><code class="xref std std-command docutils literal notranslate"><span class="pre">crawl</span></code></a></li>
<li><a class="reference internal" href="#std:command-check"><code class="xref std std-command docutils literal notranslate"><span class="pre">check</span></code></a></li>
<li><a class="reference internal" href="#std:command-list"><code class="xref std std-command docutils literal notranslate"><span class="pre">list</span></code></a></li>
<li><a class="reference internal" href="#std:command-edit"><code class="xref std std-command docutils literal notranslate"><span class="pre">edit</span></code></a></li>
<li><a class="reference internal" href="#std:command-parse"><code class="xref std std-command docutils literal notranslate"><span class="pre">parse</span></code></a></li>
<li><a class="reference internal" href="#std:command-bench"><code class="xref std std-command docutils literal notranslate"><span class="pre">bench</span></code></a></li>
</ul>
<div class="section" id="startproject">
<span id="std:command-startproject"></span><h5>startproject<a class="headerlink" href="#startproject" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">startproject</span> <span class="pre">&lt;project_name&gt;</span> <span class="pre">[project_dir]</span></code></li>
<li>Requires project: <em>no</em></li>
</ul>
<p>Creates a new Scrapy project named <code class="docutils literal notranslate"><span class="pre">project_name</span></code>, under the <code class="docutils literal notranslate"><span class="pre">project_dir</span></code>
directory.
If <code class="docutils literal notranslate"><span class="pre">project_dir</span></code> wasn’t specified, <code class="docutils literal notranslate"><span class="pre">project_dir</span></code> will be the same as <code class="docutils literal notranslate"><span class="pre">project_name</span></code>.</p>
<p>Usage example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ scrapy startproject myproject
</pre></div>
</div>
</div>
<div class="section" id="genspider">
<span id="std:command-genspider"></span><h5>genspider<a class="headerlink" href="#genspider" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">genspider</span> <span class="pre">[-t</span> <span class="pre">template]</span> <span class="pre">&lt;name&gt;</span> <span class="pre">&lt;domain&gt;</span></code></li>
<li>Requires project: <em>no</em></li>
</ul>
<p>Create a new spider in the current folder or in the current project’s <code class="docutils literal notranslate"><span class="pre">spiders</span></code> folder, if called from inside a project. The <code class="docutils literal notranslate"><span class="pre">&lt;name&gt;</span></code> parameter is set as the spider’s <code class="docutils literal notranslate"><span class="pre">name</span></code>, while <code class="docutils literal notranslate"><span class="pre">&lt;domain&gt;</span></code> is used to generate the <code class="docutils literal notranslate"><span class="pre">allowed_domains</span></code> and <code class="docutils literal notranslate"><span class="pre">start_urls</span></code> spider’s attributes.</p>
<p>Usage example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ scrapy genspider -l
Available templates:
  basic
  crawl
  csvfeed
  xmlfeed

$ scrapy genspider example example.com
Created spider &#39;example&#39; using template &#39;basic&#39;

$ scrapy genspider -t crawl scrapyorg scrapy.org
Created spider &#39;scrapyorg&#39; using template &#39;crawl&#39;
</pre></div>
</div>
<p>This is just a convenience shortcut command for creating spiders based on
pre-defined templates, but certainly not the only way to create spiders. You
can just create the spider source code files yourself, instead of using this
command.</p>
</div>
<div class="section" id="crawl">
<span id="std:command-crawl"></span><h5>crawl<a class="headerlink" href="#crawl" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">crawl</span> <span class="pre">&lt;spider&gt;</span></code></li>
<li>Requires project: <em>yes</em></li>
</ul>
<p>Start crawling using a spider.</p>
<p>Usage examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ scrapy crawl myspider
[ ... myspider starts crawling ... ]
</pre></div>
</div>
</div>
<div class="section" id="check">
<span id="std:command-check"></span><h5>check<a class="headerlink" href="#check" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">check</span> <span class="pre">[-l]</span> <span class="pre">&lt;spider&gt;</span></code></li>
<li>Requires project: <em>yes</em></li>
</ul>
<p>Run contract checks.</p>
<p>Usage examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ scrapy check -l
first_spider
  * parse
  * parse_item
second_spider
  * parse
  * parse_item

$ scrapy check
[FAILED] first_spider:parse_item
&gt;&gt;&gt; &#39;RetailPricex&#39; field is missing

[FAILED] first_spider:parse
&gt;&gt;&gt; Returned 92 requests, expected 0..4
</pre></div>
</div>
</div>
<div class="section" id="list">
<span id="std:command-list"></span><h5>list<a class="headerlink" href="#list" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">list</span></code></li>
<li>Requires project: <em>yes</em></li>
</ul>
<p>List all available spiders in the current project. The output is one spider per
line.</p>
<p>Usage example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ scrapy list
spider1
spider2
</pre></div>
</div>
</div>
<div class="section" id="edit">
<span id="std:command-edit"></span><h5>edit<a class="headerlink" href="#edit" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">edit</span> <span class="pre">&lt;spider&gt;</span></code></li>
<li>Requires project: <em>yes</em></li>
</ul>
<p>Edit the given spider using the editor defined in the <code class="docutils literal notranslate"><span class="pre">EDITOR</span></code> environment
variable or (if unset) the <a class="reference internal" href="index.html#std:setting-EDITOR"><code class="xref std std-setting docutils literal notranslate"><span class="pre">EDITOR</span></code></a> setting.</p>
<p>This command is provided only as a convenience shortcut for the most common
case, the developer is of course free to choose any tool or IDE to write and
debug spiders.</p>
<p>Usage example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ scrapy edit spider1
</pre></div>
</div>
</div>
<div class="section" id="fetch">
<span id="std:command-fetch"></span><h5>fetch<a class="headerlink" href="#fetch" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">fetch</span> <span class="pre">&lt;url&gt;</span></code></li>
<li>Requires project: <em>no</em></li>
</ul>
<p>Downloads the given URL using the Scrapy downloader and writes the contents to
standard output.</p>
<p>The interesting thing about this command is that it fetches the page how the
spider would download it. For example, if the spider has a <code class="docutils literal notranslate"><span class="pre">USER_AGENT</span></code>
attribute which overrides the User Agent, it will use that one.</p>
<p>So this command can be used to “see” how your spider would fetch a certain page.</p>
<p>If used outside a project, no particular per-spider behaviour would be applied
and it will just use the default Scrapy downloader settings.</p>
<p>Supported options:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">--spider=SPIDER</span></code>: bypass spider autodetection and force use of specific spider</li>
<li><code class="docutils literal notranslate"><span class="pre">--headers</span></code>: print the response’s HTTP headers instead of the response’s body</li>
<li><code class="docutils literal notranslate"><span class="pre">--no-redirect</span></code>: do not follow HTTP 3xx redirects (default is to follow them)</li>
</ul>
<p>Usage examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ scrapy fetch --nolog http://www.example.com/some/page.html
[ ... html content here ... ]

$ scrapy fetch --nolog --headers http://www.example.com/
{&#39;Accept-Ranges&#39;: [&#39;bytes&#39;],
 &#39;Age&#39;: [&#39;1263   &#39;],
 &#39;Connection&#39;: [&#39;close     &#39;],
 &#39;Content-Length&#39;: [&#39;596&#39;],
 &#39;Content-Type&#39;: [&#39;text/html; charset=UTF-8&#39;],
 &#39;Date&#39;: [&#39;Wed, 18 Aug 2010 23:59:46 GMT&#39;],
 &#39;Etag&#39;: [&#39;&quot;573c1-254-48c9c87349680&quot;&#39;],
 &#39;Last-Modified&#39;: [&#39;Fri, 30 Jul 2010 15:30:18 GMT&#39;],
 &#39;Server&#39;: [&#39;Apache/2.2.3 (CentOS)&#39;]}
</pre></div>
</div>
</div>
<div class="section" id="view">
<span id="std:command-view"></span><h5>view<a class="headerlink" href="#view" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">view</span> <span class="pre">&lt;url&gt;</span></code></li>
<li>Requires project: <em>no</em></li>
</ul>
<p>Opens the given URL in a browser, as your Scrapy spider would “see” it.
Sometimes spiders see pages differently from regular users, so this can be used
to check what the spider “sees” and confirm it’s what you expect.</p>
<p>Supported options:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">--spider=SPIDER</span></code>: bypass spider autodetection and force use of specific spider</li>
<li><code class="docutils literal notranslate"><span class="pre">--no-redirect</span></code>: do not follow HTTP 3xx redirects (default is to follow them)</li>
</ul>
<p>Usage example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ scrapy view http://www.example.com/some/page.html
[ ... browser starts ... ]
</pre></div>
</div>
</div>
<div class="section" id="shell">
<span id="std:command-shell"></span><h5>shell<a class="headerlink" href="#shell" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">shell</span> <span class="pre">[url]</span></code></li>
<li>Requires project: <em>no</em></li>
</ul>
<p>Starts the Scrapy shell for the given URL (if given) or empty if no URL is
given. Also supports UNIX-style local file paths, either relative with
<code class="docutils literal notranslate"><span class="pre">./</span></code> or <code class="docutils literal notranslate"><span class="pre">../</span></code> prefixes or absolute file paths.
See <a class="reference internal" href="index.html#topics-shell"><span class="std std-ref">Scrapy shell</span></a> for more info.</p>
<p>Supported options:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">--spider=SPIDER</span></code>: bypass spider autodetection and force use of specific spider</li>
<li><code class="docutils literal notranslate"><span class="pre">-c</span> <span class="pre">code</span></code>: evaluate the code in the shell, print the result and exit</li>
<li><code class="docutils literal notranslate"><span class="pre">--no-redirect</span></code>: do not follow HTTP 3xx redirects (default is to follow them);
this only affects the URL you may pass as argument on the command line;
once you are inside the shell, <code class="docutils literal notranslate"><span class="pre">fetch(url)</span></code> will still follow HTTP redirects by default.</li>
</ul>
<p>Usage example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ scrapy shell http://www.example.com/some/page.html
[ ... scrapy shell starts ... ]

$ scrapy shell --nolog http://www.example.com/ -c &#39;(response.status, response.url)&#39;
(200, &#39;http://www.example.com/&#39;)

# shell follows HTTP redirects by default
$ scrapy shell --nolog http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F -c &#39;(response.status, response.url)&#39;
(200, &#39;http://example.com/&#39;)

# you can disable this with --no-redirect
# (only for the URL passed as command line argument)
$ scrapy shell --no-redirect --nolog http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F -c &#39;(response.status, response.url)&#39;
(302, &#39;http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F&#39;)
</pre></div>
</div>
</div>
<div class="section" id="parse">
<span id="std:command-parse"></span><h5>parse<a class="headerlink" href="#parse" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">parse</span> <span class="pre">&lt;url&gt;</span> <span class="pre">[options]</span></code></li>
<li>Requires project: <em>yes</em></li>
</ul>
<p>Fetches the given URL and parses it with the spider that handles it, using the
method passed with the <code class="docutils literal notranslate"><span class="pre">--callback</span></code> option, or <code class="docutils literal notranslate"><span class="pre">parse</span></code> if not given.</p>
<p>Supported options:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">--spider=SPIDER</span></code>: bypass spider autodetection and force use of specific spider</li>
<li><code class="docutils literal notranslate"><span class="pre">--a</span> <span class="pre">NAME=VALUE</span></code>: set spider argument (may be repeated)</li>
<li><code class="docutils literal notranslate"><span class="pre">--callback</span></code> or <code class="docutils literal notranslate"><span class="pre">-c</span></code>: spider method to use as callback for parsing the
response</li>
<li><code class="docutils literal notranslate"><span class="pre">--meta</span></code> or <code class="docutils literal notranslate"><span class="pre">-m</span></code>: additional request meta that will be passed to the callback
request. This must be a valid json string. Example: –meta=’{“foo” : “bar”}’</li>
<li><code class="docutils literal notranslate"><span class="pre">--cbkwargs</span></code>: additional keyword arguments that will be passed to the callback.
This must be a valid json string. Example: –cbkwargs=’{“foo” : “bar”}’</li>
<li><code class="docutils literal notranslate"><span class="pre">--pipelines</span></code>: process items through pipelines</li>
<li><code class="docutils literal notranslate"><span class="pre">--rules</span></code> or <code class="docutils literal notranslate"><span class="pre">-r</span></code>: use <a class="reference internal" href="index.html#scrapy.spiders.CrawlSpider" title="scrapy.spiders.CrawlSpider"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlSpider</span></code></a>
rules to discover the callback (i.e. spider method) to use for parsing the
response</li>
<li><code class="docutils literal notranslate"><span class="pre">--noitems</span></code>: don’t show scraped items</li>
<li><code class="docutils literal notranslate"><span class="pre">--nolinks</span></code>: don’t show extracted links</li>
<li><code class="docutils literal notranslate"><span class="pre">--nocolour</span></code>: avoid using pygments to colorize the output</li>
<li><code class="docutils literal notranslate"><span class="pre">--depth</span></code> or <code class="docutils literal notranslate"><span class="pre">-d</span></code>: depth level for which the requests should be followed
recursively (default: 1)</li>
<li><code class="docutils literal notranslate"><span class="pre">--verbose</span></code> or <code class="docutils literal notranslate"><span class="pre">-v</span></code>: display information for each depth level</li>
</ul>
<p>Usage example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ scrapy parse http://www.example.com/ -c parse_item
[ ... scrapy log lines crawling example.com spider ... ]

&gt;&gt;&gt; STATUS DEPTH LEVEL 1 &lt;&lt;&lt;
# Scraped Items  ------------------------------------------------------------
[{&#39;name&#39;: &#39;Example item&#39;,
 &#39;category&#39;: &#39;Furniture&#39;,
 &#39;length&#39;: &#39;12 cm&#39;}]

# Requests  -----------------------------------------------------------------
[]
</pre></div>
</div>
</div>
<div class="section" id="settings">
<span id="std:command-settings"></span><h5>settings<a class="headerlink" href="#settings" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">settings</span> <span class="pre">[options]</span></code></li>
<li>Requires project: <em>no</em></li>
</ul>
<p>Get the value of a Scrapy setting.</p>
<p>If used inside a project it’ll show the project setting value, otherwise it’ll
show the default Scrapy value for that setting.</p>
<p>Example usage:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ scrapy settings --get BOT_NAME
scrapybot
$ scrapy settings --get DOWNLOAD_DELAY
0
</pre></div>
</div>
</div>
<div class="section" id="runspider">
<span id="std:command-runspider"></span><h5>runspider<a class="headerlink" href="#runspider" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">runspider</span> <span class="pre">&lt;spider_file.py&gt;</span></code></li>
<li>Requires project: <em>no</em></li>
</ul>
<p>Run a spider self-contained in a Python file, without having to create a
project.</p>
<p>Example usage:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ scrapy runspider myspider.py
[ ... spider starts crawling ... ]
</pre></div>
</div>
</div>
<div class="section" id="version">
<span id="std:command-version"></span><h5>version<a class="headerlink" href="#version" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Syntax: <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">version</span> <span class="pre">[-v]</span></code></li>
<li>Requires project: <em>no</em></li>
</ul>
<p>Prints the Scrapy version. If used with <code class="docutils literal notranslate"><span class="pre">-v</span></code> it also prints Python, Twisted
and Platform info, which is useful for bug reports.</p>
</div>
<div class="section" id="bench">
<span id="std:command-bench"></span><h5>bench<a class="headerlink" href="#bench" title="Permalink to this headline">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17.</span></p>
</div>
<ul class="simple">
<li>Syntax: <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">bench</span></code></li>
<li>Requires project: <em>no</em></li>
</ul>
<p>Run a quick benchmark test. <a class="reference internal" href="index.html#benchmarking"><span class="std std-ref">Benchmarking</span></a>.</p>
</div>
</div>
<div class="section" id="custom-project-commands">
<h4>Custom project commands<a class="headerlink" href="#custom-project-commands" title="Permalink to this headline">¶</a></h4>
<p>You can also add your custom project commands by using the
<a class="reference internal" href="#std:setting-COMMANDS_MODULE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">COMMANDS_MODULE</span></code></a> setting. See the Scrapy commands in
<a class="reference external" href="https://github.com/scrapy/scrapy/tree/master/scrapy/commands">scrapy/commands</a> for examples on how to implement your commands.</p>
<div class="section" id="commands-module">
<span id="std:setting-COMMANDS_MODULE"></span><h5>COMMANDS_MODULE<a class="headerlink" href="#commands-module" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">''</span></code> (empty string)</p>
<p>A module to use for looking up custom Scrapy commands. This is used to add custom
commands for your Scrapy project.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">COMMANDS_MODULE</span> <span class="o">=</span> <span class="s1">&#39;mybot.commands&#39;</span>
</pre></div>
</div>
</div>
<div class="section" id="register-commands-via-setup-py-entry-points">
<h5>Register commands via setup.py entry points<a class="headerlink" href="#register-commands-via-setup-py-entry-points" title="Permalink to this headline">¶</a></h5>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This is an experimental feature, use with caution.</p>
</div>
<p>You can also add Scrapy commands from an external library by adding a
<code class="docutils literal notranslate"><span class="pre">scrapy.commands</span></code> section in the entry points of the library <code class="docutils literal notranslate"><span class="pre">setup.py</span></code>
file.</p>
<p>The following example adds <code class="docutils literal notranslate"><span class="pre">my_command</span></code> command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">setuptools</span> <span class="k">import</span> <span class="n">setup</span><span class="p">,</span> <span class="n">find_packages</span>

<span class="n">setup</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;scrapy-mymodule&#39;</span><span class="p">,</span>
  <span class="n">entry_points</span><span class="o">=</span><span class="p">{</span>
    <span class="s1">&#39;scrapy.commands&#39;</span><span class="p">:</span> <span class="p">[</span>
      <span class="s1">&#39;my_command=my_scrapy_module.commands:MyCommand&#39;</span><span class="p">,</span>
    <span class="p">],</span>
  <span class="p">},</span>
 <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<span id="document-topics/spiders"></span><div class="section" id="spiders">
<span id="topics-spiders"></span><h3>Spiders<a class="headerlink" href="#spiders" title="Permalink to this headline">¶</a></h3>
<p>Spiders are classes which define how a certain site (or a group of sites) will be
scraped, including how to perform the crawl (i.e. follow links) and how to
extract structured data from their pages (i.e. scraping items). In other words,
Spiders are the place where you define the custom behaviour for crawling and
parsing pages for a particular site (or, in some cases, a group of sites).</p>
<p>For spiders, the scraping cycle goes through something like this:</p>
<ol class="arabic">
<li><p class="first">You start by generating the initial Requests to crawl the first URLs, and
specify a callback function to be called with the response downloaded from
those requests.</p>
<p>The first requests to perform are obtained by calling the
<a class="reference internal" href="#scrapy.spiders.Spider.start_requests" title="scrapy.spiders.Spider.start_requests"><code class="xref py py-meth docutils literal notranslate"><span class="pre">start_requests()</span></code></a> method which (by default)
generates <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> for the URLs specified in the
<a class="reference internal" href="#scrapy.spiders.Spider.start_urls" title="scrapy.spiders.Spider.start_urls"><code class="xref py py-attr docutils literal notranslate"><span class="pre">start_urls</span></code></a> and the
<a class="reference internal" href="#scrapy.spiders.Spider.parse" title="scrapy.spiders.Spider.parse"><code class="xref py py-attr docutils literal notranslate"><span class="pre">parse</span></code></a> method as callback function for the
Requests.</p>
</li>
<li><p class="first">In the callback function, you parse the response (web page) and return either
dicts with extracted data, <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a> objects,
<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> objects, or an iterable of these objects.
Those Requests will also contain a callback (maybe
the same) and will then be downloaded by Scrapy and then their
response handled by the specified callback.</p>
</li>
<li><p class="first">In callback functions, you parse the page contents, typically using
<a class="reference internal" href="index.html#topics-selectors"><span class="std std-ref">Selectors</span></a> (but you can also use BeautifulSoup, lxml or whatever
mechanism you prefer) and generate items with the parsed data.</p>
</li>
<li><p class="first">Finally, the items returned from the spider will be typically persisted to a
database (in some <a class="reference internal" href="index.html#topics-item-pipeline"><span class="std std-ref">Item Pipeline</span></a>) or written to
a file using <a class="reference internal" href="index.html#topics-feed-exports"><span class="std std-ref">Feed exports</span></a>.</p>
</li>
</ol>
<p>Even though this cycle applies (more or less) to any kind of spider, there are
different kinds of default spiders bundled into Scrapy for different purposes.
We will talk about those types here.</p>
<span class="target" id="module-scrapy.spiders"></span><div class="section" id="scrapy-spider">
<span id="topics-spiders-ref"></span><h4>scrapy.Spider<a class="headerlink" href="#scrapy-spider" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="scrapy.spiders.Spider">
<em class="property">class </em><code class="descclassname">scrapy.spiders.</code><code class="descname">Spider</code><a class="headerlink" href="#scrapy.spiders.Spider" title="Permalink to this definition">¶</a></dt>
<dd><p>This is the simplest spider, and the one from which every other spider
must inherit (including spiders that come bundled with Scrapy, as well as spiders
that you write yourself). It doesn’t provide any special functionality. It just
provides a default <a class="reference internal" href="#scrapy.spiders.Spider.start_requests" title="scrapy.spiders.Spider.start_requests"><code class="xref py py-meth docutils literal notranslate"><span class="pre">start_requests()</span></code></a> implementation which sends requests from
the <a class="reference internal" href="#scrapy.spiders.Spider.start_urls" title="scrapy.spiders.Spider.start_urls"><code class="xref py py-attr docutils literal notranslate"><span class="pre">start_urls</span></code></a> spider attribute and calls the spider’s method <code class="docutils literal notranslate"><span class="pre">parse</span></code>
for each of the resulting responses.</p>
<dl class="attribute">
<dt id="scrapy.spiders.Spider.name">
<code class="descname">name</code><a class="headerlink" href="#scrapy.spiders.Spider.name" title="Permalink to this definition">¶</a></dt>
<dd><p>A string which defines the name for this spider. The spider name is how
the spider is located (and instantiated) by Scrapy, so it must be
unique. However, nothing prevents you from instantiating more than one
instance of the same spider. This is the most important spider attribute
and it’s required.</p>
<p>If the spider scrapes a single domain, a common practice is to name the
spider after the domain, with or without the <a class="reference external" href="https://en.wikipedia.org/wiki/Top-level_domain">TLD</a>. So, for example, a
spider that crawls <code class="docutils literal notranslate"><span class="pre">mywebsite.com</span></code> would often be called
<code class="docutils literal notranslate"><span class="pre">mywebsite</span></code>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">In Python 2 this must be ASCII only.</p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.spiders.Spider.allowed_domains">
<code class="descname">allowed_domains</code><a class="headerlink" href="#scrapy.spiders.Spider.allowed_domains" title="Permalink to this definition">¶</a></dt>
<dd><p>An optional list of strings containing domains that this spider is
allowed to crawl. Requests for URLs not belonging to the domain names
specified in this list (or their subdomains) won’t be followed if
<a class="reference internal" href="index.html#scrapy.spidermiddlewares.offsite.OffsiteMiddleware" title="scrapy.spidermiddlewares.offsite.OffsiteMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">OffsiteMiddleware</span></code></a> is enabled.</p>
<p>Let’s say your target url is <code class="docutils literal notranslate"><span class="pre">https://www.example.com/1.html</span></code>,
then add <code class="docutils literal notranslate"><span class="pre">'example.com'</span></code> to the list.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.spiders.Spider.start_urls">
<code class="descname">start_urls</code><a class="headerlink" href="#scrapy.spiders.Spider.start_urls" title="Permalink to this definition">¶</a></dt>
<dd><p>A list of URLs where the spider will begin to crawl from, when no
particular URLs are specified. So, the first pages downloaded will be those
listed here. The subsequent <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> will be generated successively from data
contained in the start URLs.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.spiders.Spider.custom_settings">
<code class="descname">custom_settings</code><a class="headerlink" href="#scrapy.spiders.Spider.custom_settings" title="Permalink to this definition">¶</a></dt>
<dd><p>A dictionary of settings that will be overridden from the project wide
configuration when running this spider. It must be defined as a class
attribute since the settings are updated before instantiation.</p>
<p>For a list of available built-in settings see:
<a class="reference internal" href="index.html#topics-settings-ref"><span class="std std-ref">Built-in settings reference</span></a>.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.spiders.Spider.crawler">
<code class="descname">crawler</code><a class="headerlink" href="#scrapy.spiders.Spider.crawler" title="Permalink to this definition">¶</a></dt>
<dd><p>This attribute is set by the <a class="reference internal" href="index.html#from_crawler" title="from_crawler"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_crawler()</span></code></a> class method after
initializating the class, and links to the
<a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> object to which this spider instance is
bound.</p>
<p>Crawlers encapsulate a lot of components in the project for their single
entry access (such as extensions, middlewares, signals managers, etc).
See <a class="reference internal" href="index.html#topics-api-crawler"><span class="std std-ref">Crawler API</span></a> to know more about them.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.spiders.Spider.settings">
<code class="descname">settings</code><a class="headerlink" href="#scrapy.spiders.Spider.settings" title="Permalink to this definition">¶</a></dt>
<dd><p>Configuration for running this spider. This is a
<a class="reference internal" href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">Settings</span></code></a> instance, see the
<a class="reference internal" href="index.html#topics-settings"><span class="std std-ref">Settings</span></a> topic for a detailed introduction on this subject.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.spiders.Spider.logger">
<code class="descname">logger</code><a class="headerlink" href="#scrapy.spiders.Spider.logger" title="Permalink to this definition">¶</a></dt>
<dd><p>Python logger created with the Spider’s <a class="reference internal" href="#scrapy.spiders.Spider.name" title="scrapy.spiders.Spider.name"><code class="xref py py-attr docutils literal notranslate"><span class="pre">name</span></code></a>. You can use it to
send log messages through it as described on
<a class="reference internal" href="index.html#topics-logging-from-spiders"><span class="std std-ref">Logging from Spiders</span></a>.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.spiders.Spider.from_crawler">
<code class="descname">from_crawler</code><span class="sig-paren">(</span><em>crawler</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiders.Spider.from_crawler" title="Permalink to this definition">¶</a></dt>
<dd><p>This is the class method used by Scrapy to create your spiders.</p>
<p>You probably won’t need to override this directly because the default
implementation acts as a proxy to the <code class="xref py py-meth docutils literal notranslate"><span class="pre">__init__()</span></code> method, calling
it with the given arguments <code class="docutils literal notranslate"><span class="pre">args</span></code> and named arguments <code class="docutils literal notranslate"><span class="pre">kwargs</span></code>.</p>
<p>Nonetheless, this method sets the <a class="reference internal" href="#scrapy.spiders.Spider.crawler" title="scrapy.spiders.Spider.crawler"><code class="xref py py-attr docutils literal notranslate"><span class="pre">crawler</span></code></a> and <a class="reference internal" href="#scrapy.spiders.Spider.settings" title="scrapy.spiders.Spider.settings"><code class="xref py py-attr docutils literal notranslate"><span class="pre">settings</span></code></a>
attributes in the new instance so they can be accessed later inside the
spider’s code.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>crawler</strong> (<a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> instance) – crawler to which the spider will be bound</li>
<li><strong>args</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a>) – arguments passed to the <code class="xref py py-meth docutils literal notranslate"><span class="pre">__init__()</span></code> method</li>
<li><strong>kwargs</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.7)"><em>dict</em></a>) – keyword arguments passed to the <code class="xref py py-meth docutils literal notranslate"><span class="pre">__init__()</span></code> method</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.spiders.Spider.start_requests">
<code class="descname">start_requests</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiders.Spider.start_requests" title="Permalink to this definition">¶</a></dt>
<dd><p>This method must return an iterable with the first Requests to crawl for
this spider. It is called by Scrapy when the spider is opened for
scraping. Scrapy calls it only once, so it is safe to implement
<a class="reference internal" href="#scrapy.spiders.Spider.start_requests" title="scrapy.spiders.Spider.start_requests"><code class="xref py py-meth docutils literal notranslate"><span class="pre">start_requests()</span></code></a> as a generator.</p>
<p>The default implementation generates <code class="docutils literal notranslate"><span class="pre">Request(url,</span> <span class="pre">dont_filter=True)</span></code>
for each url in <a class="reference internal" href="#scrapy.spiders.Spider.start_urls" title="scrapy.spiders.Spider.start_urls"><code class="xref py py-attr docutils literal notranslate"><span class="pre">start_urls</span></code></a>.</p>
<p>If you want to change the Requests used to start scraping a domain, this is
the method to override. For example, if you need to start by logging in using
a POST request, you could do:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;myspider&#39;</span>

    <span class="k">def</span> <span class="nf">start_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">FormRequest</span><span class="p">(</span><span class="s2">&quot;http://www.example.com/login&quot;</span><span class="p">,</span>
                                   <span class="n">formdata</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;user&#39;</span><span class="p">:</span> <span class="s1">&#39;john&#39;</span><span class="p">,</span> <span class="s1">&#39;pass&#39;</span><span class="p">:</span> <span class="s1">&#39;secret&#39;</span><span class="p">},</span>
                                   <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">logged_in</span><span class="p">)]</span>

    <span class="k">def</span> <span class="nf">logged_in</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="c1"># here you would extract links to follow and return Requests for</span>
        <span class="c1"># each of them, with another callback</span>
        <span class="k">pass</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.spiders.Spider.parse">
<code class="descname">parse</code><span class="sig-paren">(</span><em>response</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiders.Spider.parse" title="Permalink to this definition">¶</a></dt>
<dd><p>This is the default callback used by Scrapy to process downloaded
responses, when their requests don’t specify a callback.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">parse</span></code> method is in charge of processing the response and returning
scraped data and/or more URLs to follow. Other Requests callbacks have
the same requirements as the <a class="reference internal" href="#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> class.</p>
<p>This method, as well as any other Request callback, must return an
iterable of <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> and/or
dicts or <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a> objects.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a>) – the response to parse</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.spiders.Spider.log">
<code class="descname">log</code><span class="sig-paren">(</span><em>message</em><span class="optional">[</span>, <em>level</em>, <em>component</em><span class="optional">]</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiders.Spider.log" title="Permalink to this definition">¶</a></dt>
<dd><p>Wrapper that sends a log message through the Spider’s <a class="reference internal" href="#scrapy.spiders.Spider.logger" title="scrapy.spiders.Spider.logger"><code class="xref py py-attr docutils literal notranslate"><span class="pre">logger</span></code></a>,
kept for backward compatibility. For more information see
<a class="reference internal" href="index.html#topics-logging-from-spiders"><span class="std std-ref">Logging from Spiders</span></a>.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.spiders.Spider.closed">
<code class="descname">closed</code><span class="sig-paren">(</span><em>reason</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiders.Spider.closed" title="Permalink to this definition">¶</a></dt>
<dd><p>Called when the spider closes. This method provides a shortcut to
signals.connect() for the <a class="reference internal" href="index.html#std:signal-spider_closed"><code class="xref std std-signal docutils literal notranslate"><span class="pre">spider_closed</span></code></a> signal.</p>
</dd></dl>

</dd></dl>

<p>Let’s see an example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;example.com&#39;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;example.com&#39;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s1">&#39;http://www.example.com/1.html&#39;</span><span class="p">,</span>
        <span class="s1">&#39;http://www.example.com/2.html&#39;</span><span class="p">,</span>
        <span class="s1">&#39;http://www.example.com/3.html&#39;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;A response from </span><span class="si">%s</span><span class="s1"> just arrived!&#39;</span><span class="p">,</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>
</pre></div>
</div>
<p>Return multiple Requests and items from a single callback:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;example.com&#39;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;example.com&#39;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s1">&#39;http://www.example.com/1.html&#39;</span><span class="p">,</span>
        <span class="s1">&#39;http://www.example.com/2.html&#39;</span><span class="p">,</span>
        <span class="s1">&#39;http://www.example.com/3.html&#39;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">h3</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//h3&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">():</span>
            <span class="k">yield</span> <span class="p">{</span><span class="s2">&quot;title&quot;</span><span class="p">:</span> <span class="n">h3</span><span class="p">}</span>

        <span class="k">for</span> <span class="n">href</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//a/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">():</span>
            <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">urljoin</span><span class="p">(</span><span class="n">href</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
</pre></div>
</div>
<p>Instead of <a class="reference internal" href="#scrapy.spiders.Spider.start_urls" title="scrapy.spiders.Spider.start_urls"><code class="xref py py-attr docutils literal notranslate"><span class="pre">start_urls</span></code></a> you can use <a class="reference internal" href="#scrapy.spiders.Spider.start_requests" title="scrapy.spiders.Spider.start_requests"><code class="xref py py-meth docutils literal notranslate"><span class="pre">start_requests()</span></code></a> directly;
to give data more structure you can use <a class="reference internal" href="index.html#topics-items"><span class="std std-ref">Items</span></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">from</span> <span class="nn">myproject.items</span> <span class="k">import</span> <span class="n">MyItem</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;example.com&#39;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;example.com&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">start_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="s1">&#39;http://www.example.com/1.html&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
        <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="s1">&#39;http://www.example.com/2.html&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
        <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="s1">&#39;http://www.example.com/3.html&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">h3</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//h3&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">():</span>
            <span class="k">yield</span> <span class="n">MyItem</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="n">h3</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">href</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//a/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">():</span>
            <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">urljoin</span><span class="p">(</span><span class="n">href</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="spider-arguments">
<span id="spiderargs"></span><h4>Spider arguments<a class="headerlink" href="#spider-arguments" title="Permalink to this headline">¶</a></h4>
<p>Spiders can receive arguments that modify their behaviour. Some common uses for
spider arguments are to define the start URLs or to restrict the crawl to
certain sections of the site, but they can be used to configure any
functionality of the spider.</p>
<p>Spider arguments are passed through the <a class="reference internal" href="index.html#std:command-crawl"><code class="xref std std-command docutils literal notranslate"><span class="pre">crawl</span></code></a> command using the
<code class="docutils literal notranslate"><span class="pre">-a</span></code> option. For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">crawl</span> <span class="n">myspider</span> <span class="o">-</span><span class="n">a</span> <span class="n">category</span><span class="o">=</span><span class="n">electronics</span>
</pre></div>
</div>
<p>Spiders can access arguments in their <cite>__init__</cite> methods:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;myspider&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MySpider</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;http://www.example.com/categories/</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">category</span><span class="p">]</span>
        <span class="c1"># ...</span>
</pre></div>
</div>
<p>The default <cite>__init__</cite> method will take any spider arguments
and copy them to the spider as attributes.
The above example can also be written as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;myspider&#39;</span>

    <span class="k">def</span> <span class="nf">start_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="s1">&#39;http://www.example.com/categories/</span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">category</span><span class="p">)</span>
</pre></div>
</div>
<p>Keep in mind that spider arguments are only strings.
The spider will not do any parsing on its own.
If you were to set the <code class="docutils literal notranslate"><span class="pre">start_urls</span></code> attribute from the command line,
you would have to parse it on your own into a list
using something like
<a class="reference external" href="https://docs.python.org/library/ast.html#ast.literal_eval">ast.literal_eval</a>
or <a class="reference external" href="https://docs.python.org/library/json.html#json.loads">json.loads</a>
and then set it as an attribute.
Otherwise, you would cause iteration over a <code class="docutils literal notranslate"><span class="pre">start_urls</span></code> string
(a very common python pitfall)
resulting in each character being seen as a separate url.</p>
<p>A valid use case is to set the http auth credentials
used by <a class="reference internal" href="index.html#scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware" title="scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">HttpAuthMiddleware</span></code></a>
or the user agent
used by <a class="reference internal" href="index.html#scrapy.downloadermiddlewares.useragent.UserAgentMiddleware" title="scrapy.downloadermiddlewares.useragent.UserAgentMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">UserAgentMiddleware</span></code></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">crawl</span> <span class="n">myspider</span> <span class="o">-</span><span class="n">a</span> <span class="n">http_user</span><span class="o">=</span><span class="n">myuser</span> <span class="o">-</span><span class="n">a</span> <span class="n">http_pass</span><span class="o">=</span><span class="n">mypassword</span> <span class="o">-</span><span class="n">a</span> <span class="n">user_agent</span><span class="o">=</span><span class="n">mybot</span>
</pre></div>
</div>
<p>Spider arguments can also be passed through the Scrapyd <code class="docutils literal notranslate"><span class="pre">schedule.json</span></code> API.
See <a class="reference external" href="https://scrapyd.readthedocs.io/en/latest/">Scrapyd documentation</a>.</p>
</div>
<div class="section" id="generic-spiders">
<span id="builtin-spiders"></span><h4>Generic Spiders<a class="headerlink" href="#generic-spiders" title="Permalink to this headline">¶</a></h4>
<p>Scrapy comes with some useful generic spiders that you can use to subclass
your spiders from. Their aim is to provide convenient functionality for a few
common scraping cases, like following all links on a site based on certain
rules, crawling from <a class="reference external" href="https://www.sitemaps.org/index.html">Sitemaps</a>, or parsing an XML/CSV feed.</p>
<p>For the examples used in the following spiders, we’ll assume you have a project
with a <code class="docutils literal notranslate"><span class="pre">TestItem</span></code> declared in a <code class="docutils literal notranslate"><span class="pre">myproject.items</span></code> module:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">TestItem</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
    <span class="nb">id</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">description</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
</pre></div>
</div>
<div class="section" id="crawlspider">
<h5>CrawlSpider<a class="headerlink" href="#crawlspider" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.spiders.CrawlSpider">
<em class="property">class </em><code class="descclassname">scrapy.spiders.</code><code class="descname">CrawlSpider</code><a class="headerlink" href="#scrapy.spiders.CrawlSpider" title="Permalink to this definition">¶</a></dt>
<dd><p>This is the most commonly used spider for crawling regular websites, as it
provides a convenient mechanism for following links by defining a set of rules.
It may not be the best suited for your particular web sites or project, but
it’s generic enough for several cases, so you can start from it and override it
as needed for more custom functionality, or just implement your own spider.</p>
<p>Apart from the attributes inherited from Spider (that you must
specify), this class supports a new attribute:</p>
<dl class="attribute">
<dt id="scrapy.spiders.CrawlSpider.rules">
<code class="descname">rules</code><a class="headerlink" href="#scrapy.spiders.CrawlSpider.rules" title="Permalink to this definition">¶</a></dt>
<dd><p>Which is a list of one (or more) <a class="reference internal" href="#scrapy.spiders.Rule" title="scrapy.spiders.Rule"><code class="xref py py-class docutils literal notranslate"><span class="pre">Rule</span></code></a> objects.  Each <a class="reference internal" href="#scrapy.spiders.Rule" title="scrapy.spiders.Rule"><code class="xref py py-class docutils literal notranslate"><span class="pre">Rule</span></code></a>
defines a certain behaviour for crawling the site. Rules objects are
described below. If multiple rules match the same link, the first one
will be used, according to the order they’re defined in this attribute.</p>
</dd></dl>

<p>This spider also exposes an overrideable method:</p>
<dl class="method">
<dt id="scrapy.spiders.CrawlSpider.parse_start_url">
<code class="descname">parse_start_url</code><span class="sig-paren">(</span><em>response</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiders.CrawlSpider.parse_start_url" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called for the start_urls responses. It allows to parse
the initial responses and must return either an
<a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a> object, a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a>
object, or an iterable containing any of them.</p>
</dd></dl>

</dd></dl>

<div class="section" id="crawling-rules">
<h6>Crawling rules<a class="headerlink" href="#crawling-rules" title="Permalink to this headline">¶</a></h6>
<dl class="class">
<dt id="scrapy.spiders.Rule">
<em class="property">class </em><code class="descclassname">scrapy.spiders.</code><code class="descname">Rule</code><span class="sig-paren">(</span><em>link_extractor</em>, <em>callback=None</em>, <em>cb_kwargs=None</em>, <em>follow=None</em>, <em>process_links=None</em>, <em>process_request=None</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiders.Rule" title="Permalink to this definition">¶</a></dt>
<dd><p><code class="docutils literal notranslate"><span class="pre">link_extractor</span></code> is a <a class="reference internal" href="index.html#topics-link-extractors"><span class="std std-ref">Link Extractor</span></a> object which
defines how links will be extracted from each crawled page. Each produced link will
be used to generate a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object, which will contain the
link’s text in its <code class="docutils literal notranslate"><span class="pre">meta</span></code> dictionary (under the <code class="docutils literal notranslate"><span class="pre">link_text</span></code> key).</p>
<p><code class="docutils literal notranslate"><span class="pre">callback</span></code> is a callable or a string (in which case a method from the spider
object with that name will be used) to be called for each link extracted with
the specified link extractor. This callback receives a <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a>
as its first argument and must return either a single instance or an iterable of
<a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a>, <code class="docutils literal notranslate"><span class="pre">dict</span></code> and/or <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> objects
(or any subclass of them). As mentioned above, the received <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a>
object will contain the text of the link that produced the <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a>
in its <code class="docutils literal notranslate"><span class="pre">meta</span></code> dictionary (under the <code class="docutils literal notranslate"><span class="pre">link_text</span></code> key)</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">When writing crawl spider rules, avoid using <code class="docutils literal notranslate"><span class="pre">parse</span></code> as
callback, since the <a class="reference internal" href="#scrapy.spiders.CrawlSpider" title="scrapy.spiders.CrawlSpider"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlSpider</span></code></a> uses the <code class="docutils literal notranslate"><span class="pre">parse</span></code> method
itself to implement its logic. So if you override the <code class="docutils literal notranslate"><span class="pre">parse</span></code> method,
the crawl spider will no longer work.</p>
</div>
<p><code class="docutils literal notranslate"><span class="pre">cb_kwargs</span></code> is a dict containing the keyword arguments to be passed to the
callback function.</p>
<p><code class="docutils literal notranslate"><span class="pre">follow</span></code> is a boolean which specifies if links should be followed from each
response extracted with this rule. If <code class="docutils literal notranslate"><span class="pre">callback</span></code> is None <code class="docutils literal notranslate"><span class="pre">follow</span></code> defaults
to <code class="docutils literal notranslate"><span class="pre">True</span></code>, otherwise it defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">process_links</span></code> is a callable, or a string (in which case a method from the
spider object with that name will be used) which will be called for each list
of links extracted from each response using the specified <code class="docutils literal notranslate"><span class="pre">link_extractor</span></code>.
This is mainly used for filtering purposes.</p>
<p><code class="docutils literal notranslate"><span class="pre">process_request</span></code> is a callable (or a string, in which case a method from
the spider object with that name will be used) which will be called for every
<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> extracted by this rule. This callable should
take said request as first argument and the <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a>
from which the request originated as second argument. It must return a
<code class="docutils literal notranslate"><span class="pre">Request</span></code> object or <code class="docutils literal notranslate"><span class="pre">None</span></code> (to filter out the request).</p>
</dd></dl>

</div>
<div class="section" id="crawlspider-example">
<h6>CrawlSpider example<a class="headerlink" href="#crawlspider-example" title="Permalink to this headline">¶</a></h6>
<p>Let’s now take a look at an example CrawlSpider with rules:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">from</span> <span class="nn">scrapy.spiders</span> <span class="k">import</span> <span class="n">CrawlSpider</span><span class="p">,</span> <span class="n">Rule</span>
<span class="kn">from</span> <span class="nn">scrapy.linkextractors</span> <span class="k">import</span> <span class="n">LinkExtractor</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">CrawlSpider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;example.com&#39;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;example.com&#39;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;http://www.example.com&#39;</span><span class="p">]</span>

    <span class="n">rules</span> <span class="o">=</span> <span class="p">(</span>
        <span class="c1"># Extract links matching &#39;category.php&#39; (but not matching &#39;subsection.php&#39;)</span>
        <span class="c1"># and follow links from them (since no callback means follow=True by default).</span>
        <span class="n">Rule</span><span class="p">(</span><span class="n">LinkExtractor</span><span class="p">(</span><span class="n">allow</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;category\.php&#39;</span><span class="p">,</span> <span class="p">),</span> <span class="n">deny</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;subsection\.php&#39;</span><span class="p">,</span> <span class="p">))),</span>

        <span class="c1"># Extract links matching &#39;item.php&#39; and parse them with the spider&#39;s method parse_item</span>
        <span class="n">Rule</span><span class="p">(</span><span class="n">LinkExtractor</span><span class="p">(</span><span class="n">allow</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;item\.php&#39;</span><span class="p">,</span> <span class="p">)),</span> <span class="n">callback</span><span class="o">=</span><span class="s1">&#39;parse_item&#39;</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">parse_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Hi, this is an item page! </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>
        <span class="n">item</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">()</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//td[@id=&quot;item_id&quot;]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">re</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;ID: (\d+)&#39;</span><span class="p">)</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//td[@id=&quot;item_name&quot;]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;description&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//td[@id=&quot;item_description&quot;]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;link_text&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s1">&#39;link_text&#39;</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
<p>This spider would start crawling example.com’s home page, collecting category
links, and item links, parsing the latter with the <code class="docutils literal notranslate"><span class="pre">parse_item</span></code> method. For
each item response, some data will be extracted from the HTML using XPath, and
an <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a> will be filled with it.</p>
</div>
</div>
<div class="section" id="xmlfeedspider">
<h5>XMLFeedSpider<a class="headerlink" href="#xmlfeedspider" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.spiders.XMLFeedSpider">
<em class="property">class </em><code class="descclassname">scrapy.spiders.</code><code class="descname">XMLFeedSpider</code><a class="headerlink" href="#scrapy.spiders.XMLFeedSpider" title="Permalink to this definition">¶</a></dt>
<dd><p>XMLFeedSpider is designed for parsing XML feeds by iterating through them by a
certain node name.  The iterator can be chosen from: <code class="docutils literal notranslate"><span class="pre">iternodes</span></code>, <code class="docutils literal notranslate"><span class="pre">xml</span></code>,
and <code class="docutils literal notranslate"><span class="pre">html</span></code>.  It’s recommended to use the <code class="docutils literal notranslate"><span class="pre">iternodes</span></code> iterator for
performance reasons, since the <code class="docutils literal notranslate"><span class="pre">xml</span></code> and <code class="docutils literal notranslate"><span class="pre">html</span></code> iterators generate the
whole DOM at once in order to parse it.  However, using <code class="docutils literal notranslate"><span class="pre">html</span></code> as the
iterator may be useful when parsing XML with bad markup.</p>
<p>To set the iterator and the tag name, you must define the following class
attributes:</p>
<dl class="attribute">
<dt id="scrapy.spiders.XMLFeedSpider.iterator">
<code class="descname">iterator</code><a class="headerlink" href="#scrapy.spiders.XMLFeedSpider.iterator" title="Permalink to this definition">¶</a></dt>
<dd><p>A string which defines the iterator to use. It can be either:</p>
<blockquote>
<div><ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">'iternodes'</span></code> - a fast iterator based on regular expressions</li>
<li><code class="docutils literal notranslate"><span class="pre">'html'</span></code> - an iterator which uses <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a>.
Keep in mind this uses DOM parsing and must load all DOM in memory
which could be a problem for big feeds</li>
<li><code class="docutils literal notranslate"><span class="pre">'xml'</span></code> - an iterator which uses <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a>.
Keep in mind this uses DOM parsing and must load all DOM in memory
which could be a problem for big feeds</li>
</ul>
</div></blockquote>
<p>It defaults to: <code class="docutils literal notranslate"><span class="pre">'iternodes'</span></code>.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.spiders.XMLFeedSpider.itertag">
<code class="descname">itertag</code><a class="headerlink" href="#scrapy.spiders.XMLFeedSpider.itertag" title="Permalink to this definition">¶</a></dt>
<dd><p>A string with the name of the node (or element) to iterate in. Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">itertag</span> <span class="o">=</span> <span class="s1">&#39;product&#39;</span>
</pre></div>
</div>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.spiders.XMLFeedSpider.namespaces">
<code class="descname">namespaces</code><a class="headerlink" href="#scrapy.spiders.XMLFeedSpider.namespaces" title="Permalink to this definition">¶</a></dt>
<dd><p>A list of <code class="docutils literal notranslate"><span class="pre">(prefix,</span> <span class="pre">uri)</span></code> tuples which define the namespaces
available in that document that will be processed with this spider. The
<code class="docutils literal notranslate"><span class="pre">prefix</span></code> and <code class="docutils literal notranslate"><span class="pre">uri</span></code> will be used to automatically register
namespaces using the
<a class="reference internal" href="index.html#scrapy.selector.Selector.register_namespace" title="scrapy.selector.Selector.register_namespace"><code class="xref py py-meth docutils literal notranslate"><span class="pre">register_namespace()</span></code></a> method.</p>
<p>You can then specify nodes with namespaces in the <a class="reference internal" href="#scrapy.spiders.XMLFeedSpider.itertag" title="scrapy.spiders.XMLFeedSpider.itertag"><code class="xref py py-attr docutils literal notranslate"><span class="pre">itertag</span></code></a>
attribute.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">YourSpider</span><span class="p">(</span><span class="n">XMLFeedSpider</span><span class="p">):</span>

    <span class="n">namespaces</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;n&#39;</span><span class="p">,</span> <span class="s1">&#39;http://www.sitemaps.org/schemas/sitemap/0.9&#39;</span><span class="p">)]</span>
    <span class="n">itertag</span> <span class="o">=</span> <span class="s1">&#39;n:url&#39;</span>
    <span class="c1"># ...</span>
</pre></div>
</div>
</dd></dl>

<p>Apart from these new attributes, this spider has the following overrideable
methods too:</p>
<dl class="method">
<dt id="scrapy.spiders.XMLFeedSpider.adapt_response">
<code class="descname">adapt_response</code><span class="sig-paren">(</span><em>response</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiders.XMLFeedSpider.adapt_response" title="Permalink to this definition">¶</a></dt>
<dd><p>A method that receives the response as soon as it arrives from the spider
middleware, before the spider starts parsing it. It can be used to modify
the response body before parsing it. This method receives a response and
also returns a response (it could be the same or another one).</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.spiders.XMLFeedSpider.parse_node">
<code class="descname">parse_node</code><span class="sig-paren">(</span><em>response</em>, <em>selector</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiders.XMLFeedSpider.parse_node" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called for the nodes matching the provided tag name
(<code class="docutils literal notranslate"><span class="pre">itertag</span></code>).  Receives the response and an
<a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a> for each node.  Overriding this
method is mandatory. Otherwise, you spider won’t work.  This method
must return either a <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a> object, a
<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object, or an iterable containing any of
them.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.spiders.XMLFeedSpider.process_results">
<code class="descname">process_results</code><span class="sig-paren">(</span><em>response</em>, <em>results</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiders.XMLFeedSpider.process_results" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called for each result (item or request) returned by the
spider, and it’s intended to perform any last time processing required
before returning the results to the framework core, for example setting the
item IDs. It receives a list of results and the response which originated
those results. It must return a list of results (Items or Requests).</p>
</dd></dl>

</dd></dl>

<div class="section" id="xmlfeedspider-example">
<h6>XMLFeedSpider example<a class="headerlink" href="#xmlfeedspider-example" title="Permalink to this headline">¶</a></h6>
<p>These spiders are pretty easy to use, let’s have a look at one example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.spiders</span> <span class="k">import</span> <span class="n">XMLFeedSpider</span>
<span class="kn">from</span> <span class="nn">myproject.items</span> <span class="k">import</span> <span class="n">TestItem</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">XMLFeedSpider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;example.com&#39;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;example.com&#39;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;http://www.example.com/feed.xml&#39;</span><span class="p">]</span>
    <span class="n">iterator</span> <span class="o">=</span> <span class="s1">&#39;iternodes&#39;</span>  <span class="c1"># This is actually unnecessary, since it&#39;s the default value</span>
    <span class="n">itertag</span> <span class="o">=</span> <span class="s1">&#39;item&#39;</span>

    <span class="k">def</span> <span class="nf">parse_node</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">node</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Hi, this is a &lt;</span><span class="si">%s</span><span class="s1">&gt; node!: </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">itertag</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">getall</span><span class="p">()))</span>

        <span class="n">item</span> <span class="o">=</span> <span class="n">TestItem</span><span class="p">()</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;@id&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;description&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;description&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
<p>Basically what we did up there was to create a spider that downloads a feed from
the given <code class="docutils literal notranslate"><span class="pre">start_urls</span></code>, and then iterates through each of its <code class="docutils literal notranslate"><span class="pre">item</span></code> tags,
prints them out, and stores some random data in an <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a>.</p>
</div>
</div>
<div class="section" id="csvfeedspider">
<h5>CSVFeedSpider<a class="headerlink" href="#csvfeedspider" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.spiders.CSVFeedSpider">
<em class="property">class </em><code class="descclassname">scrapy.spiders.</code><code class="descname">CSVFeedSpider</code><a class="headerlink" href="#scrapy.spiders.CSVFeedSpider" title="Permalink to this definition">¶</a></dt>
<dd><p>This spider is very similar to the XMLFeedSpider, except that it iterates
over rows, instead of nodes. The method that gets called in each iteration
is <a class="reference internal" href="#scrapy.spiders.CSVFeedSpider.parse_row" title="scrapy.spiders.CSVFeedSpider.parse_row"><code class="xref py py-meth docutils literal notranslate"><span class="pre">parse_row()</span></code></a>.</p>
<dl class="attribute">
<dt id="scrapy.spiders.CSVFeedSpider.delimiter">
<code class="descname">delimiter</code><a class="headerlink" href="#scrapy.spiders.CSVFeedSpider.delimiter" title="Permalink to this definition">¶</a></dt>
<dd><p>A string with the separator character for each field in the CSV file
Defaults to <code class="docutils literal notranslate"><span class="pre">','</span></code> (comma).</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.spiders.CSVFeedSpider.quotechar">
<code class="descname">quotechar</code><a class="headerlink" href="#scrapy.spiders.CSVFeedSpider.quotechar" title="Permalink to this definition">¶</a></dt>
<dd><p>A string with the enclosure character for each field in the CSV file
Defaults to <code class="docutils literal notranslate"><span class="pre">'&quot;'</span></code> (quotation mark).</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.spiders.CSVFeedSpider.headers">
<code class="descname">headers</code><a class="headerlink" href="#scrapy.spiders.CSVFeedSpider.headers" title="Permalink to this definition">¶</a></dt>
<dd><p>A list of the column names in the CSV file.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.spiders.CSVFeedSpider.parse_row">
<code class="descname">parse_row</code><span class="sig-paren">(</span><em>response</em>, <em>row</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiders.CSVFeedSpider.parse_row" title="Permalink to this definition">¶</a></dt>
<dd><p>Receives a response and a dict (representing each row) with a key for each
provided (or detected) header of the CSV file.  This spider also gives the
opportunity to override <code class="docutils literal notranslate"><span class="pre">adapt_response</span></code> and <code class="docutils literal notranslate"><span class="pre">process_results</span></code> methods
for pre- and post-processing purposes.</p>
</dd></dl>

</dd></dl>

<div class="section" id="csvfeedspider-example">
<h6>CSVFeedSpider example<a class="headerlink" href="#csvfeedspider-example" title="Permalink to this headline">¶</a></h6>
<p>Let’s see an example similar to the previous one, but using a
<a class="reference internal" href="#scrapy.spiders.CSVFeedSpider" title="scrapy.spiders.CSVFeedSpider"><code class="xref py py-class docutils literal notranslate"><span class="pre">CSVFeedSpider</span></code></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.spiders</span> <span class="k">import</span> <span class="n">CSVFeedSpider</span>
<span class="kn">from</span> <span class="nn">myproject.items</span> <span class="k">import</span> <span class="n">TestItem</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">CSVFeedSpider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;example.com&#39;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;example.com&#39;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;http://www.example.com/feed.csv&#39;</span><span class="p">]</span>
    <span class="n">delimiter</span> <span class="o">=</span> <span class="s1">&#39;;&#39;</span>
    <span class="n">quotechar</span> <span class="o">=</span> <span class="s2">&quot;&#39;&quot;</span>
    <span class="n">headers</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">,</span> <span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;description&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse_row</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">row</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Hi, this is a row!: </span><span class="si">%r</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">row</span><span class="p">)</span>

        <span class="n">item</span> <span class="o">=</span> <span class="n">TestItem</span><span class="p">()</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">]</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;description&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;description&#39;</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="sitemapspider">
<h5>SitemapSpider<a class="headerlink" href="#sitemapspider" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.spiders.SitemapSpider">
<em class="property">class </em><code class="descclassname">scrapy.spiders.</code><code class="descname">SitemapSpider</code><a class="headerlink" href="#scrapy.spiders.SitemapSpider" title="Permalink to this definition">¶</a></dt>
<dd><p>SitemapSpider allows you to crawl a site by discovering the URLs using
<a class="reference external" href="https://www.sitemaps.org/index.html">Sitemaps</a>.</p>
<p>It supports nested sitemaps and discovering sitemap urls from
<a class="reference external" href="http://www.robotstxt.org/">robots.txt</a>.</p>
<dl class="attribute">
<dt id="scrapy.spiders.SitemapSpider.sitemap_urls">
<code class="descname">sitemap_urls</code><a class="headerlink" href="#scrapy.spiders.SitemapSpider.sitemap_urls" title="Permalink to this definition">¶</a></dt>
<dd><p>A list of urls pointing to the sitemaps whose urls you want to crawl.</p>
<p>You can also point to a <a class="reference external" href="http://www.robotstxt.org/">robots.txt</a> and it will be parsed to extract
sitemap urls from it.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.spiders.SitemapSpider.sitemap_rules">
<code class="descname">sitemap_rules</code><a class="headerlink" href="#scrapy.spiders.SitemapSpider.sitemap_rules" title="Permalink to this definition">¶</a></dt>
<dd><p>A list of tuples <code class="docutils literal notranslate"><span class="pre">(regex,</span> <span class="pre">callback)</span></code> where:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">regex</span></code> is a regular expression to match urls extracted from sitemaps.
<code class="docutils literal notranslate"><span class="pre">regex</span></code> can be either a str or a compiled regex object.</li>
<li>callback is the callback to use for processing the urls that match
the regular expression. <code class="docutils literal notranslate"><span class="pre">callback</span></code> can be a string (indicating the
name of a spider method) or a callable.</li>
</ul>
<p>For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sitemap_rules</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;/product/&#39;</span><span class="p">,</span> <span class="s1">&#39;parse_product&#39;</span><span class="p">)]</span>
</pre></div>
</div>
<p>Rules are applied in order, and only the first one that matches will be
used.</p>
<p>If you omit this attribute, all urls found in sitemaps will be
processed with the <code class="docutils literal notranslate"><span class="pre">parse</span></code> callback.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.spiders.SitemapSpider.sitemap_follow">
<code class="descname">sitemap_follow</code><a class="headerlink" href="#scrapy.spiders.SitemapSpider.sitemap_follow" title="Permalink to this definition">¶</a></dt>
<dd><p>A list of regexes of sitemap that should be followed. This is only
for sites that use <a class="reference external" href="https://www.sitemaps.org/protocol.html#index">Sitemap index files</a> that point to other sitemap
files.</p>
<p>By default, all sitemaps are followed.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.spiders.SitemapSpider.sitemap_alternate_links">
<code class="descname">sitemap_alternate_links</code><a class="headerlink" href="#scrapy.spiders.SitemapSpider.sitemap_alternate_links" title="Permalink to this definition">¶</a></dt>
<dd><p>Specifies if alternate links for one <code class="docutils literal notranslate"><span class="pre">url</span></code> should be followed. These
are links for the same website in another language passed within
the same <code class="docutils literal notranslate"><span class="pre">url</span></code> block.</p>
<p>For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">url</span><span class="o">&gt;</span>
    <span class="o">&lt;</span><span class="n">loc</span><span class="o">&gt;</span><span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">example</span><span class="o">.</span><span class="n">com</span><span class="o">/&lt;/</span><span class="n">loc</span><span class="o">&gt;</span>
    <span class="o">&lt;</span><span class="n">xhtml</span><span class="p">:</span><span class="n">link</span> <span class="n">rel</span><span class="o">=</span><span class="s2">&quot;alternate&quot;</span> <span class="n">hreflang</span><span class="o">=</span><span class="s2">&quot;de&quot;</span> <span class="n">href</span><span class="o">=</span><span class="s2">&quot;http://example.com/de&quot;</span><span class="o">/&gt;</span>
<span class="o">&lt;/</span><span class="n">url</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>With <code class="docutils literal notranslate"><span class="pre">sitemap_alternate_links</span></code> set, this would retrieve both URLs. With
<code class="docutils literal notranslate"><span class="pre">sitemap_alternate_links</span></code> disabled, only <code class="docutils literal notranslate"><span class="pre">http://example.com/</span></code> would be
retrieved.</p>
<p>Default is <code class="docutils literal notranslate"><span class="pre">sitemap_alternate_links</span></code> disabled.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.spiders.SitemapSpider.sitemap_filter">
<code class="descname">sitemap_filter</code><span class="sig-paren">(</span><em>entries</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiders.SitemapSpider.sitemap_filter" title="Permalink to this definition">¶</a></dt>
<dd><p>This is a filter funtion that could be overridden to select sitemap entries
based on their attributes.</p>
<p>For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">url</span><span class="o">&gt;</span>
    <span class="o">&lt;</span><span class="n">loc</span><span class="o">&gt;</span><span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">example</span><span class="o">.</span><span class="n">com</span><span class="o">/&lt;/</span><span class="n">loc</span><span class="o">&gt;</span>
    <span class="o">&lt;</span><span class="n">lastmod</span><span class="o">&gt;</span><span class="mi">2005</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">01</span><span class="o">&lt;/</span><span class="n">lastmod</span><span class="o">&gt;</span>
<span class="o">&lt;/</span><span class="n">url</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>We can define a <code class="docutils literal notranslate"><span class="pre">sitemap_filter</span></code> function to filter <code class="docutils literal notranslate"><span class="pre">entries</span></code> by date:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">datetime</span> <span class="k">import</span> <span class="n">datetime</span>
<span class="kn">from</span> <span class="nn">scrapy.spiders</span> <span class="k">import</span> <span class="n">SitemapSpider</span>

<span class="k">class</span> <span class="nc">FilteredSitemapSpider</span><span class="p">(</span><span class="n">SitemapSpider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;filtered_sitemap_spider&#39;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;example.com&#39;</span><span class="p">]</span>
    <span class="n">sitemap_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;http://example.com/sitemap.xml&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">sitemap_filter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">entries</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">entry</span> <span class="ow">in</span> <span class="n">entries</span><span class="p">:</span>
            <span class="n">date_time</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">strptime</span><span class="p">(</span><span class="n">entry</span><span class="p">[</span><span class="s1">&#39;lastmod&#39;</span><span class="p">],</span> <span class="s1">&#39;%Y-%m-</span><span class="si">%d</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">date_time</span><span class="o">.</span><span class="n">year</span> <span class="o">&gt;=</span> <span class="mi">2005</span><span class="p">:</span>
                <span class="k">yield</span> <span class="n">entry</span>
</pre></div>
</div>
<p>This would retrieve only <code class="docutils literal notranslate"><span class="pre">entries</span></code> modified on 2005 and the following
years.</p>
<p>Entries are dict objects extracted from the sitemap document.
Usually, the key is the tag name and the value is the text inside it.</p>
<p>It’s important to notice that:</p>
<ul class="simple">
<li>as the loc attribute is required, entries without this tag are discarded</li>
<li>alternate links are stored in a list with the key <code class="docutils literal notranslate"><span class="pre">alternate</span></code>
(see <code class="docutils literal notranslate"><span class="pre">sitemap_alternate_links</span></code>)</li>
<li>namespaces are removed, so lxml tags named as <code class="docutils literal notranslate"><span class="pre">{namespace}tagname</span></code> become only <code class="docutils literal notranslate"><span class="pre">tagname</span></code></li>
</ul>
<p>If you omit this method, all entries found in sitemaps will be
processed, observing other attributes and their settings.</p>
</dd></dl>

</dd></dl>

<div class="section" id="sitemapspider-examples">
<h6>SitemapSpider examples<a class="headerlink" href="#sitemapspider-examples" title="Permalink to this headline">¶</a></h6>
<p>Simplest example: process all urls discovered through sitemaps using the
<code class="docutils literal notranslate"><span class="pre">parse</span></code> callback:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.spiders</span> <span class="k">import</span> <span class="n">SitemapSpider</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">SitemapSpider</span><span class="p">):</span>
    <span class="n">sitemap_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;http://www.example.com/sitemap.xml&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">pass</span> <span class="c1"># ... scrape item here ...</span>
</pre></div>
</div>
<p>Process some urls with certain callback and other urls with a different
callback:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.spiders</span> <span class="k">import</span> <span class="n">SitemapSpider</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">SitemapSpider</span><span class="p">):</span>
    <span class="n">sitemap_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;http://www.example.com/sitemap.xml&#39;</span><span class="p">]</span>
    <span class="n">sitemap_rules</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s1">&#39;/product/&#39;</span><span class="p">,</span> <span class="s1">&#39;parse_product&#39;</span><span class="p">),</span>
        <span class="p">(</span><span class="s1">&#39;/category/&#39;</span><span class="p">,</span> <span class="s1">&#39;parse_category&#39;</span><span class="p">),</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse_product</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">pass</span> <span class="c1"># ... scrape product ...</span>

    <span class="k">def</span> <span class="nf">parse_category</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">pass</span> <span class="c1"># ... scrape category ...</span>
</pre></div>
</div>
<p>Follow sitemaps defined in the <a class="reference external" href="http://www.robotstxt.org/">robots.txt</a> file and only follow sitemaps
whose url contains <code class="docutils literal notranslate"><span class="pre">/sitemap_shop</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.spiders</span> <span class="k">import</span> <span class="n">SitemapSpider</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">SitemapSpider</span><span class="p">):</span>
    <span class="n">sitemap_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;http://www.example.com/robots.txt&#39;</span><span class="p">]</span>
    <span class="n">sitemap_rules</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s1">&#39;/shop/&#39;</span><span class="p">,</span> <span class="s1">&#39;parse_shop&#39;</span><span class="p">),</span>
    <span class="p">]</span>
    <span class="n">sitemap_follow</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;/sitemap_shops&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse_shop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">pass</span> <span class="c1"># ... scrape shop here ...</span>
</pre></div>
</div>
<p>Combine SitemapSpider with other sources of urls:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.spiders</span> <span class="k">import</span> <span class="n">SitemapSpider</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">SitemapSpider</span><span class="p">):</span>
    <span class="n">sitemap_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;http://www.example.com/robots.txt&#39;</span><span class="p">]</span>
    <span class="n">sitemap_rules</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s1">&#39;/shop/&#39;</span><span class="p">,</span> <span class="s1">&#39;parse_shop&#39;</span><span class="p">),</span>
    <span class="p">]</span>

    <span class="n">other_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;http://www.example.com/about&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">start_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">requests</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">super</span><span class="p">(</span><span class="n">MySpider</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">start_requests</span><span class="p">())</span>
        <span class="n">requests</span> <span class="o">+=</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse_other</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">other_urls</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">requests</span>

    <span class="k">def</span> <span class="nf">parse_shop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">pass</span> <span class="c1"># ... scrape shop here ...</span>

    <span class="k">def</span> <span class="nf">parse_other</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">pass</span> <span class="c1"># ... scrape other here ...</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<span id="document-topics/selectors"></span><div class="section" id="selectors">
<span id="topics-selectors"></span><h3>Selectors<a class="headerlink" href="#selectors" title="Permalink to this headline">¶</a></h3>
<p>When you’re scraping web pages, the most common task you need to perform is
to extract data from the HTML source. There are several libraries available to
achieve this, such as:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference external" href="https://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a> is a very popular web scraping library among Python
programmers which constructs a Python object based on the structure of the
HTML code and also deals with bad markup reasonably well, but it has one
drawback: it’s slow.</li>
<li><a class="reference external" href="http://lxml.de/">lxml</a> is an XML parsing library (which also parses HTML) with a pythonic
API based on <a class="reference external" href="https://docs.python.org/2/library/xml.etree.elementtree.html">ElementTree</a>. (lxml is not part of the Python standard
library.)</li>
</ul>
</div></blockquote>
<p>Scrapy comes with its own mechanism for extracting data. They’re called
selectors because they “select” certain parts of the HTML document specified
either by <a class="reference external" href="https://www.w3.org/TR/xpath">XPath</a> or <a class="reference external" href="https://www.w3.org/TR/selectors">CSS</a> expressions.</p>
<p><a class="reference external" href="https://www.w3.org/TR/xpath">XPath</a> is a language for selecting nodes in XML documents, which can also be
used with HTML. <a class="reference external" href="https://www.w3.org/TR/selectors">CSS</a> is a language for applying styles to HTML documents. It
defines selectors to associate those styles with specific HTML elements.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>Scrapy Selectors is a thin wrapper around <a class="reference external" href="https://parsel.readthedocs.io/">parsel</a> library; the purpose of
this wrapper is to provide better integration with Scrapy Response objects.</p>
<p class="last"><a class="reference external" href="https://parsel.readthedocs.io/">parsel</a> is a stand-alone web scraping library which can be used without
Scrapy. It uses <a class="reference external" href="http://lxml.de/">lxml</a> library under the hood, and implements an
easy API on top of lxml API. It means Scrapy selectors are very similar
in speed and parsing accuracy to lxml.</p>
</div>
<div class="section" id="using-selectors">
<h4>Using selectors<a class="headerlink" href="#using-selectors" title="Permalink to this headline">¶</a></h4>
<div class="section" id="constructing-selectors">
<h5>Constructing selectors<a class="headerlink" href="#constructing-selectors" title="Permalink to this headline">¶</a></h5>
<p>Response objects expose a <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a> instance
on <code class="docutils literal notranslate"><span class="pre">.selector</span></code> attribute:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">selector</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//span/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;good&#39;</span>
</pre></div>
</div>
<p>Querying responses using XPath and CSS is so common that responses include two
more shortcuts: <code class="docutils literal notranslate"><span class="pre">response.xpath()</span></code> and <code class="docutils literal notranslate"><span class="pre">response.css()</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//span/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;good&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;span::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;good&#39;</span>
</pre></div>
</div>
<p>Scrapy selectors are instances of <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a> class
constructed by passing either <a class="reference internal" href="index.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">TextResponse</span></code></a> object or
markup as an unicode string (in <code class="docutils literal notranslate"><span class="pre">text</span></code> argument).
Usually there is no need to construct Scrapy selectors manually:
<code class="docutils literal notranslate"><span class="pre">response</span></code> object is available in Spider callbacks, so in most cases
it is more convenient to use <code class="docutils literal notranslate"><span class="pre">response.css()</span></code> and <code class="docutils literal notranslate"><span class="pre">response.xpath()</span></code>
shortcuts. By using <code class="docutils literal notranslate"><span class="pre">response.selector</span></code> or one of these shortcuts
you can also ensure the response body is parsed only once.</p>
<p>But if required, it is possible to use <code class="docutils literal notranslate"><span class="pre">Selector</span></code> directly.
Constructing from text:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.selector</span> <span class="kn">import</span> <span class="n">Selector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">body</span> <span class="o">=</span> <span class="s1">&#39;&lt;html&gt;&lt;body&gt;&lt;span&gt;good&lt;/span&gt;&lt;/body&gt;&lt;/html&gt;&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Selector</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">body</span><span class="p">)</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//span/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;good&#39;</span>
</pre></div>
</div>
<p>Constructing from response - <a class="reference internal" href="index.html#scrapy.http.HtmlResponse" title="scrapy.http.HtmlResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">HtmlResponse</span></code></a> is one of
<a class="reference internal" href="index.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">TextResponse</span></code></a> subclasses:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.selector</span> <span class="kn">import</span> <span class="n">Selector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.http</span> <span class="kn">import</span> <span class="n">HtmlResponse</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span> <span class="o">=</span> <span class="n">HtmlResponse</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s1">&#39;http://example.com&#39;</span><span class="p">,</span> <span class="n">body</span><span class="o">=</span><span class="n">body</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Selector</span><span class="p">(</span><span class="n">response</span><span class="o">=</span><span class="n">response</span><span class="p">)</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//span/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;good&#39;</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">Selector</span></code> automatically chooses the best parsing rules
(XML vs HTML) based on input type.</p>
</div>
<div class="section" id="id1">
<h5>Using selectors<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h5>
<p>To explain how to use the selectors we’ll use the <code class="docutils literal notranslate"><span class="pre">Scrapy</span> <span class="pre">shell</span></code> (which
provides interactive testing) and an example page located in the Scrapy
documentation server:</p>
<blockquote>
<div><a class="reference external" href="https://docs.scrapy.org/en/latest/_static/selectors-sample1.html">https://docs.scrapy.org/en/latest/_static/selectors-sample1.html</a></div></blockquote>
<p id="topics-selectors-htmlcode">For the sake of completeness, here’s its full HTML code:</p>
<div class="highlight-html notranslate"><div class="highlight"><pre><span></span><span class="p">&lt;</span><span class="nt">html</span><span class="p">&gt;</span>
 <span class="p">&lt;</span><span class="nt">head</span><span class="p">&gt;</span>
  <span class="p">&lt;</span><span class="nt">base</span> <span class="na">href</span><span class="o">=</span><span class="s">&#39;http://example.com/&#39;</span> <span class="p">/&gt;</span>
  <span class="p">&lt;</span><span class="nt">title</span><span class="p">&gt;</span>Example website<span class="p">&lt;/</span><span class="nt">title</span><span class="p">&gt;</span>
 <span class="p">&lt;/</span><span class="nt">head</span><span class="p">&gt;</span>
 <span class="p">&lt;</span><span class="nt">body</span><span class="p">&gt;</span>
  <span class="p">&lt;</span><span class="nt">div</span> <span class="na">id</span><span class="o">=</span><span class="s">&#39;images&#39;</span><span class="p">&gt;</span>
   <span class="p">&lt;</span><span class="nt">a</span> <span class="na">href</span><span class="o">=</span><span class="s">&#39;image1.html&#39;</span><span class="p">&gt;</span>Name: My image 1 <span class="p">&lt;</span><span class="nt">br</span> <span class="p">/&gt;&lt;</span><span class="nt">img</span> <span class="na">src</span><span class="o">=</span><span class="s">&#39;image1_thumb.jpg&#39;</span> <span class="p">/&gt;&lt;/</span><span class="nt">a</span><span class="p">&gt;</span>
   <span class="p">&lt;</span><span class="nt">a</span> <span class="na">href</span><span class="o">=</span><span class="s">&#39;image2.html&#39;</span><span class="p">&gt;</span>Name: My image 2 <span class="p">&lt;</span><span class="nt">br</span> <span class="p">/&gt;&lt;</span><span class="nt">img</span> <span class="na">src</span><span class="o">=</span><span class="s">&#39;image2_thumb.jpg&#39;</span> <span class="p">/&gt;&lt;/</span><span class="nt">a</span><span class="p">&gt;</span>
   <span class="p">&lt;</span><span class="nt">a</span> <span class="na">href</span><span class="o">=</span><span class="s">&#39;image3.html&#39;</span><span class="p">&gt;</span>Name: My image 3 <span class="p">&lt;</span><span class="nt">br</span> <span class="p">/&gt;&lt;</span><span class="nt">img</span> <span class="na">src</span><span class="o">=</span><span class="s">&#39;image3_thumb.jpg&#39;</span> <span class="p">/&gt;&lt;/</span><span class="nt">a</span><span class="p">&gt;</span>
   <span class="p">&lt;</span><span class="nt">a</span> <span class="na">href</span><span class="o">=</span><span class="s">&#39;image4.html&#39;</span><span class="p">&gt;</span>Name: My image 4 <span class="p">&lt;</span><span class="nt">br</span> <span class="p">/&gt;&lt;</span><span class="nt">img</span> <span class="na">src</span><span class="o">=</span><span class="s">&#39;image4_thumb.jpg&#39;</span> <span class="p">/&gt;&lt;/</span><span class="nt">a</span><span class="p">&gt;</span>
   <span class="p">&lt;</span><span class="nt">a</span> <span class="na">href</span><span class="o">=</span><span class="s">&#39;image5.html&#39;</span><span class="p">&gt;</span>Name: My image 5 <span class="p">&lt;</span><span class="nt">br</span> <span class="p">/&gt;&lt;</span><span class="nt">img</span> <span class="na">src</span><span class="o">=</span><span class="s">&#39;image5_thumb.jpg&#39;</span> <span class="p">/&gt;&lt;/</span><span class="nt">a</span><span class="p">&gt;</span>
  <span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
 <span class="p">&lt;/</span><span class="nt">body</span><span class="p">&gt;</span>
<span class="p">&lt;/</span><span class="nt">html</span><span class="p">&gt;</span>
</pre></div>
</div>
<p>First, let’s open the shell:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>scrapy shell https://docs.scrapy.org/en/latest/_static/selectors-sample1.html
</pre></div>
</div>
<p>Then, after the shell loads, you’ll have the response available as <code class="docutils literal notranslate"><span class="pre">response</span></code>
shell variable, and its attached selector in <code class="docutils literal notranslate"><span class="pre">response.selector</span></code> attribute.</p>
<p>Since we’re dealing with HTML, the selector will automatically use an HTML parser.</p>
<p>So, by looking at the <a class="reference internal" href="#topics-selectors-htmlcode"><span class="std std-ref">HTML code</span></a> of that
page, let’s construct an XPath for selecting the text inside the title tag:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//title/text()&#39;</span><span class="p">)</span>
<span class="go">[&lt;Selector xpath=&#39;//title/text()&#39; data=&#39;Example website&#39;&gt;]</span>
</pre></div>
</div>
<p>To actually extract the textual data, you must call the selector <code class="docutils literal notranslate"><span class="pre">.get()</span></code>
or <code class="docutils literal notranslate"><span class="pre">.getall()</span></code> methods, as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//title/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;Example website&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//title/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;Example website&#39;</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">.get()</span></code> always returns a single result; if there are several matches,
content of a first match is returned; if there are no matches, None
is returned. <code class="docutils literal notranslate"><span class="pre">.getall()</span></code> returns a list with all results.</p>
<p>Notice that CSS selectors can select text or attribute nodes using CSS3
pseudo-elements:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;title::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;Example website&#39;</span>
</pre></div>
</div>
<p>As you can see, <code class="docutils literal notranslate"><span class="pre">.xpath()</span></code> and <code class="docutils literal notranslate"><span class="pre">.css()</span></code> methods return a
<a class="reference internal" href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal notranslate"><span class="pre">SelectorList</span></code></a> instance, which is a list of new
selectors. This API can be used for quickly selecting nested data:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;img&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;@src&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;image1_thumb.jpg&#39;,</span>
<span class="go"> &#39;image2_thumb.jpg&#39;,</span>
<span class="go"> &#39;image3_thumb.jpg&#39;,</span>
<span class="go"> &#39;image4_thumb.jpg&#39;,</span>
<span class="go"> &#39;image5_thumb.jpg&#39;]</span>
</pre></div>
</div>
<p>If you want to extract only the first matched element, you can call the
selector <code class="docutils literal notranslate"><span class="pre">.get()</span></code> (or its alias <code class="docutils literal notranslate"><span class="pre">.extract_first()</span></code> commonly used in
previous Scrapy versions):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//div[@id=&quot;images&quot;]/a/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;Name: My image 1 &#39;</span>
</pre></div>
</div>
<p>It returns <code class="docutils literal notranslate"><span class="pre">None</span></code> if no element was found:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//div[@id=&quot;not-exists&quot;]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span> <span class="ow">is</span> <span class="bp">None</span>
<span class="go">True</span>
</pre></div>
</div>
<p>A default return value can be provided as an argument, to be used instead
of <code class="docutils literal notranslate"><span class="pre">None</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//div[@id=&quot;not-exists&quot;]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="s1">&#39;not-found&#39;</span><span class="p">)</span>
<span class="go">&#39;not-found&#39;</span>
</pre></div>
</div>
<p>Instead of using e.g. <code class="docutils literal notranslate"><span class="pre">'&#64;src'</span></code> XPath it is possible to query for attributes
using <code class="docutils literal notranslate"><span class="pre">.attrib</span></code> property of a <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="n">img</span><span class="o">.</span><span class="n">attrib</span><span class="p">[</span><span class="s1">&#39;src&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">img</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;img&#39;</span><span class="p">)]</span>
<span class="go">[&#39;image1_thumb.jpg&#39;,</span>
<span class="go"> &#39;image2_thumb.jpg&#39;,</span>
<span class="go"> &#39;image3_thumb.jpg&#39;,</span>
<span class="go"> &#39;image4_thumb.jpg&#39;,</span>
<span class="go"> &#39;image5_thumb.jpg&#39;]</span>
</pre></div>
</div>
<p>As a shortcut, <code class="docutils literal notranslate"><span class="pre">.attrib</span></code> is also available on SelectorList directly;
it returns attributes for the first matching element:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;img&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">attrib</span><span class="p">[</span><span class="s1">&#39;src&#39;</span><span class="p">]</span>
<span class="go">&#39;image1_thumb.jpg&#39;</span>
</pre></div>
</div>
<p>This is most useful when only a single result is expected, e.g. when selecting
by id, or selecting unique elements on a web page:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;base&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">attrib</span><span class="p">[</span><span class="s1">&#39;href&#39;</span><span class="p">]</span>
<span class="go">&#39;http://example.com/&#39;</span>
</pre></div>
</div>
<p>Now we’re going to get the base URL and some image links:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//base/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;http://example.com/&#39;</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;base::attr(href)&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;http://example.com/&#39;</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;base&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">attrib</span><span class="p">[</span><span class="s1">&#39;href&#39;</span><span class="p">]</span>
<span class="go">&#39;http://example.com/&#39;</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//a[contains(@href, &quot;image&quot;)]/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;image1.html&#39;,</span>
<span class="go"> &#39;image2.html&#39;,</span>
<span class="go"> &#39;image3.html&#39;,</span>
<span class="go"> &#39;image4.html&#39;,</span>
<span class="go"> &#39;image5.html&#39;]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;a[href*=image]::attr(href)&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;image1.html&#39;,</span>
<span class="go"> &#39;image2.html&#39;,</span>
<span class="go"> &#39;image3.html&#39;,</span>
<span class="go"> &#39;image4.html&#39;,</span>
<span class="go"> &#39;image5.html&#39;]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//a[contains(@href, &quot;image&quot;)]/img/@src&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;image1_thumb.jpg&#39;,</span>
<span class="go"> &#39;image2_thumb.jpg&#39;,</span>
<span class="go"> &#39;image3_thumb.jpg&#39;,</span>
<span class="go"> &#39;image4_thumb.jpg&#39;,</span>
<span class="go"> &#39;image5_thumb.jpg&#39;]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;a[href*=image] img::attr(src)&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;image1_thumb.jpg&#39;,</span>
<span class="go"> &#39;image2_thumb.jpg&#39;,</span>
<span class="go"> &#39;image3_thumb.jpg&#39;,</span>
<span class="go"> &#39;image4_thumb.jpg&#39;,</span>
<span class="go"> &#39;image5_thumb.jpg&#39;]</span>
</pre></div>
</div>
</div>
<div class="section" id="extensions-to-css-selectors">
<span id="topics-selectors-css-extensions"></span><h5>Extensions to CSS Selectors<a class="headerlink" href="#extensions-to-css-selectors" title="Permalink to this headline">¶</a></h5>
<p>Per W3C standards, <a class="reference external" href="https://www.w3.org/TR/css3-selectors/#selectors">CSS selectors</a> do not support selecting text nodes
or attribute values.
But selecting these is so essential in a web scraping context
that Scrapy (parsel) implements a couple of <strong>non-standard pseudo-elements</strong>:</p>
<ul class="simple">
<li>to select text nodes, use <code class="docutils literal notranslate"><span class="pre">::text</span></code></li>
<li>to select attribute values, use <code class="docutils literal notranslate"><span class="pre">::attr(name)</span></code> where <em>name</em> is the
name of the attribute that you want the value of</li>
</ul>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">These pseudo-elements are Scrapy-/Parsel-specific.
They will most probably not work with other libraries like
<a class="reference external" href="http://lxml.de/">lxml</a> or <a class="reference external" href="https://pypi.python.org/pypi/pyquery">PyQuery</a>.</p>
</div>
<p>Examples:</p>
<ul>
<li><p class="first"><code class="docutils literal notranslate"><span class="pre">title::text</span></code> selects children text nodes of a descendant <code class="docutils literal notranslate"><span class="pre">&lt;title&gt;</span></code> element:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;title::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;Example website&#39;</span>
</pre></div>
</div>
</li>
<li><p class="first"><code class="docutils literal notranslate"><span class="pre">*::text</span></code> selects all descendant text nodes of the current selector context:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;#images *::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;\n   &#39;,</span>
<span class="go"> &#39;Name: My image 1 &#39;,</span>
<span class="go"> &#39;\n   &#39;,</span>
<span class="go"> &#39;Name: My image 2 &#39;,</span>
<span class="go"> &#39;\n   &#39;,</span>
<span class="go"> &#39;Name: My image 3 &#39;,</span>
<span class="go"> &#39;\n   &#39;,</span>
<span class="go"> &#39;Name: My image 4 &#39;,</span>
<span class="go"> &#39;\n   &#39;,</span>
<span class="go"> &#39;Name: My image 5 &#39;,</span>
<span class="go"> &#39;\n  &#39;]</span>
</pre></div>
</div>
</li>
<li><p class="first"><code class="docutils literal notranslate"><span class="pre">foo::text</span></code> returns no results if <code class="docutils literal notranslate"><span class="pre">foo</span></code> element exists, but contains
no text (i.e. text is empty):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;img::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[]</span>
</pre></div>
</div>
<p>This means <code class="docutils literal notranslate"><span class="pre">.css('foo::text').get()</span></code> could return None even if an element
exists. Use <code class="docutils literal notranslate"><span class="pre">default=''</span></code> if you always want a string:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;img::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;img::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">default</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
<span class="go">&#39;&#39;</span>
</pre></div>
</div>
</li>
<li><p class="first"><code class="docutils literal notranslate"><span class="pre">a::attr(href)</span></code> selects the <em>href</em> attribute value of descendant links:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;a::attr(href)&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;image1.html&#39;,</span>
<span class="go"> &#39;image2.html&#39;,</span>
<span class="go"> &#39;image3.html&#39;,</span>
<span class="go"> &#39;image4.html&#39;,</span>
<span class="go"> &#39;image5.html&#39;]</span>
</pre></div>
</div>
</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">See also: <a class="reference internal" href="#selecting-attributes"><span class="std std-ref">Selecting element attributes</span></a>.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">You cannot chain these pseudo-elements. But in practice it would not
make much sense: text nodes do not have attributes, and attribute values
are string values already and do not have children nodes.</p>
</div>
</div>
<div class="section" id="nesting-selectors">
<span id="topics-selectors-nesting-selectors"></span><h5>Nesting selectors<a class="headerlink" href="#nesting-selectors" title="Permalink to this headline">¶</a></h5>
<p>The selection methods (<code class="docutils literal notranslate"><span class="pre">.xpath()</span></code> or <code class="docutils literal notranslate"><span class="pre">.css()</span></code>) return a list of selectors
of the same type, so you can call the selection methods for those selectors
too. Here’s an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">links</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//a[contains(@href, &quot;image&quot;)]&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">links</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;&lt;a href=&quot;image1.html&quot;&gt;Name: My image 1 &lt;br&gt;&lt;img src=&quot;image1_thumb.jpg&quot;&gt;&lt;/a&gt;&#39;,</span>
<span class="go"> &#39;&lt;a href=&quot;image2.html&quot;&gt;Name: My image 2 &lt;br&gt;&lt;img src=&quot;image2_thumb.jpg&quot;&gt;&lt;/a&gt;&#39;,</span>
<span class="go"> &#39;&lt;a href=&quot;image3.html&quot;&gt;Name: My image 3 &lt;br&gt;&lt;img src=&quot;image3_thumb.jpg&quot;&gt;&lt;/a&gt;&#39;,</span>
<span class="go"> &#39;&lt;a href=&quot;image4.html&quot;&gt;Name: My image 4 &lt;br&gt;&lt;img src=&quot;image4_thumb.jpg&quot;&gt;&lt;/a&gt;&#39;,</span>
<span class="go"> &#39;&lt;a href=&quot;image5.html&quot;&gt;Name: My image 5 &lt;br&gt;&lt;img src=&quot;image5_thumb.jpg&quot;&gt;&lt;/a&gt;&#39;]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">link</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">links</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">index</span><span class="p">,</span> <span class="n">link</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">(),</span> <span class="n">link</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;img/@src&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">())</span>
<span class="gp">... </span>    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;Link number </span><span class="si">%d</span><span class="s1"> points to url </span><span class="si">%r</span><span class="s1"> and image </span><span class="si">%r</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">args</span><span class="p">)</span>

<span class="go">Link number 0 points to url &#39;image1.html&#39; and image &#39;image1_thumb.jpg&#39;</span>
<span class="go">Link number 1 points to url &#39;image2.html&#39; and image &#39;image2_thumb.jpg&#39;</span>
<span class="go">Link number 2 points to url &#39;image3.html&#39; and image &#39;image3_thumb.jpg&#39;</span>
<span class="go">Link number 3 points to url &#39;image4.html&#39; and image &#39;image4_thumb.jpg&#39;</span>
<span class="go">Link number 4 points to url &#39;image5.html&#39; and image &#39;image5_thumb.jpg&#39;</span>
</pre></div>
</div>
</div>
<div class="section" id="selecting-element-attributes">
<span id="selecting-attributes"></span><h5>Selecting element attributes<a class="headerlink" href="#selecting-element-attributes" title="Permalink to this headline">¶</a></h5>
<p>There are several ways to get a value of an attribute. First, one can use
XPath syntax:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//a/@href&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;image1.html&#39;, &#39;image2.html&#39;, &#39;image3.html&#39;, &#39;image4.html&#39;, &#39;image5.html&#39;]</span>
</pre></div>
</div>
<p>XPath syntax has a few advantages: it is a standard XPath feature, and
<code class="docutils literal notranslate"><span class="pre">&#64;attributes</span></code> can be used in other parts of an XPath expression - e.g.
it is possible to filter by attribute value.</p>
<p>Scrapy also provides an extension to CSS selectors (<code class="docutils literal notranslate"><span class="pre">::attr(...)</span></code>)
which allows to get attribute values:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;a::attr(href)&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;image1.html&#39;, &#39;image2.html&#39;, &#39;image3.html&#39;, &#39;image4.html&#39;, &#39;image5.html&#39;]</span>
</pre></div>
</div>
<p>In addition to that, there is a <code class="docutils literal notranslate"><span class="pre">.attrib</span></code> property of Selector.
You can use it if you prefer to lookup attributes in Python
code, without using XPaths or CSS extensions:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="n">a</span><span class="o">.</span><span class="n">attrib</span><span class="p">[</span><span class="s1">&#39;href&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">)]</span>
<span class="go">[&#39;image1.html&#39;, &#39;image2.html&#39;, &#39;image3.html&#39;, &#39;image4.html&#39;, &#39;image5.html&#39;]</span>
</pre></div>
</div>
<p>This property is also available on SelectorList; it returns a dictionary
with attributes of a first matching element. It is convenient to use when
a selector is expected to give a single result (e.g. when selecting by element
ID, or when selecting an unique element on a page):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;base&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">attrib</span>
<span class="go">{&#39;href&#39;: &#39;http://example.com/&#39;}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;base&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">attrib</span><span class="p">[</span><span class="s1">&#39;href&#39;</span><span class="p">]</span>
<span class="go">&#39;http://example.com/&#39;</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">.attrib</span></code> property of an empty SelectorList is empty:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;foo&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">attrib</span>
<span class="go">{}</span>
</pre></div>
</div>
</div>
<div class="section" id="using-selectors-with-regular-expressions">
<h5>Using selectors with regular expressions<a class="headerlink" href="#using-selectors-with-regular-expressions" title="Permalink to this headline">¶</a></h5>
<p><a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a> also has a <code class="docutils literal notranslate"><span class="pre">.re()</span></code> method for extracting
data using regular expressions. However, unlike using <code class="docutils literal notranslate"><span class="pre">.xpath()</span></code> or
<code class="docutils literal notranslate"><span class="pre">.css()</span></code> methods, <code class="docutils literal notranslate"><span class="pre">.re()</span></code> returns a list of unicode strings. So you
can’t construct nested <code class="docutils literal notranslate"><span class="pre">.re()</span></code> calls.</p>
<p>Here’s an example used to extract image names from the <a class="reference internal" href="#topics-selectors-htmlcode"><span class="std std-ref">HTML code</span></a> above:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//a[contains(@href, &quot;image&quot;)]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">re</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Name:\s*(.*)&#39;</span><span class="p">)</span>
<span class="go">[&#39;My image 1&#39;,</span>
<span class="go"> &#39;My image 2&#39;,</span>
<span class="go"> &#39;My image 3&#39;,</span>
<span class="go"> &#39;My image 4&#39;,</span>
<span class="go"> &#39;My image 5&#39;]</span>
</pre></div>
</div>
<p>There’s an additional helper reciprocating <code class="docutils literal notranslate"><span class="pre">.get()</span></code> (and its
alias <code class="docutils literal notranslate"><span class="pre">.extract_first()</span></code>) for <code class="docutils literal notranslate"><span class="pre">.re()</span></code>, named <code class="docutils literal notranslate"><span class="pre">.re_first()</span></code>.
Use it to extract just the first matching string:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//a[contains(@href, &quot;image&quot;)]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">re_first</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Name:\s*(.*)&#39;</span><span class="p">)</span>
<span class="go">&#39;My image 1&#39;</span>
</pre></div>
</div>
</div>
<div class="section" id="extract-and-extract-first">
<span id="old-extraction-api"></span><h5>extract() and extract_first()<a class="headerlink" href="#extract-and-extract-first" title="Permalink to this headline">¶</a></h5>
<p>If you’re a long-time Scrapy user, you’re probably familiar
with <code class="docutils literal notranslate"><span class="pre">.extract()</span></code> and <code class="docutils literal notranslate"><span class="pre">.extract_first()</span></code> selector methods. Many blog posts
and tutorials are using them as well. These methods are still supported
by Scrapy, there are <strong>no plans</strong> to deprecate them.</p>
<p>However, Scrapy usage docs are now written using <code class="docutils literal notranslate"><span class="pre">.get()</span></code> and
<code class="docutils literal notranslate"><span class="pre">.getall()</span></code> methods. We feel that these new methods result in a more concise
and readable code.</p>
<p>The following examples show how these methods map to each other.</p>
<ol class="arabic">
<li><p class="first"><code class="docutils literal notranslate"><span class="pre">SelectorList.get()</span></code> is the same as <code class="docutils literal notranslate"><span class="pre">SelectorList.extract_first()</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;a::attr(href)&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;image1.html&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;a::attr(href)&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract_first</span><span class="p">()</span>
<span class="go">&#39;image1.html&#39;</span>
</pre></div>
</div>
</li>
<li><p class="first"><code class="docutils literal notranslate"><span class="pre">SelectorList.getall()</span></code> is the same as <code class="docutils literal notranslate"><span class="pre">SelectorList.extract()</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;a::attr(href)&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;image1.html&#39;, &#39;image2.html&#39;, &#39;image3.html&#39;, &#39;image4.html&#39;, &#39;image5.html&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;a::attr(href)&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">[&#39;image1.html&#39;, &#39;image2.html&#39;, &#39;image3.html&#39;, &#39;image4.html&#39;, &#39;image5.html&#39;]</span>
</pre></div>
</div>
</li>
<li><p class="first"><code class="docutils literal notranslate"><span class="pre">Selector.get()</span></code> is the same as <code class="docutils literal notranslate"><span class="pre">Selector.extract()</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;a::attr(href)&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;image1.html&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;a::attr(href)&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
<span class="go">&#39;image1.html&#39;</span>
</pre></div>
</div>
</li>
<li><p class="first">For consistency, there is also <code class="docutils literal notranslate"><span class="pre">Selector.getall()</span></code>, which returns a list:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;a::attr(href)&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;image1.html&#39;]</span>
</pre></div>
</div>
</li>
</ol>
<p>So, the main difference is that output of <code class="docutils literal notranslate"><span class="pre">.get()</span></code> and <code class="docutils literal notranslate"><span class="pre">.getall()</span></code> methods
is more predictable: <code class="docutils literal notranslate"><span class="pre">.get()</span></code> always returns a single result, <code class="docutils literal notranslate"><span class="pre">.getall()</span></code>
always returns a list of all extracted results. With <code class="docutils literal notranslate"><span class="pre">.extract()</span></code> method
it was not always obvious if a result is a list or not; to get a single
result either <code class="docutils literal notranslate"><span class="pre">.extract()</span></code> or <code class="docutils literal notranslate"><span class="pre">.extract_first()</span></code> should be called.</p>
</div>
</div>
<div class="section" id="working-with-xpaths">
<span id="topics-selectors-xpaths"></span><h4>Working with XPaths<a class="headerlink" href="#working-with-xpaths" title="Permalink to this headline">¶</a></h4>
<p>Here are some tips which may help you to use XPath with Scrapy selectors
effectively. If you are not much familiar with XPath yet,
you may want to take a look first at this <a class="reference external" href="http://www.zvon.org/comp/r/tut-XPath_1.html">XPath tutorial</a>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Some of the tips are based on <a class="reference external" href="https://blog.scrapinghub.com/2014/07/17/xpath-tips-from-the-web-scraping-trenches/">this post from ScrapingHub’s blog</a>.</p>
</div>
<div class="section" id="working-with-relative-xpaths">
<span id="topics-selectors-relative-xpaths"></span><h5>Working with relative XPaths<a class="headerlink" href="#working-with-relative-xpaths" title="Permalink to this headline">¶</a></h5>
<p>Keep in mind that if you are nesting selectors and use an XPath that starts
with <code class="docutils literal notranslate"><span class="pre">/</span></code>, that XPath will be absolute to the document and not relative to the
<code class="docutils literal notranslate"><span class="pre">Selector</span></code> you’re calling it from.</p>
<p>For example, suppose you want to extract all <code class="docutils literal notranslate"><span class="pre">&lt;p&gt;</span></code> elements inside <code class="docutils literal notranslate"><span class="pre">&lt;div&gt;</span></code>
elements. First, you would get all <code class="docutils literal notranslate"><span class="pre">&lt;div&gt;</span></code> elements:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">divs</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//div&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>At first, you may be tempted to use the following approach, which is wrong, as
it actually extracts all <code class="docutils literal notranslate"><span class="pre">&lt;p&gt;</span></code> elements from the document, not only those
inside <code class="docutils literal notranslate"><span class="pre">&lt;div&gt;</span></code> elements:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">divs</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//p&#39;</span><span class="p">):</span>  <span class="c1"># this is wrong - gets all &lt;p&gt; from the whole document</span>
<span class="gp">... </span>    <span class="k">print</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">get</span><span class="p">())</span>
</pre></div>
</div>
<p>This is the proper way to do it (note the dot prefixing the <code class="docutils literal notranslate"><span class="pre">.//p</span></code> XPath):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">divs</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;.//p&#39;</span><span class="p">):</span>  <span class="c1"># extracts all &lt;p&gt; inside</span>
<span class="gp">... </span>    <span class="k">print</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">get</span><span class="p">())</span>
</pre></div>
</div>
<p>Another common case would be to extract all direct <code class="docutils literal notranslate"><span class="pre">&lt;p&gt;</span></code> children:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">divs</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;p&#39;</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">print</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">get</span><span class="p">())</span>
</pre></div>
</div>
<p>For more details about relative XPaths see the <a class="reference external" href="https://www.w3.org/TR/xpath#location-paths">Location Paths</a> section in the
XPath specification.</p>
</div>
<div class="section" id="when-querying-by-class-consider-using-css">
<h5>When querying by class, consider using CSS<a class="headerlink" href="#when-querying-by-class-consider-using-css" title="Permalink to this headline">¶</a></h5>
<p>Because an element can contain multiple CSS classes, the XPath way to select elements
by class is the rather verbose:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">*</span><span class="p">[</span><span class="n">contains</span><span class="p">(</span><span class="n">concat</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="n">normalize</span><span class="o">-</span><span class="n">space</span><span class="p">(</span><span class="nd">@class</span><span class="p">),</span> <span class="s1">&#39; &#39;</span><span class="p">),</span> <span class="s1">&#39; someclass &#39;</span><span class="p">)]</span>
</pre></div>
</div>
<p>If you use <code class="docutils literal notranslate"><span class="pre">&#64;class='someclass'</span></code> you may end up missing elements that have
other classes, and if you just use <code class="docutils literal notranslate"><span class="pre">contains(&#64;class,</span> <span class="pre">'someclass')</span></code> to make up
for that you may end up with more elements that you want, if they have a different
class name that shares the string <code class="docutils literal notranslate"><span class="pre">someclass</span></code>.</p>
<p>As it turns out, Scrapy selectors allow you to chain selectors, so most of the time
you can just select by class using CSS and then switch to XPath when needed:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">Selector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="s1">&#39;&lt;div class=&quot;hero shout&quot;&gt;&lt;time datetime=&quot;2014-07-23 19:00&quot;&gt;Special date&lt;/time&gt;&lt;/div&gt;&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;.shout&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;./time/@datetime&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;2014-07-23 19:00&#39;]</span>
</pre></div>
</div>
<p>This is cleaner than using the verbose XPath trick shown above. Just remember
to use the <code class="docutils literal notranslate"><span class="pre">.</span></code> in the XPath expressions that will follow.</p>
</div>
<div class="section" id="beware-of-the-difference-between-node-1-and-node-1">
<h5>Beware of the difference between //node[1] and (//node)[1]<a class="headerlink" href="#beware-of-the-difference-between-node-1-and-node-1" title="Permalink to this headline">¶</a></h5>
<p><code class="docutils literal notranslate"><span class="pre">//node[1]</span></code> selects all the nodes occurring first under their respective parents.</p>
<p><code class="docutils literal notranslate"><span class="pre">(//node)[1]</span></code> selects all the nodes in the document, and then gets only the first of them.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">Selector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="s2">&quot;&quot;&quot;</span>
<span class="go">....:     &lt;ul class=&quot;list&quot;&gt;</span>
<span class="go">....:         &lt;li&gt;1&lt;/li&gt;</span>
<span class="go">....:         &lt;li&gt;2&lt;/li&gt;</span>
<span class="go">....:         &lt;li&gt;3&lt;/li&gt;</span>
<span class="go">....:     &lt;/ul&gt;</span>
<span class="go">....:     &lt;ul class=&quot;list&quot;&gt;</span>
<span class="go">....:         &lt;li&gt;4&lt;/li&gt;</span>
<span class="go">....:         &lt;li&gt;5&lt;/li&gt;</span>
<span class="go">....:         &lt;li&gt;6&lt;/li&gt;</span>
<span class="go">....:     &lt;/ul&gt;&quot;&quot;&quot;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">xp</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
</pre></div>
</div>
<p>This gets all first <code class="docutils literal notranslate"><span class="pre">&lt;li&gt;</span></code>  elements under whatever it is its parent:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">xp</span><span class="p">(</span><span class="s2">&quot;//li[1]&quot;</span><span class="p">)</span>
<span class="go">[&#39;&lt;li&gt;1&lt;/li&gt;&#39;, &#39;&lt;li&gt;4&lt;/li&gt;&#39;]</span>
</pre></div>
</div>
<p>And this gets the first <code class="docutils literal notranslate"><span class="pre">&lt;li&gt;</span></code>  element in the whole document:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">xp</span><span class="p">(</span><span class="s2">&quot;(//li)[1]&quot;</span><span class="p">)</span>
<span class="go">[&#39;&lt;li&gt;1&lt;/li&gt;&#39;]</span>
</pre></div>
</div>
<p>This gets all first <code class="docutils literal notranslate"><span class="pre">&lt;li&gt;</span></code>  elements under an <code class="docutils literal notranslate"><span class="pre">&lt;ul&gt;</span></code>  parent:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">xp</span><span class="p">(</span><span class="s2">&quot;//ul/li[1]&quot;</span><span class="p">)</span>
<span class="go">[&#39;&lt;li&gt;1&lt;/li&gt;&#39;, &#39;&lt;li&gt;4&lt;/li&gt;&#39;]</span>
</pre></div>
</div>
<p>And this gets the first <code class="docutils literal notranslate"><span class="pre">&lt;li&gt;</span></code>  element under an <code class="docutils literal notranslate"><span class="pre">&lt;ul&gt;</span></code>  parent in the whole document:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">xp</span><span class="p">(</span><span class="s2">&quot;(//ul/li)[1]&quot;</span><span class="p">)</span>
<span class="go">[&#39;&lt;li&gt;1&lt;/li&gt;&#39;]</span>
</pre></div>
</div>
</div>
<div class="section" id="using-text-nodes-in-a-condition">
<h5>Using text nodes in a condition<a class="headerlink" href="#using-text-nodes-in-a-condition" title="Permalink to this headline">¶</a></h5>
<p>When you need to use the text content as argument to an <a class="reference external" href="https://www.w3.org/TR/xpath/#section-String-Functions">XPath string function</a>,
avoid using <code class="docutils literal notranslate"><span class="pre">.//text()</span></code> and use just <code class="docutils literal notranslate"><span class="pre">.</span></code> instead.</p>
<p>This is because the expression <code class="docutils literal notranslate"><span class="pre">.//text()</span></code> yields a collection of text elements – a <em>node-set</em>.
And when a node-set is converted to a string, which happens when it is passed as argument to
a string function like <code class="docutils literal notranslate"><span class="pre">contains()</span></code> or <code class="docutils literal notranslate"><span class="pre">starts-with()</span></code>, it results in the text for the first element only.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">Selector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="s1">&#39;&lt;a href=&quot;#&quot;&gt;Click here to go to the &lt;strong&gt;Next Page&lt;/strong&gt;&lt;/a&gt;&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Converting a <em>node-set</em> to string:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//a//text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span> <span class="c1"># take a peek at the node-set</span>
<span class="go">[&#39;Click here to go to the &#39;, &#39;Next Page&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;string(//a[1]//text())&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span> <span class="c1"># convert it to string</span>
<span class="go">[&#39;Click here to go to the &#39;]</span>
</pre></div>
</div>
<p>A <em>node</em> converted to a string, however, puts together the text of itself plus of all its descendants:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//a[1]&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span> <span class="c1"># select the first node</span>
<span class="go">[&#39;&lt;a href=&quot;#&quot;&gt;Click here to go to the &lt;strong&gt;Next Page&lt;/strong&gt;&lt;/a&gt;&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;string(//a[1])&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span> <span class="c1"># convert it to string</span>
<span class="go">[&#39;Click here to go to the Next Page&#39;]</span>
</pre></div>
</div>
<p>So, using the <code class="docutils literal notranslate"><span class="pre">.//text()</span></code> node-set won’t select anything in this case:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//a[contains(.//text(), &#39;Next Page&#39;)]&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[]</span>
</pre></div>
</div>
<p>But using the <code class="docutils literal notranslate"><span class="pre">.</span></code> to mean the node, works:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//a[contains(., &#39;Next Page&#39;)]&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;&lt;a href=&quot;#&quot;&gt;Click here to go to the &lt;strong&gt;Next Page&lt;/strong&gt;&lt;/a&gt;&#39;]</span>
</pre></div>
</div>
</div>
<div class="section" id="variables-in-xpath-expressions">
<span id="topics-selectors-xpath-variables"></span><h5>Variables in XPath expressions<a class="headerlink" href="#variables-in-xpath-expressions" title="Permalink to this headline">¶</a></h5>
<p>XPath allows you to reference variables in your XPath expressions, using
the <code class="docutils literal notranslate"><span class="pre">$somevariable</span></code> syntax. This is somewhat similar to parameterized
queries or prepared statements in the SQL world where you replace
some arguments in your queries with placeholders like <code class="docutils literal notranslate"><span class="pre">?</span></code>,
which are then substituted with values passed with the query.</p>
<p>Here’s an example to match an element based on its “id” attribute value,
without hard-coding it (that was shown previously):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># `$val` used in the expression, a `val` argument needs to be passed</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//div[@id=$val]/a/text()&#39;</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="s1">&#39;images&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;Name: My image 1 &#39;</span>
</pre></div>
</div>
<p>Here’s another example, to find the “id” attribute of a <code class="docutils literal notranslate"><span class="pre">&lt;div&gt;</span></code> tag containing
five <code class="docutils literal notranslate"><span class="pre">&lt;a&gt;</span></code> children (here we pass the value <code class="docutils literal notranslate"><span class="pre">5</span></code> as an integer):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//div[count(a)=$cnt]/@id&#39;</span><span class="p">,</span> <span class="n">cnt</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;images&#39;</span>
</pre></div>
</div>
<p>All variable references must have a binding value when calling <code class="docutils literal notranslate"><span class="pre">.xpath()</span></code>
(otherwise you’ll get a <code class="docutils literal notranslate"><span class="pre">ValueError:</span> <span class="pre">XPath</span> <span class="pre">error:</span></code> exception).
This is done by passing as many named arguments as necessary.</p>
<p><a class="reference external" href="https://parsel.readthedocs.io/">parsel</a>, the library powering Scrapy selectors, has more details and examples
on <a class="reference external" href="https://parsel.readthedocs.io/en/latest/usage.html#variables-in-xpath-expressions">XPath variables</a>.</p>
</div>
<div class="section" id="removing-namespaces">
<span id="id2"></span><h5>Removing namespaces<a class="headerlink" href="#removing-namespaces" title="Permalink to this headline">¶</a></h5>
<p>When dealing with scraping projects, it is often quite convenient to get rid of
namespaces altogether and just work with element names, to write more
simple/convenient XPaths. You can use the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">Selector.remove_namespaces()</span></code> method for that.</p>
<p>Let’s show an example that illustrates this with the Python Insider blog atom feed.</p>
<p>First, we open the shell with the url we want to scrape:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ scrapy shell https://feeds.feedburner.com/PythonInsider
</pre></div>
</div>
<p>This is how the file starts:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>&lt;?xml <span class="nv">version</span><span class="o">=</span><span class="s2">&quot;1.0&quot;</span> <span class="nv">encoding</span><span class="o">=</span><span class="s2">&quot;UTF-8&quot;</span>?&gt;
&lt;?xml-stylesheet ...
&lt;feed <span class="nv">xmlns</span><span class="o">=</span><span class="s2">&quot;http://www.w3.org/2005/Atom&quot;</span>
      xmlns:openSearch<span class="o">=</span><span class="s2">&quot;http://a9.com/-/spec/opensearchrss/1.0/&quot;</span>
      xmlns:blogger<span class="o">=</span><span class="s2">&quot;http://schemas.google.com/blogger/2008&quot;</span>
      xmlns:georss<span class="o">=</span><span class="s2">&quot;http://www.georss.org/georss&quot;</span>
      xmlns:gd<span class="o">=</span><span class="s2">&quot;http://schemas.google.com/g/2005&quot;</span>
      xmlns:thr<span class="o">=</span><span class="s2">&quot;http://purl.org/syndication/thread/1.0&quot;</span>
      xmlns:feedburner<span class="o">=</span><span class="s2">&quot;http://rssnamespace.org/feedburner/ext/1.0&quot;</span>&gt;
  ...
</pre></div>
</div>
<p>You can see several namespace declarations including a default
“<a class="reference external" href="http://www.w3.org/2005/Atom">http://www.w3.org/2005/Atom</a>” and another one using the “gd:” prefix for
“<a class="reference external" href="http://schemas.google.com/g/2005">http://schemas.google.com/g/2005</a>”.</p>
<p>Once in the shell we can try selecting all <code class="docutils literal notranslate"><span class="pre">&lt;link&gt;</span></code> objects and see that it
doesn’t work (because the Atom XML namespace is obfuscating those nodes):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//link&quot;</span><span class="p">)</span>
<span class="go">[]</span>
</pre></div>
</div>
<p>But once we call the <code class="xref py py-meth docutils literal notranslate"><span class="pre">Selector.remove_namespaces()</span></code> method, all
nodes can be accessed directly by their names:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">selector</span><span class="o">.</span><span class="n">remove_namespaces</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//link&quot;</span><span class="p">)</span>
<span class="go">[&lt;Selector xpath=&#39;//link&#39; data=&#39;&lt;link rel=&quot;alternate&quot; type=&quot;text/html&quot; h&#39;&gt;,</span>
<span class="go"> &lt;Selector xpath=&#39;//link&#39; data=&#39;&lt;link rel=&quot;next&quot; type=&quot;application/atom+&#39;&gt;,</span>
<span class="go"> ...</span>
</pre></div>
</div>
<p>If you wonder why the namespace removal procedure isn’t always called by default
instead of having to call it manually, this is because of two reasons, which, in order
of relevance, are:</p>
<ol class="arabic simple">
<li>Removing namespaces requires to iterate and modify all nodes in the
document, which is a reasonably expensive operation to perform by default
for all documents crawled by Scrapy</li>
<li>There could be some cases where using namespaces is actually required, in
case some element names clash between namespaces. These cases are very rare
though.</li>
</ol>
</div>
<div class="section" id="using-exslt-extensions">
<h5>Using EXSLT extensions<a class="headerlink" href="#using-exslt-extensions" title="Permalink to this headline">¶</a></h5>
<p>Being built atop <a class="reference external" href="http://lxml.de/">lxml</a>, Scrapy selectors support some <a class="reference external" href="http://exslt.org/">EXSLT</a> extensions
and come with these pre-registered namespaces to use in XPath expressions:</p>
<table border="1" class="docutils">
<colgroup>
<col width="9%" />
<col width="56%" />
<col width="35%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">prefix</th>
<th class="head">namespace</th>
<th class="head">usage</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>re</td>
<td>http://exslt.org/regular-expressions</td>
<td><a class="reference external" href="http://exslt.org/regexp/index.html">regular expressions</a></td>
</tr>
<tr class="row-odd"><td>set</td>
<td>http://exslt.org/sets</td>
<td><a class="reference external" href="http://exslt.org/set/index.html">set manipulation</a></td>
</tr>
</tbody>
</table>
<div class="section" id="regular-expressions">
<h6>Regular expressions<a class="headerlink" href="#regular-expressions" title="Permalink to this headline">¶</a></h6>
<p>The <code class="docutils literal notranslate"><span class="pre">test()</span></code> function, for example, can prove quite useful when XPath’s
<code class="docutils literal notranslate"><span class="pre">starts-with()</span></code> or <code class="docutils literal notranslate"><span class="pre">contains()</span></code> are not sufficient.</p>
<p>Example selecting links in list item with a “class” attribute ending with a digit:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy</span> <span class="kn">import</span> <span class="n">Selector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">doc</span> <span class="o">=</span> <span class="sa">u</span><span class="s2">&quot;&quot;&quot;</span>
<span class="gp">... </span><span class="s2">&lt;div&gt;</span>
<span class="gp">... </span><span class="s2">    &lt;ul&gt;</span>
<span class="gp">... </span><span class="s2">        &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link1.html&quot;&gt;first item&lt;/a&gt;&lt;/li&gt;</span>
<span class="gp">... </span><span class="s2">        &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link2.html&quot;&gt;second item&lt;/a&gt;&lt;/li&gt;</span>
<span class="gp">... </span><span class="s2">        &lt;li class=&quot;item-inactive&quot;&gt;&lt;a href=&quot;link3.html&quot;&gt;third item&lt;/a&gt;&lt;/li&gt;</span>
<span class="gp">... </span><span class="s2">        &lt;li class=&quot;item-1&quot;&gt;&lt;a href=&quot;link4.html&quot;&gt;fourth item&lt;/a&gt;&lt;/li&gt;</span>
<span class="gp">... </span><span class="s2">        &lt;li class=&quot;item-0&quot;&gt;&lt;a href=&quot;link5.html&quot;&gt;fifth item&lt;/a&gt;&lt;/li&gt;</span>
<span class="gp">... </span><span class="s2">    &lt;/ul&gt;</span>
<span class="gp">... </span><span class="s2">&lt;/div&gt;</span>
<span class="gp">... </span><span class="s2">&quot;&quot;&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">doc</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;html&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//li//@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;link1.html&#39;, &#39;link2.html&#39;, &#39;link3.html&#39;, &#39;link4.html&#39;, &#39;link5.html&#39;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//li[re:test(@class, &quot;item-\d$&quot;)]//@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="go">[&#39;link1.html&#39;, &#39;link2.html&#39;, &#39;link4.html&#39;, &#39;link5.html&#39;]</span>
<span class="go">&gt;&gt;&gt;</span>
</pre></div>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">C library <code class="docutils literal notranslate"><span class="pre">libxslt</span></code> doesn’t natively support EXSLT regular
expressions so <a class="reference external" href="http://lxml.de/">lxml</a>’s implementation uses hooks to Python’s <code class="docutils literal notranslate"><span class="pre">re</span></code> module.
Thus, using regexp functions in your XPath expressions may add a small
performance penalty.</p>
</div>
</div>
<div class="section" id="set-operations">
<h6>Set operations<a class="headerlink" href="#set-operations" title="Permalink to this headline">¶</a></h6>
<p>These can be handy for excluding parts of a document tree before
extracting text elements for example.</p>
<p>Example extracting microdata (sample content taken from <a class="reference external" href="http://schema.org/Product">http://schema.org/Product</a>)
with groups of itemscopes and corresponding itemprops:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">doc</span> <span class="o">=</span> <span class="sa">u</span><span class="s2">&quot;&quot;&quot;</span>
<span class="gp">... </span><span class="s2">&lt;div itemscope itemtype=&quot;http://schema.org/Product&quot;&gt;</span>
<span class="gp">... </span><span class="s2">  &lt;span itemprop=&quot;name&quot;&gt;Kenmore White 17&quot; Microwave&lt;/span&gt;</span>
<span class="gp">... </span><span class="s2">  &lt;img src=&quot;kenmore-microwave-17in.jpg&quot; alt=&#39;Kenmore 17&quot; Microwave&#39; /&gt;</span>
<span class="gp">... </span><span class="s2">  &lt;div itemprop=&quot;aggregateRating&quot;</span>
<span class="gp">... </span><span class="s2">    itemscope itemtype=&quot;http://schema.org/AggregateRating&quot;&gt;</span>
<span class="gp">... </span><span class="s2">   Rated &lt;span itemprop=&quot;ratingValue&quot;&gt;3.5&lt;/span&gt;/5</span>
<span class="gp">... </span><span class="s2">   based on &lt;span itemprop=&quot;reviewCount&quot;&gt;11&lt;/span&gt; customer reviews</span>
<span class="gp">... </span><span class="s2">  &lt;/div&gt;</span>
<span class="gp">...</span><span class="s2"></span>
<span class="gp">... </span><span class="s2">  &lt;div itemprop=&quot;offers&quot; itemscope itemtype=&quot;http://schema.org/Offer&quot;&gt;</span>
<span class="gp">... </span><span class="s2">    &lt;span itemprop=&quot;price&quot;&gt;$55.00&lt;/span&gt;</span>
<span class="gp">... </span><span class="s2">    &lt;link itemprop=&quot;availability&quot; href=&quot;http://schema.org/InStock&quot; /&gt;In stock</span>
<span class="gp">... </span><span class="s2">  &lt;/div&gt;</span>
<span class="gp">...</span><span class="s2"></span>
<span class="gp">... </span><span class="s2">  Product description:</span>
<span class="gp">... </span><span class="s2">  &lt;span itemprop=&quot;description&quot;&gt;0.7 cubic feet countertop microwave.</span>
<span class="gp">... </span><span class="s2">  Has six preset cooking categories and convenience features like</span>
<span class="gp">... </span><span class="s2">  Add-A-Minute and Child Lock.&lt;/span&gt;</span>
<span class="gp">...</span><span class="s2"></span>
<span class="gp">... </span><span class="s2">  Customer reviews:</span>
<span class="gp">...</span><span class="s2"></span>
<span class="gp">... </span><span class="s2">  &lt;div itemprop=&quot;review&quot; itemscope itemtype=&quot;http://schema.org/Review&quot;&gt;</span>
<span class="gp">... </span><span class="s2">    &lt;span itemprop=&quot;name&quot;&gt;Not a happy camper&lt;/span&gt; -</span>
<span class="gp">... </span><span class="s2">    by &lt;span itemprop=&quot;author&quot;&gt;Ellie&lt;/span&gt;,</span>
<span class="gp">... </span><span class="s2">    &lt;meta itemprop=&quot;datePublished&quot; content=&quot;2011-04-01&quot;&gt;April 1, 2011</span>
<span class="gp">... </span><span class="s2">    &lt;div itemprop=&quot;reviewRating&quot; itemscope itemtype=&quot;http://schema.org/Rating&quot;&gt;</span>
<span class="gp">... </span><span class="s2">      &lt;meta itemprop=&quot;worstRating&quot; content = &quot;1&quot;&gt;</span>
<span class="gp">... </span><span class="s2">      &lt;span itemprop=&quot;ratingValue&quot;&gt;1&lt;/span&gt;/</span>
<span class="gp">... </span><span class="s2">      &lt;span itemprop=&quot;bestRating&quot;&gt;5&lt;/span&gt;stars</span>
<span class="gp">... </span><span class="s2">    &lt;/div&gt;</span>
<span class="gp">... </span><span class="s2">    &lt;span itemprop=&quot;description&quot;&gt;The lamp burned out and now I have to replace</span>
<span class="gp">... </span><span class="s2">    it. &lt;/span&gt;</span>
<span class="gp">... </span><span class="s2">  &lt;/div&gt;</span>
<span class="gp">...</span><span class="s2"></span>
<span class="gp">... </span><span class="s2">  &lt;div itemprop=&quot;review&quot; itemscope itemtype=&quot;http://schema.org/Review&quot;&gt;</span>
<span class="gp">... </span><span class="s2">    &lt;span itemprop=&quot;name&quot;&gt;Value purchase&lt;/span&gt; -</span>
<span class="gp">... </span><span class="s2">    by &lt;span itemprop=&quot;author&quot;&gt;Lucas&lt;/span&gt;,</span>
<span class="gp">... </span><span class="s2">    &lt;meta itemprop=&quot;datePublished&quot; content=&quot;2011-03-25&quot;&gt;March 25, 2011</span>
<span class="gp">... </span><span class="s2">    &lt;div itemprop=&quot;reviewRating&quot; itemscope itemtype=&quot;http://schema.org/Rating&quot;&gt;</span>
<span class="gp">... </span><span class="s2">      &lt;meta itemprop=&quot;worstRating&quot; content = &quot;1&quot;/&gt;</span>
<span class="gp">... </span><span class="s2">      &lt;span itemprop=&quot;ratingValue&quot;&gt;4&lt;/span&gt;/</span>
<span class="gp">... </span><span class="s2">      &lt;span itemprop=&quot;bestRating&quot;&gt;5&lt;/span&gt;stars</span>
<span class="gp">... </span><span class="s2">    &lt;/div&gt;</span>
<span class="gp">... </span><span class="s2">    &lt;span itemprop=&quot;description&quot;&gt;Great microwave for the price. It is small and</span>
<span class="gp">... </span><span class="s2">    fits in my apartment.&lt;/span&gt;</span>
<span class="gp">... </span><span class="s2">  &lt;/div&gt;</span>
<span class="gp">... </span><span class="s2">  ...</span>
<span class="gp">... </span><span class="s2">&lt;/div&gt;</span>
<span class="gp">... </span><span class="s2">&quot;&quot;&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sel</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">doc</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;html&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">scope</span> <span class="ow">in</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//div[@itemscope]&#39;</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;current scope:&quot;</span><span class="p">,</span> <span class="n">scope</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;@itemtype&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">())</span>
<span class="gp">... </span>    <span class="n">props</span> <span class="o">=</span> <span class="n">scope</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="gp">... </span><span class="s1">                set:difference(./descendant::*/@itemprop,</span>
<span class="gp">... </span><span class="s1">                               .//*[@itemscope]/*/@itemprop)&#39;&#39;&#39;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;    properties: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">props</span><span class="o">.</span><span class="n">getall</span><span class="p">()))</span>
<span class="gp">... </span>    <span class="k">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)</span>

<span class="go">current scope: [&#39;http://schema.org/Product&#39;]</span>
<span class="go">    properties: [&#39;name&#39;, &#39;aggregateRating&#39;, &#39;offers&#39;, &#39;description&#39;, &#39;review&#39;, &#39;review&#39;]</span>

<span class="go">current scope: [&#39;http://schema.org/AggregateRating&#39;]</span>
<span class="go">    properties: [&#39;ratingValue&#39;, &#39;reviewCount&#39;]</span>

<span class="go">current scope: [&#39;http://schema.org/Offer&#39;]</span>
<span class="go">    properties: [&#39;price&#39;, &#39;availability&#39;]</span>

<span class="go">current scope: [&#39;http://schema.org/Review&#39;]</span>
<span class="go">    properties: [&#39;name&#39;, &#39;author&#39;, &#39;datePublished&#39;, &#39;reviewRating&#39;, &#39;description&#39;]</span>

<span class="go">current scope: [&#39;http://schema.org/Rating&#39;]</span>
<span class="go">    properties: [&#39;worstRating&#39;, &#39;ratingValue&#39;, &#39;bestRating&#39;]</span>

<span class="go">current scope: [&#39;http://schema.org/Review&#39;]</span>
<span class="go">    properties: [&#39;name&#39;, &#39;author&#39;, &#39;datePublished&#39;, &#39;reviewRating&#39;, &#39;description&#39;]</span>

<span class="go">current scope: [&#39;http://schema.org/Rating&#39;]</span>
<span class="go">    properties: [&#39;worstRating&#39;, &#39;ratingValue&#39;, &#39;bestRating&#39;]</span>

<span class="go">&gt;&gt;&gt;</span>
</pre></div>
</div>
<p>Here we first iterate over <code class="docutils literal notranslate"><span class="pre">itemscope</span></code> elements, and for each one,
we look for all <code class="docutils literal notranslate"><span class="pre">itemprops</span></code> elements and exclude those that are themselves
inside another <code class="docutils literal notranslate"><span class="pre">itemscope</span></code>.</p>
</div>
</div>
<div class="section" id="other-xpath-extensions">
<h5>Other XPath extensions<a class="headerlink" href="#other-xpath-extensions" title="Permalink to this headline">¶</a></h5>
<p>Scrapy selectors also provide a sorely missed XPath extension function
<code class="docutils literal notranslate"><span class="pre">has-class</span></code> that returns <code class="docutils literal notranslate"><span class="pre">True</span></code> for nodes that have all of the specified
HTML classes.</p>
<p>For the following HTML:</p>
<div class="highlight-html notranslate"><div class="highlight"><pre><span></span><span class="p">&lt;</span><span class="nt">p</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;foo bar-baz&quot;</span><span class="p">&gt;</span>First<span class="p">&lt;/</span><span class="nt">p</span><span class="p">&gt;</span>
<span class="p">&lt;</span><span class="nt">p</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;foo&quot;</span><span class="p">&gt;</span>Second<span class="p">&lt;/</span><span class="nt">p</span><span class="p">&gt;</span>
<span class="p">&lt;</span><span class="nt">p</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;bar&quot;</span><span class="p">&gt;</span>Third<span class="p">&lt;/</span><span class="nt">p</span><span class="p">&gt;</span>
<span class="p">&lt;</span><span class="nt">p</span><span class="p">&gt;</span>Fourth<span class="p">&lt;/</span><span class="nt">p</span><span class="p">&gt;</span>
</pre></div>
</div>
<p>You can use it like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//p[has-class(&quot;foo&quot;)]&#39;</span><span class="p">)</span>
<span class="go">[&lt;Selector xpath=&#39;//p[has-class(&quot;foo&quot;)]&#39; data=&#39;&lt;p class=&quot;foo bar-baz&quot;&gt;First&lt;/p&gt;&#39;&gt;,</span>
<span class="go"> &lt;Selector xpath=&#39;//p[has-class(&quot;foo&quot;)]&#39; data=&#39;&lt;p class=&quot;foo&quot;&gt;Second&lt;/p&gt;&#39;&gt;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//p[has-class(&quot;foo&quot;, &quot;bar-baz&quot;)]&#39;</span><span class="p">)</span>
<span class="go">[&lt;Selector xpath=&#39;//p[has-class(&quot;foo&quot;, &quot;bar-baz&quot;)]&#39; data=&#39;&lt;p class=&quot;foo bar-baz&quot;&gt;First&lt;/p&gt;&#39;&gt;]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//p[has-class(&quot;foo&quot;, &quot;bar&quot;)]&#39;</span><span class="p">)</span>
<span class="go">[]</span>
</pre></div>
</div>
<p>So XPath <code class="docutils literal notranslate"><span class="pre">//p[has-class(&quot;foo&quot;,</span> <span class="pre">&quot;bar-baz&quot;)]</span></code> is roughly equivalent to CSS
<code class="docutils literal notranslate"><span class="pre">p.foo.bar-baz</span></code>.  Please note, that it is slower in most of the cases,
because it’s a pure-Python function that’s invoked for every node in question
whereas the CSS lookup is translated into XPath and thus runs more efficiently,
so performance-wise its uses are limited to situations that are not easily
described with CSS selectors.</p>
<p>Parsel also simplifies adding your own XPath extensions.</p>
<dl class="function">
<dt id="parsel.xpathfuncs.set_xpathfunc">
<code class="descclassname">parsel.xpathfuncs.</code><code class="descname">set_xpathfunc</code><span class="sig-paren">(</span><em>fname</em>, <em>func</em><span class="sig-paren">)</span><a class="headerlink" href="#parsel.xpathfuncs.set_xpathfunc" title="Permalink to this definition">¶</a></dt>
<dd><p>Register a custom extension function to use in XPath expressions.</p>
<p>The function <code class="docutils literal notranslate"><span class="pre">func</span></code> registered under <code class="docutils literal notranslate"><span class="pre">fname</span></code> identifier will be called
for every matching node, being passed a <code class="docutils literal notranslate"><span class="pre">context</span></code> parameter as well as
any parameters passed from the corresponding XPath expression.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">func</span></code> is <code class="docutils literal notranslate"><span class="pre">None</span></code>, the extension function will be removed.</p>
<p>See more <a class="reference external" href="http://lxml.de/extensions.html#xpath-extension-functions">in lxml documentation</a>.</p>
</dd></dl>

</div>
</div>
<div class="section" id="module-scrapy.selector">
<span id="built-in-selectors-reference"></span><span id="topics-selectors-ref"></span><h4>Built-in Selectors reference<a class="headerlink" href="#module-scrapy.selector" title="Permalink to this headline">¶</a></h4>
<div class="section" id="selector-objects">
<h5>Selector objects<a class="headerlink" href="#selector-objects" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.selector.Selector">
<em class="property">class </em><code class="descclassname">scrapy.selector.</code><code class="descname">Selector</code><span class="sig-paren">(</span><em>response=None</em>, <em>text=None</em>, <em>type=None</em>, <em>root=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.selector.Selector" title="Permalink to this definition">¶</a></dt>
<dd><p>An instance of <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a> is a wrapper over response to select
certain parts of its content.</p>
<p><code class="docutils literal notranslate"><span class="pre">response</span></code> is an <a class="reference internal" href="index.html#scrapy.http.HtmlResponse" title="scrapy.http.HtmlResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">HtmlResponse</span></code></a> or an
<a class="reference internal" href="index.html#scrapy.http.XmlResponse" title="scrapy.http.XmlResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">XmlResponse</span></code></a> object that will be used for selecting
and extracting data.</p>
<p><code class="docutils literal notranslate"><span class="pre">text</span></code> is a unicode string or utf-8 encoded text for cases when a
<code class="docutils literal notranslate"><span class="pre">response</span></code> isn’t available. Using <code class="docutils literal notranslate"><span class="pre">text</span></code> and <code class="docutils literal notranslate"><span class="pre">response</span></code> together is
undefined behavior.</p>
<p><code class="docutils literal notranslate"><span class="pre">type</span></code> defines the selector type, it can be <code class="docutils literal notranslate"><span class="pre">&quot;html&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;xml&quot;</span></code>
or <code class="docutils literal notranslate"><span class="pre">None</span></code> (default).</p>
<p>If <code class="docutils literal notranslate"><span class="pre">type</span></code> is <code class="docutils literal notranslate"><span class="pre">None</span></code>, the selector automatically chooses the best type
based on <code class="docutils literal notranslate"><span class="pre">response</span></code> type (see below), or defaults to <code class="docutils literal notranslate"><span class="pre">&quot;html&quot;</span></code> in case it
is used together with <code class="docutils literal notranslate"><span class="pre">text</span></code>.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">type</span></code> is <code class="docutils literal notranslate"><span class="pre">None</span></code> and a <code class="docutils literal notranslate"><span class="pre">response</span></code> is passed, the selector type is
inferred from the response type as follows:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">&quot;html&quot;</span></code> for <a class="reference internal" href="index.html#scrapy.http.HtmlResponse" title="scrapy.http.HtmlResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">HtmlResponse</span></code></a> type</li>
<li><code class="docutils literal notranslate"><span class="pre">&quot;xml&quot;</span></code> for <a class="reference internal" href="index.html#scrapy.http.XmlResponse" title="scrapy.http.XmlResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">XmlResponse</span></code></a> type</li>
<li><code class="docutils literal notranslate"><span class="pre">&quot;html&quot;</span></code> for anything else</li>
</ul>
<p>Otherwise, if <code class="docutils literal notranslate"><span class="pre">type</span></code> is set, the selector type will be forced and no
detection will occur.</p>
<dl class="method">
<dt id="scrapy.selector.Selector.xpath">
<code class="descname">xpath</code><span class="sig-paren">(</span><em>query</em>, <em>namespaces=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.selector.Selector.xpath" title="Permalink to this definition">¶</a></dt>
<dd><p>Find nodes matching the xpath <code class="docutils literal notranslate"><span class="pre">query</span></code> and return the result as a
<a class="reference internal" href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal notranslate"><span class="pre">SelectorList</span></code></a> instance with all elements flattened. List
elements implement <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a> interface too.</p>
<p><code class="docutils literal notranslate"><span class="pre">query</span></code> is a string containing the XPATH query to apply.</p>
<p><code class="docutils literal notranslate"><span class="pre">namespaces</span></code> is an optional <code class="docutils literal notranslate"><span class="pre">prefix:</span> <span class="pre">namespace-uri</span></code> mapping (dict)
for additional prefixes to those registered with <code class="docutils literal notranslate"><span class="pre">register_namespace(prefix,</span> <span class="pre">uri)</span></code>.
Contrary to <code class="docutils literal notranslate"><span class="pre">register_namespace()</span></code>, these prefixes are not
saved for future calls.</p>
<p>Any additional named arguments can be used to pass values for XPath
variables in the XPath expression, e.g.:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">selector</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//a[href=$url]&#39;</span><span class="p">,</span> <span class="n">url</span><span class="o">=</span><span class="s2">&quot;http://www.example.com&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">For convenience, this method can be called as <code class="docutils literal notranslate"><span class="pre">response.xpath()</span></code></p>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.Selector.css">
<code class="descname">css</code><span class="sig-paren">(</span><em>query</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.selector.Selector.css" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply the given CSS selector and return a <a class="reference internal" href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal notranslate"><span class="pre">SelectorList</span></code></a> instance.</p>
<p><code class="docutils literal notranslate"><span class="pre">query</span></code> is a string containing the CSS selector to apply.</p>
<p>In the background, CSS queries are translated into XPath queries using
<a class="reference external" href="https://pypi.python.org/pypi/cssselect/">cssselect</a> library and run <code class="docutils literal notranslate"><span class="pre">.xpath()</span></code> method.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">For convenience, this method can be called as <code class="docutils literal notranslate"><span class="pre">response.css()</span></code></p>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.Selector.get">
<code class="descname">get</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.selector.Selector.get" title="Permalink to this definition">¶</a></dt>
<dd><p>Serialize and return the matched nodes in a single unicode string.
Percent encoded content is unquoted.</p>
<p>See also: <a class="reference internal" href="#old-extraction-api"><span class="std std-ref">extract() and extract_first()</span></a></p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.selector.Selector.attrib">
<code class="descname">attrib</code><a class="headerlink" href="#scrapy.selector.Selector.attrib" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the attributes dictionary for underlying element.</p>
<p>See also: <a class="reference internal" href="#selecting-attributes"><span class="std std-ref">Selecting element attributes</span></a>.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.Selector.re">
<code class="descname">re</code><span class="sig-paren">(</span><em>regex</em>, <em>replace_entities=True</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.selector.Selector.re" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply the given regex and return a list of unicode strings with the
matches.</p>
<p><code class="docutils literal notranslate"><span class="pre">regex</span></code> can be either a compiled regular expression or a string which
will be compiled to a regular expression using <code class="docutils literal notranslate"><span class="pre">re.compile(regex)</span></code>.</p>
<p>By default, character entity references are replaced by their
corresponding character (except for <code class="docutils literal notranslate"><span class="pre">&amp;amp;</span></code> and <code class="docutils literal notranslate"><span class="pre">&amp;lt;</span></code>).
Passing <code class="docutils literal notranslate"><span class="pre">replace_entities</span></code> as <code class="docutils literal notranslate"><span class="pre">False</span></code> switches off these
replacements.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.Selector.re_first">
<code class="descname">re_first</code><span class="sig-paren">(</span><em>regex</em>, <em>default=None</em>, <em>replace_entities=True</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.selector.Selector.re_first" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply the given regex and return the first unicode string which
matches. If there is no match, return the default value (<code class="docutils literal notranslate"><span class="pre">None</span></code> if
the argument is not provided).</p>
<p>By default, character entity references are replaced by their
corresponding character (except for <code class="docutils literal notranslate"><span class="pre">&amp;amp;</span></code> and <code class="docutils literal notranslate"><span class="pre">&amp;lt;</span></code>).
Passing <code class="docutils literal notranslate"><span class="pre">replace_entities</span></code> as <code class="docutils literal notranslate"><span class="pre">False</span></code> switches off these
replacements.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.Selector.register_namespace">
<code class="descname">register_namespace</code><span class="sig-paren">(</span><em>prefix</em>, <em>uri</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.selector.Selector.register_namespace" title="Permalink to this definition">¶</a></dt>
<dd><p>Register the given namespace to be used in this <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a>.
Without registering namespaces you can’t select or extract data from
non-standard namespaces. See <a class="reference internal" href="#selector-examples-xml"><span class="std std-ref">Selector examples on XML response</span></a>.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.Selector.remove_namespaces">
<code class="descname">remove_namespaces</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.selector.Selector.remove_namespaces" title="Permalink to this definition">¶</a></dt>
<dd><p>Remove all namespaces, allowing to traverse the document using
namespace-less xpaths. See <a class="reference internal" href="#removing-namespaces"><span class="std std-ref">Removing namespaces</span></a>.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.Selector.__bool__">
<code class="descname">__bool__</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.selector.Selector.__bool__" title="Permalink to this definition">¶</a></dt>
<dd><p>Return <code class="docutils literal notranslate"><span class="pre">True</span></code> if there is any real content selected or <code class="docutils literal notranslate"><span class="pre">False</span></code>
otherwise.  In other words, the boolean value of a <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a> is
given by the contents it selects.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.Selector.getall">
<code class="descname">getall</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.selector.Selector.getall" title="Permalink to this definition">¶</a></dt>
<dd><p>Serialize and return the matched node in a 1-element list of unicode strings.</p>
<p>This method is added to Selector for consistency; it is more useful
with SelectorList. See also: <a class="reference internal" href="#old-extraction-api"><span class="std std-ref">extract() and extract_first()</span></a></p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="selectorlist-objects">
<h5>SelectorList objects<a class="headerlink" href="#selectorlist-objects" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.selector.SelectorList">
<em class="property">class </em><code class="descclassname">scrapy.selector.</code><code class="descname">SelectorList</code><a class="headerlink" href="#scrapy.selector.SelectorList" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal notranslate"><span class="pre">SelectorList</span></code></a> class is a subclass of the builtin <code class="docutils literal notranslate"><span class="pre">list</span></code>
class, which provides a few additional methods.</p>
<dl class="method">
<dt id="scrapy.selector.SelectorList.xpath">
<code class="descname">xpath</code><span class="sig-paren">(</span><em>xpath</em>, <em>namespaces=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.selector.SelectorList.xpath" title="Permalink to this definition">¶</a></dt>
<dd><p>Call the <code class="docutils literal notranslate"><span class="pre">.xpath()</span></code> method for each element in this list and return
their results flattened as another <a class="reference internal" href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal notranslate"><span class="pre">SelectorList</span></code></a>.</p>
<p><code class="docutils literal notranslate"><span class="pre">query</span></code> is the same argument as the one in <a class="reference internal" href="#scrapy.selector.Selector.xpath" title="scrapy.selector.Selector.xpath"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Selector.xpath()</span></code></a></p>
<p><code class="docutils literal notranslate"><span class="pre">namespaces</span></code> is an optional <code class="docutils literal notranslate"><span class="pre">prefix:</span> <span class="pre">namespace-uri</span></code> mapping (dict)
for additional prefixes to those registered with <code class="docutils literal notranslate"><span class="pre">register_namespace(prefix,</span> <span class="pre">uri)</span></code>.
Contrary to <code class="docutils literal notranslate"><span class="pre">register_namespace()</span></code>, these prefixes are not
saved for future calls.</p>
<p>Any additional named arguments can be used to pass values for XPath
variables in the XPath expression, e.g.:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">selector</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//a[href=$url]&#39;</span><span class="p">,</span> <span class="n">url</span><span class="o">=</span><span class="s2">&quot;http://www.example.com&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.SelectorList.css">
<code class="descname">css</code><span class="sig-paren">(</span><em>query</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.selector.SelectorList.css" title="Permalink to this definition">¶</a></dt>
<dd><p>Call the <code class="docutils literal notranslate"><span class="pre">.css()</span></code> method for each element in this list and return
their results flattened as another <a class="reference internal" href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal notranslate"><span class="pre">SelectorList</span></code></a>.</p>
<p><code class="docutils literal notranslate"><span class="pre">query</span></code> is the same argument as the one in <a class="reference internal" href="#scrapy.selector.Selector.css" title="scrapy.selector.Selector.css"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Selector.css()</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.SelectorList.getall">
<code class="descname">getall</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.selector.SelectorList.getall" title="Permalink to this definition">¶</a></dt>
<dd><p>Call the <code class="docutils literal notranslate"><span class="pre">.get()</span></code> method for each element is this list and return
their results flattened, as a list of unicode strings.</p>
<p>See also: <a class="reference internal" href="#old-extraction-api"><span class="std std-ref">extract() and extract_first()</span></a></p>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.SelectorList.get">
<code class="descname">get</code><span class="sig-paren">(</span><em>default=None</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.selector.SelectorList.get" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the result of <code class="docutils literal notranslate"><span class="pre">.get()</span></code> for the first element in this list.
If the list is empty, return the default value.</p>
<p>See also: <a class="reference internal" href="#old-extraction-api"><span class="std std-ref">extract() and extract_first()</span></a></p>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.SelectorList.re">
<code class="descname">re</code><span class="sig-paren">(</span><em>regex</em>, <em>replace_entities=True</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.selector.SelectorList.re" title="Permalink to this definition">¶</a></dt>
<dd><p>Call the <code class="docutils literal notranslate"><span class="pre">.re()</span></code> method for each element in this list and return
their results flattened, as a list of unicode strings.</p>
<p>By default, character entity references are replaced by their
corresponding character (except for <code class="docutils literal notranslate"><span class="pre">&amp;amp;</span></code> and <code class="docutils literal notranslate"><span class="pre">&amp;lt;</span></code>.
Passing <code class="docutils literal notranslate"><span class="pre">replace_entities</span></code> as <code class="docutils literal notranslate"><span class="pre">False</span></code> switches off these
replacements.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.selector.SelectorList.re_first">
<code class="descname">re_first</code><span class="sig-paren">(</span><em>regex</em>, <em>default=None</em>, <em>replace_entities=True</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.selector.SelectorList.re_first" title="Permalink to this definition">¶</a></dt>
<dd><p>Call the <code class="docutils literal notranslate"><span class="pre">.re()</span></code> method for the first element in this list and
return the result in an unicode string. If the list is empty or the
regex doesn’t match anything, return the default value (<code class="docutils literal notranslate"><span class="pre">None</span></code> if
the argument is not provided).</p>
<p>By default, character entity references are replaced by their
corresponding character (except for <code class="docutils literal notranslate"><span class="pre">&amp;amp;</span></code> and <code class="docutils literal notranslate"><span class="pre">&amp;lt;</span></code>.
Passing <code class="docutils literal notranslate"><span class="pre">replace_entities</span></code> as <code class="docutils literal notranslate"><span class="pre">False</span></code> switches off these
replacements.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.selector.SelectorList.attrib">
<code class="descname">attrib</code><a class="headerlink" href="#scrapy.selector.SelectorList.attrib" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the attributes dictionary for the first element.
If the list is empty, return an empty dict.</p>
<p>See also: <a class="reference internal" href="#selecting-attributes"><span class="std std-ref">Selecting element attributes</span></a>.</p>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="examples">
<span id="selector-examples"></span><h4>Examples<a class="headerlink" href="#examples" title="Permalink to this headline">¶</a></h4>
<div class="section" id="selector-examples-on-html-response">
<span id="selector-examples-html"></span><h5>Selector examples on HTML response<a class="headerlink" href="#selector-examples-on-html-response" title="Permalink to this headline">¶</a></h5>
<p>Here are some <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a> examples to illustrate several concepts.
In all cases, we assume there is already a <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a> instantiated with
a <a class="reference internal" href="index.html#scrapy.http.HtmlResponse" title="scrapy.http.HtmlResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">HtmlResponse</span></code></a> object like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sel</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">html_response</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic">
<li><p class="first">Select all <code class="docutils literal notranslate"><span class="pre">&lt;h1&gt;</span></code> elements from an HTML response body, returning a list of
<a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a> objects (ie. a <a class="reference internal" href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal notranslate"><span class="pre">SelectorList</span></code></a> object):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//h1&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p class="first">Extract the text of all <code class="docutils literal notranslate"><span class="pre">&lt;h1&gt;</span></code> elements from an HTML response body,
returning a list of unicode strings:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//h1&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>         <span class="c1"># this includes the h1 tag</span>
<span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//h1/text()&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>  <span class="c1"># this excludes the h1 tag</span>
</pre></div>
</div>
</li>
<li><p class="first">Iterate over all <code class="docutils literal notranslate"><span class="pre">&lt;p&gt;</span></code> tags and print their class attribute:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//p&quot;</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">attrib</span><span class="p">[</span><span class="s1">&#39;class&#39;</span><span class="p">])</span>
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="selector-examples-on-xml-response">
<span id="selector-examples-xml"></span><h5>Selector examples on XML response<a class="headerlink" href="#selector-examples-on-xml-response" title="Permalink to this headline">¶</a></h5>
<p>Here are some examples to illustrate concepts for <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a> objects
instantiated with an <a class="reference internal" href="index.html#scrapy.http.XmlResponse" title="scrapy.http.XmlResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">XmlResponse</span></code></a> object:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sel</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">xml_response</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic">
<li><p class="first">Select all <code class="docutils literal notranslate"><span class="pre">&lt;product&gt;</span></code> elements from an XML response body, returning a list
of <a class="reference internal" href="#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a> objects (ie. a <a class="reference internal" href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal notranslate"><span class="pre">SelectorList</span></code></a> object):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//product&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p class="first">Extract all prices from a <a class="reference external" href="https://support.google.com/merchants/answer/160589?hl=en&amp;ref_topic=2473799">Google Base XML feed</a> which requires registering
a namespace:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sel</span><span class="o">.</span><span class="n">register_namespace</span><span class="p">(</span><span class="s2">&quot;g&quot;</span><span class="p">,</span> <span class="s2">&quot;http://base.google.com/ns/1.0&quot;</span><span class="p">)</span>
<span class="n">sel</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s2">&quot;//g:price&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
</pre></div>
</div>
</li>
</ol>
</div>
</div>
</div>
<span id="document-topics/items"></span><div class="section" id="module-scrapy.item">
<span id="items"></span><span id="topics-items"></span><h3>Items<a class="headerlink" href="#module-scrapy.item" title="Permalink to this headline">¶</a></h3>
<p>The main goal in scraping is to extract structured data from unstructured
sources, typically, web pages. Scrapy spiders can return the extracted data
as Python dicts. While convenient and familiar, Python dicts lack structure:
it is easy to make a typo in a field name or return inconsistent data,
especially in a larger project with many spiders.</p>
<p>To define common output data format Scrapy provides the <a class="reference internal" href="#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a> class.
<a class="reference internal" href="#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a> objects are simple containers used to collect the scraped data.
They provide a <a class="reference external" href="https://docs.python.org/2/library/stdtypes.html#dict">dictionary-like</a> API with a convenient syntax for declaring
their available fields.</p>
<p>Various Scrapy components use extra information provided by Items:
exporters look at declared fields to figure out columns to export,
serialization can be customized using Item fields metadata, <code class="xref py py-mod docutils literal notranslate"><span class="pre">trackref</span></code>
tracks Item instances to help find memory leaks
(see <a class="reference internal" href="index.html#topics-leaks-trackrefs"><span class="std std-ref">Debugging memory leaks with trackref</span></a>), etc.</p>
<div class="section" id="declaring-items">
<span id="topics-items-declaring"></span><h4>Declaring Items<a class="headerlink" href="#declaring-items" title="Permalink to this headline">¶</a></h4>
<p>Items are declared using a simple class definition syntax and <a class="reference internal" href="#scrapy.item.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal notranslate"><span class="pre">Field</span></code></a>
objects. Here is an example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">Product</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">price</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">stock</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">tags</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">last_updated</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">serializer</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Those familiar with <a class="reference external" href="https://www.djangoproject.com/">Django</a> will notice that Scrapy Items are
declared similar to <a class="reference external" href="https://docs.djangoproject.com/en/dev/topics/db/models/">Django Models</a>, except that Scrapy Items are much
simpler as there is no concept of different field types.</p>
</div>
</div>
<div class="section" id="item-fields">
<span id="topics-items-fields"></span><h4>Item Fields<a class="headerlink" href="#item-fields" title="Permalink to this headline">¶</a></h4>
<p><a class="reference internal" href="#scrapy.item.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal notranslate"><span class="pre">Field</span></code></a> objects are used to specify metadata for each field. For
example, the serializer function for the <code class="docutils literal notranslate"><span class="pre">last_updated</span></code> field illustrated in
the example above.</p>
<p>You can specify any kind of metadata for each field. There is no restriction on
the values accepted by <a class="reference internal" href="#scrapy.item.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal notranslate"><span class="pre">Field</span></code></a> objects. For this same
reason, there is no reference list of all available metadata keys. Each key
defined in <a class="reference internal" href="#scrapy.item.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal notranslate"><span class="pre">Field</span></code></a> objects could be used by a different component, and
only those components know about it. You can also define and use any other
<a class="reference internal" href="#scrapy.item.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal notranslate"><span class="pre">Field</span></code></a> key in your project too, for your own needs. The main goal of
<a class="reference internal" href="#scrapy.item.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal notranslate"><span class="pre">Field</span></code></a> objects is to provide a way to define all field metadata in one
place. Typically, those components whose behaviour depends on each field use
certain field keys to configure that behaviour. You must refer to their
documentation to see which metadata keys are used by each component.</p>
<p>It’s important to note that the <a class="reference internal" href="#scrapy.item.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal notranslate"><span class="pre">Field</span></code></a> objects used to declare the item
do not stay assigned as class attributes. Instead, they can be accessed through
the <a class="reference internal" href="#scrapy.item.Item.fields" title="scrapy.item.Item.fields"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Item.fields</span></code></a> attribute.</p>
</div>
<div class="section" id="working-with-items">
<h4>Working with Items<a class="headerlink" href="#working-with-items" title="Permalink to this headline">¶</a></h4>
<p>Here are some examples of common tasks performed with items, using the
<code class="docutils literal notranslate"><span class="pre">Product</span></code> item <a class="reference internal" href="#topics-items-declaring"><span class="std std-ref">declared above</span></a>. You will
notice the API is very similar to the <a class="reference external" href="https://docs.python.org/2/library/stdtypes.html#dict">dict API</a>.</p>
<div class="section" id="creating-items">
<h5>Creating items<a class="headerlink" href="#creating-items" title="Permalink to this headline">¶</a></h5>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">product</span> <span class="o">=</span> <span class="n">Product</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;Desktop PC&#39;</span><span class="p">,</span> <span class="n">price</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">product</span><span class="p">)</span>
<span class="go">Product(name=&#39;Desktop PC&#39;, price=1000)</span>
</pre></div>
</div>
</div>
<div class="section" id="getting-field-values">
<h5>Getting field values<a class="headerlink" href="#getting-field-values" title="Permalink to this headline">¶</a></h5>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span>
<span class="go">Desktop PC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">)</span>
<span class="go">Desktop PC</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="p">[</span><span class="s1">&#39;price&#39;</span><span class="p">]</span>
<span class="go">1000</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="p">[</span><span class="s1">&#39;last_updated&#39;</span><span class="p">]</span>
<span class="gt">Traceback (most recent call last):</span>
    <span class="o">...</span>
<span class="gr">KeyError</span>: <span class="n">&#39;last_updated&#39;</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;last_updated&#39;</span><span class="p">,</span> <span class="s1">&#39;not set&#39;</span><span class="p">)</span>
<span class="go">not set</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="p">[</span><span class="s1">&#39;lala&#39;</span><span class="p">]</span> <span class="c1"># getting unknown field</span>
<span class="gt">Traceback (most recent call last):</span>
    <span class="o">...</span>
<span class="gr">KeyError</span>: <span class="n">&#39;lala&#39;</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;lala&#39;</span><span class="p">,</span> <span class="s1">&#39;unknown field&#39;</span><span class="p">)</span>
<span class="go">&#39;unknown field&#39;</span>

<span class="gp">&gt;&gt;&gt; </span><span class="s1">&#39;name&#39;</span> <span class="ow">in</span> <span class="n">product</span>  <span class="c1"># is name field populated?</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="s1">&#39;last_updated&#39;</span> <span class="ow">in</span> <span class="n">product</span>  <span class="c1"># is last_updated populated?</span>
<span class="go">False</span>

<span class="gp">&gt;&gt;&gt; </span><span class="s1">&#39;last_updated&#39;</span> <span class="ow">in</span> <span class="n">product</span><span class="o">.</span><span class="n">fields</span>  <span class="c1"># is last_updated a declared field?</span>
<span class="go">True</span>

<span class="gp">&gt;&gt;&gt; </span><span class="s1">&#39;lala&#39;</span> <span class="ow">in</span> <span class="n">product</span><span class="o">.</span><span class="n">fields</span>  <span class="c1"># is lala a declared field?</span>
<span class="go">False</span>
</pre></div>
</div>
</div>
<div class="section" id="setting-field-values">
<h5>Setting field values<a class="headerlink" href="#setting-field-values" title="Permalink to this headline">¶</a></h5>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="p">[</span><span class="s1">&#39;last_updated&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;today&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="p">[</span><span class="s1">&#39;last_updated&#39;</span><span class="p">]</span>
<span class="go">today</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="p">[</span><span class="s1">&#39;lala&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;test&#39;</span> <span class="c1"># setting unknown field</span>
<span class="gt">Traceback (most recent call last):</span>
    <span class="o">...</span>
<span class="gr">KeyError</span>: <span class="n">&#39;Product does not support field: lala&#39;</span>
</pre></div>
</div>
</div>
<div class="section" id="accessing-all-populated-values">
<h5>Accessing all populated values<a class="headerlink" href="#accessing-all-populated-values" title="Permalink to this headline">¶</a></h5>
<p>To access all populated values, just use the typical <a class="reference external" href="https://docs.python.org/2/library/stdtypes.html#dict">dict API</a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
<span class="go">[&#39;price&#39;, &#39;name&#39;]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">product</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
<span class="go">[(&#39;price&#39;, 1000), (&#39;name&#39;, &#39;Desktop PC&#39;)]</span>
</pre></div>
</div>
</div>
<div class="section" id="copying-items">
<span id="id1"></span><h5>Copying items<a class="headerlink" href="#copying-items" title="Permalink to this headline">¶</a></h5>
<p>To copy an item, you must first decide whether you want a shallow copy or a
deep copy.</p>
<p>If your item contains <a class="reference external" href="https://docs.python.org/glossary.html#term-mutable">mutable</a> values like lists or dictionaries, a shallow
copy will keep references to the same mutable values across all different
copies.</p>
<p>For example, if you have an item with a list of tags, and you create a shallow
copy of that item, both the original item and the copy have the same list of
tags. Adding a tag to the list of one of the items will add the tag to the
other item as well.</p>
<p>If that is not the desired behavior, use a deep copy instead.</p>
<p>See the <a class="reference external" href="https://docs.python.org/library/copy.html">documentation of the copy module</a> for more information.</p>
<p>To create a shallow copy of an item, you can either call
<code class="xref py py-meth docutils literal notranslate"><span class="pre">copy()</span></code> on an existing item
(<code class="docutils literal notranslate"><span class="pre">product2</span> <span class="pre">=</span> <span class="pre">product.copy()</span></code>) or instantiate your item class from an existing
item (<code class="docutils literal notranslate"><span class="pre">product2</span> <span class="pre">=</span> <span class="pre">Product(product)</span></code>).</p>
<p>To create a deep copy, call <code class="xref py py-meth docutils literal notranslate"><span class="pre">deepcopy()</span></code> instead
(<code class="docutils literal notranslate"><span class="pre">product2</span> <span class="pre">=</span> <span class="pre">product.deepcopy()</span></code>).</p>
</div>
<div class="section" id="other-common-tasks">
<h5>Other common tasks<a class="headerlink" href="#other-common-tasks" title="Permalink to this headline">¶</a></h5>
<p>Creating dicts from items:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">dict</span><span class="p">(</span><span class="n">product</span><span class="p">)</span> <span class="c1"># create a dict from all populated values</span>
<span class="go">{&#39;price&#39;: 1000, &#39;name&#39;: &#39;Desktop PC&#39;}</span>
</pre></div>
</div>
<p>Creating items from dicts:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">Product</span><span class="p">({</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Laptop PC&#39;</span><span class="p">,</span> <span class="s1">&#39;price&#39;</span><span class="p">:</span> <span class="mi">1500</span><span class="p">})</span>
<span class="go">Product(price=1500, name=&#39;Laptop PC&#39;)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">Product</span><span class="p">({</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;Laptop PC&#39;</span><span class="p">,</span> <span class="s1">&#39;lala&#39;</span><span class="p">:</span> <span class="mi">1500</span><span class="p">})</span> <span class="c1"># warning: unknown field in dict</span>
<span class="gt">Traceback (most recent call last):</span>
    <span class="o">...</span>
<span class="gr">KeyError</span>: <span class="n">&#39;Product does not support field: lala&#39;</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="extending-items">
<h4>Extending Items<a class="headerlink" href="#extending-items" title="Permalink to this headline">¶</a></h4>
<p>You can extend Items (to add more fields or to change some metadata for some
fields) by declaring a subclass of your original Item.</p>
<p>For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DiscountedProduct</span><span class="p">(</span><span class="n">Product</span><span class="p">):</span>
    <span class="n">discount_percent</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">serializer</span><span class="o">=</span><span class="nb">str</span><span class="p">)</span>
    <span class="n">discount_expiration_date</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
</pre></div>
</div>
<p>You can also extend field metadata by using the previous field metadata and
appending more values, or changing existing values, like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SpecificProduct</span><span class="p">(</span><span class="n">Product</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">Product</span><span class="o">.</span><span class="n">fields</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">],</span> <span class="n">serializer</span><span class="o">=</span><span class="n">my_serializer</span><span class="p">)</span>
</pre></div>
</div>
<p>That adds (or replaces) the <code class="docutils literal notranslate"><span class="pre">serializer</span></code> metadata key for the <code class="docutils literal notranslate"><span class="pre">name</span></code> field,
keeping all the previously existing metadata values.</p>
</div>
<div class="section" id="item-objects">
<h4>Item objects<a class="headerlink" href="#item-objects" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="scrapy.item.Item">
<em class="property">class </em><code class="descclassname">scrapy.item.</code><code class="descname">Item</code><span class="sig-paren">(</span><span class="optional">[</span><em>arg</em><span class="optional">]</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.item.Item" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a new Item optionally initialized from the given argument.</p>
<p>Items replicate the standard <a class="reference external" href="https://docs.python.org/2/library/stdtypes.html#dict">dict API</a>, including its constructor. The
only additional attribute provided by Items is:</p>
<dl class="attribute">
<dt id="scrapy.item.Item.fields">
<code class="descname">fields</code><a class="headerlink" href="#scrapy.item.Item.fields" title="Permalink to this definition">¶</a></dt>
<dd><p>A dictionary containing <em>all declared fields</em> for this Item, not only
those populated. The keys are the field names and the values are the
<a class="reference internal" href="#scrapy.item.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal notranslate"><span class="pre">Field</span></code></a> objects used in the <a class="reference internal" href="#topics-items-declaring"><span class="std std-ref">Item declaration</span></a>.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="field-objects">
<h4>Field objects<a class="headerlink" href="#field-objects" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="scrapy.item.Field">
<em class="property">class </em><code class="descclassname">scrapy.item.</code><code class="descname">Field</code><span class="sig-paren">(</span><span class="optional">[</span><em>arg</em><span class="optional">]</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.item.Field" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#scrapy.item.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal notranslate"><span class="pre">Field</span></code></a> class is just an alias to the built-in <a class="reference external" href="https://docs.python.org/2/library/stdtypes.html#dict">dict</a> class and
doesn’t provide any extra functionality or attributes. In other words,
<a class="reference internal" href="#scrapy.item.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal notranslate"><span class="pre">Field</span></code></a> objects are plain-old Python dicts. A separate class is used
to support the <a class="reference internal" href="#topics-items-declaring"><span class="std std-ref">item declaration syntax</span></a>
based on class attributes.</p>
</dd></dl>

</div>
</div>
<span id="document-topics/loaders"></span><div class="section" id="module-scrapy.loader">
<span id="item-loaders"></span><span id="topics-loaders"></span><h3>Item Loaders<a class="headerlink" href="#module-scrapy.loader" title="Permalink to this headline">¶</a></h3>
<p>Item Loaders provide a convenient mechanism for populating scraped <a class="reference internal" href="index.html#topics-items"><span class="std std-ref">Items</span></a>. Even though Items can be populated using their own
dictionary-like API, Item Loaders provide a much more convenient API for
populating them from a scraping process, by automating some common tasks like
parsing the raw extracted data before assigning it.</p>
<p>In other words, <a class="reference internal" href="index.html#topics-items"><span class="std std-ref">Items</span></a> provide the <em>container</em> of
scraped data, while Item Loaders provide the mechanism for <em>populating</em> that
container.</p>
<p>Item Loaders are designed to provide a flexible, efficient and easy mechanism
for extending and overriding different field parsing rules, either by spider,
or by source format (HTML, XML, etc) without becoming a nightmare to maintain.</p>
<div class="section" id="using-item-loaders-to-populate-items">
<h4>Using Item Loaders to populate items<a class="headerlink" href="#using-item-loaders-to-populate-items" title="Permalink to this headline">¶</a></h4>
<p>To use an Item Loader, you must first instantiate it. You can either
instantiate it with a dict-like object (e.g. Item or dict) or without one, in
which case an Item is automatically instantiated in the Item Loader constructor
using the Item class specified in the <a class="reference internal" href="#scrapy.loader.ItemLoader.default_item_class" title="scrapy.loader.ItemLoader.default_item_class"><code class="xref py py-attr docutils literal notranslate"><span class="pre">ItemLoader.default_item_class</span></code></a>
attribute.</p>
<p>Then, you start collecting values into the Item Loader, typically using
<a class="reference internal" href="index.html#topics-selectors"><span class="std std-ref">Selectors</span></a>. You can add more than one value to
the same item field; the Item Loader will know how to “join” those values later
using a proper processing function.</p>
<p>Here is a typical Item Loader usage in a <a class="reference internal" href="index.html#topics-spiders"><span class="std std-ref">Spider</span></a>, using
the <a class="reference internal" href="index.html#topics-items-declaring"><span class="std std-ref">Product item</span></a> declared in the <a class="reference internal" href="index.html#topics-items"><span class="std std-ref">Items
chapter</span></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.loader</span> <span class="k">import</span> <span class="n">ItemLoader</span>
<span class="kn">from</span> <span class="nn">myproject.items</span> <span class="k">import</span> <span class="n">Product</span>

<span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="n">l</span> <span class="o">=</span> <span class="n">ItemLoader</span><span class="p">(</span><span class="n">item</span><span class="o">=</span><span class="n">Product</span><span class="p">(),</span> <span class="n">response</span><span class="o">=</span><span class="n">response</span><span class="p">)</span>
    <span class="n">l</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;//div[@class=&quot;product_name&quot;]&#39;</span><span class="p">)</span>
    <span class="n">l</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;//div[@class=&quot;product_title&quot;]&#39;</span><span class="p">)</span>
    <span class="n">l</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s1">&#39;price&#39;</span><span class="p">,</span> <span class="s1">&#39;//p[@id=&quot;price&quot;]&#39;</span><span class="p">)</span>
    <span class="n">l</span><span class="o">.</span><span class="n">add_css</span><span class="p">(</span><span class="s1">&#39;stock&#39;</span><span class="p">,</span> <span class="s1">&#39;p#stock]&#39;</span><span class="p">)</span>
    <span class="n">l</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="s1">&#39;last_updated&#39;</span><span class="p">,</span> <span class="s1">&#39;today&#39;</span><span class="p">)</span> <span class="c1"># you can also use literal values</span>
    <span class="k">return</span> <span class="n">l</span><span class="o">.</span><span class="n">load_item</span><span class="p">()</span>
</pre></div>
</div>
<p>By quickly looking at that code, we can see the <code class="docutils literal notranslate"><span class="pre">name</span></code> field is being
extracted from two different XPath locations in the page:</p>
<ol class="arabic simple">
<li><code class="docutils literal notranslate"><span class="pre">//div[&#64;class=&quot;product_name&quot;]</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">//div[&#64;class=&quot;product_title&quot;]</span></code></li>
</ol>
<p>In other words, data is being collected by extracting it from two XPath
locations, using the <a class="reference internal" href="#scrapy.loader.ItemLoader.add_xpath" title="scrapy.loader.ItemLoader.add_xpath"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_xpath()</span></code></a> method. This is the
data that will be assigned to the <code class="docutils literal notranslate"><span class="pre">name</span></code> field later.</p>
<p>Afterwards, similar calls are used for <code class="docutils literal notranslate"><span class="pre">price</span></code> and <code class="docutils literal notranslate"><span class="pre">stock</span></code> fields
(the latter using a CSS selector with the <a class="reference internal" href="#scrapy.loader.ItemLoader.add_css" title="scrapy.loader.ItemLoader.add_css"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_css()</span></code></a> method),
and finally the <code class="docutils literal notranslate"><span class="pre">last_update</span></code> field is populated directly with a literal value
(<code class="docutils literal notranslate"><span class="pre">today</span></code>) using a different method: <a class="reference internal" href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_value()</span></code></a>.</p>
<p>Finally, when all data is collected, the <a class="reference internal" href="#scrapy.loader.ItemLoader.load_item" title="scrapy.loader.ItemLoader.load_item"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ItemLoader.load_item()</span></code></a> method is
called which actually returns the item populated with the data
previously extracted and collected with the <a class="reference internal" href="#scrapy.loader.ItemLoader.add_xpath" title="scrapy.loader.ItemLoader.add_xpath"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_xpath()</span></code></a>,
<a class="reference internal" href="#scrapy.loader.ItemLoader.add_css" title="scrapy.loader.ItemLoader.add_css"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_css()</span></code></a>, and <a class="reference internal" href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_value()</span></code></a> calls.</p>
</div>
<div class="section" id="input-and-output-processors">
<span id="topics-loaders-processors"></span><h4>Input and Output processors<a class="headerlink" href="#input-and-output-processors" title="Permalink to this headline">¶</a></h4>
<p>An Item Loader contains one input processor and one output processor for each
(item) field. The input processor processes the extracted data as soon as it’s
received (through the <a class="reference internal" href="#scrapy.loader.ItemLoader.add_xpath" title="scrapy.loader.ItemLoader.add_xpath"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_xpath()</span></code></a>, <a class="reference internal" href="#scrapy.loader.ItemLoader.add_css" title="scrapy.loader.ItemLoader.add_css"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_css()</span></code></a> or
<a class="reference internal" href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_value()</span></code></a> methods) and the result of the input processor is
collected and kept inside the ItemLoader. After collecting all data, the
<a class="reference internal" href="#scrapy.loader.ItemLoader.load_item" title="scrapy.loader.ItemLoader.load_item"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ItemLoader.load_item()</span></code></a> method is called to populate and get the populated
<a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a> object.  That’s when the output processor is
called with the data previously collected (and processed using the input
processor). The result of the output processor is the final value that gets
assigned to the item.</p>
<p>Let’s see an example to illustrate how the input and output processors are
called for a particular field (the same applies for any other field):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">l</span> <span class="o">=</span> <span class="n">ItemLoader</span><span class="p">(</span><span class="n">Product</span><span class="p">(),</span> <span class="n">some_selector</span><span class="p">)</span>
<span class="n">l</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="n">xpath1</span><span class="p">)</span> <span class="c1"># (1)</span>
<span class="n">l</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="n">xpath2</span><span class="p">)</span> <span class="c1"># (2)</span>
<span class="n">l</span><span class="o">.</span><span class="n">add_css</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="n">css</span><span class="p">)</span> <span class="c1"># (3)</span>
<span class="n">l</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">)</span> <span class="c1"># (4)</span>
<span class="k">return</span> <span class="n">l</span><span class="o">.</span><span class="n">load_item</span><span class="p">()</span> <span class="c1"># (5)</span>
</pre></div>
</div>
<p>So what happens is:</p>
<ol class="arabic simple">
<li>Data from <code class="docutils literal notranslate"><span class="pre">xpath1</span></code> is extracted, and passed through the <em>input processor</em> of
the <code class="docutils literal notranslate"><span class="pre">name</span></code> field. The result of the input processor is collected and kept in
the Item Loader (but not yet assigned to the item).</li>
<li>Data from <code class="docutils literal notranslate"><span class="pre">xpath2</span></code> is extracted, and passed through the same <em>input
processor</em> used in (1). The result of the input processor is appended to the
data collected in (1) (if any).</li>
<li>This case is similar to the previous ones, except that the data is extracted
from the <code class="docutils literal notranslate"><span class="pre">css</span></code> CSS selector, and passed through the same <em>input
processor</em> used in (1) and (2). The result of the input processor is appended to the
data collected in (1) and (2) (if any).</li>
<li>This case is also similar to the previous ones, except that the value to be
collected is assigned directly, instead of being extracted from a XPath
expression or a CSS selector.
However, the value is still passed through the input processors. In this
case, since the value is not iterable it is converted to an iterable of a
single element before passing it to the input processor, because input
processor always receive iterables.</li>
<li>The data collected in steps (1), (2), (3) and (4) is passed through
the <em>output processor</em> of the <code class="docutils literal notranslate"><span class="pre">name</span></code> field.
The result of the output processor is the value assigned to the <code class="docutils literal notranslate"><span class="pre">name</span></code>
field in the item.</li>
</ol>
<p>It’s worth noticing that processors are just callable objects, which are called
with the data to be parsed, and return a parsed value. So you can use any
function as input or output processor. The only requirement is that they must
accept one (and only one) positional argument, which will be an iterator.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Both input and output processors must receive an iterator as their
first argument. The output of those functions can be anything. The result of
input processors will be appended to an internal list (in the Loader)
containing the collected values (for that field). The result of the output
processors is the value that will be finally assigned to the item.</p>
</div>
<p>If you want to use a plain function as a processor, make sure it receives
<code class="docutils literal notranslate"><span class="pre">self</span></code> as the first argument:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">lowercase_processor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">values</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">values</span><span class="p">:</span>
        <span class="k">yield</span> <span class="n">v</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>

<span class="k">class</span> <span class="nc">MyItemLoader</span><span class="p">(</span><span class="n">ItemLoader</span><span class="p">):</span>
    <span class="n">name_in</span> <span class="o">=</span> <span class="n">lowercase_processor</span>
</pre></div>
</div>
<p>This is because whenever a function is assigned as a class variable, it becomes
a method and would be passed the instance as the the first argument when being
called. See <a class="reference external" href="https://stackoverflow.com/a/35322635">this answer on stackoverflow</a> for more details.</p>
<p>The other thing you need to keep in mind is that the values returned by input
processors are collected internally (in lists) and then passed to output
processors to populate the fields.</p>
<p>Last, but not least, Scrapy comes with some <a class="reference internal" href="#topics-loaders-available-processors"><span class="std std-ref">commonly used processors</span></a> built-in for convenience.</p>
</div>
<div class="section" id="declaring-item-loaders">
<h4>Declaring Item Loaders<a class="headerlink" href="#declaring-item-loaders" title="Permalink to this headline">¶</a></h4>
<p>Item Loaders are declared like Items, by using a class definition syntax. Here
is an example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.loader</span> <span class="k">import</span> <span class="n">ItemLoader</span>
<span class="kn">from</span> <span class="nn">scrapy.loader.processors</span> <span class="k">import</span> <span class="n">TakeFirst</span><span class="p">,</span> <span class="n">MapCompose</span><span class="p">,</span> <span class="n">Join</span>

<span class="k">class</span> <span class="nc">ProductLoader</span><span class="p">(</span><span class="n">ItemLoader</span><span class="p">):</span>

    <span class="n">default_output_processor</span> <span class="o">=</span> <span class="n">TakeFirst</span><span class="p">()</span>

    <span class="n">name_in</span> <span class="o">=</span> <span class="n">MapCompose</span><span class="p">(</span><span class="n">unicode</span><span class="o">.</span><span class="n">title</span><span class="p">)</span>
    <span class="n">name_out</span> <span class="o">=</span> <span class="n">Join</span><span class="p">()</span>

    <span class="n">price_in</span> <span class="o">=</span> <span class="n">MapCompose</span><span class="p">(</span><span class="n">unicode</span><span class="o">.</span><span class="n">strip</span><span class="p">)</span>

    <span class="c1"># ...</span>
</pre></div>
</div>
<p>As you can see, input processors are declared using the <code class="docutils literal notranslate"><span class="pre">_in</span></code> suffix while
output processors are declared using the <code class="docutils literal notranslate"><span class="pre">_out</span></code> suffix. And you can also
declare a default input/output processors using the
<a class="reference internal" href="#scrapy.loader.ItemLoader.default_input_processor" title="scrapy.loader.ItemLoader.default_input_processor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">ItemLoader.default_input_processor</span></code></a> and
<a class="reference internal" href="#scrapy.loader.ItemLoader.default_output_processor" title="scrapy.loader.ItemLoader.default_output_processor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">ItemLoader.default_output_processor</span></code></a> attributes.</p>
</div>
<div class="section" id="declaring-input-and-output-processors">
<span id="topics-loaders-processors-declaring"></span><h4>Declaring Input and Output Processors<a class="headerlink" href="#declaring-input-and-output-processors" title="Permalink to this headline">¶</a></h4>
<p>As seen in the previous section, input and output processors can be declared in
the Item Loader definition, and it’s very common to declare input processors
this way. However, there is one more place where you can specify the input and
output processors to use: in the <a class="reference internal" href="index.html#topics-items-fields"><span class="std std-ref">Item Field</span></a>
metadata. Here is an example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">from</span> <span class="nn">scrapy.loader.processors</span> <span class="k">import</span> <span class="n">Join</span><span class="p">,</span> <span class="n">MapCompose</span><span class="p">,</span> <span class="n">TakeFirst</span>
<span class="kn">from</span> <span class="nn">w3lib.html</span> <span class="k">import</span> <span class="n">remove_tags</span>

<span class="k">def</span> <span class="nf">filter_price</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">value</span><span class="o">.</span><span class="n">isdigit</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">value</span>

<span class="k">class</span> <span class="nc">Product</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span>
        <span class="n">input_processor</span><span class="o">=</span><span class="n">MapCompose</span><span class="p">(</span><span class="n">remove_tags</span><span class="p">),</span>
        <span class="n">output_processor</span><span class="o">=</span><span class="n">Join</span><span class="p">(),</span>
    <span class="p">)</span>
    <span class="n">price</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span>
        <span class="n">input_processor</span><span class="o">=</span><span class="n">MapCompose</span><span class="p">(</span><span class="n">remove_tags</span><span class="p">,</span> <span class="n">filter_price</span><span class="p">),</span>
        <span class="n">output_processor</span><span class="o">=</span><span class="n">TakeFirst</span><span class="p">(),</span>
    <span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.loader</span> <span class="k">import</span> <span class="n">ItemLoader</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">il</span> <span class="o">=</span> <span class="n">ItemLoader</span><span class="p">(</span><span class="n">item</span><span class="o">=</span><span class="n">Product</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">il</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="p">[</span><span class="sa">u</span><span class="s1">&#39;Welcome to my&#39;</span><span class="p">,</span> <span class="sa">u</span><span class="s1">&#39;&lt;strong&gt;website&lt;/strong&gt;&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">il</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="s1">&#39;price&#39;</span><span class="p">,</span> <span class="p">[</span><span class="sa">u</span><span class="s1">&#39;&amp;euro;&#39;</span><span class="p">,</span> <span class="sa">u</span><span class="s1">&#39;&lt;span&gt;1000&lt;/span&gt;&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">il</span><span class="o">.</span><span class="n">load_item</span><span class="p">()</span>
<span class="go">{&#39;name&#39;: u&#39;Welcome to my website&#39;, &#39;price&#39;: u&#39;1000&#39;}</span>
</pre></div>
</div>
<p>The precedence order, for both input and output processors, is as follows:</p>
<ol class="arabic simple">
<li>Item Loader field-specific attributes: <code class="docutils literal notranslate"><span class="pre">field_in</span></code> and <code class="docutils literal notranslate"><span class="pre">field_out</span></code> (most
precedence)</li>
<li>Field metadata (<code class="docutils literal notranslate"><span class="pre">input_processor</span></code> and <code class="docutils literal notranslate"><span class="pre">output_processor</span></code> key)</li>
<li>Item Loader defaults: <a class="reference internal" href="#scrapy.loader.ItemLoader.default_input_processor" title="scrapy.loader.ItemLoader.default_input_processor"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ItemLoader.default_input_processor()</span></code></a> and
<a class="reference internal" href="#scrapy.loader.ItemLoader.default_output_processor" title="scrapy.loader.ItemLoader.default_output_processor"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ItemLoader.default_output_processor()</span></code></a> (least precedence)</li>
</ol>
<p>See also: <a class="reference internal" href="#topics-loaders-extending"><span class="std std-ref">Reusing and extending Item Loaders</span></a>.</p>
</div>
<div class="section" id="item-loader-context">
<span id="topics-loaders-context"></span><h4>Item Loader Context<a class="headerlink" href="#item-loader-context" title="Permalink to this headline">¶</a></h4>
<p>The Item Loader Context is a dict of arbitrary key/values which is shared among
all input and output processors in the Item Loader. It can be passed when
declaring, instantiating or using Item Loader. They are used to modify the
behaviour of the input/output processors.</p>
<p>For example, suppose you have a function <code class="docutils literal notranslate"><span class="pre">parse_length</span></code> which receives a text
value and extracts a length from it:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">parse_length</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">loader_context</span><span class="p">):</span>
    <span class="n">unit</span> <span class="o">=</span> <span class="n">loader_context</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;unit&#39;</span><span class="p">,</span> <span class="s1">&#39;m&#39;</span><span class="p">)</span>
    <span class="c1"># ... length parsing code goes here ...</span>
    <span class="k">return</span> <span class="n">parsed_length</span>
</pre></div>
</div>
<p>By accepting a <code class="docutils literal notranslate"><span class="pre">loader_context</span></code> argument the function is explicitly telling
the Item Loader that it’s able to receive an Item Loader context, so the Item
Loader passes the currently active context when calling it, and the processor
function (<code class="docutils literal notranslate"><span class="pre">parse_length</span></code> in this case) can thus use them.</p>
<p>There are several ways to modify Item Loader context values:</p>
<ol class="arabic">
<li><p class="first">By modifying the currently active Item Loader context
(<a class="reference internal" href="#scrapy.loader.ItemLoader.context" title="scrapy.loader.ItemLoader.context"><code class="xref py py-attr docutils literal notranslate"><span class="pre">context</span></code></a> attribute):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">loader</span> <span class="o">=</span> <span class="n">ItemLoader</span><span class="p">(</span><span class="n">product</span><span class="p">)</span>
<span class="n">loader</span><span class="o">.</span><span class="n">context</span><span class="p">[</span><span class="s1">&#39;unit&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;cm&#39;</span>
</pre></div>
</div>
</li>
<li><p class="first">On Item Loader instantiation (the keyword arguments of Item Loader
constructor are stored in the Item Loader context):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">loader</span> <span class="o">=</span> <span class="n">ItemLoader</span><span class="p">(</span><span class="n">product</span><span class="p">,</span> <span class="n">unit</span><span class="o">=</span><span class="s1">&#39;cm&#39;</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p class="first">On Item Loader declaration, for those input/output processors that support
instantiating them with an Item Loader context. <code class="xref py py-class docutils literal notranslate"><span class="pre">MapCompose</span></code> is one of
them:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ProductLoader</span><span class="p">(</span><span class="n">ItemLoader</span><span class="p">):</span>
    <span class="n">length_out</span> <span class="o">=</span> <span class="n">MapCompose</span><span class="p">(</span><span class="n">parse_length</span><span class="p">,</span> <span class="n">unit</span><span class="o">=</span><span class="s1">&#39;cm&#39;</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="itemloader-objects">
<h4>ItemLoader objects<a class="headerlink" href="#itemloader-objects" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="scrapy.loader.ItemLoader">
<em class="property">class </em><code class="descclassname">scrapy.loader.</code><code class="descname">ItemLoader</code><span class="sig-paren">(</span><span class="optional">[</span><em>item</em>, <em>selector</em>, <em>response</em>, <span class="optional">]</span><em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.ItemLoader" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a new Item Loader for populating the given Item. If no item is
given, one is instantiated automatically using the class in
<a class="reference internal" href="#scrapy.loader.ItemLoader.default_item_class" title="scrapy.loader.ItemLoader.default_item_class"><code class="xref py py-attr docutils literal notranslate"><span class="pre">default_item_class</span></code></a>.</p>
<p>When instantiated with a <code class="docutils literal notranslate"><span class="pre">selector</span></code> or a <code class="docutils literal notranslate"><span class="pre">response</span></code> parameters
the <a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">ItemLoader</span></code></a> class provides convenient mechanisms for extracting
data from web pages using <a class="reference internal" href="index.html#topics-selectors"><span class="std std-ref">selectors</span></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>item</strong> (<a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a> object) – The item instance to populate using subsequent calls to
<a class="reference internal" href="#scrapy.loader.ItemLoader.add_xpath" title="scrapy.loader.ItemLoader.add_xpath"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_xpath()</span></code></a>, <a class="reference internal" href="#scrapy.loader.ItemLoader.add_css" title="scrapy.loader.ItemLoader.add_css"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_css()</span></code></a>,
or <a class="reference internal" href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_value()</span></code></a>.</li>
<li><strong>selector</strong> (<a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a> object) – The selector to extract data from, when using the
<a class="reference internal" href="#scrapy.loader.ItemLoader.add_xpath" title="scrapy.loader.ItemLoader.add_xpath"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_xpath()</span></code></a> (resp. <a class="reference internal" href="#scrapy.loader.ItemLoader.add_css" title="scrapy.loader.ItemLoader.add_css"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_css()</span></code></a>) or <a class="reference internal" href="#scrapy.loader.ItemLoader.replace_xpath" title="scrapy.loader.ItemLoader.replace_xpath"><code class="xref py py-meth docutils literal notranslate"><span class="pre">replace_xpath()</span></code></a>
(resp. <a class="reference internal" href="#scrapy.loader.ItemLoader.replace_css" title="scrapy.loader.ItemLoader.replace_css"><code class="xref py py-meth docutils literal notranslate"><span class="pre">replace_css()</span></code></a>) method.</li>
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object) – The response used to construct the selector using the
<a class="reference internal" href="#scrapy.loader.ItemLoader.default_selector_class" title="scrapy.loader.ItemLoader.default_selector_class"><code class="xref py py-attr docutils literal notranslate"><span class="pre">default_selector_class</span></code></a>, unless the selector argument is given,
in which case this argument is ignored.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>The item, selector, response and the remaining keyword arguments are
assigned to the Loader context (accessible through the <a class="reference internal" href="#scrapy.loader.ItemLoader.context" title="scrapy.loader.ItemLoader.context"><code class="xref py py-attr docutils literal notranslate"><span class="pre">context</span></code></a> attribute).</p>
<p><a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">ItemLoader</span></code></a> instances have the following methods:</p>
<dl class="method">
<dt id="scrapy.loader.ItemLoader.get_value">
<code class="descname">get_value</code><span class="sig-paren">(</span><em>value</em>, <em>*processors</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.ItemLoader.get_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Process the given <code class="docutils literal notranslate"><span class="pre">value</span></code> by the given <code class="docutils literal notranslate"><span class="pre">processors</span></code> and keyword
arguments.</p>
<p>Available keyword arguments:</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>re</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a><em> or </em><em>compiled regex</em>) – a regular expression to use for extracting data from the
given value using <code class="xref py py-meth docutils literal notranslate"><span class="pre">extract_regex()</span></code> method,
applied before processors</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.loader.processors</span> <span class="k">import</span> <span class="n">TakeFirst</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">loader</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="sa">u</span><span class="s1">&#39;name: foo&#39;</span><span class="p">,</span> <span class="n">TakeFirst</span><span class="p">(),</span> <span class="n">unicode</span><span class="o">.</span><span class="n">upper</span><span class="p">,</span> <span class="n">re</span><span class="o">=</span><span class="s1">&#39;name: (.+)&#39;</span><span class="p">)</span>
<span class="go">&#39;FOO`</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.loader.ItemLoader.add_value">
<code class="descname">add_value</code><span class="sig-paren">(</span><em>field_name</em>, <em>value</em>, <em>*processors</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.ItemLoader.add_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Process and then add the given <code class="docutils literal notranslate"><span class="pre">value</span></code> for the given field.</p>
<p>The value is first passed through <a class="reference internal" href="#scrapy.loader.ItemLoader.get_value" title="scrapy.loader.ItemLoader.get_value"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_value()</span></code></a> by giving the
<code class="docutils literal notranslate"><span class="pre">processors</span></code> and <code class="docutils literal notranslate"><span class="pre">kwargs</span></code>, and then passed through the
<a class="reference internal" href="#topics-loaders-processors"><span class="std std-ref">field input processor</span></a> and its result
appended to the data collected for that field. If the field already
contains collected data, the new data is added.</p>
<p>The given <code class="docutils literal notranslate"><span class="pre">field_name</span></code> can be <code class="docutils literal notranslate"><span class="pre">None</span></code>, in which case values for
multiple fields may be added. And the processed value should be a dict
with field_name mapped to values.</p>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">loader</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="sa">u</span><span class="s1">&#39;Color TV&#39;</span><span class="p">)</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="s1">&#39;colours&#39;</span><span class="p">,</span> <span class="p">[</span><span class="sa">u</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="sa">u</span><span class="s1">&#39;blue&#39;</span><span class="p">])</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="s1">&#39;length&#39;</span><span class="p">,</span> <span class="sa">u</span><span class="s1">&#39;100&#39;</span><span class="p">)</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="sa">u</span><span class="s1">&#39;name: foo&#39;</span><span class="p">,</span> <span class="n">TakeFirst</span><span class="p">(),</span> <span class="n">re</span><span class="o">=</span><span class="s1">&#39;name: (.+)&#39;</span><span class="p">)</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_value</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="sa">u</span><span class="s1">&#39;foo&#39;</span><span class="p">,</span> <span class="s1">&#39;sex&#39;</span><span class="p">:</span> <span class="sa">u</span><span class="s1">&#39;male&#39;</span><span class="p">})</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.loader.ItemLoader.replace_value">
<code class="descname">replace_value</code><span class="sig-paren">(</span><em>field_name</em>, <em>value</em>, <em>*processors</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.ItemLoader.replace_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_value()</span></code></a> but replaces the collected data with the
new value instead of adding it.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.loader.ItemLoader.get_xpath">
<code class="descname">get_xpath</code><span class="sig-paren">(</span><em>xpath</em>, <em>*processors</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.ItemLoader.get_xpath" title="Permalink to this definition">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="#scrapy.loader.ItemLoader.get_value" title="scrapy.loader.ItemLoader.get_value"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ItemLoader.get_value()</span></code></a> but receives an XPath instead of a
value, which is used to extract a list of unicode strings from the
selector associated with this <a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">ItemLoader</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>xpath</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – the XPath to extract data from</li>
<li><strong>re</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a><em> or </em><em>compiled regex</em>) – a regular expression to use for extracting data from the
selected XPath region</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># HTML snippet: &lt;p class=&quot;product-name&quot;&gt;Color TV&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">get_xpath</span><span class="p">(</span><span class="s1">&#39;//p[@class=&quot;product-name&quot;]&#39;</span><span class="p">)</span>
<span class="c1"># HTML snippet: &lt;p id=&quot;price&quot;&gt;the price is $1200&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">get_xpath</span><span class="p">(</span><span class="s1">&#39;//p[@id=&quot;price&quot;]&#39;</span><span class="p">,</span> <span class="n">TakeFirst</span><span class="p">(),</span> <span class="n">re</span><span class="o">=</span><span class="s1">&#39;the price is (.*)&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.loader.ItemLoader.add_xpath">
<code class="descname">add_xpath</code><span class="sig-paren">(</span><em>field_name</em>, <em>xpath</em>, <em>*processors</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.ItemLoader.add_xpath" title="Permalink to this definition">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ItemLoader.add_value()</span></code></a> but receives an XPath instead of a
value, which is used to extract a list of unicode strings from the
selector associated with this <a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">ItemLoader</span></code></a>.</p>
<p>See <a class="reference internal" href="#scrapy.loader.ItemLoader.get_xpath" title="scrapy.loader.ItemLoader.get_xpath"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_xpath()</span></code></a> for <code class="docutils literal notranslate"><span class="pre">kwargs</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>xpath</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – the XPath to extract data from</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># HTML snippet: &lt;p class=&quot;product-name&quot;&gt;Color TV&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;//p[@class=&quot;product-name&quot;]&#39;</span><span class="p">)</span>
<span class="c1"># HTML snippet: &lt;p id=&quot;price&quot;&gt;the price is $1200&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s1">&#39;price&#39;</span><span class="p">,</span> <span class="s1">&#39;//p[@id=&quot;price&quot;]&#39;</span><span class="p">,</span> <span class="n">re</span><span class="o">=</span><span class="s1">&#39;the price is (.*)&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.loader.ItemLoader.replace_xpath">
<code class="descname">replace_xpath</code><span class="sig-paren">(</span><em>field_name</em>, <em>xpath</em>, <em>*processors</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.ItemLoader.replace_xpath" title="Permalink to this definition">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="#scrapy.loader.ItemLoader.add_xpath" title="scrapy.loader.ItemLoader.add_xpath"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_xpath()</span></code></a> but replaces collected data instead of
adding it.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.loader.ItemLoader.get_css">
<code class="descname">get_css</code><span class="sig-paren">(</span><em>css</em>, <em>*processors</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.ItemLoader.get_css" title="Permalink to this definition">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="#scrapy.loader.ItemLoader.get_value" title="scrapy.loader.ItemLoader.get_value"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ItemLoader.get_value()</span></code></a> but receives a CSS selector
instead of a value, which is used to extract a list of unicode strings
from the selector associated with this <a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">ItemLoader</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>css</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – the CSS selector to extract data from</li>
<li><strong>re</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a><em> or </em><em>compiled regex</em>) – a regular expression to use for extracting data from the
selected CSS region</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># HTML snippet: &lt;p class=&quot;product-name&quot;&gt;Color TV&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">get_css</span><span class="p">(</span><span class="s1">&#39;p.product-name&#39;</span><span class="p">)</span>
<span class="c1"># HTML snippet: &lt;p id=&quot;price&quot;&gt;the price is $1200&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">get_css</span><span class="p">(</span><span class="s1">&#39;p#price&#39;</span><span class="p">,</span> <span class="n">TakeFirst</span><span class="p">(),</span> <span class="n">re</span><span class="o">=</span><span class="s1">&#39;the price is (.*)&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.loader.ItemLoader.add_css">
<code class="descname">add_css</code><span class="sig-paren">(</span><em>field_name</em>, <em>css</em>, <em>*processors</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.ItemLoader.add_css" title="Permalink to this definition">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ItemLoader.add_value()</span></code></a> but receives a CSS selector
instead of a value, which is used to extract a list of unicode strings
from the selector associated with this <a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">ItemLoader</span></code></a>.</p>
<p>See <a class="reference internal" href="#scrapy.loader.ItemLoader.get_css" title="scrapy.loader.ItemLoader.get_css"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_css()</span></code></a> for <code class="docutils literal notranslate"><span class="pre">kwargs</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>css</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – the CSS selector to extract data from</td>
</tr>
</tbody>
</table>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># HTML snippet: &lt;p class=&quot;product-name&quot;&gt;Color TV&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_css</span><span class="p">(</span><span class="s1">&#39;name&#39;</span><span class="p">,</span> <span class="s1">&#39;p.product-name&#39;</span><span class="p">)</span>
<span class="c1"># HTML snippet: &lt;p id=&quot;price&quot;&gt;the price is $1200&lt;/p&gt;</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_css</span><span class="p">(</span><span class="s1">&#39;price&#39;</span><span class="p">,</span> <span class="s1">&#39;p#price&#39;</span><span class="p">,</span> <span class="n">re</span><span class="o">=</span><span class="s1">&#39;the price is (.*)&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.loader.ItemLoader.replace_css">
<code class="descname">replace_css</code><span class="sig-paren">(</span><em>field_name</em>, <em>css</em>, <em>*processors</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.ItemLoader.replace_css" title="Permalink to this definition">¶</a></dt>
<dd><p>Similar to <a class="reference internal" href="#scrapy.loader.ItemLoader.add_css" title="scrapy.loader.ItemLoader.add_css"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_css()</span></code></a> but replaces collected data instead of
adding it.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.loader.ItemLoader.load_item">
<code class="descname">load_item</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.ItemLoader.load_item" title="Permalink to this definition">¶</a></dt>
<dd><p>Populate the item with the data collected so far, and return it. The
data collected is first passed through the <a class="reference internal" href="#topics-loaders-processors"><span class="std std-ref">output processors</span></a> to get the final value to assign to each
item field.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.loader.ItemLoader.nested_xpath">
<code class="descname">nested_xpath</code><span class="sig-paren">(</span><em>xpath</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.ItemLoader.nested_xpath" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a nested loader with an xpath selector.
The supplied selector is applied relative to selector associated
with this <a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">ItemLoader</span></code></a>. The nested loader shares the <code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code>
with the parent <a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">ItemLoader</span></code></a> so calls to <a class="reference internal" href="#scrapy.loader.ItemLoader.add_xpath" title="scrapy.loader.ItemLoader.add_xpath"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_xpath()</span></code></a>,
<a class="reference internal" href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_value()</span></code></a>, <a class="reference internal" href="#scrapy.loader.ItemLoader.replace_value" title="scrapy.loader.ItemLoader.replace_value"><code class="xref py py-meth docutils literal notranslate"><span class="pre">replace_value()</span></code></a>, etc. will behave as expected.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.loader.ItemLoader.nested_css">
<code class="descname">nested_css</code><span class="sig-paren">(</span><em>css</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.ItemLoader.nested_css" title="Permalink to this definition">¶</a></dt>
<dd><p>Create a nested loader with a css selector.
The supplied selector is applied relative to selector associated
with this <a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">ItemLoader</span></code></a>. The nested loader shares the <code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code>
with the parent <a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">ItemLoader</span></code></a> so calls to <a class="reference internal" href="#scrapy.loader.ItemLoader.add_xpath" title="scrapy.loader.ItemLoader.add_xpath"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_xpath()</span></code></a>,
<a class="reference internal" href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value"><code class="xref py py-meth docutils literal notranslate"><span class="pre">add_value()</span></code></a>, <a class="reference internal" href="#scrapy.loader.ItemLoader.replace_value" title="scrapy.loader.ItemLoader.replace_value"><code class="xref py py-meth docutils literal notranslate"><span class="pre">replace_value()</span></code></a>, etc. will behave as expected.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.loader.ItemLoader.get_collected_values">
<code class="descname">get_collected_values</code><span class="sig-paren">(</span><em>field_name</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.ItemLoader.get_collected_values" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the collected values for the given field.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.loader.ItemLoader.get_output_value">
<code class="descname">get_output_value</code><span class="sig-paren">(</span><em>field_name</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.ItemLoader.get_output_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the collected values parsed using the output processor, for the
given field. This method doesn’t populate or modify the item at all.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.loader.ItemLoader.get_input_processor">
<code class="descname">get_input_processor</code><span class="sig-paren">(</span><em>field_name</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.ItemLoader.get_input_processor" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the input processor for the given field.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.loader.ItemLoader.get_output_processor">
<code class="descname">get_output_processor</code><span class="sig-paren">(</span><em>field_name</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.ItemLoader.get_output_processor" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the output processor for the given field.</p>
</dd></dl>

<p><a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">ItemLoader</span></code></a> instances have the following attributes:</p>
<dl class="attribute">
<dt id="scrapy.loader.ItemLoader.item">
<code class="descname">item</code><a class="headerlink" href="#scrapy.loader.ItemLoader.item" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a> object being parsed by this Item Loader.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.loader.ItemLoader.context">
<code class="descname">context</code><a class="headerlink" href="#scrapy.loader.ItemLoader.context" title="Permalink to this definition">¶</a></dt>
<dd><p>The currently active <a class="reference internal" href="#topics-loaders-context"><span class="std std-ref">Context</span></a> of this
Item Loader.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.loader.ItemLoader.default_item_class">
<code class="descname">default_item_class</code><a class="headerlink" href="#scrapy.loader.ItemLoader.default_item_class" title="Permalink to this definition">¶</a></dt>
<dd><p>An Item class (or factory), used to instantiate items when not given in
the constructor.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.loader.ItemLoader.default_input_processor">
<code class="descname">default_input_processor</code><a class="headerlink" href="#scrapy.loader.ItemLoader.default_input_processor" title="Permalink to this definition">¶</a></dt>
<dd><p>The default input processor to use for those fields which don’t specify
one.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.loader.ItemLoader.default_output_processor">
<code class="descname">default_output_processor</code><a class="headerlink" href="#scrapy.loader.ItemLoader.default_output_processor" title="Permalink to this definition">¶</a></dt>
<dd><p>The default output processor to use for those fields which don’t specify
one.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.loader.ItemLoader.default_selector_class">
<code class="descname">default_selector_class</code><a class="headerlink" href="#scrapy.loader.ItemLoader.default_selector_class" title="Permalink to this definition">¶</a></dt>
<dd><p>The class used to construct the <a class="reference internal" href="#scrapy.loader.ItemLoader.selector" title="scrapy.loader.ItemLoader.selector"><code class="xref py py-attr docutils literal notranslate"><span class="pre">selector</span></code></a> of this
<a class="reference internal" href="#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">ItemLoader</span></code></a>, if only a response is given in the constructor.
If a selector is given in the constructor this attribute is ignored.
This attribute is sometimes overridden in subclasses.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.loader.ItemLoader.selector">
<code class="descname">selector</code><a class="headerlink" href="#scrapy.loader.ItemLoader.selector" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a> object to extract data from.
It’s either the selector given in the constructor or one created from
the response given in the constructor using the
<a class="reference internal" href="#scrapy.loader.ItemLoader.default_selector_class" title="scrapy.loader.ItemLoader.default_selector_class"><code class="xref py py-attr docutils literal notranslate"><span class="pre">default_selector_class</span></code></a>. This attribute is meant to be
read-only.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="nested-loaders">
<span id="topics-loaders-nested"></span><h4>Nested Loaders<a class="headerlink" href="#nested-loaders" title="Permalink to this headline">¶</a></h4>
<p>When parsing related values from a subsection of a document, it can be
useful to create nested loaders.  Imagine you’re extracting details from
a footer of a page that looks something like:</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">footer</span><span class="o">&gt;</span>
    <span class="o">&lt;</span><span class="n">a</span> <span class="n">class</span><span class="o">=</span><span class="s2">&quot;social&quot;</span> <span class="n">href</span><span class="o">=</span><span class="s2">&quot;https://facebook.com/whatever&quot;</span><span class="o">&gt;</span><span class="n">Like</span> <span class="n">Us</span><span class="o">&lt;/</span><span class="n">a</span><span class="o">&gt;</span>
    <span class="o">&lt;</span><span class="n">a</span> <span class="n">class</span><span class="o">=</span><span class="s2">&quot;social&quot;</span> <span class="n">href</span><span class="o">=</span><span class="s2">&quot;https://twitter.com/whatever&quot;</span><span class="o">&gt;</span><span class="n">Follow</span> <span class="n">Us</span><span class="o">&lt;/</span><span class="n">a</span><span class="o">&gt;</span>
    <span class="o">&lt;</span><span class="n">a</span> <span class="n">class</span><span class="o">=</span><span class="s2">&quot;email&quot;</span> <span class="n">href</span><span class="o">=</span><span class="s2">&quot;mailto:whatever@example.com&quot;</span><span class="o">&gt;</span><span class="n">Email</span> <span class="n">Us</span><span class="o">&lt;/</span><span class="n">a</span><span class="o">&gt;</span>
<span class="o">&lt;/</span><span class="n">footer</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>Without nested loaders, you need to specify the full xpath (or css) for each value
that you wish to extract.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">loader</span> <span class="o">=</span> <span class="n">ItemLoader</span><span class="p">(</span><span class="n">item</span><span class="o">=</span><span class="n">Item</span><span class="p">())</span>
<span class="c1"># load stuff not in the footer</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s1">&#39;social&#39;</span><span class="p">,</span> <span class="s1">&#39;//footer/a[@class = &quot;social&quot;]/@href&#39;</span><span class="p">)</span>
<span class="n">loader</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s1">&#39;email&#39;</span><span class="p">,</span> <span class="s1">&#39;//footer/a[@class = &quot;email&quot;]/@href&#39;</span><span class="p">)</span>
<span class="n">loader</span><span class="o">.</span><span class="n">load_item</span><span class="p">()</span>
</pre></div>
</div>
<p>Instead, you can create a nested loader with the footer selector and add values
relative to the footer.  The functionality is the same but you avoid repeating
the footer selector.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">loader</span> <span class="o">=</span> <span class="n">ItemLoader</span><span class="p">(</span><span class="n">item</span><span class="o">=</span><span class="n">Item</span><span class="p">())</span>
<span class="c1"># load stuff not in the footer</span>
<span class="n">footer_loader</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">nested_xpath</span><span class="p">(</span><span class="s1">&#39;//footer&#39;</span><span class="p">)</span>
<span class="n">footer_loader</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s1">&#39;social&#39;</span><span class="p">,</span> <span class="s1">&#39;a[@class = &quot;social&quot;]/@href&#39;</span><span class="p">)</span>
<span class="n">footer_loader</span><span class="o">.</span><span class="n">add_xpath</span><span class="p">(</span><span class="s1">&#39;email&#39;</span><span class="p">,</span> <span class="s1">&#39;a[@class = &quot;email&quot;]/@href&#39;</span><span class="p">)</span>
<span class="c1"># no need to call footer_loader.load_item()</span>
<span class="n">loader</span><span class="o">.</span><span class="n">load_item</span><span class="p">()</span>
</pre></div>
</div>
<p>You can nest loaders arbitrarily and they work with either xpath or css selectors.
As a general guideline, use nested loaders when they make your code simpler but do
not go overboard with nesting or your parser can become difficult to read.</p>
</div>
<div class="section" id="reusing-and-extending-item-loaders">
<span id="topics-loaders-extending"></span><h4>Reusing and extending Item Loaders<a class="headerlink" href="#reusing-and-extending-item-loaders" title="Permalink to this headline">¶</a></h4>
<p>As your project grows bigger and acquires more and more spiders, maintenance
becomes a fundamental problem, especially when you have to deal with many
different parsing rules for each spider, having a lot of exceptions, but also
wanting to reuse the common processors.</p>
<p>Item Loaders are designed to ease the maintenance burden of parsing rules,
without losing flexibility and, at the same time, providing a convenient
mechanism for extending and overriding them. For this reason Item Loaders
support traditional Python class inheritance for dealing with differences of
specific spiders (or groups of spiders).</p>
<p>Suppose, for example, that some particular site encloses their product names in
three dashes (e.g. <code class="docutils literal notranslate"><span class="pre">---Plasma</span> <span class="pre">TV---</span></code>) and you don’t want to end up scraping
those dashes in the final product names.</p>
<p>Here’s how you can remove those dashes by reusing and extending the default
Product Item Loader (<code class="docutils literal notranslate"><span class="pre">ProductLoader</span></code>):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.loader.processors</span> <span class="k">import</span> <span class="n">MapCompose</span>
<span class="kn">from</span> <span class="nn">myproject.ItemLoaders</span> <span class="k">import</span> <span class="n">ProductLoader</span>

<span class="k">def</span> <span class="nf">strip_dashes</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">SiteSpecificLoader</span><span class="p">(</span><span class="n">ProductLoader</span><span class="p">):</span>
    <span class="n">name_in</span> <span class="o">=</span> <span class="n">MapCompose</span><span class="p">(</span><span class="n">strip_dashes</span><span class="p">,</span> <span class="n">ProductLoader</span><span class="o">.</span><span class="n">name_in</span><span class="p">)</span>
</pre></div>
</div>
<p>Another case where extending Item Loaders can be very helpful is when you have
multiple source formats, for example XML and HTML. In the XML version you may
want to remove <code class="docutils literal notranslate"><span class="pre">CDATA</span></code> occurrences. Here’s an example of how to do it:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.loader.processors</span> <span class="k">import</span> <span class="n">MapCompose</span>
<span class="kn">from</span> <span class="nn">myproject.ItemLoaders</span> <span class="k">import</span> <span class="n">ProductLoader</span>
<span class="kn">from</span> <span class="nn">myproject.utils.xml</span> <span class="k">import</span> <span class="n">remove_cdata</span>

<span class="k">class</span> <span class="nc">XmlProductLoader</span><span class="p">(</span><span class="n">ProductLoader</span><span class="p">):</span>
    <span class="n">name_in</span> <span class="o">=</span> <span class="n">MapCompose</span><span class="p">(</span><span class="n">remove_cdata</span><span class="p">,</span> <span class="n">ProductLoader</span><span class="o">.</span><span class="n">name_in</span><span class="p">)</span>
</pre></div>
</div>
<p>And that’s how you typically extend input processors.</p>
<p>As for output processors, it is more common to declare them in the field metadata,
as they usually depend only on the field and not on each specific site parsing
rule (as input processors do). See also:
<a class="reference internal" href="#topics-loaders-processors-declaring"><span class="std std-ref">Declaring Input and Output Processors</span></a>.</p>
<p>There are many other possible ways to extend, inherit and override your Item
Loaders, and different Item Loaders hierarchies may fit better for different
projects. Scrapy only provides the mechanism; it doesn’t impose any specific
organization of your Loaders collection - that’s up to you and your project’s
needs.</p>
</div>
<div class="section" id="module-scrapy.loader.processors">
<span id="available-built-in-processors"></span><span id="topics-loaders-available-processors"></span><h4>Available built-in processors<a class="headerlink" href="#module-scrapy.loader.processors" title="Permalink to this headline">¶</a></h4>
<p>Even though you can use any callable function as input and output processors,
Scrapy provides some commonly used processors, which are described below. Some
of them, like the <a class="reference internal" href="#scrapy.loader.processors.MapCompose" title="scrapy.loader.processors.MapCompose"><code class="xref py py-class docutils literal notranslate"><span class="pre">MapCompose</span></code></a> (which is typically used as input
processor) compose the output of several functions executed in order, to
produce the final parsed value.</p>
<p>Here is a list of all built-in processors:</p>
<dl class="class">
<dt id="scrapy.loader.processors.Identity">
<em class="property">class </em><code class="descclassname">scrapy.loader.processors.</code><code class="descname">Identity</code><a class="headerlink" href="#scrapy.loader.processors.Identity" title="Permalink to this definition">¶</a></dt>
<dd><p>The simplest processor, which doesn’t do anything. It returns the original
values unchanged. It doesn’t receive any constructor arguments, nor does it
accept Loader contexts.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.loader.processors</span> <span class="k">import</span> <span class="n">Identity</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span> <span class="o">=</span> <span class="n">Identity</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span><span class="p">([</span><span class="s1">&#39;one&#39;</span><span class="p">,</span> <span class="s1">&#39;two&#39;</span><span class="p">,</span> <span class="s1">&#39;three&#39;</span><span class="p">])</span>
<span class="go">[&#39;one&#39;, &#39;two&#39;, &#39;three&#39;]</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="scrapy.loader.processors.TakeFirst">
<em class="property">class </em><code class="descclassname">scrapy.loader.processors.</code><code class="descname">TakeFirst</code><a class="headerlink" href="#scrapy.loader.processors.TakeFirst" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the first non-null/non-empty value from the values received,
so it’s typically used as an output processor to single-valued fields.
It doesn’t receive any constructor arguments, nor does it accept Loader contexts.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.loader.processors</span> <span class="k">import</span> <span class="n">TakeFirst</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span> <span class="o">=</span> <span class="n">TakeFirst</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span><span class="p">([</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="s1">&#39;one&#39;</span><span class="p">,</span> <span class="s1">&#39;two&#39;</span><span class="p">,</span> <span class="s1">&#39;three&#39;</span><span class="p">])</span>
<span class="go">&#39;one&#39;</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="scrapy.loader.processors.Join">
<em class="property">class </em><code class="descclassname">scrapy.loader.processors.</code><code class="descname">Join</code><span class="sig-paren">(</span><em>separator=u' '</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.processors.Join" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the values joined with the separator given in the constructor, which
defaults to <code class="docutils literal notranslate"><span class="pre">u'</span> <span class="pre">'</span></code>. It doesn’t accept Loader contexts.</p>
<p>When using the default separator, this processor is equivalent to the
function: <code class="docutils literal notranslate"><span class="pre">u'</span> <span class="pre">'.join</span></code></p>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.loader.processors</span> <span class="k">import</span> <span class="n">Join</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span> <span class="o">=</span> <span class="n">Join</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span><span class="p">([</span><span class="s1">&#39;one&#39;</span><span class="p">,</span> <span class="s1">&#39;two&#39;</span><span class="p">,</span> <span class="s1">&#39;three&#39;</span><span class="p">])</span>
<span class="go">&#39;one two three&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span> <span class="o">=</span> <span class="n">Join</span><span class="p">(</span><span class="s1">&#39;&lt;br&gt;&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span><span class="p">([</span><span class="s1">&#39;one&#39;</span><span class="p">,</span> <span class="s1">&#39;two&#39;</span><span class="p">,</span> <span class="s1">&#39;three&#39;</span><span class="p">])</span>
<span class="go">&#39;one&lt;br&gt;two&lt;br&gt;three&#39;</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="scrapy.loader.processors.Compose">
<em class="property">class </em><code class="descclassname">scrapy.loader.processors.</code><code class="descname">Compose</code><span class="sig-paren">(</span><em>*functions</em>, <em>**default_loader_context</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.processors.Compose" title="Permalink to this definition">¶</a></dt>
<dd><p>A processor which is constructed from the composition of the given
functions. This means that each input value of this processor is passed to
the first function, and the result of that function is passed to the second
function, and so on, until the last function returns the output value of
this processor.</p>
<p>By default, stop process on <code class="docutils literal notranslate"><span class="pre">None</span></code> value. This behaviour can be changed by
passing keyword argument <code class="docutils literal notranslate"><span class="pre">stop_on_none=False</span></code>.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.loader.processors</span> <span class="k">import</span> <span class="n">Compose</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span> <span class="o">=</span> <span class="n">Compose</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">str</span><span class="o">.</span><span class="n">upper</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span><span class="p">([</span><span class="s1">&#39;hello&#39;</span><span class="p">,</span> <span class="s1">&#39;world&#39;</span><span class="p">])</span>
<span class="go">&#39;HELLO&#39;</span>
</pre></div>
</div>
<p>Each function can optionally receive a <code class="docutils literal notranslate"><span class="pre">loader_context</span></code> parameter. For
those which do, this processor will pass the currently active <a class="reference internal" href="#topics-loaders-context"><span class="std std-ref">Loader
context</span></a> through that parameter.</p>
<p>The keyword arguments passed in the constructor are used as the default
Loader context values passed to each function call. However, the final
Loader context values passed to functions are overridden with the currently
active Loader context accessible through the <code class="xref py py-meth docutils literal notranslate"><span class="pre">ItemLoader.context()</span></code>
attribute.</p>
</dd></dl>

<dl class="class">
<dt id="scrapy.loader.processors.MapCompose">
<em class="property">class </em><code class="descclassname">scrapy.loader.processors.</code><code class="descname">MapCompose</code><span class="sig-paren">(</span><em>*functions</em>, <em>**default_loader_context</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.processors.MapCompose" title="Permalink to this definition">¶</a></dt>
<dd><p>A processor which is constructed from the composition of the given
functions, similar to the <a class="reference internal" href="#scrapy.loader.processors.Compose" title="scrapy.loader.processors.Compose"><code class="xref py py-class docutils literal notranslate"><span class="pre">Compose</span></code></a> processor. The difference with
this processor is the way internal results are passed among functions,
which is as follows:</p>
<p>The input value of this processor is <em>iterated</em> and the first function is
applied to each element. The results of these function calls (one for each element)
are concatenated to construct a new iterable, which is then used to apply the
second function, and so on, until the last function is applied to each
value of the list of values collected so far. The output values of the last
function are concatenated together to produce the output of this processor.</p>
<p>Each particular function can return a value or a list of values, which is
flattened with the list of values returned by the same function applied to
the other input values. The functions can also return <code class="docutils literal notranslate"><span class="pre">None</span></code> in which
case the output of that function is ignored for further processing over the
chain.</p>
<p>This processor provides a convenient way to compose functions that only
work with single values (instead of iterables). For this reason the
<a class="reference internal" href="#scrapy.loader.processors.MapCompose" title="scrapy.loader.processors.MapCompose"><code class="xref py py-class docutils literal notranslate"><span class="pre">MapCompose</span></code></a> processor is typically used as input processor, since
data is often extracted using the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">extract()</span></code> method of <a class="reference internal" href="index.html#topics-selectors"><span class="std std-ref">selectors</span></a>, which returns a list of unicode strings.</p>
<p>The example below should clarify how it works:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">filter_world</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">x</span> <span class="o">==</span> <span class="s1">&#39;world&#39;</span> <span class="k">else</span> <span class="n">x</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.loader.processors</span> <span class="k">import</span> <span class="n">MapCompose</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span> <span class="o">=</span> <span class="n">MapCompose</span><span class="p">(</span><span class="n">filter_world</span><span class="p">,</span> <span class="nb">str</span><span class="o">.</span><span class="n">upper</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span><span class="p">([</span><span class="s1">&#39;hello&#39;</span><span class="p">,</span> <span class="s1">&#39;world&#39;</span><span class="p">,</span> <span class="s1">&#39;this&#39;</span><span class="p">,</span> <span class="s1">&#39;is&#39;</span><span class="p">,</span> <span class="s1">&#39;scrapy&#39;</span><span class="p">])</span>
<span class="go">[&#39;HELLO, &#39;THIS&#39;, &#39;IS&#39;, &#39;SCRAPY&#39;]</span>
</pre></div>
</div>
<p>As with the Compose processor, functions can receive Loader contexts, and
constructor keyword arguments are used as default context values. See
<a class="reference internal" href="#scrapy.loader.processors.Compose" title="scrapy.loader.processors.Compose"><code class="xref py py-class docutils literal notranslate"><span class="pre">Compose</span></code></a> processor for more info.</p>
</dd></dl>

<dl class="class">
<dt id="scrapy.loader.processors.SelectJmes">
<em class="property">class </em><code class="descclassname">scrapy.loader.processors.</code><code class="descname">SelectJmes</code><span class="sig-paren">(</span><em>json_path</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.loader.processors.SelectJmes" title="Permalink to this definition">¶</a></dt>
<dd><p>Queries the value using the json path provided to the constructor and returns the output.
Requires jmespath (<a class="reference external" href="https://github.com/jmespath/jmespath.py">https://github.com/jmespath/jmespath.py</a>) to run.
This processor takes only one input at a time.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.loader.processors</span> <span class="k">import</span> <span class="n">SelectJmes</span><span class="p">,</span> <span class="n">Compose</span><span class="p">,</span> <span class="n">MapCompose</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span> <span class="o">=</span> <span class="n">SelectJmes</span><span class="p">(</span><span class="s2">&quot;foo&quot;</span><span class="p">)</span> <span class="c1">#for direct use on lists and dictionaries</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span><span class="p">({</span><span class="s1">&#39;foo&#39;</span><span class="p">:</span> <span class="s1">&#39;bar&#39;</span><span class="p">})</span>
<span class="go">&#39;bar&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc</span><span class="p">({</span><span class="s1">&#39;foo&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;bar&#39;</span><span class="p">:</span> <span class="s1">&#39;baz&#39;</span><span class="p">}})</span>
<span class="go">{&#39;bar&#39;: &#39;baz&#39;}</span>
</pre></div>
</div>
<p>Working with Json:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">json</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc_single_json_str</span> <span class="o">=</span> <span class="n">Compose</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">,</span> <span class="n">SelectJmes</span><span class="p">(</span><span class="s2">&quot;foo&quot;</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc_single_json_str</span><span class="p">(</span><span class="s1">&#39;{&quot;foo&quot;: &quot;bar&quot;}&#39;</span><span class="p">)</span>
<span class="go">&#39;bar&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc_json_list</span> <span class="o">=</span> <span class="n">Compose</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">,</span> <span class="n">MapCompose</span><span class="p">(</span><span class="n">SelectJmes</span><span class="p">(</span><span class="s1">&#39;foo&#39;</span><span class="p">)))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">proc_json_list</span><span class="p">(</span><span class="s1">&#39;[{&quot;foo&quot;:&quot;bar&quot;}, {&quot;baz&quot;:&quot;tar&quot;}]&#39;</span><span class="p">)</span>
<span class="go">[&#39;bar&#39;]</span>
</pre></div>
</div>
</dd></dl>

</div>
</div>
<span id="document-topics/shell"></span><div class="section" id="scrapy-shell">
<span id="topics-shell"></span><h3>Scrapy shell<a class="headerlink" href="#scrapy-shell" title="Permalink to this headline">¶</a></h3>
<p>The Scrapy shell is an interactive shell where you can try and debug your
scraping code very quickly, without having to run the spider. It’s meant to be
used for testing data extraction code, but you can actually use it for testing
any kind of code as it is also a regular Python shell.</p>
<p>The shell is used for testing XPath or CSS expressions and see how they work
and what data they extract from the web pages you’re trying to scrape. It
allows you to interactively test your expressions while you’re writing your
spider, without having to run the spider to test every change.</p>
<p>Once you get familiarized with the Scrapy shell, you’ll see that it’s an
invaluable tool for developing and debugging your spiders.</p>
<div class="section" id="configuring-the-shell">
<h4>Configuring the shell<a class="headerlink" href="#configuring-the-shell" title="Permalink to this headline">¶</a></h4>
<p>If you have <a class="reference external" href="https://ipython.org/">IPython</a> installed, the Scrapy shell will use it (instead of the
standard Python console). The <a class="reference external" href="https://ipython.org/">IPython</a> console is much more powerful and
provides smart auto-completion and colorized output, among other things.</p>
<p>We highly recommend you install <a class="reference external" href="https://ipython.org/">IPython</a>, specially if you’re working on
Unix systems (where <a class="reference external" href="https://ipython.org/">IPython</a> excels). See the <a class="reference external" href="https://ipython.org/install.html">IPython installation guide</a>
for more info.</p>
<p>Scrapy also has support for <a class="reference external" href="https://www.bpython-interpreter.org/">bpython</a>, and will try to use it where <a class="reference external" href="https://ipython.org/">IPython</a>
is unavailable.</p>
<p>Through scrapy’s settings you can configure it to use any one of
<code class="docutils literal notranslate"><span class="pre">ipython</span></code>, <code class="docutils literal notranslate"><span class="pre">bpython</span></code> or the standard <code class="docutils literal notranslate"><span class="pre">python</span></code> shell, regardless of which
are installed. This is done by setting the <code class="docutils literal notranslate"><span class="pre">SCRAPY_PYTHON_SHELL</span></code> environment
variable; or by defining it in your <a class="reference internal" href="index.html#topics-config-settings"><span class="std std-ref">scrapy.cfg</span></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">settings</span><span class="p">]</span>
<span class="n">shell</span> <span class="o">=</span> <span class="n">bpython</span>
</pre></div>
</div>
</div>
<div class="section" id="launch-the-shell">
<h4>Launch the shell<a class="headerlink" href="#launch-the-shell" title="Permalink to this headline">¶</a></h4>
<p>To launch the Scrapy shell you can use the <a class="reference internal" href="index.html#std:command-shell"><code class="xref std std-command docutils literal notranslate"><span class="pre">shell</span></code></a> command like
this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">shell</span> <span class="o">&lt;</span><span class="n">url</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>Where the <code class="docutils literal notranslate"><span class="pre">&lt;url&gt;</span></code> is the URL you want to scrape.</p>
<p><a class="reference internal" href="index.html#std:command-shell"><code class="xref std std-command docutils literal notranslate"><span class="pre">shell</span></code></a> also works for local files. This can be handy if you want
to play around with a local copy of a web page. <a class="reference internal" href="index.html#std:command-shell"><code class="xref std std-command docutils literal notranslate"><span class="pre">shell</span></code></a> understands
the following syntaxes for local files:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># UNIX-style</span>
<span class="n">scrapy</span> <span class="n">shell</span> <span class="o">./</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">file</span><span class="o">.</span><span class="n">html</span>
<span class="n">scrapy</span> <span class="n">shell</span> <span class="o">../</span><span class="n">other</span><span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">file</span><span class="o">.</span><span class="n">html</span>
<span class="n">scrapy</span> <span class="n">shell</span> <span class="o">/</span><span class="n">absolute</span><span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">file</span><span class="o">.</span><span class="n">html</span>

<span class="c1"># File URI</span>
<span class="n">scrapy</span> <span class="n">shell</span> <span class="n">file</span><span class="p">:</span><span class="o">///</span><span class="n">absolute</span><span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">file</span><span class="o">.</span><span class="n">html</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>When using relative file paths, be explicit and prepend them
with <code class="docutils literal notranslate"><span class="pre">./</span></code> (or <code class="docutils literal notranslate"><span class="pre">../</span></code> when relevant).
<code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">shell</span> <span class="pre">index.html</span></code> will not work as one might expect (and
this is by design, not a bug).</p>
<p>Because <a class="reference internal" href="index.html#std:command-shell"><code class="xref std std-command docutils literal notranslate"><span class="pre">shell</span></code></a> favors HTTP URLs over File URIs,
and <code class="docutils literal notranslate"><span class="pre">index.html</span></code> being syntactically similar to <code class="docutils literal notranslate"><span class="pre">example.com</span></code>,
<a class="reference internal" href="index.html#std:command-shell"><code class="xref std std-command docutils literal notranslate"><span class="pre">shell</span></code></a> will treat <code class="docutils literal notranslate"><span class="pre">index.html</span></code> as a domain name and trigger
a DNS lookup error:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ scrapy shell index.html
[ ... scrapy shell starts ... ]
[ ... traceback ... ]
twisted.internet.error.DNSLookupError: DNS lookup failed:
address &#39;index.html&#39; not found: [Errno -5] No address associated with hostname.
</pre></div>
</div>
<p class="last"><a class="reference internal" href="index.html#std:command-shell"><code class="xref std std-command docutils literal notranslate"><span class="pre">shell</span></code></a> will not test beforehand if a file called <code class="docutils literal notranslate"><span class="pre">index.html</span></code>
exists in the current directory. Again, be explicit.</p>
</div>
</div>
<div class="section" id="using-the-shell">
<h4>Using the shell<a class="headerlink" href="#using-the-shell" title="Permalink to this headline">¶</a></h4>
<p>The Scrapy shell is just a regular Python console (or <a class="reference external" href="https://ipython.org/">IPython</a> console if you
have it available) which provides some additional shortcut functions for
convenience.</p>
<div class="section" id="available-shortcuts">
<h5>Available Shortcuts<a class="headerlink" href="#available-shortcuts" title="Permalink to this headline">¶</a></h5>
<blockquote>
<div><ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">shelp()</span></code> - print a help with the list of available objects and shortcuts</li>
<li><code class="docutils literal notranslate"><span class="pre">fetch(url[,</span> <span class="pre">redirect=True])</span></code> - fetch a new response from the given
URL and update all related objects accordingly. You can optionaly ask for
HTTP 3xx redirections to not be followed by passing <code class="docutils literal notranslate"><span class="pre">redirect=False</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">fetch(request)</span></code> - fetch a new response from the given request and
update all related objects accordingly.</li>
<li><code class="docutils literal notranslate"><span class="pre">view(response)</span></code> - open the given response in your local web browser, for
inspection. This will add a <a class="reference external" href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/base">&lt;base&gt; tag</a> to the response body in order
for external links (such as images and style sheets) to display properly.
Note, however, that this will create a temporary file in your computer,
which won’t be removed automatically.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="available-scrapy-objects">
<h5>Available Scrapy objects<a class="headerlink" href="#available-scrapy-objects" title="Permalink to this headline">¶</a></h5>
<p>The Scrapy shell automatically creates some convenient objects from the
downloaded page, like the <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object and the
<a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a> objects (for both HTML and XML
content).</p>
<p>Those objects are:</p>
<blockquote>
<div><ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">crawler</span></code> - the current <a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> object.</li>
<li><code class="docutils literal notranslate"><span class="pre">spider</span></code> - the Spider which is known to handle the URL, or a
<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object if there is no spider found for
the current URL</li>
<li><code class="docutils literal notranslate"><span class="pre">request</span></code> - a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object of the last fetched
page. You can modify this request using <a class="reference internal" href="index.html#scrapy.http.Request.replace" title="scrapy.http.Request.replace"><code class="xref py py-meth docutils literal notranslate"><span class="pre">replace()</span></code></a>
or fetch a new request (without leaving the shell) using the <code class="docutils literal notranslate"><span class="pre">fetch</span></code>
shortcut.</li>
<li><code class="docutils literal notranslate"><span class="pre">response</span></code> - a <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object containing the last
fetched page</li>
<li><code class="docutils literal notranslate"><span class="pre">settings</span></code> - the current <a class="reference internal" href="index.html#topics-settings"><span class="std std-ref">Scrapy settings</span></a></li>
</ul>
</div></blockquote>
</div>
</div>
<div class="section" id="example-of-shell-session">
<h4>Example of shell session<a class="headerlink" href="#example-of-shell-session" title="Permalink to this headline">¶</a></h4>
<p>Here’s an example of a typical shell session where we start by scraping the
<a class="reference external" href="https://scrapy.org">https://scrapy.org</a> page, and then proceed to scrape the <a class="reference external" href="https://reddit.com">https://reddit.com</a>
page. Finally, we modify the (Reddit) request method to POST and re-fetch it
getting an error. We end the session by typing Ctrl-D (in Unix systems) or
Ctrl-Z in Windows.</p>
<p>Keep in mind that the data extracted here may not be the same when you try it,
as those pages are not static and could have changed by the time you test this.
The only purpose of this example is to get you familiarized with how the Scrapy
shell works.</p>
<p>First, we launch the shell:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">shell</span> <span class="s1">&#39;https://scrapy.org&#39;</span> <span class="o">--</span><span class="n">nolog</span>
</pre></div>
</div>
<p>Then, the shell fetches the URL (using the Scrapy downloader) and prints the
list of available objects and useful shortcuts (you’ll notice that these lines
all start with the <code class="docutils literal notranslate"><span class="pre">[s]</span></code> prefix):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="n">Available</span> <span class="n">Scrapy</span> <span class="n">objects</span><span class="p">:</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">scrapy</span>     <span class="n">scrapy</span> <span class="n">module</span> <span class="p">(</span><span class="n">contains</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">,</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Selector</span><span class="p">,</span> <span class="n">etc</span><span class="p">)</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">crawler</span>    <span class="o">&lt;</span><span class="n">scrapy</span><span class="o">.</span><span class="n">crawler</span><span class="o">.</span><span class="n">Crawler</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x7f07395dd690</span><span class="o">&gt;</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">item</span>       <span class="p">{}</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">request</span>    <span class="o">&lt;</span><span class="n">GET</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">scrapy</span><span class="o">.</span><span class="n">org</span><span class="o">&gt;</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">response</span>   <span class="o">&lt;</span><span class="mi">200</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">scrapy</span><span class="o">.</span><span class="n">org</span><span class="o">/&gt;</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">settings</span>   <span class="o">&lt;</span><span class="n">scrapy</span><span class="o">.</span><span class="n">settings</span><span class="o">.</span><span class="n">Settings</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x7f07395dd710</span><span class="o">&gt;</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">spider</span>     <span class="o">&lt;</span><span class="n">DefaultSpider</span> <span class="s1">&#39;default&#39;</span> <span class="n">at</span> <span class="mh">0x7f0735891690</span><span class="o">&gt;</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="n">Useful</span> <span class="n">shortcuts</span><span class="p">:</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">fetch</span><span class="p">(</span><span class="n">url</span><span class="p">[,</span> <span class="n">redirect</span><span class="o">=</span><span class="kc">True</span><span class="p">])</span> <span class="n">Fetch</span> <span class="n">URL</span> <span class="ow">and</span> <span class="n">update</span> <span class="n">local</span> <span class="n">objects</span> <span class="p">(</span><span class="n">by</span> <span class="n">default</span><span class="p">,</span> <span class="n">redirects</span> <span class="n">are</span> <span class="n">followed</span><span class="p">)</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">fetch</span><span class="p">(</span><span class="n">req</span><span class="p">)</span>                  <span class="n">Fetch</span> <span class="n">a</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span> <span class="ow">and</span> <span class="n">update</span> <span class="n">local</span> <span class="n">objects</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">shelp</span><span class="p">()</span>           <span class="n">Shell</span> <span class="n">help</span> <span class="p">(</span><span class="nb">print</span> <span class="n">this</span> <span class="n">help</span><span class="p">)</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">view</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>    <span class="n">View</span> <span class="n">response</span> <span class="ow">in</span> <span class="n">a</span> <span class="n">browser</span>

<span class="o">&gt;&gt;&gt;</span>
</pre></div>
</div>
<p>After that, we can start playing with the objects:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//title/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;Scrapy | A Fast and Powerful Scraping and Web Crawling Framework&#39;</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">fetch</span><span class="p">(</span><span class="s2">&quot;https://reddit.com&quot;</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//title/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;reddit: the front page of the internet&#39;</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">request</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s2">&quot;POST&quot;</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">fetch</span><span class="p">(</span><span class="n">request</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">status</span>
<span class="go">404</span>

<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pprint</span> <span class="k">import</span> <span class="n">pprint</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">pprint</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">headers</span><span class="p">)</span>
<span class="go">{&#39;Accept-Ranges&#39;: [&#39;bytes&#39;],</span>
<span class="go"> &#39;Cache-Control&#39;: [&#39;max-age=0, must-revalidate&#39;],</span>
<span class="go"> &#39;Content-Type&#39;: [&#39;text/html; charset=UTF-8&#39;],</span>
<span class="go"> &#39;Date&#39;: [&#39;Thu, 08 Dec 2016 16:21:19 GMT&#39;],</span>
<span class="go"> &#39;Server&#39;: [&#39;snooserv&#39;],</span>
<span class="go"> &#39;Set-Cookie&#39;: [&#39;loid=KqNLou0V9SKMX4qb4n; Domain=reddit.com; Max-Age=63071999; Path=/; expires=Sat, 08-Dec-2018 16:21:19 GMT; secure&#39;,</span>
<span class="go">                &#39;loidcreated=2016-12-08T16%3A21%3A19.445Z; Domain=reddit.com; Max-Age=63071999; Path=/; expires=Sat, 08-Dec-2018 16:21:19 GMT; secure&#39;,</span>
<span class="go">                &#39;loid=vi0ZVe4NkxNWdlH7r7; Domain=reddit.com; Max-Age=63071999; Path=/; expires=Sat, 08-Dec-2018 16:21:19 GMT; secure&#39;,</span>
<span class="go">                &#39;loidcreated=2016-12-08T16%3A21%3A19.459Z; Domain=reddit.com; Max-Age=63071999; Path=/; expires=Sat, 08-Dec-2018 16:21:19 GMT; secure&#39;],</span>
<span class="go"> &#39;Vary&#39;: [&#39;accept-encoding&#39;],</span>
<span class="go"> &#39;Via&#39;: [&#39;1.1 varnish&#39;],</span>
<span class="go"> &#39;X-Cache&#39;: [&#39;MISS&#39;],</span>
<span class="go"> &#39;X-Cache-Hits&#39;: [&#39;0&#39;],</span>
<span class="go"> &#39;X-Content-Type-Options&#39;: [&#39;nosniff&#39;],</span>
<span class="go"> &#39;X-Frame-Options&#39;: [&#39;SAMEORIGIN&#39;],</span>
<span class="go"> &#39;X-Moose&#39;: [&#39;majestic&#39;],</span>
<span class="go"> &#39;X-Served-By&#39;: [&#39;cache-cdg8730-CDG&#39;],</span>
<span class="go"> &#39;X-Timer&#39;: [&#39;S1481214079.394283,VS0,VE159&#39;],</span>
<span class="go"> &#39;X-Ua-Compatible&#39;: [&#39;IE=edge&#39;],</span>
<span class="go"> &#39;X-Xss-Protection&#39;: [&#39;1; mode=block&#39;]}</span>
<span class="go">&gt;&gt;&gt;</span>
</pre></div>
</div>
</div>
<div class="section" id="invoking-the-shell-from-spiders-to-inspect-responses">
<span id="topics-shell-inspect-response"></span><h4>Invoking the shell from spiders to inspect responses<a class="headerlink" href="#invoking-the-shell-from-spiders-to-inspect-responses" title="Permalink to this headline">¶</a></h4>
<p>Sometimes you want to inspect the responses that are being processed in a
certain point of your spider, if only to check that response you expect is
getting there.</p>
<p>This can be achieved by using the <code class="docutils literal notranslate"><span class="pre">scrapy.shell.inspect_response</span></code> function.</p>
<p>Here’s an example of how you would call it from your spider:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;myspider&quot;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;http://example.com&quot;</span><span class="p">,</span>
        <span class="s2">&quot;http://example.org&quot;</span><span class="p">,</span>
        <span class="s2">&quot;http://example.net&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="c1"># We want to inspect one specific response.</span>
        <span class="k">if</span> <span class="s2">&quot;.org&quot;</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">scrapy.shell</span> <span class="k">import</span> <span class="n">inspect_response</span>
            <span class="n">inspect_response</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

        <span class="c1"># Rest of parsing code.</span>
</pre></div>
</div>
<p>When you run the spider, you will get something similar to this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">2014</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">23</span> <span class="mi">17</span><span class="p">:</span><span class="mi">48</span><span class="p">:</span><span class="mi">31</span><span class="o">-</span><span class="mi">0400</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">engine</span><span class="p">]</span> <span class="n">DEBUG</span><span class="p">:</span> <span class="n">Crawled</span> <span class="p">(</span><span class="mi">200</span><span class="p">)</span> <span class="o">&lt;</span><span class="n">GET</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">example</span><span class="o">.</span><span class="n">com</span><span class="o">&gt;</span> <span class="p">(</span><span class="n">referer</span><span class="p">:</span> <span class="kc">None</span><span class="p">)</span>
<span class="mi">2014</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">23</span> <span class="mi">17</span><span class="p">:</span><span class="mi">48</span><span class="p">:</span><span class="mi">31</span><span class="o">-</span><span class="mi">0400</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">engine</span><span class="p">]</span> <span class="n">DEBUG</span><span class="p">:</span> <span class="n">Crawled</span> <span class="p">(</span><span class="mi">200</span><span class="p">)</span> <span class="o">&lt;</span><span class="n">GET</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">example</span><span class="o">.</span><span class="n">org</span><span class="o">&gt;</span> <span class="p">(</span><span class="n">referer</span><span class="p">:</span> <span class="kc">None</span><span class="p">)</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="n">Available</span> <span class="n">Scrapy</span> <span class="n">objects</span><span class="p">:</span>
<span class="p">[</span><span class="n">s</span><span class="p">]</span>   <span class="n">crawler</span>    <span class="o">&lt;</span><span class="n">scrapy</span><span class="o">.</span><span class="n">crawler</span><span class="o">.</span><span class="n">Crawler</span> <span class="nb">object</span> <span class="n">at</span> <span class="mh">0x1e16b50</span><span class="o">&gt;</span>
<span class="o">...</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span>
<span class="s1">&#39;http://example.org&#39;</span>
</pre></div>
</div>
<p>Then, you can check if the extraction code is working:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//h1[@class=&quot;fn&quot;]&#39;</span><span class="p">)</span>
<span class="go">[]</span>
</pre></div>
</div>
<p>Nope, it doesn’t. So you can open the response in your web browser and see if
it’s the response you were expecting:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">view</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
<span class="go">True</span>
</pre></div>
</div>
<p>Finally you hit Ctrl-D (or Ctrl-Z in Windows) to exit the shell and resume the
crawling:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="o">^</span><span class="n">D</span>
<span class="go">2014-01-23 17:50:03-0400 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://example.net&gt; (referer: None)</span>
<span class="gp">...</span>
</pre></div>
</div>
<p>Note that you can’t use the <code class="docutils literal notranslate"><span class="pre">fetch</span></code> shortcut here since the Scrapy engine is
blocked by the shell. However, after you leave the shell, the spider will
continue crawling where it stopped, as shown above.</p>
</div>
</div>
<span id="document-topics/item-pipeline"></span><div class="section" id="item-pipeline">
<span id="topics-item-pipeline"></span><h3>Item Pipeline<a class="headerlink" href="#item-pipeline" title="Permalink to this headline">¶</a></h3>
<p>After an item has been scraped by a spider, it is sent to the Item Pipeline
which processes it through several components that are executed sequentially.</p>
<p>Each item pipeline component (sometimes referred as just “Item Pipeline”) is a
Python class that implements a simple method. They receive an item and perform
an action over it, also deciding if the item should continue through the
pipeline or be dropped and no longer processed.</p>
<p>Typical uses of item pipelines are:</p>
<ul class="simple">
<li>cleansing HTML data</li>
<li>validating scraped data (checking that the items contain certain fields)</li>
<li>checking for duplicates (and dropping them)</li>
<li>storing the scraped item in a database</li>
</ul>
<div class="section" id="writing-your-own-item-pipeline">
<h4>Writing your own item pipeline<a class="headerlink" href="#writing-your-own-item-pipeline" title="Permalink to this headline">¶</a></h4>
<p>Each item pipeline component is a Python class that must implement the following method:</p>
<dl class="method">
<dt id="process_item">
<code class="descname">process_item</code><span class="sig-paren">(</span><em>self</em>, <em>item</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#process_item" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called for every item pipeline component. <a class="reference internal" href="#process_item" title="process_item"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_item()</span></code></a>
must either: return a dict with data, return an <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a>
(or any descendant class) object, return a <a class="reference external" href="https://twistedmatrix.com/documents/current/core/howto/defer.html">Twisted Deferred</a> or raise
<a class="reference internal" href="index.html#scrapy.exceptions.DropItem" title="scrapy.exceptions.DropItem"><code class="xref py py-exc docutils literal notranslate"><span class="pre">DropItem</span></code></a> exception. Dropped items are no longer
processed by further pipeline components.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>item</strong> (<a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a> object or a dict) – the item scraped</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider which scraped the item</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<p>Additionally, they may also implement the following methods:</p>
<dl class="method">
<dt id="open_spider">
<code class="descname">open_spider</code><span class="sig-paren">(</span><em>self</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#open_spider" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called when the spider is opened.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider which was opened</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="close_spider">
<code class="descname">close_spider</code><span class="sig-paren">(</span><em>self</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#close_spider" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called when the spider is closed.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider which was closed</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="from_crawler">
<code class="descname">from_crawler</code><span class="sig-paren">(</span><em>cls</em>, <em>crawler</em><span class="sig-paren">)</span><a class="headerlink" href="#from_crawler" title="Permalink to this definition">¶</a></dt>
<dd><p>If present, this classmethod is called to create a pipeline instance
from a <a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a>. It must return a new instance
of the pipeline. Crawler object provides access to all Scrapy core
components like settings and signals; it is a way for pipeline to
access them and hook its functionality into Scrapy.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>crawler</strong> (<a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> object) – crawler that uses this pipeline</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="item-pipeline-example">
<h4>Item pipeline example<a class="headerlink" href="#item-pipeline-example" title="Permalink to this headline">¶</a></h4>
<div class="section" id="price-validation-and-dropping-items-with-no-prices">
<h5>Price validation and dropping items with no prices<a class="headerlink" href="#price-validation-and-dropping-items-with-no-prices" title="Permalink to this headline">¶</a></h5>
<p>Let’s take a look at the following hypothetical pipeline that adjusts the
<code class="docutils literal notranslate"><span class="pre">price</span></code> attribute for those items that do not include VAT
(<code class="docutils literal notranslate"><span class="pre">price_excludes_vat</span></code> attribute), and drops those items which don’t
contain a price:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.exceptions</span> <span class="k">import</span> <span class="n">DropItem</span>

<span class="k">class</span> <span class="nc">PricePipeline</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="n">vat_factor</span> <span class="o">=</span> <span class="mf">1.15</span>

    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">item</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;price&#39;</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">item</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;price_excludes_vat&#39;</span><span class="p">):</span>
                <span class="n">item</span><span class="p">[</span><span class="s1">&#39;price&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">item</span><span class="p">[</span><span class="s1">&#39;price&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">vat_factor</span>
            <span class="k">return</span> <span class="n">item</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">DropItem</span><span class="p">(</span><span class="s2">&quot;Missing price in </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">item</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="write-items-to-a-json-file">
<h5>Write items to a JSON file<a class="headerlink" href="#write-items-to-a-json-file" title="Permalink to this headline">¶</a></h5>
<p>The following pipeline stores all scraped items (from all spiders) into a
single <code class="docutils literal notranslate"><span class="pre">items.jl</span></code> file, containing one item per line serialized in JSON
format:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">json</span>

<span class="k">class</span> <span class="nc">JsonWriterPipeline</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">open_spider</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">file</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;items.jl&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">close_spider</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">file</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="n">line</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">item</span><span class="p">))</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The purpose of JsonWriterPipeline is just to introduce how to write
item pipelines. If you really want to store all scraped items into a JSON
file you should use the <a class="reference internal" href="index.html#topics-feed-exports"><span class="std std-ref">Feed exports</span></a>.</p>
</div>
</div>
<div class="section" id="write-items-to-mongodb">
<h5>Write items to MongoDB<a class="headerlink" href="#write-items-to-mongodb" title="Permalink to this headline">¶</a></h5>
<p>In this example we’ll write items to <a class="reference external" href="https://www.mongodb.org/">MongoDB</a> using <a class="reference external" href="https://api.mongodb.org/python/current/">pymongo</a>.
MongoDB address and database name are specified in Scrapy settings;
MongoDB collection is named after item class.</p>
<p>The main point of this example is to show how to use <a class="reference internal" href="#from_crawler" title="from_crawler"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_crawler()</span></code></a>
method and how to clean up the resources properly.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pymongo</span>

<span class="k">class</span> <span class="nc">MongoPipeline</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="n">collection_name</span> <span class="o">=</span> <span class="s1">&#39;scrapy_items&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mongo_uri</span><span class="p">,</span> <span class="n">mongo_db</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mongo_uri</span> <span class="o">=</span> <span class="n">mongo_uri</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mongo_db</span> <span class="o">=</span> <span class="n">mongo_db</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_crawler</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">crawler</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span>
            <span class="n">mongo_uri</span><span class="o">=</span><span class="n">crawler</span><span class="o">.</span><span class="n">settings</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;MONGO_URI&#39;</span><span class="p">),</span>
            <span class="n">mongo_db</span><span class="o">=</span><span class="n">crawler</span><span class="o">.</span><span class="n">settings</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;MONGO_DATABASE&#39;</span><span class="p">,</span> <span class="s1">&#39;items&#39;</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">open_spider</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">client</span> <span class="o">=</span> <span class="n">pymongo</span><span class="o">.</span><span class="n">MongoClient</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mongo_uri</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">db</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">mongo_db</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">close_spider</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">client</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">db</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">collection_name</span><span class="p">]</span><span class="o">.</span><span class="n">insert_one</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">item</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
</div>
<div class="section" id="take-screenshot-of-item">
<h5>Take screenshot of item<a class="headerlink" href="#take-screenshot-of-item" title="Permalink to this headline">¶</a></h5>
<p>This example demonstrates how to return <a class="reference external" href="https://twistedmatrix.com/documents/current/core/howto/defer.html">Deferred</a> from <a class="reference internal" href="#process_item" title="process_item"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_item()</span></code></a> method.
It uses <a class="reference external" href="https://splash.readthedocs.io/en/stable/">Splash</a> to render screenshot of item url. Pipeline
makes request to locally running instance of <a class="reference external" href="https://splash.readthedocs.io/en/stable/">Splash</a>. After request is downloaded
and Deferred callback fires, it saves item to a file and adds filename to an item.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">import</span> <span class="nn">hashlib</span>
<span class="kn">from</span> <span class="nn">urllib.parse</span> <span class="k">import</span> <span class="n">quote</span>


<span class="k">class</span> <span class="nc">ScreenshotPipeline</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Pipeline that uses Splash to render screenshot of</span>
<span class="sd">    every Scrapy item.&quot;&quot;&quot;</span>

    <span class="n">SPLASH_URL</span> <span class="o">=</span> <span class="s2">&quot;http://localhost:8050/render.png?url=</span><span class="si">{}</span><span class="s2">&quot;</span>

    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="n">encoded_item_url</span> <span class="o">=</span> <span class="n">quote</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="s2">&quot;url&quot;</span><span class="p">])</span>
        <span class="n">screenshot_url</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">SPLASH_URL</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">encoded_item_url</span><span class="p">)</span>
        <span class="n">request</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">screenshot_url</span><span class="p">)</span>
        <span class="n">dfd</span> <span class="o">=</span> <span class="n">spider</span><span class="o">.</span><span class="n">crawler</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="n">request</span><span class="p">,</span> <span class="n">spider</span><span class="p">)</span>
        <span class="n">dfd</span><span class="o">.</span><span class="n">addBoth</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">return_item</span><span class="p">,</span> <span class="n">item</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">dfd</span>

    <span class="k">def</span> <span class="nf">return_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">response</span><span class="o">.</span><span class="n">status</span> <span class="o">!=</span> <span class="mi">200</span><span class="p">:</span>
            <span class="c1"># Error happened, return item.</span>
            <span class="k">return</span> <span class="n">item</span>

        <span class="c1"># Save screenshot to file, filename will be hash of url.</span>
        <span class="n">url</span> <span class="o">=</span> <span class="n">item</span><span class="p">[</span><span class="s2">&quot;url&quot;</span><span class="p">]</span>
        <span class="n">url_hash</span> <span class="o">=</span> <span class="n">hashlib</span><span class="o">.</span><span class="n">md5</span><span class="p">(</span><span class="n">url</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;utf8&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">hexdigest</span><span class="p">()</span>
        <span class="n">filename</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">{}</span><span class="s2">.png&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">url_hash</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">body</span><span class="p">)</span>

        <span class="c1"># Store filename in item.</span>
        <span class="n">item</span><span class="p">[</span><span class="s2">&quot;screenshot_filename&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">filename</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
</div>
<div class="section" id="duplicates-filter">
<h5>Duplicates filter<a class="headerlink" href="#duplicates-filter" title="Permalink to this headline">¶</a></h5>
<p>A filter that looks for duplicate items, and drops those items that were
already processed. Let’s say that our items have a unique id, but our spider
returns multiples items with the same id:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.exceptions</span> <span class="k">import</span> <span class="n">DropItem</span>

<span class="k">class</span> <span class="nc">DuplicatesPipeline</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ids_seen</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">item</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">]</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ids_seen</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">DropItem</span><span class="p">(</span><span class="s2">&quot;Duplicate item found: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">item</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ids_seen</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="activating-an-item-pipeline-component">
<h4>Activating an Item Pipeline component<a class="headerlink" href="#activating-an-item-pipeline-component" title="Permalink to this headline">¶</a></h4>
<p>To activate an Item Pipeline component you must add its class to the
<a class="reference internal" href="index.html#std:setting-ITEM_PIPELINES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ITEM_PIPELINES</span></code></a> setting, like in the following example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ITEM_PIPELINES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;myproject.pipelines.PricePipeline&#39;</span><span class="p">:</span> <span class="mi">300</span><span class="p">,</span>
    <span class="s1">&#39;myproject.pipelines.JsonWriterPipeline&#39;</span><span class="p">:</span> <span class="mi">800</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The integer values you assign to classes in this setting determine the
order in which they run: items go through from lower valued to higher
valued classes. It’s customary to define these numbers in the 0-1000 range.</p>
</div>
</div>
<span id="document-topics/feed-exports"></span><div class="section" id="feed-exports">
<span id="topics-feed-exports"></span><h3>Feed exports<a class="headerlink" href="#feed-exports" title="Permalink to this headline">¶</a></h3>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.10.</span></p>
</div>
<p>One of the most frequently required features when implementing scrapers is
being able to store the scraped data properly and, quite often, that means
generating an “export file” with the scraped data (commonly called “export
feed”) to be consumed by other systems.</p>
<p>Scrapy provides this functionality out of the box with the Feed Exports, which
allows you to generate a feed with the scraped items, using multiple
serialization formats and storage backends.</p>
<div class="section" id="serialization-formats">
<span id="topics-feed-format"></span><h4>Serialization formats<a class="headerlink" href="#serialization-formats" title="Permalink to this headline">¶</a></h4>
<p>For serializing the scraped data, the feed exports use the <a class="reference internal" href="index.html#topics-exporters"><span class="std std-ref">Item exporters</span></a>. These formats are supported out of the box:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="#topics-feed-format-json"><span class="std std-ref">JSON</span></a></li>
<li><a class="reference internal" href="#topics-feed-format-jsonlines"><span class="std std-ref">JSON lines</span></a></li>
<li><a class="reference internal" href="#topics-feed-format-csv"><span class="std std-ref">CSV</span></a></li>
<li><a class="reference internal" href="#topics-feed-format-xml"><span class="std std-ref">XML</span></a></li>
</ul>
</div></blockquote>
<p>But you can also extend the supported format through the
<a class="reference internal" href="#std:setting-FEED_EXPORTERS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_EXPORTERS</span></code></a> setting.</p>
<div class="section" id="json">
<span id="topics-feed-format-json"></span><h5>JSON<a class="headerlink" href="#json" title="Permalink to this headline">¶</a></h5>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="#std:setting-FEED_FORMAT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_FORMAT</span></code></a>: <code class="docutils literal notranslate"><span class="pre">json</span></code></li>
<li>Exporter used: <a class="reference internal" href="index.html#scrapy.exporters.JsonItemExporter" title="scrapy.exporters.JsonItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">JsonItemExporter</span></code></a></li>
<li>See <a class="reference internal" href="index.html#json-with-large-data"><span class="std std-ref">this warning</span></a> if you’re using JSON with
large feeds.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="json-lines">
<span id="topics-feed-format-jsonlines"></span><h5>JSON lines<a class="headerlink" href="#json-lines" title="Permalink to this headline">¶</a></h5>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="#std:setting-FEED_FORMAT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_FORMAT</span></code></a>: <code class="docutils literal notranslate"><span class="pre">jsonlines</span></code></li>
<li>Exporter used: <a class="reference internal" href="index.html#scrapy.exporters.JsonLinesItemExporter" title="scrapy.exporters.JsonLinesItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">JsonLinesItemExporter</span></code></a></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="csv">
<span id="topics-feed-format-csv"></span><h5>CSV<a class="headerlink" href="#csv" title="Permalink to this headline">¶</a></h5>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="#std:setting-FEED_FORMAT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_FORMAT</span></code></a>: <code class="docutils literal notranslate"><span class="pre">csv</span></code></li>
<li>Exporter used: <a class="reference internal" href="index.html#scrapy.exporters.CsvItemExporter" title="scrapy.exporters.CsvItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">CsvItemExporter</span></code></a></li>
<li>To specify columns to export and their order use
<a class="reference internal" href="#std:setting-FEED_EXPORT_FIELDS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_EXPORT_FIELDS</span></code></a>. Other feed exporters can also use this
option, but it is important for CSV because unlike many other export
formats CSV uses a fixed header.</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="xml">
<span id="topics-feed-format-xml"></span><h5>XML<a class="headerlink" href="#xml" title="Permalink to this headline">¶</a></h5>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="#std:setting-FEED_FORMAT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_FORMAT</span></code></a>: <code class="docutils literal notranslate"><span class="pre">xml</span></code></li>
<li>Exporter used: <a class="reference internal" href="index.html#scrapy.exporters.XmlItemExporter" title="scrapy.exporters.XmlItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">XmlItemExporter</span></code></a></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="pickle">
<span id="topics-feed-format-pickle"></span><h5>Pickle<a class="headerlink" href="#pickle" title="Permalink to this headline">¶</a></h5>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="#std:setting-FEED_FORMAT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_FORMAT</span></code></a>: <code class="docutils literal notranslate"><span class="pre">pickle</span></code></li>
<li>Exporter used: <a class="reference internal" href="index.html#scrapy.exporters.PickleItemExporter" title="scrapy.exporters.PickleItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">PickleItemExporter</span></code></a></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="marshal">
<span id="topics-feed-format-marshal"></span><h5>Marshal<a class="headerlink" href="#marshal" title="Permalink to this headline">¶</a></h5>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="#std:setting-FEED_FORMAT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_FORMAT</span></code></a>: <code class="docutils literal notranslate"><span class="pre">marshal</span></code></li>
<li>Exporter used: <code class="xref py py-class docutils literal notranslate"><span class="pre">MarshalItemExporter</span></code></li>
</ul>
</div></blockquote>
</div>
</div>
<div class="section" id="storages">
<span id="topics-feed-storage"></span><h4>Storages<a class="headerlink" href="#storages" title="Permalink to this headline">¶</a></h4>
<p>When using the feed exports you define where to store the feed using a <a class="reference external" href="https://en.wikipedia.org/wiki/Uniform_Resource_Identifier">URI</a>
(through the <a class="reference internal" href="#std:setting-FEED_URI"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_URI</span></code></a> setting). The feed exports supports multiple
storage backend types which are defined by the URI scheme.</p>
<p>The storages backends supported out of the box are:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="#topics-feed-storage-fs"><span class="std std-ref">Local filesystem</span></a></li>
<li><a class="reference internal" href="#topics-feed-storage-ftp"><span class="std std-ref">FTP</span></a></li>
<li><a class="reference internal" href="#topics-feed-storage-s3"><span class="std std-ref">S3</span></a> (requires <a class="reference external" href="https://github.com/boto/botocore">botocore</a> or <a class="reference external" href="https://github.com/boto/boto">boto</a>)</li>
<li><a class="reference internal" href="#topics-feed-storage-stdout"><span class="std std-ref">Standard output</span></a></li>
</ul>
</div></blockquote>
<p>Some storage backends may be unavailable if the required external libraries are
not available. For example, the S3 backend is only available if the <a class="reference external" href="https://github.com/boto/botocore">botocore</a>
or <a class="reference external" href="https://github.com/boto/boto">boto</a> library is installed (Scrapy supports <a class="reference external" href="https://github.com/boto/boto">boto</a> only on Python 2).</p>
</div>
<div class="section" id="storage-uri-parameters">
<span id="topics-feed-uri-params"></span><h4>Storage URI parameters<a class="headerlink" href="#storage-uri-parameters" title="Permalink to this headline">¶</a></h4>
<p>The storage URI can also contain parameters that get replaced when the feed is
being created. These parameters are:</p>
<blockquote>
<div><ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">%(time)s</span></code> - gets replaced by a timestamp when the feed is being created</li>
<li><code class="docutils literal notranslate"><span class="pre">%(name)s</span></code> - gets replaced by the spider name</li>
</ul>
</div></blockquote>
<p>Any other named parameter gets replaced by the spider attribute of the same
name. For example, <code class="docutils literal notranslate"><span class="pre">%(site_id)s</span></code> would get replaced by the <code class="docutils literal notranslate"><span class="pre">spider.site_id</span></code>
attribute the moment the feed is being created.</p>
<p>Here are some examples to illustrate:</p>
<blockquote>
<div><ul class="simple">
<li>Store in FTP using one directory per spider:<ul>
<li><code class="docutils literal notranslate"><span class="pre">ftp://user:password&#64;ftp.example.com/scraping/feeds/%(name)s/%(time)s.json</span></code></li>
</ul>
</li>
<li>Store in S3 using one directory per spider:<ul>
<li><code class="docutils literal notranslate"><span class="pre">s3://mybucket/scraping/feeds/%(name)s/%(time)s.json</span></code></li>
</ul>
</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="storage-backends">
<span id="topics-feed-storage-backends"></span><h4>Storage backends<a class="headerlink" href="#storage-backends" title="Permalink to this headline">¶</a></h4>
<div class="section" id="local-filesystem">
<span id="topics-feed-storage-fs"></span><h5>Local filesystem<a class="headerlink" href="#local-filesystem" title="Permalink to this headline">¶</a></h5>
<p>The feeds are stored in the local filesystem.</p>
<blockquote>
<div><ul class="simple">
<li>URI scheme: <code class="docutils literal notranslate"><span class="pre">file</span></code></li>
<li>Example URI: <code class="docutils literal notranslate"><span class="pre">file:///tmp/export.csv</span></code></li>
<li>Required external libraries: none</li>
</ul>
</div></blockquote>
<p>Note that for the local filesystem storage (only) you can omit the scheme if
you specify an absolute path like <code class="docutils literal notranslate"><span class="pre">/tmp/export.csv</span></code>. This only works on Unix
systems though.</p>
</div>
<div class="section" id="ftp">
<span id="topics-feed-storage-ftp"></span><h5>FTP<a class="headerlink" href="#ftp" title="Permalink to this headline">¶</a></h5>
<p>The feeds are stored in a FTP server.</p>
<blockquote>
<div><ul class="simple">
<li>URI scheme: <code class="docutils literal notranslate"><span class="pre">ftp</span></code></li>
<li>Example URI: <code class="docutils literal notranslate"><span class="pre">ftp://user:pass&#64;ftp.example.com/path/to/export.csv</span></code></li>
<li>Required external libraries: none</li>
</ul>
</div></blockquote>
<p>FTP supports two different connection modes: <a class="reference external" href="https://stackoverflow.com/a/1699163">active or passive</a>. Scrapy uses the passive connection
mode by default. To use the active connection mode instead, set the
<a class="reference internal" href="#std:setting-FEED_STORAGE_FTP_ACTIVE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_STORAGE_FTP_ACTIVE</span></code></a> setting to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</div>
<div class="section" id="s3">
<span id="topics-feed-storage-s3"></span><h5>S3<a class="headerlink" href="#s3" title="Permalink to this headline">¶</a></h5>
<p>The feeds are stored on <a class="reference external" href="https://aws.amazon.com/s3/">Amazon S3</a>.</p>
<blockquote>
<div><ul class="simple">
<li>URI scheme: <code class="docutils literal notranslate"><span class="pre">s3</span></code></li>
<li>Example URIs:<ul>
<li><code class="docutils literal notranslate"><span class="pre">s3://mybucket/path/to/export.csv</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">s3://aws_key:aws_secret&#64;mybucket/path/to/export.csv</span></code></li>
</ul>
</li>
<li>Required external libraries: <a class="reference external" href="https://github.com/boto/botocore">botocore</a> (Python 2 and Python 3) or <a class="reference external" href="https://github.com/boto/boto">boto</a> (Python 2 only)</li>
</ul>
</div></blockquote>
<p>The AWS credentials can be passed as user/password in the URI, or they can be
passed through the following settings:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-AWS_ACCESS_KEY_ID"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AWS_ACCESS_KEY_ID</span></code></a></li>
<li><a class="reference internal" href="index.html#std:setting-AWS_SECRET_ACCESS_KEY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AWS_SECRET_ACCESS_KEY</span></code></a></li>
</ul>
</div></blockquote>
<p>You can also define a custom ACL for exported feeds using this setting:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="#std:setting-FEED_STORAGE_S3_ACL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_STORAGE_S3_ACL</span></code></a></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="standard-output">
<span id="topics-feed-storage-stdout"></span><h5>Standard output<a class="headerlink" href="#standard-output" title="Permalink to this headline">¶</a></h5>
<p>The feeds are written to the standard output of the Scrapy process.</p>
<blockquote>
<div><ul class="simple">
<li>URI scheme: <code class="docutils literal notranslate"><span class="pre">stdout</span></code></li>
<li>Example URI: <code class="docutils literal notranslate"><span class="pre">stdout:</span></code></li>
<li>Required external libraries: none</li>
</ul>
</div></blockquote>
</div>
</div>
<div class="section" id="settings">
<h4>Settings<a class="headerlink" href="#settings" title="Permalink to this headline">¶</a></h4>
<p>These are the settings used for configuring the feed exports:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="#std:setting-FEED_URI"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_URI</span></code></a> (mandatory)</li>
<li><a class="reference internal" href="#std:setting-FEED_FORMAT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_FORMAT</span></code></a></li>
<li><a class="reference internal" href="#std:setting-FEED_STORAGES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_STORAGES</span></code></a></li>
<li><a class="reference internal" href="#std:setting-FEED_STORAGE_FTP_ACTIVE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_STORAGE_FTP_ACTIVE</span></code></a></li>
<li><a class="reference internal" href="#std:setting-FEED_STORAGE_S3_ACL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_STORAGE_S3_ACL</span></code></a></li>
<li><a class="reference internal" href="#std:setting-FEED_EXPORTERS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_EXPORTERS</span></code></a></li>
<li><a class="reference internal" href="#std:setting-FEED_STORE_EMPTY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_STORE_EMPTY</span></code></a></li>
<li><a class="reference internal" href="#std:setting-FEED_EXPORT_ENCODING"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_EXPORT_ENCODING</span></code></a></li>
<li><a class="reference internal" href="#std:setting-FEED_EXPORT_FIELDS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_EXPORT_FIELDS</span></code></a></li>
<li><a class="reference internal" href="#std:setting-FEED_EXPORT_INDENT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_EXPORT_INDENT</span></code></a></li>
</ul>
</div></blockquote>
<div class="section" id="feed-uri">
<span id="std:setting-FEED_URI"></span><h5>FEED_URI<a class="headerlink" href="#feed-uri" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p>
<p>The URI of the export feed. See <a class="reference internal" href="#topics-feed-storage-backends"><span class="std std-ref">Storage backends</span></a> for
supported URI schemes.</p>
<p>This setting is required for enabling the feed exports.</p>
</div>
<div class="section" id="feed-format">
<span id="std:setting-FEED_FORMAT"></span><h5>FEED_FORMAT<a class="headerlink" href="#feed-format" title="Permalink to this headline">¶</a></h5>
<p>The serialization format to be used for the feed. See
<a class="reference internal" href="#topics-feed-format"><span class="std std-ref">Serialization formats</span></a> for possible values.</p>
</div>
<div class="section" id="feed-export-encoding">
<span id="std:setting-FEED_EXPORT_ENCODING"></span><h5>FEED_EXPORT_ENCODING<a class="headerlink" href="#feed-export-encoding" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p>
<p>The encoding to be used for the feed.</p>
<p>If unset or set to <code class="docutils literal notranslate"><span class="pre">None</span></code> (default) it uses UTF-8 for everything except JSON output,
which uses safe numeric encoding (<code class="docutils literal notranslate"><span class="pre">\uXXXX</span></code> sequences) for historic reasons.</p>
<p>Use <code class="docutils literal notranslate"><span class="pre">utf-8</span></code> if you want UTF-8 for JSON too.</p>
</div>
<div class="section" id="feed-export-fields">
<span id="std:setting-FEED_EXPORT_FIELDS"></span><h5>FEED_EXPORT_FIELDS<a class="headerlink" href="#feed-export-fields" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p>
<p>A list of fields to export, optional.
Example: <code class="docutils literal notranslate"><span class="pre">FEED_EXPORT_FIELDS</span> <span class="pre">=</span> <span class="pre">[&quot;foo&quot;,</span> <span class="pre">&quot;bar&quot;,</span> <span class="pre">&quot;baz&quot;]</span></code>.</p>
<p>Use FEED_EXPORT_FIELDS option to define fields to export and their order.</p>
<p>When FEED_EXPORT_FIELDS is empty or None (default), Scrapy uses fields
defined in dicts or <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a> subclasses a spider is yielding.</p>
<p>If an exporter requires a fixed set of fields (this is the case for
<a class="reference internal" href="#topics-feed-format-csv"><span class="std std-ref">CSV</span></a> export format) and FEED_EXPORT_FIELDS
is empty or None, then Scrapy tries to infer field names from the
exported data - currently it uses field names from the first item.</p>
</div>
<div class="section" id="feed-export-indent">
<span id="std:setting-FEED_EXPORT_INDENT"></span><h5>FEED_EXPORT_INDENT<a class="headerlink" href="#feed-export-indent" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">0</span></code></p>
<p>Amount of spaces used to indent the output on each level. If <code class="docutils literal notranslate"><span class="pre">FEED_EXPORT_INDENT</span></code>
is a non-negative integer, then array elements and object members will be pretty-printed
with that indent level. An indent level of <code class="docutils literal notranslate"><span class="pre">0</span></code> (the default), or negative,
will put each item on a new line. <code class="docutils literal notranslate"><span class="pre">None</span></code> selects the most compact representation.</p>
<p>Currently implemented only by <a class="reference internal" href="index.html#scrapy.exporters.JsonItemExporter" title="scrapy.exporters.JsonItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">JsonItemExporter</span></code></a>
and <a class="reference internal" href="index.html#scrapy.exporters.XmlItemExporter" title="scrapy.exporters.XmlItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">XmlItemExporter</span></code></a>, i.e. when you are exporting
to <code class="docutils literal notranslate"><span class="pre">.json</span></code> or <code class="docutils literal notranslate"><span class="pre">.xml</span></code>.</p>
</div>
<div class="section" id="feed-store-empty">
<span id="std:setting-FEED_STORE_EMPTY"></span><h5>FEED_STORE_EMPTY<a class="headerlink" href="#feed-store-empty" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>Whether to export empty feeds (ie. feeds with no items).</p>
</div>
<div class="section" id="feed-storages">
<span id="std:setting-FEED_STORAGES"></span><h5>FEED_STORAGES<a class="headerlink" href="#feed-storages" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">{}</span></code></p>
<p>A dict containing additional feed storage backends supported by your project.
The keys are URI schemes and the values are paths to storage classes.</p>
</div>
<div class="section" id="feed-storage-ftp-active">
<span id="std:setting-FEED_STORAGE_FTP_ACTIVE"></span><h5>FEED_STORAGE_FTP_ACTIVE<a class="headerlink" href="#feed-storage-ftp-active" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>Whether to use the active connection mode when exporting feeds to an FTP server
(<code class="docutils literal notranslate"><span class="pre">True</span></code>) or use the passive connection mode instead (<code class="docutils literal notranslate"><span class="pre">False</span></code>, default).</p>
<p>For information about FTP connection modes, see <a class="reference external" href="https://stackoverflow.com/a/1699163">What is the difference between
active and passive FTP?</a>.</p>
</div>
<div class="section" id="feed-storage-s3-acl">
<span id="std:setting-FEED_STORAGE_S3_ACL"></span><h5>FEED_STORAGE_S3_ACL<a class="headerlink" href="#feed-storage-s3-acl" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">''</span></code> (empty string)</p>
<p>A string containing a custom ACL for feeds exported to Amazon S3 by your project.</p>
<p>For a complete list of available values, access the <a class="reference external" href="https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl">Canned ACL</a> section on Amazon S3 docs.</p>
</div>
<div class="section" id="feed-storages-base">
<span id="std:setting-FEED_STORAGES_BASE"></span><h5>FEED_STORAGES_BASE<a class="headerlink" href="#feed-storages-base" title="Permalink to this headline">¶</a></h5>
<p>Default:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.extensions.feedexport.FileFeedStorage&#39;</span><span class="p">,</span>
    <span class="s1">&#39;file&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.extensions.feedexport.FileFeedStorage&#39;</span><span class="p">,</span>
    <span class="s1">&#39;stdout&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.extensions.feedexport.StdoutFeedStorage&#39;</span><span class="p">,</span>
    <span class="s1">&#39;s3&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.extensions.feedexport.S3FeedStorage&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ftp&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.extensions.feedexport.FTPFeedStorage&#39;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>A dict containing the built-in feed storage backends supported by Scrapy. You
can disable any of these backends by assigning <code class="docutils literal notranslate"><span class="pre">None</span></code> to their URI scheme in
<a class="reference internal" href="#std:setting-FEED_STORAGES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_STORAGES</span></code></a>. E.g., to disable the built-in FTP storage backend
(without replacement), place this in your <code class="docutils literal notranslate"><span class="pre">settings.py</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">FEED_STORAGES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;ftp&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="feed-exporters">
<span id="std:setting-FEED_EXPORTERS"></span><h5>FEED_EXPORTERS<a class="headerlink" href="#feed-exporters" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">{}</span></code></p>
<p>A dict containing additional exporters supported by your project. The keys are
serialization formats and the values are paths to <a class="reference internal" href="index.html#topics-exporters"><span class="std std-ref">Item exporter</span></a> classes.</p>
</div>
<div class="section" id="feed-exporters-base">
<span id="std:setting-FEED_EXPORTERS_BASE"></span><h5>FEED_EXPORTERS_BASE<a class="headerlink" href="#feed-exporters-base" title="Permalink to this headline">¶</a></h5>
<p>Default:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;json&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.exporters.JsonItemExporter&#39;</span><span class="p">,</span>
    <span class="s1">&#39;jsonlines&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.exporters.JsonLinesItemExporter&#39;</span><span class="p">,</span>
    <span class="s1">&#39;jl&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.exporters.JsonLinesItemExporter&#39;</span><span class="p">,</span>
    <span class="s1">&#39;csv&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.exporters.CsvItemExporter&#39;</span><span class="p">,</span>
    <span class="s1">&#39;xml&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.exporters.XmlItemExporter&#39;</span><span class="p">,</span>
    <span class="s1">&#39;marshal&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.exporters.MarshalItemExporter&#39;</span><span class="p">,</span>
    <span class="s1">&#39;pickle&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.exporters.PickleItemExporter&#39;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>A dict containing the built-in feed exporters supported by Scrapy. You can
disable any of these exporters by assigning <code class="docutils literal notranslate"><span class="pre">None</span></code> to their serialization
format in <a class="reference internal" href="#std:setting-FEED_EXPORTERS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_EXPORTERS</span></code></a>. E.g., to disable the built-in CSV exporter
(without replacement), place this in your <code class="docutils literal notranslate"><span class="pre">settings.py</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">FEED_EXPORTERS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;csv&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
</div>
<span id="document-topics/request-response"></span><div class="section" id="module-scrapy.http">
<span id="requests-and-responses"></span><span id="topics-request-response"></span><h3>Requests and Responses<a class="headerlink" href="#module-scrapy.http" title="Permalink to this headline">¶</a></h3>
<p>Scrapy uses <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> and <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> objects for crawling web
sites.</p>
<p>Typically, <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> objects are generated in the spiders and pass
across the system until they reach the Downloader, which executes the request
and returns a <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object which travels back to the spider that
issued the request.</p>
<p>Both <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> and <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> classes have subclasses which add
functionality not required in the base classes. These are described
below in <a class="reference internal" href="#topics-request-response-ref-request-subclasses"><span class="std std-ref">Request subclasses</span></a> and
<a class="reference internal" href="#topics-request-response-ref-response-subclasses"><span class="std std-ref">Response subclasses</span></a>.</p>
<div class="section" id="request-objects">
<h4>Request objects<a class="headerlink" href="#request-objects" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="scrapy.http.Request">
<em class="property">class </em><code class="descclassname">scrapy.http.</code><code class="descname">Request</code><span class="sig-paren">(</span><em>url</em><span class="optional">[</span>, <em>callback</em>, <em>method='GET'</em>, <em>headers</em>, <em>body</em>, <em>cookies</em>, <em>meta</em>, <em>encoding='utf-8'</em>, <em>priority=0</em>, <em>dont_filter=False</em>, <em>errback</em>, <em>flags</em>, <em>cb_kwargs</em><span class="optional">]</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.Request" title="Permalink to this definition">¶</a></dt>
<dd><p>A <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object represents an HTTP request, which is usually
generated in the Spider and executed by the Downloader, and thus generating
a <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>url</strong> (<em>string</em>) – the URL of this request</li>
<li><strong>callback</strong> (<em>callable</em>) – the function that will be called with the response of this
request (once its downloaded) as its first parameter. For more information
see <a class="reference internal" href="#topics-request-response-ref-request-callback-arguments"><span class="std std-ref">Passing additional data to callback functions</span></a> below.
If a Request doesn’t specify a callback, the spider’s
<a class="reference internal" href="index.html#scrapy.spiders.Spider.parse" title="scrapy.spiders.Spider.parse"><code class="xref py py-meth docutils literal notranslate"><span class="pre">parse()</span></code></a> method will be used.
Note that if exceptions are raised during processing, errback is called instead.</li>
<li><strong>method</strong> (<em>string</em>) – the HTTP method of this request. Defaults to <code class="docutils literal notranslate"><span class="pre">'GET'</span></code>.</li>
<li><strong>meta</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.7)"><em>dict</em></a>) – the initial values for the <a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code></a> attribute. If
given, the dict passed in this parameter will be shallow copied.</li>
<li><strong>body</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a><em> or </em><em>unicode</em>) – the request body. If a <code class="docutils literal notranslate"><span class="pre">unicode</span></code> is passed, then it’s encoded to
<code class="docutils literal notranslate"><span class="pre">str</span></code> using the <code class="docutils literal notranslate"><span class="pre">encoding</span></code> passed (which defaults to <code class="docutils literal notranslate"><span class="pre">utf-8</span></code>). If
<code class="docutils literal notranslate"><span class="pre">body</span></code> is not given, an empty string is stored. Regardless of the
type of this argument, the final value stored will be a <code class="docutils literal notranslate"><span class="pre">str</span></code> (never
<code class="docutils literal notranslate"><span class="pre">unicode</span></code> or <code class="docutils literal notranslate"><span class="pre">None</span></code>).</li>
<li><strong>headers</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.7)"><em>dict</em></a>) – the headers of this request. The dict values can be strings
(for single valued headers) or lists (for multi-valued headers). If
<code class="docutils literal notranslate"><span class="pre">None</span></code> is passed as value, the HTTP header will not be sent at all.</li>
<li><strong>cookies</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.7)"><em>dict</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a>) – <p>the request cookies. These can be sent in two forms.</p>
<ol class="arabic">
<li>Using a dict:<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">request_with_cookies</span> <span class="o">=</span> <span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s2">&quot;http://www.example.com&quot;</span><span class="p">,</span>
                               <span class="n">cookies</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;currency&#39;</span><span class="p">:</span> <span class="s1">&#39;USD&#39;</span><span class="p">,</span> <span class="s1">&#39;country&#39;</span><span class="p">:</span> <span class="s1">&#39;UY&#39;</span><span class="p">})</span>
</pre></div>
</div>
</li>
<li>Using a list of dicts:<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">request_with_cookies</span> <span class="o">=</span> <span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s2">&quot;http://www.example.com&quot;</span><span class="p">,</span>
                               <span class="n">cookies</span><span class="o">=</span><span class="p">[{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;currency&#39;</span><span class="p">,</span>
                                        <span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="s1">&#39;USD&#39;</span><span class="p">,</span>
                                        <span class="s1">&#39;domain&#39;</span><span class="p">:</span> <span class="s1">&#39;example.com&#39;</span><span class="p">,</span>
                                        <span class="s1">&#39;path&#39;</span><span class="p">:</span> <span class="s1">&#39;/currency&#39;</span><span class="p">}])</span>
</pre></div>
</div>
</li>
</ol>
<p>The latter form allows for customizing the <code class="docutils literal notranslate"><span class="pre">domain</span></code> and <code class="docutils literal notranslate"><span class="pre">path</span></code>
attributes of the cookie. This is only useful if the cookies are saved
for later requests.</p>
<span class="target" id="std:reqmeta-dont_merge_cookies"></span><p>When some site returns cookies (in a response) those are stored in the
cookies for that domain and will be sent again in future requests. That’s
the typical behaviour of any regular web browser. However, if, for some
reason, you want to avoid merging with existing cookies you can instruct
Scrapy to do so by setting the <code class="docutils literal notranslate"><span class="pre">dont_merge_cookies</span></code> key to True in the
<a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code></a>.</p>
<p>Example of request without merging cookies:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">request_with_cookies</span> <span class="o">=</span> <span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s2">&quot;http://www.example.com&quot;</span><span class="p">,</span>
                               <span class="n">cookies</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;currency&#39;</span><span class="p">:</span> <span class="s1">&#39;USD&#39;</span><span class="p">,</span> <span class="s1">&#39;country&#39;</span><span class="p">:</span> <span class="s1">&#39;UY&#39;</span><span class="p">},</span>
                               <span class="n">meta</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;dont_merge_cookies&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">})</span>
</pre></div>
</div>
<p>For more info see <a class="reference internal" href="index.html#cookies-mw"><span class="std std-ref">CookiesMiddleware</span></a>.</p>
</li>
<li><strong>encoding</strong> (<em>string</em>) – the encoding of this request (defaults to <code class="docutils literal notranslate"><span class="pre">'utf-8'</span></code>).
This encoding will be used to percent-encode the URL and to convert the
body to <code class="docutils literal notranslate"><span class="pre">str</span></code> (if given as <code class="docutils literal notranslate"><span class="pre">unicode</span></code>).</li>
<li><strong>priority</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the priority of this request (defaults to <code class="docutils literal notranslate"><span class="pre">0</span></code>).
The priority is used by the scheduler to define the order used to process
requests.  Requests with a higher priority value will execute earlier.
Negative values are allowed in order to indicate relatively low-priority.</li>
<li><strong>dont_filter</strong> (<em>boolean</em>) – indicates that this request should not be filtered by
the scheduler. This is used when you want to perform an identical
request multiple times, to ignore the duplicates filter. Use it with
care, or you will get into crawling loops. Default to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</li>
<li><strong>errback</strong> (<em>callable</em>) – a function that will be called if any exception was
raised while processing the request. This includes pages that failed
with 404 HTTP errors and such. It receives a <a class="reference external" href="https://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html">Twisted Failure</a> instance
as first parameter.
For more information,
see <a class="reference internal" href="#topics-request-response-ref-errbacks"><span class="std std-ref">Using errbacks to catch exceptions in request processing</span></a> below.</li>
<li><strong>flags</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a>) – Flags sent to the request, can be used for logging or similar purposes.</li>
<li><strong>cb_kwargs</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.7)"><em>dict</em></a>) – A dict with arbitrary data that will be passed as keyword arguments to the Request’s callback.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="scrapy.http.Request.url">
<code class="descname">url</code><a class="headerlink" href="#scrapy.http.Request.url" title="Permalink to this definition">¶</a></dt>
<dd><p>A string containing the URL of this request. Keep in mind that this
attribute contains the escaped URL, so it can differ from the URL passed in
the constructor.</p>
<p>This attribute is read-only. To change the URL of a Request use
<a class="reference internal" href="#scrapy.http.Request.replace" title="scrapy.http.Request.replace"><code class="xref py py-meth docutils literal notranslate"><span class="pre">replace()</span></code></a>.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Request.method">
<code class="descname">method</code><a class="headerlink" href="#scrapy.http.Request.method" title="Permalink to this definition">¶</a></dt>
<dd><p>A string representing the HTTP method in the request. This is guaranteed to
be uppercase. Example: <code class="docutils literal notranslate"><span class="pre">&quot;GET&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;POST&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;PUT&quot;</span></code>, etc</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Request.headers">
<code class="descname">headers</code><a class="headerlink" href="#scrapy.http.Request.headers" title="Permalink to this definition">¶</a></dt>
<dd><p>A dictionary-like object which contains the request headers.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Request.body">
<code class="descname">body</code><a class="headerlink" href="#scrapy.http.Request.body" title="Permalink to this definition">¶</a></dt>
<dd><p>A str that contains the request body.</p>
<p>This attribute is read-only. To change the body of a Request use
<a class="reference internal" href="#scrapy.http.Request.replace" title="scrapy.http.Request.replace"><code class="xref py py-meth docutils literal notranslate"><span class="pre">replace()</span></code></a>.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Request.meta">
<code class="descname">meta</code><a class="headerlink" href="#scrapy.http.Request.meta" title="Permalink to this definition">¶</a></dt>
<dd><p>A dict that contains arbitrary metadata for this request. This dict is
empty for new Requests, and is usually  populated by different Scrapy
components (extensions, middlewares, etc). So the data contained in this
dict depends on the extensions you have enabled.</p>
<p>See <a class="reference internal" href="#topics-request-meta"><span class="std std-ref">Request.meta special keys</span></a> for a list of special meta keys
recognized by Scrapy.</p>
<p>This dict is <a class="reference external" href="https://docs.python.org/2/library/copy.html">shallow copied</a> when the request is cloned using the
<code class="docutils literal notranslate"><span class="pre">copy()</span></code> or <code class="docutils literal notranslate"><span class="pre">replace()</span></code> methods, and can also be accessed, in your
spider, from the <code class="docutils literal notranslate"><span class="pre">response.meta</span></code> attribute.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Request.cb_kwargs">
<code class="descname">cb_kwargs</code><a class="headerlink" href="#scrapy.http.Request.cb_kwargs" title="Permalink to this definition">¶</a></dt>
<dd><p>A dictionary that contains arbitrary metadata for this request. Its contents
will be passed to the Request’s callback as keyword arguments. It is empty
for new Requests, which means by default callbacks only get a <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a>
object as argument.</p>
<p>This dict is <a class="reference external" href="https://docs.python.org/2/library/copy.html">shallow copied</a> when the request is cloned using the
<code class="docutils literal notranslate"><span class="pre">copy()</span></code> or <code class="docutils literal notranslate"><span class="pre">replace()</span></code> methods, and can also be accessed, in your
spider, from the <code class="docutils literal notranslate"><span class="pre">response.cb_kwargs</span></code> attribute.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.http.Request.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.Request.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a new Request which is a copy of this Request. See also:
<a class="reference internal" href="#topics-request-response-ref-request-callback-arguments"><span class="std std-ref">Passing additional data to callback functions</span></a>.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.http.Request.replace">
<code class="descname">replace</code><span class="sig-paren">(</span><span class="optional">[</span><em>url</em>, <em>method</em>, <em>headers</em>, <em>body</em>, <em>cookies</em>, <em>meta</em>, <em>flags</em>, <em>encoding</em>, <em>priority</em>, <em>dont_filter</em>, <em>callback</em>, <em>errback</em>, <em>cb_kwargs</em><span class="optional">]</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.Request.replace" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a Request object with the same members, except for those members
given new values by whichever keyword arguments are specified. The
<a class="reference internal" href="#scrapy.http.Request.cb_kwargs" title="scrapy.http.Request.cb_kwargs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.cb_kwargs</span></code></a> and <a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code></a> attributes are shallow
copied by default (unless new values are given as arguments). See also
<a class="reference internal" href="#topics-request-response-ref-request-callback-arguments"><span class="std std-ref">Passing additional data to callback functions</span></a>.</p>
</dd></dl>

</dd></dl>

<div class="section" id="passing-additional-data-to-callback-functions">
<span id="topics-request-response-ref-request-callback-arguments"></span><h5>Passing additional data to callback functions<a class="headerlink" href="#passing-additional-data-to-callback-functions" title="Permalink to this headline">¶</a></h5>
<p>The callback of a request is a function that will be called when the response
of that request is downloaded. The callback function will be called with the
downloaded <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object as its first argument.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">parse_page1</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="s2">&quot;http://www.example.com/some_page.html&quot;</span><span class="p">,</span>
                          <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse_page2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">parse_page2</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="c1"># this would log http://www.example.com/some_page.html</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Visited </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>
</pre></div>
</div>
<p>In some cases you may be interested in passing arguments to those callback
functions so you can receive the arguments later, in the second callback.
The following example shows how to achieve this by using the
<a class="reference internal" href="#scrapy.http.Request.cb_kwargs" title="scrapy.http.Request.cb_kwargs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.cb_kwargs</span></code></a> attribute:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="n">request</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="s1">&#39;http://www.example.com/index.html&#39;</span><span class="p">,</span>
                             <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse_page2</span><span class="p">,</span>
                             <span class="n">cb_kwargs</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">main_url</span><span class="o">=</span><span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">))</span>
    <span class="n">request</span><span class="o">.</span><span class="n">cb_kwargs</span><span class="p">[</span><span class="s1">&#39;foo&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;bar&#39;</span>  <span class="c1"># add more arguments for the callback</span>
    <span class="k">yield</span> <span class="n">request</span>

<span class="k">def</span> <span class="nf">parse_page2</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">main_url</span><span class="p">,</span> <span class="n">foo</span><span class="p">):</span>
    <span class="k">yield</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">main_url</span><span class="o">=</span><span class="n">main_url</span><span class="p">,</span>
        <span class="n">other_url</span><span class="o">=</span><span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">,</span>
        <span class="n">foo</span><span class="o">=</span><span class="n">foo</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
<div class="admonition caution">
<p class="first admonition-title">Caution</p>
<p class="last"><a class="reference internal" href="#scrapy.http.Request.cb_kwargs" title="scrapy.http.Request.cb_kwargs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.cb_kwargs</span></code></a> was introduced in version <code class="docutils literal notranslate"><span class="pre">1.7</span></code>.
Prior to that, using <a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code></a> was recommended for passing
information around callbacks. After <code class="docutils literal notranslate"><span class="pre">1.7</span></code>, <a class="reference internal" href="#scrapy.http.Request.cb_kwargs" title="scrapy.http.Request.cb_kwargs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.cb_kwargs</span></code></a>
became the preferred way for handling user information, leaving <a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code></a>
for communication with components like middlewares and extensions.</p>
</div>
</div>
<div class="section" id="using-errbacks-to-catch-exceptions-in-request-processing">
<span id="topics-request-response-ref-errbacks"></span><h5>Using errbacks to catch exceptions in request processing<a class="headerlink" href="#using-errbacks-to-catch-exceptions-in-request-processing" title="Permalink to this headline">¶</a></h5>
<p>The errback of a request is a function that will be called when an exception
is raise while processing it.</p>
<p>It receives a <a class="reference external" href="https://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html">Twisted Failure</a> instance as first parameter and can be
used to track connection establishment timeouts, DNS errors etc.</p>
<p>Here’s an example spider logging all errors and catching some specific
errors if needed:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="kn">from</span> <span class="nn">scrapy.spidermiddlewares.httperror</span> <span class="k">import</span> <span class="n">HttpError</span>
<span class="kn">from</span> <span class="nn">twisted.internet.error</span> <span class="k">import</span> <span class="n">DNSLookupError</span>
<span class="kn">from</span> <span class="nn">twisted.internet.error</span> <span class="k">import</span> <span class="ne">TimeoutError</span><span class="p">,</span> <span class="n">TCPTimedOutError</span>

<span class="k">class</span> <span class="nc">ErrbackSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;errback_example&quot;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;http://www.httpbin.org/&quot;</span><span class="p">,</span>              <span class="c1"># HTTP 200 expected</span>
        <span class="s2">&quot;http://www.httpbin.org/status/404&quot;</span><span class="p">,</span>    <span class="c1"># Not found error</span>
        <span class="s2">&quot;http://www.httpbin.org/status/500&quot;</span><span class="p">,</span>    <span class="c1"># server issue</span>
        <span class="s2">&quot;http://www.httpbin.org:12345/&quot;</span><span class="p">,</span>        <span class="c1"># non-responding host, timeout expected</span>
        <span class="s2">&quot;http://www.httphttpbinbin.org/&quot;</span><span class="p">,</span>       <span class="c1"># DNS error expected</span>
    <span class="p">]</span>

    <span class="k">def</span> <span class="nf">start_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">u</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_urls</span><span class="p">:</span>
            <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse_httpbin</span><span class="p">,</span>
                                    <span class="n">errback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">errback_httpbin</span><span class="p">,</span>
                                    <span class="n">dont_filter</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">parse_httpbin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Got successful response from </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">))</span>
        <span class="c1"># do something useful here...</span>

    <span class="k">def</span> <span class="nf">errback_httpbin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">failure</span><span class="p">):</span>
        <span class="c1"># log all failures</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="nb">repr</span><span class="p">(</span><span class="n">failure</span><span class="p">))</span>

        <span class="c1"># in case you want to do something special for some errors,</span>
        <span class="c1"># you may need the failure&#39;s type:</span>

        <span class="k">if</span> <span class="n">failure</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="n">HttpError</span><span class="p">):</span>
            <span class="c1"># these exceptions come from HttpError spider middleware</span>
            <span class="c1"># you can get the non-200 response</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">failure</span><span class="o">.</span><span class="n">value</span><span class="o">.</span><span class="n">response</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s1">&#39;HttpError on </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>

        <span class="k">elif</span> <span class="n">failure</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="n">DNSLookupError</span><span class="p">):</span>
            <span class="c1"># this is the original request</span>
            <span class="n">request</span> <span class="o">=</span> <span class="n">failure</span><span class="o">.</span><span class="n">request</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s1">&#39;DNSLookupError on </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">request</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>

        <span class="k">elif</span> <span class="n">failure</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="ne">TimeoutError</span><span class="p">,</span> <span class="n">TCPTimedOutError</span><span class="p">):</span>
            <span class="n">request</span> <span class="o">=</span> <span class="n">failure</span><span class="o">.</span><span class="n">request</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s1">&#39;TimeoutError on </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">request</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="request-meta-special-keys">
<span id="topics-request-meta"></span><h4>Request.meta special keys<a class="headerlink" href="#request-meta-special-keys" title="Permalink to this headline">¶</a></h4>
<p>The <a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code></a> attribute can contain any arbitrary data, but there
are some special keys recognized by Scrapy and its built-in extensions.</p>
<p>Those are:</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:reqmeta-dont_redirect"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">dont_redirect</span></code></a></li>
<li><a class="reference internal" href="index.html#std:reqmeta-dont_retry"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">dont_retry</span></code></a></li>
<li><a class="reference internal" href="index.html#std:reqmeta-handle_httpstatus_list"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">handle_httpstatus_list</span></code></a></li>
<li><a class="reference internal" href="index.html#std:reqmeta-handle_httpstatus_all"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">handle_httpstatus_all</span></code></a></li>
<li><a class="reference internal" href="#std:reqmeta-dont_merge_cookies"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">dont_merge_cookies</span></code></a></li>
<li><a class="reference internal" href="index.html#std:reqmeta-cookiejar"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">cookiejar</span></code></a></li>
<li><a class="reference internal" href="index.html#std:reqmeta-dont_cache"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">dont_cache</span></code></a></li>
<li><a class="reference internal" href="index.html#std:reqmeta-redirect_reasons"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">redirect_reasons</span></code></a></li>
<li><a class="reference internal" href="index.html#std:reqmeta-redirect_urls"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">redirect_urls</span></code></a></li>
<li><a class="reference internal" href="#std:reqmeta-bindaddress"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">bindaddress</span></code></a></li>
<li><a class="reference internal" href="index.html#std:reqmeta-dont_obey_robotstxt"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">dont_obey_robotstxt</span></code></a></li>
<li><a class="reference internal" href="#std:reqmeta-download_timeout"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">download_timeout</span></code></a></li>
<li><a class="reference internal" href="index.html#std:reqmeta-download_maxsize"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">download_maxsize</span></code></a></li>
<li><a class="reference internal" href="#std:reqmeta-download_latency"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">download_latency</span></code></a></li>
<li><a class="reference internal" href="#std:reqmeta-download_fail_on_dataloss"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">download_fail_on_dataloss</span></code></a></li>
<li><a class="reference internal" href="index.html#std:reqmeta-proxy"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">proxy</span></code></a></li>
<li><code class="docutils literal notranslate"><span class="pre">ftp_user</span></code> (See <a class="reference internal" href="index.html#std:setting-FTP_USER"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FTP_USER</span></code></a> for more info)</li>
<li><code class="docutils literal notranslate"><span class="pre">ftp_password</span></code> (See <a class="reference internal" href="index.html#std:setting-FTP_PASSWORD"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FTP_PASSWORD</span></code></a> for more info)</li>
<li><a class="reference internal" href="index.html#std:reqmeta-referrer_policy"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">referrer_policy</span></code></a></li>
<li><a class="reference internal" href="#std:reqmeta-max_retry_times"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">max_retry_times</span></code></a></li>
</ul>
<div class="section" id="bindaddress">
<span id="std:reqmeta-bindaddress"></span><h5>bindaddress<a class="headerlink" href="#bindaddress" title="Permalink to this headline">¶</a></h5>
<p>The IP of the outgoing IP address to use for the performing the request.</p>
</div>
<div class="section" id="download-timeout">
<span id="std:reqmeta-download_timeout"></span><h5>download_timeout<a class="headerlink" href="#download-timeout" title="Permalink to this headline">¶</a></h5>
<p>The amount of time (in secs) that the downloader will wait before timing out.
See also: <a class="reference internal" href="index.html#std:setting-DOWNLOAD_TIMEOUT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_TIMEOUT</span></code></a>.</p>
</div>
<div class="section" id="download-latency">
<span id="std:reqmeta-download_latency"></span><h5>download_latency<a class="headerlink" href="#download-latency" title="Permalink to this headline">¶</a></h5>
<p>The amount of time spent to fetch the response, since the request has been
started, i.e. HTTP message sent over the network. This meta key only becomes
available when the response has been downloaded. While most other meta keys are
used to control Scrapy behavior, this one is supposed to be read-only.</p>
</div>
<div class="section" id="download-fail-on-dataloss">
<span id="std:reqmeta-download_fail_on_dataloss"></span><h5>download_fail_on_dataloss<a class="headerlink" href="#download-fail-on-dataloss" title="Permalink to this headline">¶</a></h5>
<p>Whether or not to fail on broken responses. See:
<a class="reference internal" href="index.html#std:setting-DOWNLOAD_FAIL_ON_DATALOSS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_FAIL_ON_DATALOSS</span></code></a>.</p>
</div>
<div class="section" id="max-retry-times">
<span id="std:reqmeta-max_retry_times"></span><h5>max_retry_times<a class="headerlink" href="#max-retry-times" title="Permalink to this headline">¶</a></h5>
<p>The meta key is used set retry times per request. When initialized, the
<a class="reference internal" href="#std:reqmeta-max_retry_times"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">max_retry_times</span></code></a> meta key takes higher precedence over the
<a class="reference internal" href="index.html#std:setting-RETRY_TIMES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_TIMES</span></code></a> setting.</p>
</div>
</div>
<div class="section" id="request-subclasses">
<span id="topics-request-response-ref-request-subclasses"></span><h4>Request subclasses<a class="headerlink" href="#request-subclasses" title="Permalink to this headline">¶</a></h4>
<p>Here is the list of built-in <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> subclasses. You can also subclass
it to implement your own custom functionality.</p>
<div class="section" id="formrequest-objects">
<h5>FormRequest objects<a class="headerlink" href="#formrequest-objects" title="Permalink to this headline">¶</a></h5>
<p>The FormRequest class extends the base <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> with functionality for
dealing with HTML forms. It uses <a class="reference external" href="http://lxml.de/lxmlhtml.html#forms">lxml.html forms</a>  to pre-populate form
fields with form data from <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> objects.</p>
<dl class="class">
<dt id="scrapy.http.FormRequest">
<em class="property">class </em><code class="descclassname">scrapy.http.</code><code class="descname">FormRequest</code><span class="sig-paren">(</span><em>url</em><span class="optional">[</span>, <em>formdata</em>, <em>...</em><span class="optional">]</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.FormRequest" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#scrapy.http.FormRequest" title="scrapy.http.FormRequest"><code class="xref py py-class docutils literal notranslate"><span class="pre">FormRequest</span></code></a> class adds a new argument to the constructor. The
remaining arguments are the same as for the <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> class and are
not documented here.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>formdata</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.7)"><em>dict</em></a><em> or </em><em>iterable of tuples</em>) – is a dictionary (or iterable of (key, value) tuples)
containing HTML Form data which will be url-encoded and assigned to the
body of the request.</td>
</tr>
</tbody>
</table>
<p>The <a class="reference internal" href="#scrapy.http.FormRequest" title="scrapy.http.FormRequest"><code class="xref py py-class docutils literal notranslate"><span class="pre">FormRequest</span></code></a> objects support the following class method in
addition to the standard <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> methods:</p>
<dl class="classmethod">
<dt id="scrapy.http.FormRequest.from_response">
<em class="property">classmethod </em><code class="descname">from_response</code><span class="sig-paren">(</span><em>response</em><span class="optional">[</span>, <em>formname=None</em>, <em>formid=None</em>, <em>formnumber=0</em>, <em>formdata=None</em>, <em>formxpath=None</em>, <em>formcss=None</em>, <em>clickdata=None</em>, <em>dont_click=False</em>, <em>...</em><span class="optional">]</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.FormRequest.from_response" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new <a class="reference internal" href="#scrapy.http.FormRequest" title="scrapy.http.FormRequest"><code class="xref py py-class docutils literal notranslate"><span class="pre">FormRequest</span></code></a> object with its form field values
pre-populated with those found in the HTML <code class="docutils literal notranslate"><span class="pre">&lt;form&gt;</span></code> element contained
in the given response. For an example see
<a class="reference internal" href="#topics-request-response-ref-request-userlogin"><span class="std std-ref">Using FormRequest.from_response() to simulate a user login</span></a>.</p>
<p>The policy is to automatically simulate a click, by default, on any form
control that looks clickable, like a <code class="docutils literal notranslate"><span class="pre">&lt;input</span> <span class="pre">type=&quot;submit&quot;&gt;</span></code>.  Even
though this is quite convenient, and often the desired behaviour,
sometimes it can cause problems which could be hard to debug. For
example, when working with forms that are filled and/or submitted using
javascript, the default <a class="reference internal" href="#scrapy.http.FormRequest.from_response" title="scrapy.http.FormRequest.from_response"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_response()</span></code></a> behaviour may not be the
most appropriate. To disable this behaviour you can set the
<code class="docutils literal notranslate"><span class="pre">dont_click</span></code> argument to <code class="docutils literal notranslate"><span class="pre">True</span></code>. Also, if you want to change the
control clicked (instead of disabling it) you can also use the
<code class="docutils literal notranslate"><span class="pre">clickdata</span></code> argument.</p>
<div class="admonition caution">
<p class="first admonition-title">Caution</p>
<p class="last">Using this method with select elements which have leading
or trailing whitespace in the option values will not work due to a
<a class="reference external" href="https://bugs.launchpad.net/lxml/+bug/1665241">bug in lxml</a>, which should be fixed in lxml 3.8 and above.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>response</strong> (<a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object) – the response containing a HTML form which will be used
to pre-populate the form fields</li>
<li><strong>formname</strong> (<em>string</em>) – if given, the form with name attribute set to this value will be used.</li>
<li><strong>formid</strong> (<em>string</em>) – if given, the form with id attribute set to this value will be used.</li>
<li><strong>formxpath</strong> (<em>string</em>) – if given, the first form that matches the xpath will be used.</li>
<li><strong>formcss</strong> (<em>string</em>) – if given, the first form that matches the css selector will be used.</li>
<li><strong>formnumber</strong> (<em>integer</em>) – the number of form to use, when the response contains
multiple forms. The first one (and also the default) is <code class="docutils literal notranslate"><span class="pre">0</span></code>.</li>
<li><strong>formdata</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.7)"><em>dict</em></a>) – fields to override in the form data. If a field was
already present in the response <code class="docutils literal notranslate"><span class="pre">&lt;form&gt;</span></code> element, its value is
overridden by the one passed in this parameter. If a value passed in
this parameter is <code class="docutils literal notranslate"><span class="pre">None</span></code>, the field will not be included in the
request, even if it was present in the response <code class="docutils literal notranslate"><span class="pre">&lt;form&gt;</span></code> element.</li>
<li><strong>clickdata</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.7)"><em>dict</em></a>) – attributes to lookup the control clicked. If it’s not
given, the form data will be submitted simulating a click on the
first clickable element. In addition to html attributes, the control
can be identified by its zero-based index relative to other
submittable inputs inside the form, via the <code class="docutils literal notranslate"><span class="pre">nr</span></code> attribute.</li>
<li><strong>dont_click</strong> (<em>boolean</em>) – If True, the form data will be submitted without
clicking in any element.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>The other parameters of this class method are passed directly to the
<a class="reference internal" href="#scrapy.http.FormRequest" title="scrapy.http.FormRequest"><code class="xref py py-class docutils literal notranslate"><span class="pre">FormRequest</span></code></a> constructor.</p>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.10.3: </span>The <code class="docutils literal notranslate"><span class="pre">formname</span></code> parameter.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17: </span>The <code class="docutils literal notranslate"><span class="pre">formxpath</span></code> parameter.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.1.0: </span>The <code class="docutils literal notranslate"><span class="pre">formcss</span></code> parameter.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.1.0: </span>The <code class="docutils literal notranslate"><span class="pre">formid</span></code> parameter.</p>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="request-usage-examples">
<h5>Request usage examples<a class="headerlink" href="#request-usage-examples" title="Permalink to this headline">¶</a></h5>
<div class="section" id="using-formrequest-to-send-data-via-http-post">
<h6>Using FormRequest to send data via HTTP POST<a class="headerlink" href="#using-formrequest-to-send-data-via-http-post" title="Permalink to this headline">¶</a></h6>
<p>If you want to simulate a HTML Form POST in your spider and send a couple of
key-value fields, you can return a <a class="reference internal" href="#scrapy.http.FormRequest" title="scrapy.http.FormRequest"><code class="xref py py-class docutils literal notranslate"><span class="pre">FormRequest</span></code></a> object (from your
spider) like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">return</span> <span class="p">[</span><span class="n">FormRequest</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s2">&quot;http://www.example.com/post/action&quot;</span><span class="p">,</span>
                    <span class="n">formdata</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="s1">&#39;John Doe&#39;</span><span class="p">,</span> <span class="s1">&#39;age&#39;</span><span class="p">:</span> <span class="s1">&#39;27&#39;</span><span class="p">},</span>
                    <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">after_post</span><span class="p">)]</span>
</pre></div>
</div>
</div>
<div class="section" id="using-formrequest-from-response-to-simulate-a-user-login">
<span id="topics-request-response-ref-request-userlogin"></span><h6>Using FormRequest.from_response() to simulate a user login<a class="headerlink" href="#using-formrequest-from-response-to-simulate-a-user-login" title="Permalink to this headline">¶</a></h6>
<p>It is usual for web sites to provide pre-populated form fields through <code class="docutils literal notranslate"><span class="pre">&lt;input</span>
<span class="pre">type=&quot;hidden&quot;&gt;</span></code> elements, such as session related data or authentication
tokens (for login pages). When scraping, you’ll want these fields to be
automatically pre-populated and only override a couple of them, such as the
user name and password. You can use the <a class="reference internal" href="#scrapy.http.FormRequest.from_response" title="scrapy.http.FormRequest.from_response"><code class="xref py py-meth docutils literal notranslate"><span class="pre">FormRequest.from_response()</span></code></a>
method for this job. Here’s an example spider which uses it:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">def</span> <span class="nf">authentication_failed</span><span class="p">(</span><span class="n">response</span><span class="p">):</span>
    <span class="c1"># TODO: Check the contents of the response and return True if it failed</span>
    <span class="c1"># or False if it succeeded.</span>
    <span class="k">pass</span>

<span class="k">class</span> <span class="nc">LoginSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;example.com&#39;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;http://www.example.com/users/login.php&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">FormRequest</span><span class="o">.</span><span class="n">from_response</span><span class="p">(</span>
            <span class="n">response</span><span class="p">,</span>
            <span class="n">formdata</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;username&#39;</span><span class="p">:</span> <span class="s1">&#39;john&#39;</span><span class="p">,</span> <span class="s1">&#39;password&#39;</span><span class="p">:</span> <span class="s1">&#39;secret&#39;</span><span class="p">},</span>
            <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">after_login</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">after_login</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">authentication_failed</span><span class="p">(</span><span class="n">response</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;Login failed&quot;</span><span class="p">)</span>
            <span class="k">return</span>

        <span class="c1"># continue scraping with authenticated session...</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="jsonrequest">
<h5>JSONRequest<a class="headerlink" href="#jsonrequest" title="Permalink to this headline">¶</a></h5>
<p>The JSONRequest class extends the base <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> class with functionality for
dealing with JSON requests.</p>
<dl class="class">
<dt id="scrapy.http.JSONRequest">
<em class="property">class </em><code class="descclassname">scrapy.http.</code><code class="descname">JSONRequest</code><span class="sig-paren">(</span><em>url</em><span class="optional">[</span>, <em>... data</em>, <em>dumps_kwargs</em><span class="optional">]</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.JSONRequest" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#scrapy.http.JSONRequest" title="scrapy.http.JSONRequest"><code class="xref py py-class docutils literal notranslate"><span class="pre">JSONRequest</span></code></a> class adds two new argument to the constructor. The
remaining arguments are the same as for the <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> class and are
not documented here.</p>
<p>Using the <a class="reference internal" href="#scrapy.http.JSONRequest" title="scrapy.http.JSONRequest"><code class="xref py py-class docutils literal notranslate"><span class="pre">JSONRequest</span></code></a> will set the <code class="docutils literal notranslate"><span class="pre">Content-Type</span></code> header to <code class="docutils literal notranslate"><span class="pre">application/json</span></code>
and <code class="docutils literal notranslate"><span class="pre">Accept</span></code> header to <code class="docutils literal notranslate"><span class="pre">application/json,</span> <span class="pre">text/javascript,</span> <span class="pre">*/*;</span> <span class="pre">q=0.01</span></code></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>data</strong> (<em>JSON serializable object</em>) – is any JSON serializable object that needs to be JSON encoded and assigned to body.
if <a class="reference internal" href="#scrapy.http.Request.body" title="scrapy.http.Request.body"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.body</span></code></a> argument is provided this parameter will be ignored.
if <a class="reference internal" href="#scrapy.http.Request.body" title="scrapy.http.Request.body"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.body</span></code></a> argument is not provided and data argument is provided <a class="reference internal" href="#scrapy.http.Request.method" title="scrapy.http.Request.method"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.method</span></code></a> will be
set to <code class="docutils literal notranslate"><span class="pre">'POST'</span></code> automatically.</li>
<li><strong>dumps_kwargs</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.7)"><em>dict</em></a>) – Parameters that will be passed to underlying <a class="reference external" href="https://docs.python.org/3/library/json.html#json.dumps">json.dumps</a> method which is used to serialize
data into JSON format.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="jsonrequest-usage-example">
<h5>JSONRequest usage example<a class="headerlink" href="#jsonrequest-usage-example" title="Permalink to this headline">¶</a></h5>
<p>Sending a JSON POST request with a JSON payload:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;name1&#39;</span><span class="p">:</span> <span class="s1">&#39;value1&#39;</span><span class="p">,</span>
    <span class="s1">&#39;name2&#39;</span><span class="p">:</span> <span class="s1">&#39;value2&#39;</span><span class="p">,</span>
<span class="p">}</span>
<span class="k">yield</span> <span class="n">JSONRequest</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s1">&#39;http://www.example.com/post/action&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="response-objects">
<h4>Response objects<a class="headerlink" href="#response-objects" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="scrapy.http.Response">
<em class="property">class </em><code class="descclassname">scrapy.http.</code><code class="descname">Response</code><span class="sig-paren">(</span><em>url</em><span class="optional">[</span>, <em>status=200</em>, <em>headers=None</em>, <em>body=b''</em>, <em>flags=None</em>, <em>request=None</em><span class="optional">]</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.Response" title="Permalink to this definition">¶</a></dt>
<dd><p>A <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object represents an HTTP response, which is usually
downloaded (by the Downloader) and fed to the Spiders for processing.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>url</strong> (<em>string</em>) – the URL of this response</li>
<li><strong>status</strong> (<em>integer</em>) – the HTTP status of the response. Defaults to <code class="docutils literal notranslate"><span class="pre">200</span></code>.</li>
<li><strong>headers</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.7)"><em>dict</em></a>) – the headers of this response. The dict values can be strings
(for single valued headers) or lists (for multi-valued headers).</li>
<li><strong>body</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.7)"><em>bytes</em></a>) – the response body. To access the decoded text as str (unicode
in Python 2) you can use <code class="docutils literal notranslate"><span class="pre">response.text</span></code> from an encoding-aware
<a class="reference internal" href="#topics-request-response-ref-response-subclasses"><span class="std std-ref">Response subclass</span></a>,
such as <a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">TextResponse</span></code></a>.</li>
<li><strong>flags</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a>) – is a list containing the initial values for the
<a class="reference internal" href="#scrapy.http.Response.flags" title="scrapy.http.Response.flags"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Response.flags</span></code></a> attribute. If given, the list will be shallow
copied.</li>
<li><strong>request</strong> (<a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object) – the initial value of the <a class="reference internal" href="#scrapy.http.Response.request" title="scrapy.http.Response.request"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Response.request</span></code></a> attribute.
This represents the <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> that generated this response.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="scrapy.http.Response.url">
<code class="descname">url</code><a class="headerlink" href="#scrapy.http.Response.url" title="Permalink to this definition">¶</a></dt>
<dd><p>A string containing the URL of the response.</p>
<p>This attribute is read-only. To change the URL of a Response use
<a class="reference internal" href="#scrapy.http.Response.replace" title="scrapy.http.Response.replace"><code class="xref py py-meth docutils literal notranslate"><span class="pre">replace()</span></code></a>.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Response.status">
<code class="descname">status</code><a class="headerlink" href="#scrapy.http.Response.status" title="Permalink to this definition">¶</a></dt>
<dd><p>An integer representing the HTTP status of the response. Example: <code class="docutils literal notranslate"><span class="pre">200</span></code>,
<code class="docutils literal notranslate"><span class="pre">404</span></code>.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Response.headers">
<code class="descname">headers</code><a class="headerlink" href="#scrapy.http.Response.headers" title="Permalink to this definition">¶</a></dt>
<dd><p>A dictionary-like object which contains the response headers. Values can
be accessed using <code class="xref py py-meth docutils literal notranslate"><span class="pre">get()</span></code> to return the first header value with the
specified name or <code class="xref py py-meth docutils literal notranslate"><span class="pre">getlist()</span></code> to return all header values with the
specified name. For example, this call will give you all cookies in the
headers:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">response</span><span class="o">.</span><span class="n">headers</span><span class="o">.</span><span class="n">getlist</span><span class="p">(</span><span class="s1">&#39;Set-Cookie&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Response.body">
<code class="descname">body</code><a class="headerlink" href="#scrapy.http.Response.body" title="Permalink to this definition">¶</a></dt>
<dd><p>The body of this Response. Keep in mind that Response.body
is always a bytes object. If you want the unicode version use
<a class="reference internal" href="#scrapy.http.TextResponse.text" title="scrapy.http.TextResponse.text"><code class="xref py py-attr docutils literal notranslate"><span class="pre">TextResponse.text</span></code></a> (only available in <a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">TextResponse</span></code></a>
and subclasses).</p>
<p>This attribute is read-only. To change the body of a Response use
<a class="reference internal" href="#scrapy.http.Response.replace" title="scrapy.http.Response.replace"><code class="xref py py-meth docutils literal notranslate"><span class="pre">replace()</span></code></a>.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Response.request">
<code class="descname">request</code><a class="headerlink" href="#scrapy.http.Response.request" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object that generated this response. This attribute is
assigned in the Scrapy engine, after the response and the request have passed
through all <a class="reference internal" href="index.html#topics-downloader-middleware"><span class="std std-ref">Downloader Middlewares</span></a>.
In particular, this means that:</p>
<ul class="simple">
<li>HTTP redirections will cause the original request (to the URL before
redirection) to be assigned to the redirected response (with the final
URL after redirection).</li>
<li>Response.request.url doesn’t always equal Response.url</li>
<li>This attribute is only available in the spider code, and in the
<a class="reference internal" href="index.html#topics-spider-middleware"><span class="std std-ref">Spider Middlewares</span></a>, but not in
Downloader Middlewares (although you have the Request available there by
other means) and handlers of the <a class="reference internal" href="index.html#std:signal-response_downloaded"><code class="xref std std-signal docutils literal notranslate"><span class="pre">response_downloaded</span></code></a> signal.</li>
</ul>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Response.meta">
<code class="descname">meta</code><a class="headerlink" href="#scrapy.http.Response.meta" title="Permalink to this definition">¶</a></dt>
<dd><p>A shortcut to the <a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code></a> attribute of the
<a class="reference internal" href="#scrapy.http.Response.request" title="scrapy.http.Response.request"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Response.request</span></code></a> object (ie. <code class="docutils literal notranslate"><span class="pre">self.request.meta</span></code>).</p>
<p>Unlike the <a class="reference internal" href="#scrapy.http.Response.request" title="scrapy.http.Response.request"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Response.request</span></code></a> attribute, the <a class="reference internal" href="#scrapy.http.Response.meta" title="scrapy.http.Response.meta"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Response.meta</span></code></a>
attribute is propagated along redirects and retries, so you will get
the original <a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code></a> sent from your spider.</p>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference internal" href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code></a> attribute</p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.Response.flags">
<code class="descname">flags</code><a class="headerlink" href="#scrapy.http.Response.flags" title="Permalink to this definition">¶</a></dt>
<dd><p>A list that contains flags for this response. Flags are labels used for
tagging Responses. For example: <code class="docutils literal notranslate"><span class="pre">'cached'</span></code>, <code class="docutils literal notranslate"><span class="pre">'redirected</span></code>’, etc. And
they’re shown on the string representation of the Response (<cite>__str__</cite>
method) which is used by the engine for logging.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.http.Response.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.Response.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a new Response which is a copy of this Response.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.http.Response.replace">
<code class="descname">replace</code><span class="sig-paren">(</span><span class="optional">[</span><em>url</em>, <em>status</em>, <em>headers</em>, <em>body</em>, <em>request</em>, <em>flags</em>, <em>cls</em><span class="optional">]</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.Response.replace" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a Response object with the same members, except for those members
given new values by whichever keyword arguments are specified. The
attribute <a class="reference internal" href="#scrapy.http.Response.meta" title="scrapy.http.Response.meta"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Response.meta</span></code></a> is copied by default.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.http.Response.urljoin">
<code class="descname">urljoin</code><span class="sig-paren">(</span><em>url</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.Response.urljoin" title="Permalink to this definition">¶</a></dt>
<dd><p>Constructs an absolute url by combining the Response’s <a class="reference internal" href="#scrapy.http.Response.url" title="scrapy.http.Response.url"><code class="xref py py-attr docutils literal notranslate"><span class="pre">url</span></code></a> with
a possible relative url.</p>
<p>This is a wrapper over <a class="reference external" href="https://docs.python.org/2/library/urlparse.html#urlparse.urljoin">urlparse.urljoin</a>, it’s merely an alias for
making this call:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">urlparse</span><span class="o">.</span><span class="n">urljoin</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">,</span> <span class="n">url</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.http.Response.follow">
<code class="descname">follow</code><span class="sig-paren">(</span><em>url</em>, <em>callback=None</em>, <em>method='GET'</em>, <em>headers=None</em>, <em>body=None</em>, <em>cookies=None</em>, <em>meta=None</em>, <em>encoding='utf-8'</em>, <em>priority=0</em>, <em>dont_filter=False</em>, <em>errback=None</em>, <em>cb_kwargs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.Response.follow" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> instance to follow a link <code class="docutils literal notranslate"><span class="pre">url</span></code>.
It accepts the same arguments as <code class="docutils literal notranslate"><span class="pre">Request.__init__</span></code> method,
but <code class="docutils literal notranslate"><span class="pre">url</span></code> can be a relative URL or a <code class="docutils literal notranslate"><span class="pre">scrapy.link.Link</span></code> object,
not only an absolute URL.</p>
<p><a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">TextResponse</span></code></a> provides a <a class="reference internal" href="#scrapy.http.TextResponse.follow" title="scrapy.http.TextResponse.follow"><code class="xref py py-meth docutils literal notranslate"><span class="pre">follow()</span></code></a> 
method which supports selectors in addition to absolute/relative URLs
and Link objects.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="response-subclasses">
<span id="topics-request-response-ref-response-subclasses"></span><h4>Response subclasses<a class="headerlink" href="#response-subclasses" title="Permalink to this headline">¶</a></h4>
<p>Here is the list of available built-in Response subclasses. You can also
subclass the Response class to implement your own functionality.</p>
<div class="section" id="textresponse-objects">
<h5>TextResponse objects<a class="headerlink" href="#textresponse-objects" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.http.TextResponse">
<em class="property">class </em><code class="descclassname">scrapy.http.</code><code class="descname">TextResponse</code><span class="sig-paren">(</span><em>url</em><span class="optional">[</span>, <em>encoding</em><span class="optional">[</span>, <em>...</em><span class="optional">]</span><span class="optional">]</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.TextResponse" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">TextResponse</span></code></a> objects adds encoding capabilities to the base
<a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> class, which is meant to be used only for binary data,
such as images, sounds or any media file.</p>
<p><a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">TextResponse</span></code></a> objects support a new constructor argument, in
addition to the base <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> objects. The remaining functionality
is the same as for the <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> class and is not documented here.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>encoding</strong> (<em>string</em>) – is a string which contains the encoding to use for this
response. If you create a <a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">TextResponse</span></code></a> object with a unicode
body, it will be encoded using this encoding (remember the body attribute
is always a string). If <code class="docutils literal notranslate"><span class="pre">encoding</span></code> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default value), the
encoding will be looked up in the response headers and body instead.</td>
</tr>
</tbody>
</table>
<p><a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">TextResponse</span></code></a> objects support the following attributes in addition
to the standard <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> ones:</p>
<dl class="attribute">
<dt id="scrapy.http.TextResponse.text">
<code class="descname">text</code><a class="headerlink" href="#scrapy.http.TextResponse.text" title="Permalink to this definition">¶</a></dt>
<dd><p>Response body, as unicode.</p>
<p>The same as <code class="docutils literal notranslate"><span class="pre">response.body.decode(response.encoding)</span></code>, but the
result is cached after the first call, so you can access
<code class="docutils literal notranslate"><span class="pre">response.text</span></code> multiple times without extra overhead.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><code class="docutils literal notranslate"><span class="pre">unicode(response.body)</span></code> is not a correct way to convert response
body to unicode: you would be using the system default encoding
(typically <code class="docutils literal notranslate"><span class="pre">ascii</span></code>) instead of the response encoding.</p>
</div>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.TextResponse.encoding">
<code class="descname">encoding</code><a class="headerlink" href="#scrapy.http.TextResponse.encoding" title="Permalink to this definition">¶</a></dt>
<dd><p>A string with the encoding of this response. The encoding is resolved by
trying the following mechanisms, in order:</p>
<ol class="arabic simple">
<li>the encoding passed in the constructor <code class="docutils literal notranslate"><span class="pre">encoding</span></code> argument</li>
<li>the encoding declared in the Content-Type HTTP header. If this
encoding is not valid (ie. unknown), it is ignored and the next
resolution mechanism is tried.</li>
<li>the encoding declared in the response body. The TextResponse class
doesn’t provide any special functionality for this. However, the
<a class="reference internal" href="#scrapy.http.HtmlResponse" title="scrapy.http.HtmlResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">HtmlResponse</span></code></a> and <a class="reference internal" href="#scrapy.http.XmlResponse" title="scrapy.http.XmlResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">XmlResponse</span></code></a> classes do.</li>
<li>the encoding inferred by looking at the response body. This is the more
fragile method but also the last one tried.</li>
</ol>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.http.TextResponse.selector">
<code class="descname">selector</code><a class="headerlink" href="#scrapy.http.TextResponse.selector" title="Permalink to this definition">¶</a></dt>
<dd><p>A <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a> instance using the response as
target. The selector is lazily instantiated on first access.</p>
</dd></dl>

<p><a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">TextResponse</span></code></a> objects support the following methods in addition to
the standard <a class="reference internal" href="#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> ones:</p>
<dl class="method">
<dt id="scrapy.http.TextResponse.xpath">
<code class="descname">xpath</code><span class="sig-paren">(</span><em>query</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.TextResponse.xpath" title="Permalink to this definition">¶</a></dt>
<dd><p>A shortcut to <code class="docutils literal notranslate"><span class="pre">TextResponse.selector.xpath(query)</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//p&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.http.TextResponse.css">
<code class="descname">css</code><span class="sig-paren">(</span><em>query</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.TextResponse.css" title="Permalink to this definition">¶</a></dt>
<dd><p>A shortcut to <code class="docutils literal notranslate"><span class="pre">TextResponse.selector.css(query)</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;p&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="scrapy.http.TextResponse.follow">
<code class="descname">follow</code><span class="sig-paren">(</span><em>url</em>, <em>callback=None</em>, <em>method='GET'</em>, <em>headers=None</em>, <em>body=None</em>, <em>cookies=None</em>, <em>meta=None</em>, <em>encoding=None</em>, <em>priority=0</em>, <em>dont_filter=False</em>, <em>errback=None</em>, <em>cb_kwargs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.TextResponse.follow" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a <a class="reference internal" href="#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> instance to follow a link <code class="docutils literal notranslate"><span class="pre">url</span></code>.
It accepts the same arguments as <code class="docutils literal notranslate"><span class="pre">Request.__init__</span></code> method,
but <code class="docutils literal notranslate"><span class="pre">url</span></code> can be not only an absolute URL, but also</p>
<ul class="simple">
<li>a relative URL;</li>
<li>a scrapy.link.Link object (e.g. a link extractor result);</li>
<li>an attribute Selector (not SelectorList) - e.g.
<code class="docutils literal notranslate"><span class="pre">response.css('a::attr(href)')[0]</span></code> or
<code class="docutils literal notranslate"><span class="pre">response.xpath('//img/&#64;src')[0]</span></code>.</li>
<li>a Selector for <code class="docutils literal notranslate"><span class="pre">&lt;a&gt;</span></code> or <code class="docutils literal notranslate"><span class="pre">&lt;link&gt;</span></code> element, e.g.
<code class="docutils literal notranslate"><span class="pre">response.css('a.my_link')[0]</span></code>.</li>
</ul>
<p>See <a class="reference internal" href="index.html#response-follow-example"><span class="std std-ref">A shortcut for creating Requests</span></a> for usage examples.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.http.TextResponse.body_as_unicode">
<code class="descname">body_as_unicode</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.TextResponse.body_as_unicode" title="Permalink to this definition">¶</a></dt>
<dd><p>The same as <a class="reference internal" href="#scrapy.http.TextResponse.text" title="scrapy.http.TextResponse.text"><code class="xref py py-attr docutils literal notranslate"><span class="pre">text</span></code></a>, but available as a method. This method is
kept for backward compatibility; please prefer <code class="docutils literal notranslate"><span class="pre">response.text</span></code>.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="htmlresponse-objects">
<h5>HtmlResponse objects<a class="headerlink" href="#htmlresponse-objects" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.http.HtmlResponse">
<em class="property">class </em><code class="descclassname">scrapy.http.</code><code class="descname">HtmlResponse</code><span class="sig-paren">(</span><em>url</em><span class="optional">[</span>, <em>...</em><span class="optional">]</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.HtmlResponse" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#scrapy.http.HtmlResponse" title="scrapy.http.HtmlResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">HtmlResponse</span></code></a> class is a subclass of <a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">TextResponse</span></code></a>
which adds encoding auto-discovering support by looking into the HTML <a class="reference external" href="https://www.w3schools.com/TAGS/att_meta_http_equiv.asp">meta
http-equiv</a> attribute.  See <a class="reference internal" href="#scrapy.http.TextResponse.encoding" title="scrapy.http.TextResponse.encoding"><code class="xref py py-attr docutils literal notranslate"><span class="pre">TextResponse.encoding</span></code></a>.</p>
</dd></dl>

</div>
<div class="section" id="xmlresponse-objects">
<h5>XmlResponse objects<a class="headerlink" href="#xmlresponse-objects" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.http.XmlResponse">
<em class="property">class </em><code class="descclassname">scrapy.http.</code><code class="descname">XmlResponse</code><span class="sig-paren">(</span><em>url</em><span class="optional">[</span>, <em>...</em><span class="optional">]</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.http.XmlResponse" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#scrapy.http.XmlResponse" title="scrapy.http.XmlResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">XmlResponse</span></code></a> class is a subclass of <a class="reference internal" href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse"><code class="xref py py-class docutils literal notranslate"><span class="pre">TextResponse</span></code></a> which
adds encoding auto-discovering support by looking into the XML declaration
line.  See <a class="reference internal" href="#scrapy.http.TextResponse.encoding" title="scrapy.http.TextResponse.encoding"><code class="xref py py-attr docutils literal notranslate"><span class="pre">TextResponse.encoding</span></code></a>.</p>
</dd></dl>

</div>
</div>
</div>
<span id="document-topics/link-extractors"></span><div class="section" id="link-extractors">
<span id="topics-link-extractors"></span><h3>Link Extractors<a class="headerlink" href="#link-extractors" title="Permalink to this headline">¶</a></h3>
<p>Link extractors are objects whose only purpose is to extract links from web
pages (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.http.Response</span></code></a> objects) which will be eventually
followed.</p>
<p>There is <code class="docutils literal notranslate"><span class="pre">scrapy.linkextractors.LinkExtractor</span></code> available
in Scrapy, but you can create your own custom Link Extractors to suit your
needs by implementing a simple interface.</p>
<p>The only public method that every link extractor has is <code class="docutils literal notranslate"><span class="pre">extract_links</span></code>,
which receives a <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object and returns a list
of <code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.link.Link</span></code> objects. Link extractors are meant to be
instantiated once and their <code class="docutils literal notranslate"><span class="pre">extract_links</span></code> method called several times
with different responses to extract links to follow.</p>
<p>Link extractors are used in the <a class="reference internal" href="index.html#scrapy.spiders.CrawlSpider" title="scrapy.spiders.CrawlSpider"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlSpider</span></code></a>
class (available in Scrapy), through a set of rules, but you can also use it in
your spiders, even if you don’t subclass from
<a class="reference internal" href="index.html#scrapy.spiders.CrawlSpider" title="scrapy.spiders.CrawlSpider"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlSpider</span></code></a>, as its purpose is very simple: to
extract links.</p>
<div class="section" id="module-scrapy.linkextractors">
<span id="built-in-link-extractors-reference"></span><span id="topics-link-extractors-ref"></span><h4>Built-in link extractors reference<a class="headerlink" href="#module-scrapy.linkextractors" title="Permalink to this headline">¶</a></h4>
<p>Link extractors classes bundled with Scrapy are provided in the
<a class="reference internal" href="#module-scrapy.linkextractors" title="scrapy.linkextractors: Link extractors classes"><code class="xref py py-mod docutils literal notranslate"><span class="pre">scrapy.linkextractors</span></code></a> module.</p>
<p>The default link extractor is <code class="docutils literal notranslate"><span class="pre">LinkExtractor</span></code>, which is the same as
<a class="reference internal" href="#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"><code class="xref py py-class docutils literal notranslate"><span class="pre">LxmlLinkExtractor</span></code></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.linkextractors</span> <span class="k">import</span> <span class="n">LinkExtractor</span>
</pre></div>
</div>
<p>There used to be other link extractor classes in previous Scrapy versions,
but they are deprecated now.</p>
<div class="section" id="module-scrapy.linkextractors.lxmlhtml">
<span id="lxmllinkextractor"></span><h5>LxmlLinkExtractor<a class="headerlink" href="#module-scrapy.linkextractors.lxmlhtml" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor">
<em class="property">class </em><code class="descclassname">scrapy.linkextractors.lxmlhtml.</code><code class="descname">LxmlLinkExtractor</code><span class="sig-paren">(</span><em>allow=()</em>, <em>deny=()</em>, <em>allow_domains=()</em>, <em>deny_domains=()</em>, <em>deny_extensions=None</em>, <em>restrict_xpaths=()</em>, <em>restrict_css=()</em>, <em>tags=('a'</em>, <em>'area')</em>, <em>attrs=('href'</em>, <em>)</em>, <em>canonicalize=False</em>, <em>unique=True</em>, <em>process_value=None</em>, <em>strip=True</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="Permalink to this definition">¶</a></dt>
<dd><p>LxmlLinkExtractor is the recommended link extractor with handy filtering
options. It is implemented using lxml’s robust HTMLParser.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>allow</strong> (<em>a regular expression</em><em> (or </em><em>list of</em><em>)</em>) – a single regular expression (or list of regular expressions)
that the (absolute) urls must match in order to be extracted. If not
given (or empty), it will match all links.</li>
<li><strong>deny</strong> (<em>a regular expression</em><em> (or </em><em>list of</em><em>)</em>) – a single regular expression (or list of regular expressions)
that the (absolute) urls must match in order to be excluded (ie. not
extracted). It has precedence over the <code class="docutils literal notranslate"><span class="pre">allow</span></code> parameter. If not
given (or empty) it won’t exclude any links.</li>
<li><strong>allow_domains</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a>) – a single value or a list of string containing
domains which will be considered for extracting the links</li>
<li><strong>deny_domains</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a>) – a single value or a list of strings containing
domains which won’t be considered for extracting the links</li>
<li><strong>deny_extensions</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a>) – a single value or list of strings containing
extensions that should be ignored when extracting links.
If not given, it will default to the
<code class="docutils literal notranslate"><span class="pre">IGNORED_EXTENSIONS</span></code> list defined in the
<a class="reference external" href="https://github.com/scrapy/scrapy/blob/master/scrapy/linkextractors/__init__.py">scrapy.linkextractors</a> package.</li>
<li><strong>restrict_xpaths</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a>) – is an XPath (or list of XPath’s) which defines
regions inside the response where links should be extracted from.
If given, only the text selected by those XPath will be scanned for
links. See examples below.</li>
<li><strong>restrict_css</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a>) – a CSS selector (or list of selectors) which defines
regions inside the response where links should be extracted from.
Has the same behaviour as <code class="docutils literal notranslate"><span class="pre">restrict_xpaths</span></code>.</li>
<li><strong>restrict_text</strong> (<em>a regular expression</em><em> (or </em><em>list of</em><em>)</em>) – a single regular expression (or list of regular expressions)
that the link’s text must match in order to be extracted. If not
given (or empty), it will match all links. If a list of regular expressions is
given, the link will be extracted if it matches at least one.</li>
<li><strong>tags</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a>) – a tag or a list of tags to consider when extracting links.
Defaults to <code class="docutils literal notranslate"><span class="pre">('a',</span> <span class="pre">'area')</span></code>.</li>
<li><strong>attrs</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a>) – an attribute or list of attributes which should be considered when looking
for links to extract (only for those tags specified in the <code class="docutils literal notranslate"><span class="pre">tags</span></code>
parameter). Defaults to <code class="docutils literal notranslate"><span class="pre">('href',)</span></code></li>
<li><strong>canonicalize</strong> (<em>boolean</em>) – canonicalize each extracted url (using
w3lib.url.canonicalize_url). Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>.
Note that canonicalize_url is meant for duplicate checking;
it can change the URL visible at server side, so the response can be
different for requests with canonicalized and raw URLs. If you’re
using LinkExtractor to follow links it is more robust to
keep the default <code class="docutils literal notranslate"><span class="pre">canonicalize=False</span></code>.</li>
<li><strong>unique</strong> (<em>boolean</em>) – whether duplicate filtering should be applied to extracted
links.</li>
<li><strong>process_value</strong> (<em>callable</em>) – <p>a function which receives each value extracted from
the tag and attributes scanned and can modify the value and return a
new one, or return <code class="docutils literal notranslate"><span class="pre">None</span></code> to ignore the link altogether. If not
given, <code class="docutils literal notranslate"><span class="pre">process_value</span></code> defaults to <code class="docutils literal notranslate"><span class="pre">lambda</span> <span class="pre">x:</span> <span class="pre">x</span></code>.</p>
<p>For example, to extract links from this code:</p>
<div class="highlight-html notranslate"><div class="highlight"><pre><span></span><span class="p">&lt;</span><span class="nt">a</span> <span class="na">href</span><span class="o">=</span><span class="s">&quot;javascript:goToPage(&#39;../other/page.html&#39;); return false&quot;</span><span class="p">&gt;</span>Link text<span class="p">&lt;/</span><span class="nt">a</span><span class="p">&gt;</span>
</pre></div>
</div>
<p>You can use the following function in <code class="docutils literal notranslate"><span class="pre">process_value</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">process_value</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="s2">&quot;javascript:goToPage\(&#39;(.*?)&#39;&quot;</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">m</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">m</span><span class="o">.</span><span class="n">group</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><strong>strip</strong> (<em>boolean</em>) – whether to strip whitespaces from extracted attributes.
According to HTML5 standard, leading and trailing whitespaces
must be stripped from <code class="docutils literal notranslate"><span class="pre">href</span></code> attributes of <code class="docutils literal notranslate"><span class="pre">&lt;a&gt;</span></code>, <code class="docutils literal notranslate"><span class="pre">&lt;area&gt;</span></code>
and many other elements, <code class="docutils literal notranslate"><span class="pre">src</span></code> attribute of <code class="docutils literal notranslate"><span class="pre">&lt;img&gt;</span></code>, <code class="docutils literal notranslate"><span class="pre">&lt;iframe&gt;</span></code>
elements, etc., so LinkExtractor strips space chars by default.
Set <code class="docutils literal notranslate"><span class="pre">strip=False</span></code> to turn it off (e.g. if you’re extracting urls
from elements or attributes which allow leading/trailing whitespaces).</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
</div>
<span id="document-topics/settings"></span><div class="section" id="settings">
<span id="topics-settings"></span><h3>Settings<a class="headerlink" href="#settings" title="Permalink to this headline">¶</a></h3>
<p>The Scrapy settings allows you to customize the behaviour of all Scrapy
components, including the core, extensions, pipelines and spiders themselves.</p>
<p>The infrastructure of the settings provides a global namespace of key-value mappings
that the code can use to pull configuration values from. The settings can be
populated through different mechanisms, which are described below.</p>
<p>The settings are also the mechanism for selecting the currently active Scrapy
project (in case you have many).</p>
<p>For a list of available built-in settings see: <a class="reference internal" href="#topics-settings-ref"><span class="std std-ref">Built-in settings reference</span></a>.</p>
<div class="section" id="designating-the-settings">
<span id="topics-settings-module-envvar"></span><h4>Designating the settings<a class="headerlink" href="#designating-the-settings" title="Permalink to this headline">¶</a></h4>
<p>When you use Scrapy, you have to tell it which settings you’re using. You can
do this by using an environment variable, <code class="docutils literal notranslate"><span class="pre">SCRAPY_SETTINGS_MODULE</span></code>.</p>
<p>The value of <code class="docutils literal notranslate"><span class="pre">SCRAPY_SETTINGS_MODULE</span></code> should be in Python path syntax, e.g.
<code class="docutils literal notranslate"><span class="pre">myproject.settings</span></code>. Note that the settings module should be on the
Python <a class="reference external" href="https://docs.python.org/2/tutorial/modules.html#the-module-search-path">import search path</a>.</p>
</div>
<div class="section" id="populating-the-settings">
<span id="populating-settings"></span><h4>Populating the settings<a class="headerlink" href="#populating-the-settings" title="Permalink to this headline">¶</a></h4>
<p>Settings can be populated using different mechanisms, each of which having a
different precedence. Here is the list of them in decreasing order of
precedence:</p>
<blockquote>
<div><ol class="arabic simple">
<li>Command line options (most precedence)</li>
<li>Settings per-spider</li>
<li>Project settings module</li>
<li>Default settings per-command</li>
<li>Default global settings (less precedence)</li>
</ol>
</div></blockquote>
<p>The population of these settings sources is taken care of internally, but a
manual handling is possible using API calls. See the
<a class="reference internal" href="index.html#topics-api-settings"><span class="std std-ref">Settings API</span></a> topic for reference.</p>
<p>These mechanisms are described in more detail below.</p>
<div class="section" id="command-line-options">
<h5>1. Command line options<a class="headerlink" href="#command-line-options" title="Permalink to this headline">¶</a></h5>
<p>Arguments provided by the command line are the ones that take most precedence,
overriding any other options. You can explicitly override one (or more)
settings using the <code class="docutils literal notranslate"><span class="pre">-s</span></code> (or <code class="docutils literal notranslate"><span class="pre">--set</span></code>) command line option.</p>
<p>Example:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>scrapy crawl myspider -s <span class="nv">LOG_FILE</span><span class="o">=</span>scrapy.log
</pre></div>
</div>
</div>
<div class="section" id="settings-per-spider">
<h5>2. Settings per-spider<a class="headerlink" href="#settings-per-spider" title="Permalink to this headline">¶</a></h5>
<p>Spiders (See the <a class="reference internal" href="index.html#topics-spiders"><span class="std std-ref">Spiders</span></a> chapter for reference) can define their
own settings that will take precedence and override the project ones. They can
do so by setting their <a class="reference internal" href="index.html#scrapy.spiders.Spider.custom_settings" title="scrapy.spiders.Spider.custom_settings"><code class="xref py py-attr docutils literal notranslate"><span class="pre">custom_settings</span></code></a> attribute:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>class MySpider<span class="o">(</span>scrapy.Spider<span class="o">)</span>:
    <span class="nv">name</span> <span class="o">=</span> <span class="s1">&#39;myspider&#39;</span>

    <span class="nv">custom_settings</span> <span class="o">=</span> <span class="o">{</span>
        <span class="s1">&#39;SOME_SETTING&#39;</span>: <span class="s1">&#39;some value&#39;</span>,
    <span class="o">}</span>
</pre></div>
</div>
</div>
<div class="section" id="project-settings-module">
<h5>3. Project settings module<a class="headerlink" href="#project-settings-module" title="Permalink to this headline">¶</a></h5>
<p>The project settings module is the standard configuration file for your Scrapy
project, it’s where most of your custom settings will be populated. For a
standard Scrapy project, this means you’ll be adding or changing the settings
in the <code class="docutils literal notranslate"><span class="pre">settings.py</span></code> file created for your project.</p>
</div>
<div class="section" id="default-settings-per-command">
<h5>4. Default settings per-command<a class="headerlink" href="#default-settings-per-command" title="Permalink to this headline">¶</a></h5>
<p>Each <a class="reference internal" href="index.html#document-topics/commands"><span class="doc">Scrapy tool</span></a> command can have its own default
settings, which override the global default settings. Those custom command
settings are specified in the <code class="docutils literal notranslate"><span class="pre">default_settings</span></code> attribute of the command
class.</p>
</div>
<div class="section" id="default-global-settings">
<h5>5. Default global settings<a class="headerlink" href="#default-global-settings" title="Permalink to this headline">¶</a></h5>
<p>The global defaults are located in the <code class="docutils literal notranslate"><span class="pre">scrapy.settings.default_settings</span></code>
module and documented in the <a class="reference internal" href="#topics-settings-ref"><span class="std std-ref">Built-in settings reference</span></a> section.</p>
</div>
</div>
<div class="section" id="how-to-access-settings">
<h4>How to access settings<a class="headerlink" href="#how-to-access-settings" title="Permalink to this headline">¶</a></h4>
<p>In a spider, the settings are available through <code class="docutils literal notranslate"><span class="pre">self.settings</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;myspider&#39;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;http://example.com&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Existing settings: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">settings</span><span class="o">.</span><span class="n">attributes</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The <code class="docutils literal notranslate"><span class="pre">settings</span></code> attribute is set in the base Spider class after the spider
is initialized.  If you want to use the settings before the initialization
(e.g., in your spider’s <code class="docutils literal notranslate"><span class="pre">__init__()</span></code> method), you’ll need to override the
<a class="reference internal" href="index.html#scrapy.spiders.Spider.from_crawler" title="scrapy.spiders.Spider.from_crawler"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_crawler()</span></code></a> method.</p>
</div>
<p>Settings can be accessed through the <a class="reference internal" href="index.html#scrapy.crawler.Crawler.settings" title="scrapy.crawler.Crawler.settings"><code class="xref py py-attr docutils literal notranslate"><span class="pre">scrapy.crawler.Crawler.settings</span></code></a>
attribute of the Crawler that is passed to <code class="docutils literal notranslate"><span class="pre">from_crawler</span></code> method in
extensions, middlewares and item pipelines:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyExtension</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">log_is_enabled</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">log_is_enabled</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">&quot;log is enabled!&quot;</span><span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_crawler</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">crawler</span><span class="p">):</span>
        <span class="n">settings</span> <span class="o">=</span> <span class="n">crawler</span><span class="o">.</span><span class="n">settings</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">settings</span><span class="o">.</span><span class="n">getbool</span><span class="p">(</span><span class="s1">&#39;LOG_ENABLED&#39;</span><span class="p">))</span>
</pre></div>
</div>
<p>The settings object can be used like a dict (e.g.,
<code class="docutils literal notranslate"><span class="pre">settings['LOG_ENABLED']</span></code>), but it’s usually preferred to extract the setting
in the format you need it to avoid type errors, using one of the methods
provided by the <a class="reference internal" href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">Settings</span></code></a> API.</p>
</div>
<div class="section" id="rationale-for-setting-names">
<h4>Rationale for setting names<a class="headerlink" href="#rationale-for-setting-names" title="Permalink to this headline">¶</a></h4>
<p>Setting names are usually prefixed with the component that they configure. For
example, proper setting names for a fictional robots.txt extension would be
<code class="docutils literal notranslate"><span class="pre">ROBOTSTXT_ENABLED</span></code>, <code class="docutils literal notranslate"><span class="pre">ROBOTSTXT_OBEY</span></code>, <code class="docutils literal notranslate"><span class="pre">ROBOTSTXT_CACHEDIR</span></code>, etc.</p>
</div>
<div class="section" id="built-in-settings-reference">
<span id="topics-settings-ref"></span><h4>Built-in settings reference<a class="headerlink" href="#built-in-settings-reference" title="Permalink to this headline">¶</a></h4>
<p>Here’s a list of all available Scrapy settings, in alphabetical order, along
with their default values and the scope where they apply.</p>
<p>The scope, where available, shows where the setting is being used, if it’s tied
to any particular component. In that case the module of that component will be
shown, typically an extension, middleware or pipeline. It also means that the
component must be enabled in order for the setting to have any effect.</p>
<div class="section" id="aws-access-key-id">
<span id="std:setting-AWS_ACCESS_KEY_ID"></span><h5>AWS_ACCESS_KEY_ID<a class="headerlink" href="#aws-access-key-id" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p>
<p>The AWS access key used by code that requires access to <a class="reference external" href="https://aws.amazon.com/">Amazon Web services</a>,
such as the <a class="reference internal" href="index.html#topics-feed-storage-s3"><span class="std std-ref">S3 feed storage backend</span></a>.</p>
</div>
<div class="section" id="aws-secret-access-key">
<span id="std:setting-AWS_SECRET_ACCESS_KEY"></span><h5>AWS_SECRET_ACCESS_KEY<a class="headerlink" href="#aws-secret-access-key" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p>
<p>The AWS secret key used by code that requires access to <a class="reference external" href="https://aws.amazon.com/">Amazon Web services</a>,
such as the <a class="reference internal" href="index.html#topics-feed-storage-s3"><span class="std std-ref">S3 feed storage backend</span></a>.</p>
</div>
<div class="section" id="aws-endpoint-url">
<span id="std:setting-AWS_ENDPOINT_URL"></span><h5>AWS_ENDPOINT_URL<a class="headerlink" href="#aws-endpoint-url" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p>
<p>Endpoint URL used for S3-like storage, for example Minio or s3.scality.
Only supported with <code class="docutils literal notranslate"><span class="pre">botocore</span></code> library.</p>
</div>
<div class="section" id="aws-use-ssl">
<span id="std:setting-AWS_USE_SSL"></span><h5>AWS_USE_SSL<a class="headerlink" href="#aws-use-ssl" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p>
<p>Use this option if you want to disable SSL connection for communication with
S3 or S3-like storage. By default SSL will be used.
Only supported with <code class="docutils literal notranslate"><span class="pre">botocore</span></code> library.</p>
</div>
<div class="section" id="aws-verify">
<span id="std:setting-AWS_VERIFY"></span><h5>AWS_VERIFY<a class="headerlink" href="#aws-verify" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p>
<p>Verify SSL connection between Scrapy and S3 or S3-like storage. By default
SSL verification will occur. Only supported with <code class="docutils literal notranslate"><span class="pre">botocore</span></code> library.</p>
</div>
<div class="section" id="aws-region-name">
<span id="std:setting-AWS_REGION_NAME"></span><h5>AWS_REGION_NAME<a class="headerlink" href="#aws-region-name" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p>
<p>The name of the region associated with the AWS client.
Only supported with <code class="docutils literal notranslate"><span class="pre">botocore</span></code> library.</p>
</div>
<div class="section" id="bot-name">
<span id="std:setting-BOT_NAME"></span><h5>BOT_NAME<a class="headerlink" href="#bot-name" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapybot'</span></code></p>
<p>The name of the bot implemented by this Scrapy project (also known as the
project name). This will be used to construct the User-Agent by default, and
also for logging.</p>
<p>It’s automatically populated with your project name when you create your
project with the <a class="reference internal" href="index.html#std:command-startproject"><code class="xref std std-command docutils literal notranslate"><span class="pre">startproject</span></code></a> command.</p>
</div>
<div class="section" id="concurrent-items">
<span id="std:setting-CONCURRENT_ITEMS"></span><h5>CONCURRENT_ITEMS<a class="headerlink" href="#concurrent-items" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">100</span></code></p>
<p>Maximum number of concurrent items (per response) to process in parallel in the
Item Processor (also known as the <a class="reference internal" href="index.html#topics-item-pipeline"><span class="std std-ref">Item Pipeline</span></a>).</p>
</div>
<div class="section" id="concurrent-requests">
<span id="std:setting-CONCURRENT_REQUESTS"></span><h5>CONCURRENT_REQUESTS<a class="headerlink" href="#concurrent-requests" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">16</span></code></p>
<p>The maximum number of concurrent (ie. simultaneous) requests that will be
performed by the Scrapy downloader.</p>
</div>
<div class="section" id="concurrent-requests-per-domain">
<span id="std:setting-CONCURRENT_REQUESTS_PER_DOMAIN"></span><h5>CONCURRENT_REQUESTS_PER_DOMAIN<a class="headerlink" href="#concurrent-requests-per-domain" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">8</span></code></p>
<p>The maximum number of concurrent (ie. simultaneous) requests that will be
performed to any single domain.</p>
<p>See also: <a class="reference internal" href="index.html#topics-autothrottle"><span class="std std-ref">AutoThrottle extension</span></a> and its
<a class="reference internal" href="index.html#std:setting-AUTOTHROTTLE_TARGET_CONCURRENCY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code></a> option.</p>
</div>
<div class="section" id="concurrent-requests-per-ip">
<span id="std:setting-CONCURRENT_REQUESTS_PER_IP"></span><h5>CONCURRENT_REQUESTS_PER_IP<a class="headerlink" href="#concurrent-requests-per-ip" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">0</span></code></p>
<p>The maximum number of concurrent (ie. simultaneous) requests that will be
performed to any single IP. If non-zero, the
<a class="reference internal" href="#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a> setting is ignored, and this one is
used instead. In other words, concurrency limits will be applied per IP, not
per domain.</p>
<p>This setting also affects <a class="reference internal" href="#std:setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_DELAY</span></code></a> and
<a class="reference internal" href="index.html#topics-autothrottle"><span class="std std-ref">AutoThrottle extension</span></a>: if <a class="reference internal" href="#std:setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a>
is non-zero, download delay is enforced per IP, not per domain.</p>
</div>
<div class="section" id="default-item-class">
<span id="std:setting-DEFAULT_ITEM_CLASS"></span><h5>DEFAULT_ITEM_CLASS<a class="headerlink" href="#default-item-class" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapy.item.Item'</span></code></p>
<p>The default class that will be used for instantiating items in the <a class="reference internal" href="index.html#topics-shell"><span class="std std-ref">the
Scrapy shell</span></a>.</p>
</div>
<div class="section" id="default-request-headers">
<span id="std:setting-DEFAULT_REQUEST_HEADERS"></span><h5>DEFAULT_REQUEST_HEADERS<a class="headerlink" href="#default-request-headers" title="Permalink to this headline">¶</a></h5>
<p>Default:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;Accept&#39;</span><span class="p">:</span> <span class="s1">&#39;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#39;</span><span class="p">,</span>
    <span class="s1">&#39;Accept-Language&#39;</span><span class="p">:</span> <span class="s1">&#39;en&#39;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The default headers used for Scrapy HTTP Requests. They’re populated in the
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware" title="scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">DefaultHeadersMiddleware</span></code></a>.</p>
</div>
<div class="section" id="depth-limit">
<span id="std:setting-DEPTH_LIMIT"></span><h5>DEPTH_LIMIT<a class="headerlink" href="#depth-limit" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">0</span></code></p>
<p>Scope: <code class="docutils literal notranslate"><span class="pre">scrapy.spidermiddlewares.depth.DepthMiddleware</span></code></p>
<p>The maximum depth that will be allowed to crawl for any site. If zero, no limit
will be imposed.</p>
</div>
<div class="section" id="depth-priority">
<span id="std:setting-DEPTH_PRIORITY"></span><h5>DEPTH_PRIORITY<a class="headerlink" href="#depth-priority" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">0</span></code></p>
<p>Scope: <code class="docutils literal notranslate"><span class="pre">scrapy.spidermiddlewares.depth.DepthMiddleware</span></code></p>
<p>An integer that is used to adjust the <code class="xref py py-attr docutils literal notranslate"><span class="pre">priority</span></code> of
a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> based on its depth.</p>
<p>The priority of a request is adjusted as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">request</span><span class="o">.</span><span class="n">priority</span> <span class="o">=</span> <span class="n">request</span><span class="o">.</span><span class="n">priority</span> <span class="o">-</span> <span class="p">(</span> <span class="n">depth</span> <span class="o">*</span> <span class="n">DEPTH_PRIORITY</span> <span class="p">)</span>
</pre></div>
</div>
<p>As depth increases, positive values of <code class="docutils literal notranslate"><span class="pre">DEPTH_PRIORITY</span></code> decrease request
priority (BFO), while negative values increase request priority (DFO). See
also <a class="reference internal" href="index.html#faq-bfo-dfo"><span class="std std-ref">Does Scrapy crawl in breadth-first or depth-first order?</span></a>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This setting adjusts priority <strong>in the opposite way</strong> compared to
other priority settings <a class="reference internal" href="#std:setting-REDIRECT_PRIORITY_ADJUST"><code class="xref std std-setting docutils literal notranslate"><span class="pre">REDIRECT_PRIORITY_ADJUST</span></code></a>
and <a class="reference internal" href="#std:setting-RETRY_PRIORITY_ADJUST"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_PRIORITY_ADJUST</span></code></a>.</p>
</div>
</div>
<div class="section" id="depth-stats-verbose">
<span id="std:setting-DEPTH_STATS_VERBOSE"></span><h5>DEPTH_STATS_VERBOSE<a class="headerlink" href="#depth-stats-verbose" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>Scope: <code class="docutils literal notranslate"><span class="pre">scrapy.spidermiddlewares.depth.DepthMiddleware</span></code></p>
<p>Whether to collect verbose depth stats. If this is enabled, the number of
requests for each depth is collected in the stats.</p>
</div>
<div class="section" id="dnscache-enabled">
<span id="std:setting-DNSCACHE_ENABLED"></span><h5>DNSCACHE_ENABLED<a class="headerlink" href="#dnscache-enabled" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>Whether to enable DNS in-memory cache.</p>
</div>
<div class="section" id="dnscache-size">
<span id="std:setting-DNSCACHE_SIZE"></span><h5>DNSCACHE_SIZE<a class="headerlink" href="#dnscache-size" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">10000</span></code></p>
<p>DNS in-memory cache size.</p>
</div>
<div class="section" id="dns-timeout">
<span id="std:setting-DNS_TIMEOUT"></span><h5>DNS_TIMEOUT<a class="headerlink" href="#dns-timeout" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">60</span></code></p>
<p>Timeout for processing of DNS queries in seconds. Float is supported.</p>
</div>
<div class="section" id="downloader">
<span id="std:setting-DOWNLOADER"></span><h5>DOWNLOADER<a class="headerlink" href="#downloader" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapy.core.downloader.Downloader'</span></code></p>
<p>The downloader to use for crawling.</p>
</div>
<div class="section" id="downloader-httpclientfactory">
<span id="std:setting-DOWNLOADER_HTTPCLIENTFACTORY"></span><h5>DOWNLOADER_HTTPCLIENTFACTORY<a class="headerlink" href="#downloader-httpclientfactory" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapy.core.downloader.webclient.ScrapyHTTPClientFactory'</span></code></p>
<p>Defines a Twisted <code class="docutils literal notranslate"><span class="pre">protocol.ClientFactory</span></code>  class to use for HTTP/1.0
connections (for <code class="docutils literal notranslate"><span class="pre">HTTP10DownloadHandler</span></code>).</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">HTTP/1.0 is rarely used nowadays so you can safely ignore this setting,
unless you use Twisted&lt;11.1, or if you really want to use HTTP/1.0
and override <a class="reference internal" href="#std:setting-DOWNLOAD_HANDLERS_BASE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_HANDLERS_BASE</span></code></a> for <code class="docutils literal notranslate"><span class="pre">http(s)</span></code> scheme
accordingly, i.e. to
<code class="docutils literal notranslate"><span class="pre">'scrapy.core.downloader.handlers.http.HTTP10DownloadHandler'</span></code>.</p>
</div>
</div>
<div class="section" id="downloader-clientcontextfactory">
<span id="std:setting-DOWNLOADER_CLIENTCONTEXTFACTORY"></span><h5>DOWNLOADER_CLIENTCONTEXTFACTORY<a class="headerlink" href="#downloader-clientcontextfactory" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapy.core.downloader.contextfactory.ScrapyClientContextFactory'</span></code></p>
<p>Represents the classpath to the ContextFactory to use.</p>
<p>Here, “ContextFactory” is a Twisted term for SSL/TLS contexts, defining
the TLS/SSL protocol version to use, whether to do certificate verification,
or even enable client-side authentication (and various other things).</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>Scrapy default context factory <strong>does NOT perform remote server
certificate verification</strong>. This is usually fine for web scraping.</p>
<p class="last">If you do need remote server certificate verification enabled,
Scrapy also has another context factory class that you can set,
<code class="docutils literal notranslate"><span class="pre">'scrapy.core.downloader.contextfactory.BrowserLikeContextFactory'</span></code>,
which uses the platform’s certificates to validate remote endpoints.
<strong>This is only available if you use Twisted&gt;=14.0.</strong></p>
</div>
<p>If you do use a custom ContextFactory, make sure it accepts a <code class="docutils literal notranslate"><span class="pre">method</span></code>
parameter at init (this is the <code class="docutils literal notranslate"><span class="pre">OpenSSL.SSL</span></code> method mapping
<a class="reference internal" href="#std:setting-DOWNLOADER_CLIENT_TLS_METHOD"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_CLIENT_TLS_METHOD</span></code></a>).</p>
</div>
<div class="section" id="downloader-client-tls-method">
<span id="std:setting-DOWNLOADER_CLIENT_TLS_METHOD"></span><h5>DOWNLOADER_CLIENT_TLS_METHOD<a class="headerlink" href="#downloader-client-tls-method" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'TLS'</span></code></p>
<p>Use this setting to customize the TLS/SSL method used by the default
HTTP/1.1 downloader.</p>
<p>This setting must be one of these string values:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">'TLS'</span></code>: maps to OpenSSL’s <code class="docutils literal notranslate"><span class="pre">TLS_method()</span></code> (a.k.a <code class="docutils literal notranslate"><span class="pre">SSLv23_method()</span></code>),
which allows protocol negotiation, starting from the highest supported
by the platform; <strong>default, recommended</strong></li>
<li><code class="docutils literal notranslate"><span class="pre">'TLSv1.0'</span></code>: this value forces HTTPS connections to use TLS version 1.0 ;
set this if you want the behavior of Scrapy&lt;1.1</li>
<li><code class="docutils literal notranslate"><span class="pre">'TLSv1.1'</span></code>: forces TLS version 1.1</li>
<li><code class="docutils literal notranslate"><span class="pre">'TLSv1.2'</span></code>: forces TLS version 1.2</li>
<li><code class="docutils literal notranslate"><span class="pre">'SSLv3'</span></code>: forces SSL version 3 (<strong>not recommended</strong>)</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">We recommend that you use PyOpenSSL&gt;=0.13 and Twisted&gt;=0.13
or above (Twisted&gt;=14.0 if you can).</p>
</div>
</div>
<div class="section" id="downloader-middlewares">
<span id="std:setting-DOWNLOADER_MIDDLEWARES"></span><h5>DOWNLOADER_MIDDLEWARES<a class="headerlink" href="#downloader-middlewares" title="Permalink to this headline">¶</a></h5>
<p>Default:: <code class="docutils literal notranslate"><span class="pre">{}</span></code></p>
<p>A dict containing the downloader middlewares enabled in your project, and their
orders. For more info see <a class="reference internal" href="index.html#topics-downloader-middleware-setting"><span class="std std-ref">Activating a downloader middleware</span></a>.</p>
</div>
<div class="section" id="downloader-middlewares-base">
<span id="std:setting-DOWNLOADER_MIDDLEWARES_BASE"></span><h5>DOWNLOADER_MIDDLEWARES_BASE<a class="headerlink" href="#downloader-middlewares-base" title="Permalink to this headline">¶</a></h5>
<p>Default:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware&#39;</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware&#39;</span><span class="p">:</span> <span class="mi">300</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware&#39;</span><span class="p">:</span> <span class="mi">350</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware&#39;</span><span class="p">:</span> <span class="mi">400</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&#39;</span><span class="p">:</span> <span class="mi">500</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.retry.RetryMiddleware&#39;</span><span class="p">:</span> <span class="mi">550</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware&#39;</span><span class="p">:</span> <span class="mi">560</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware&#39;</span><span class="p">:</span> <span class="mi">580</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&#39;</span><span class="p">:</span> <span class="mi">590</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.redirect.RedirectMiddleware&#39;</span><span class="p">:</span> <span class="mi">600</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.cookies.CookiesMiddleware&#39;</span><span class="p">:</span> <span class="mi">700</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware&#39;</span><span class="p">:</span> <span class="mi">750</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.stats.DownloaderStats&#39;</span><span class="p">:</span> <span class="mi">850</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware&#39;</span><span class="p">:</span> <span class="mi">900</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>A dict containing the downloader middlewares enabled by default in Scrapy. Low
orders are closer to the engine, high orders are closer to the downloader. You
should never modify this setting in your project, modify
<a class="reference internal" href="#std:setting-DOWNLOADER_MIDDLEWARES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_MIDDLEWARES</span></code></a> instead.  For more info see
<a class="reference internal" href="index.html#topics-downloader-middleware-setting"><span class="std std-ref">Activating a downloader middleware</span></a>.</p>
</div>
<div class="section" id="downloader-stats">
<span id="std:setting-DOWNLOADER_STATS"></span><h5>DOWNLOADER_STATS<a class="headerlink" href="#downloader-stats" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>Whether to enable downloader stats collection.</p>
</div>
<div class="section" id="download-delay">
<span id="std:setting-DOWNLOAD_DELAY"></span><h5>DOWNLOAD_DELAY<a class="headerlink" href="#download-delay" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">0</span></code></p>
<p>The amount of time (in secs) that the downloader should wait before downloading
consecutive pages from the same website. This can be used to throttle the
crawling speed to avoid hitting servers too hard. Decimal numbers are
supported.  Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">DOWNLOAD_DELAY</span> <span class="o">=</span> <span class="mf">0.25</span>    <span class="c1"># 250 ms of delay</span>
</pre></div>
</div>
<p>This setting is also affected by the <a class="reference internal" href="#std:setting-RANDOMIZE_DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RANDOMIZE_DOWNLOAD_DELAY</span></code></a>
setting (which is enabled by default). By default, Scrapy doesn’t wait a fixed
amount of time between requests, but uses a random interval between 0.5 * <a class="reference internal" href="#std:setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_DELAY</span></code></a> and 1.5 * <a class="reference internal" href="#std:setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_DELAY</span></code></a>.</p>
<p>When <a class="reference internal" href="#std:setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a> is non-zero, delays are enforced
per ip address instead of per domain.</p>
<p id="spider-download-delay-attribute">You can also change this setting per spider by setting <code class="docutils literal notranslate"><span class="pre">download_delay</span></code>
spider attribute.</p>
</div>
<div class="section" id="download-handlers">
<span id="std:setting-DOWNLOAD_HANDLERS"></span><h5>DOWNLOAD_HANDLERS<a class="headerlink" href="#download-handlers" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">{}</span></code></p>
<p>A dict containing the request downloader handlers enabled in your project.
See <a class="reference internal" href="#std:setting-DOWNLOAD_HANDLERS_BASE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_HANDLERS_BASE</span></code></a> for example format.</p>
</div>
<div class="section" id="download-handlers-base">
<span id="std:setting-DOWNLOAD_HANDLERS_BASE"></span><h5>DOWNLOAD_HANDLERS_BASE<a class="headerlink" href="#download-handlers-base" title="Permalink to this headline">¶</a></h5>
<p>Default:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;file&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.core.downloader.handlers.file.FileDownloadHandler&#39;</span><span class="p">,</span>
    <span class="s1">&#39;http&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.core.downloader.handlers.http.HTTPDownloadHandler&#39;</span><span class="p">,</span>
    <span class="s1">&#39;https&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.core.downloader.handlers.http.HTTPDownloadHandler&#39;</span><span class="p">,</span>
    <span class="s1">&#39;s3&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.core.downloader.handlers.s3.S3DownloadHandler&#39;</span><span class="p">,</span>
    <span class="s1">&#39;ftp&#39;</span><span class="p">:</span> <span class="s1">&#39;scrapy.core.downloader.handlers.ftp.FTPDownloadHandler&#39;</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>A dict containing the request download handlers enabled by default in Scrapy.
You should never modify this setting in your project, modify
<a class="reference internal" href="#std:setting-DOWNLOAD_HANDLERS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_HANDLERS</span></code></a> instead.</p>
<p>You can disable any of these download handlers by assigning <code class="docutils literal notranslate"><span class="pre">None</span></code> to their
URI scheme in <a class="reference internal" href="#std:setting-DOWNLOAD_HANDLERS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_HANDLERS</span></code></a>. E.g., to disable the built-in FTP
handler (without replacement), place this in your <code class="docutils literal notranslate"><span class="pre">settings.py</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">DOWNLOAD_HANDLERS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;ftp&#39;</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="download-timeout">
<span id="std:setting-DOWNLOAD_TIMEOUT"></span><h5>DOWNLOAD_TIMEOUT<a class="headerlink" href="#download-timeout" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">180</span></code></p>
<p>The amount of time (in secs) that the downloader will wait before timing out.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This timeout can be set per spider using <code class="xref py py-attr docutils literal notranslate"><span class="pre">download_timeout</span></code>
spider attribute and per-request using <a class="reference internal" href="index.html#std:reqmeta-download_timeout"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">download_timeout</span></code></a>
Request.meta key.</p>
</div>
</div>
<div class="section" id="download-maxsize">
<span id="std:setting-DOWNLOAD_MAXSIZE"></span><h5>DOWNLOAD_MAXSIZE<a class="headerlink" href="#download-maxsize" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">1073741824</span></code> (1024MB)</p>
<p>The maximum response size (in bytes) that downloader will download.</p>
<p>If you want to disable it set to 0.</p>
<div class="admonition note" id="std:reqmeta-download_maxsize">
<p class="first admonition-title">Note</p>
<p>This size can be set per spider using <code class="xref py py-attr docutils literal notranslate"><span class="pre">download_maxsize</span></code>
spider attribute and per-request using <a class="reference internal" href="#std:reqmeta-download_maxsize"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">download_maxsize</span></code></a>
Request.meta key.</p>
<p class="last">This feature needs Twisted &gt;= 11.1.</p>
</div>
</div>
<div class="section" id="download-warnsize">
<span id="std:setting-DOWNLOAD_WARNSIZE"></span><h5>DOWNLOAD_WARNSIZE<a class="headerlink" href="#download-warnsize" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">33554432</span></code> (32MB)</p>
<p>The response size (in bytes) that downloader will start to warn.</p>
<p>If you want to disable it set to 0.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>This size can be set per spider using <code class="xref py py-attr docutils literal notranslate"><span class="pre">download_warnsize</span></code>
spider attribute and per-request using <code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">download_warnsize</span></code>
Request.meta key.</p>
<p class="last">This feature needs Twisted &gt;= 11.1.</p>
</div>
</div>
<div class="section" id="download-fail-on-dataloss">
<span id="std:setting-DOWNLOAD_FAIL_ON_DATALOSS"></span><h5>DOWNLOAD_FAIL_ON_DATALOSS<a class="headerlink" href="#download-fail-on-dataloss" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>Whether or not to fail on broken responses, that is, declared
<code class="docutils literal notranslate"><span class="pre">Content-Length</span></code> does not match content sent by the server or chunked
response was not properly finish. If <code class="docutils literal notranslate"><span class="pre">True</span></code>, these responses raise a
<code class="docutils literal notranslate"><span class="pre">ResponseFailed([_DataLoss])</span></code> error. If <code class="docutils literal notranslate"><span class="pre">False</span></code>, these responses
are passed through and the flag <code class="docutils literal notranslate"><span class="pre">dataloss</span></code> is added to the response, i.e.:
<code class="docutils literal notranslate"><span class="pre">'dataloss'</span> <span class="pre">in</span> <span class="pre">response.flags</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p>Optionally, this can be set per-request basis by using the
<a class="reference internal" href="index.html#std:reqmeta-download_fail_on_dataloss"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">download_fail_on_dataloss</span></code></a> Request.meta key to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">A broken response, or data loss error, may happen under several
circumstances, from server misconfiguration to network errors to data
corruption. It is up to the user to decide if it makes sense to process
broken responses considering they may contain partial or incomplete content.
If <a class="reference internal" href="index.html#std:setting-RETRY_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_ENABLED</span></code></a> is <code class="docutils literal notranslate"><span class="pre">True</span></code> and this setting is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>,
the <code class="docutils literal notranslate"><span class="pre">ResponseFailed([_DataLoss])</span></code> failure will be retried as usual.</p>
</div>
</div>
<div class="section" id="dupefilter-class">
<span id="std:setting-DUPEFILTER_CLASS"></span><h5>DUPEFILTER_CLASS<a class="headerlink" href="#dupefilter-class" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapy.dupefilters.RFPDupeFilter'</span></code></p>
<p>The class used to detect and filter duplicate requests.</p>
<p>The default (<code class="docutils literal notranslate"><span class="pre">RFPDupeFilter</span></code>) filters based on request fingerprint using
the <code class="docutils literal notranslate"><span class="pre">scrapy.utils.request.request_fingerprint</span></code> function. In order to change
the way duplicates are checked you could subclass <code class="docutils literal notranslate"><span class="pre">RFPDupeFilter</span></code> and
override its <code class="docutils literal notranslate"><span class="pre">request_fingerprint</span></code> method. This method should accept
scrapy <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object and return its fingerprint
(a string).</p>
<p>You can disable filtering of duplicate requests by setting
<a class="reference internal" href="#std:setting-DUPEFILTER_CLASS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DUPEFILTER_CLASS</span></code></a> to <code class="docutils literal notranslate"><span class="pre">'scrapy.dupefilters.BaseDupeFilter'</span></code>.
Be very careful about this however, because you can get into crawling loops.
It’s usually a better idea to set the <code class="docutils literal notranslate"><span class="pre">dont_filter</span></code> parameter to
<code class="docutils literal notranslate"><span class="pre">True</span></code> on the specific <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> that should not be
filtered.</p>
</div>
<div class="section" id="dupefilter-debug">
<span id="std:setting-DUPEFILTER_DEBUG"></span><h5>DUPEFILTER_DEBUG<a class="headerlink" href="#dupefilter-debug" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>By default, <code class="docutils literal notranslate"><span class="pre">RFPDupeFilter</span></code> only logs the first duplicate request.
Setting <a class="reference internal" href="#std:setting-DUPEFILTER_DEBUG"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DUPEFILTER_DEBUG</span></code></a> to <code class="docutils literal notranslate"><span class="pre">True</span></code> will make it log all duplicate requests.</p>
</div>
<div class="section" id="editor">
<span id="std:setting-EDITOR"></span><h5>EDITOR<a class="headerlink" href="#editor" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">vi</span></code> (on Unix systems) or the IDLE editor (on Windows)</p>
<p>The editor to use for editing spiders with the <a class="reference internal" href="index.html#std:command-edit"><code class="xref std std-command docutils literal notranslate"><span class="pre">edit</span></code></a> command.
Additionally, if the <code class="docutils literal notranslate"><span class="pre">EDITOR</span></code> environment variable is set, the <a class="reference internal" href="index.html#std:command-edit"><code class="xref std std-command docutils literal notranslate"><span class="pre">edit</span></code></a>
command will prefer it over the default setting.</p>
</div>
<div class="section" id="extensions">
<span id="std:setting-EXTENSIONS"></span><h5>EXTENSIONS<a class="headerlink" href="#extensions" title="Permalink to this headline">¶</a></h5>
<p>Default:: <code class="docutils literal notranslate"><span class="pre">{}</span></code></p>
<p>A dict containing the extensions enabled in your project, and their orders.</p>
</div>
<div class="section" id="extensions-base">
<span id="std:setting-EXTENSIONS_BASE"></span><h5>EXTENSIONS_BASE<a class="headerlink" href="#extensions-base" title="Permalink to this headline">¶</a></h5>
<p>Default:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;scrapy.extensions.corestats.CoreStats&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.extensions.telnet.TelnetConsole&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.extensions.memusage.MemoryUsage&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.extensions.memdebug.MemoryDebugger&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.extensions.closespider.CloseSpider&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.extensions.feedexport.FeedExporter&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.extensions.logstats.LogStats&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.extensions.spiderstate.SpiderState&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.extensions.throttle.AutoThrottle&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>A dict containing the extensions available by default in Scrapy, and their
orders. This setting contains all stable built-in extensions. Keep in mind that
some of them need to be enabled through a setting.</p>
<p>For more information See the <a class="reference internal" href="index.html#topics-extensions"><span class="std std-ref">extensions user guide</span></a>
and the <a class="reference internal" href="index.html#topics-extensions-ref"><span class="std std-ref">list of available extensions</span></a>.</p>
</div>
<div class="section" id="feed-tempdir">
<span id="std:setting-FEED_TEMPDIR"></span><h5>FEED_TEMPDIR<a class="headerlink" href="#feed-tempdir" title="Permalink to this headline">¶</a></h5>
<p>The Feed Temp dir allows you to set a custom folder to save crawler
temporary files before uploading with <a class="reference internal" href="index.html#topics-feed-storage-ftp"><span class="std std-ref">FTP feed storage</span></a> and
<a class="reference internal" href="index.html#topics-feed-storage-s3"><span class="std std-ref">Amazon S3</span></a>.</p>
</div>
<div class="section" id="ftp-passive-mode">
<span id="std:setting-FTP_PASSIVE_MODE"></span><h5>FTP_PASSIVE_MODE<a class="headerlink" href="#ftp-passive-mode" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>Whether or not to use passive mode when initiating FTP transfers.</p>
</div>
<div class="section" id="ftp-password">
<span id="std:setting-FTP_PASSWORD"></span><h5>FTP_PASSWORD<a class="headerlink" href="#ftp-password" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">&quot;guest&quot;</span></code></p>
<p>The password to use for FTP connections when there is no <code class="docutils literal notranslate"><span class="pre">&quot;ftp_password&quot;</span></code>
in <code class="docutils literal notranslate"><span class="pre">Request</span></code> meta.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Paraphrasing <a class="reference external" href="https://tools.ietf.org/html/rfc1635">RFC 1635</a>, although it is common to use either the password
“guest” or one’s e-mail address for anonymous FTP,
some FTP servers explicitly ask for the user’s e-mail address
and will not allow login with the “guest” password.</p>
</div>
</div>
<div class="section" id="ftp-user">
<span id="std:setting-FTP_USER"></span><h5>FTP_USER<a class="headerlink" href="#ftp-user" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">&quot;anonymous&quot;</span></code></p>
<p>The username to use for FTP connections when there is no <code class="docutils literal notranslate"><span class="pre">&quot;ftp_user&quot;</span></code>
in <code class="docutils literal notranslate"><span class="pre">Request</span></code> meta.</p>
</div>
<div class="section" id="item-pipelines">
<span id="std:setting-ITEM_PIPELINES"></span><h5>ITEM_PIPELINES<a class="headerlink" href="#item-pipelines" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">{}</span></code></p>
<p>A dict containing the item pipelines to use, and their orders. Order values are
arbitrary, but it is customary to define them in the 0-1000 range. Lower orders
process before higher orders.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ITEM_PIPELINES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;mybot.pipelines.validate.ValidateMyItem&#39;</span><span class="p">:</span> <span class="mi">300</span><span class="p">,</span>
    <span class="s1">&#39;mybot.pipelines.validate.StoreMyItem&#39;</span><span class="p">:</span> <span class="mi">800</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="item-pipelines-base">
<span id="std:setting-ITEM_PIPELINES_BASE"></span><h5>ITEM_PIPELINES_BASE<a class="headerlink" href="#item-pipelines-base" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">{}</span></code></p>
<p>A dict containing the pipelines enabled by default in Scrapy. You should never
modify this setting in your project, modify <a class="reference internal" href="#std:setting-ITEM_PIPELINES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ITEM_PIPELINES</span></code></a> instead.</p>
</div>
<div class="section" id="log-enabled">
<span id="std:setting-LOG_ENABLED"></span><h5>LOG_ENABLED<a class="headerlink" href="#log-enabled" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>Whether to enable logging.</p>
</div>
<div class="section" id="log-encoding">
<span id="std:setting-LOG_ENCODING"></span><h5>LOG_ENCODING<a class="headerlink" href="#log-encoding" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'utf-8'</span></code></p>
<p>The encoding to use for logging.</p>
</div>
<div class="section" id="log-file">
<span id="std:setting-LOG_FILE"></span><h5>LOG_FILE<a class="headerlink" href="#log-file" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p>
<p>File name to use for logging output. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, standard error will be used.</p>
</div>
<div class="section" id="log-format">
<span id="std:setting-LOG_FORMAT"></span><h5>LOG_FORMAT<a class="headerlink" href="#log-format" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'%(asctime)s</span> <span class="pre">[%(name)s]</span> <span class="pre">%(levelname)s:</span> <span class="pre">%(message)s'</span></code></p>
<p>String for formatting log messsages. Refer to the <a class="reference external" href="https://docs.python.org/2/library/logging.html#logrecord-attributes">Python logging documentation</a> for the whole list of available
placeholders.</p>
</div>
<div class="section" id="log-dateformat">
<span id="std:setting-LOG_DATEFORMAT"></span><h5>LOG_DATEFORMAT<a class="headerlink" href="#log-dateformat" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'%Y-%m-%d</span> <span class="pre">%H:%M:%S'</span></code></p>
<p>String for formatting date/time, expansion of the <code class="docutils literal notranslate"><span class="pre">%(asctime)s</span></code> placeholder
in <a class="reference internal" href="#std:setting-LOG_FORMAT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_FORMAT</span></code></a>. Refer to the <a class="reference external" href="https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior">Python datetime documentation</a> for the whole list of available
directives.</p>
</div>
<div class="section" id="log-level">
<span id="std:setting-LOG_LEVEL"></span><h5>LOG_LEVEL<a class="headerlink" href="#log-level" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'DEBUG'</span></code></p>
<p>Minimum level to log. Available levels are: CRITICAL, ERROR, WARNING,
INFO, DEBUG. For more info see <a class="reference internal" href="index.html#topics-logging"><span class="std std-ref">Logging</span></a>.</p>
</div>
<div class="section" id="log-stdout">
<span id="std:setting-LOG_STDOUT"></span><h5>LOG_STDOUT<a class="headerlink" href="#log-stdout" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, all standard output (and error) of your process will be redirected
to the log. For example if you <code class="docutils literal notranslate"><span class="pre">print('hello')</span></code> it will appear in the Scrapy
log.</p>
</div>
<div class="section" id="log-short-names">
<span id="std:setting-LOG_SHORT_NAMES"></span><h5>LOG_SHORT_NAMES<a class="headerlink" href="#log-short-names" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the logs will just contain the root path. If it is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>
then it displays the component responsible for the log output</p>
</div>
<div class="section" id="logstats-interval">
<span id="std:setting-LOGSTATS_INTERVAL"></span><h5>LOGSTATS_INTERVAL<a class="headerlink" href="#logstats-interval" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">60.0</span></code></p>
<p>The interval (in seconds) between each logging printout of the stats
by <code class="xref py py-class docutils literal notranslate"><span class="pre">LogStats</span></code>.</p>
</div>
<div class="section" id="memdebug-enabled">
<span id="std:setting-MEMDEBUG_ENABLED"></span><h5>MEMDEBUG_ENABLED<a class="headerlink" href="#memdebug-enabled" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>Whether to enable memory debugging.</p>
</div>
<div class="section" id="memdebug-notify">
<span id="std:setting-MEMDEBUG_NOTIFY"></span><h5>MEMDEBUG_NOTIFY<a class="headerlink" href="#memdebug-notify" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">[]</span></code></p>
<p>When memory debugging is enabled a memory report will be sent to the specified
addresses if this setting is not empty, otherwise the report will be written to
the log.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">MEMDEBUG_NOTIFY</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;user@example.com&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="section" id="memusage-enabled">
<span id="std:setting-MEMUSAGE_ENABLED"></span><h5>MEMUSAGE_ENABLED<a class="headerlink" href="#memusage-enabled" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>Scope: <code class="docutils literal notranslate"><span class="pre">scrapy.extensions.memusage</span></code></p>
<p>Whether to enable the memory usage extension. This extension keeps track of
a peak memory used by the process (it writes it to stats). It can also
optionally shutdown the Scrapy process when it exceeds a memory limit
(see <a class="reference internal" href="#std:setting-MEMUSAGE_LIMIT_MB"><code class="xref std std-setting docutils literal notranslate"><span class="pre">MEMUSAGE_LIMIT_MB</span></code></a>), and notify by email when that happened
(see <a class="reference internal" href="#std:setting-MEMUSAGE_NOTIFY_MAIL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">MEMUSAGE_NOTIFY_MAIL</span></code></a>).</p>
<p>See <a class="reference internal" href="index.html#topics-extensions-ref-memusage"><span class="std std-ref">Memory usage extension</span></a>.</p>
</div>
<div class="section" id="memusage-limit-mb">
<span id="std:setting-MEMUSAGE_LIMIT_MB"></span><h5>MEMUSAGE_LIMIT_MB<a class="headerlink" href="#memusage-limit-mb" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">0</span></code></p>
<p>Scope: <code class="docutils literal notranslate"><span class="pre">scrapy.extensions.memusage</span></code></p>
<p>The maximum amount of memory to allow (in megabytes) before shutting down
Scrapy  (if MEMUSAGE_ENABLED is True). If zero, no check will be performed.</p>
<p>See <a class="reference internal" href="index.html#topics-extensions-ref-memusage"><span class="std std-ref">Memory usage extension</span></a>.</p>
</div>
<div class="section" id="memusage-check-interval-seconds">
<span id="std:setting-MEMUSAGE_CHECK_INTERVAL_SECONDS"></span><h5>MEMUSAGE_CHECK_INTERVAL_SECONDS<a class="headerlink" href="#memusage-check-interval-seconds" title="Permalink to this headline">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.1.</span></p>
</div>
<p>Default: <code class="docutils literal notranslate"><span class="pre">60.0</span></code></p>
<p>Scope: <code class="docutils literal notranslate"><span class="pre">scrapy.extensions.memusage</span></code></p>
<p>The <a class="reference internal" href="index.html#topics-extensions-ref-memusage"><span class="std std-ref">Memory usage extension</span></a>
checks the current memory usage, versus the limits set by
<a class="reference internal" href="#std:setting-MEMUSAGE_LIMIT_MB"><code class="xref std std-setting docutils literal notranslate"><span class="pre">MEMUSAGE_LIMIT_MB</span></code></a> and <a class="reference internal" href="#std:setting-MEMUSAGE_WARNING_MB"><code class="xref std std-setting docutils literal notranslate"><span class="pre">MEMUSAGE_WARNING_MB</span></code></a>,
at fixed time intervals.</p>
<p>This sets the length of these intervals, in seconds.</p>
<p>See <a class="reference internal" href="index.html#topics-extensions-ref-memusage"><span class="std std-ref">Memory usage extension</span></a>.</p>
</div>
<div class="section" id="memusage-notify-mail">
<span id="std:setting-MEMUSAGE_NOTIFY_MAIL"></span><h5>MEMUSAGE_NOTIFY_MAIL<a class="headerlink" href="#memusage-notify-mail" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>Scope: <code class="docutils literal notranslate"><span class="pre">scrapy.extensions.memusage</span></code></p>
<p>A list of emails to notify if the memory limit has been reached.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">MEMUSAGE_NOTIFY_MAIL</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;user@example.com&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>See <a class="reference internal" href="index.html#topics-extensions-ref-memusage"><span class="std std-ref">Memory usage extension</span></a>.</p>
</div>
<div class="section" id="memusage-warning-mb">
<span id="std:setting-MEMUSAGE_WARNING_MB"></span><h5>MEMUSAGE_WARNING_MB<a class="headerlink" href="#memusage-warning-mb" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">0</span></code></p>
<p>Scope: <code class="docutils literal notranslate"><span class="pre">scrapy.extensions.memusage</span></code></p>
<p>The maximum amount of memory to allow (in megabytes) before sending a warning
email notifying about it. If zero, no warning will be produced.</p>
</div>
<div class="section" id="newspider-module">
<span id="std:setting-NEWSPIDER_MODULE"></span><h5>NEWSPIDER_MODULE<a class="headerlink" href="#newspider-module" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">''</span></code></p>
<p>Module where to create new spiders using the <a class="reference internal" href="index.html#std:command-genspider"><code class="xref std std-command docutils literal notranslate"><span class="pre">genspider</span></code></a> command.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">NEWSPIDER_MODULE</span> <span class="o">=</span> <span class="s1">&#39;mybot.spiders_dev&#39;</span>
</pre></div>
</div>
</div>
<div class="section" id="randomize-download-delay">
<span id="std:setting-RANDOMIZE_DOWNLOAD_DELAY"></span><h5>RANDOMIZE_DOWNLOAD_DELAY<a class="headerlink" href="#randomize-download-delay" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>If enabled, Scrapy will wait a random amount of time (between 0.5 * <a class="reference internal" href="#std:setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_DELAY</span></code></a> and 1.5 * <a class="reference internal" href="#std:setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_DELAY</span></code></a>) while fetching requests from the same
website.</p>
<p>This randomization decreases the chance of the crawler being detected (and
subsequently blocked) by sites which analyze requests looking for statistically
significant similarities in the time between their requests.</p>
<p>The randomization policy is the same used by <a class="reference external" href="https://www.gnu.org/software/wget/manual/wget.html">wget</a> <code class="docutils literal notranslate"><span class="pre">--random-wait</span></code> option.</p>
<p>If <a class="reference internal" href="#std:setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_DELAY</span></code></a> is zero (default) this option has no effect.</p>
</div>
<div class="section" id="reactor-threadpool-maxsize">
<span id="std:setting-REACTOR_THREADPOOL_MAXSIZE"></span><h5>REACTOR_THREADPOOL_MAXSIZE<a class="headerlink" href="#reactor-threadpool-maxsize" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">10</span></code></p>
<p>The maximum limit for Twisted Reactor thread pool size. This is common
multi-purpose thread pool used by various Scrapy components. Threaded
DNS Resolver, BlockingFeedStorage, S3FilesStore just to name a few. Increase
this value if you’re experiencing problems with insufficient blocking IO.</p>
</div>
<div class="section" id="redirect-max-times">
<span id="std:setting-REDIRECT_MAX_TIMES"></span><h5>REDIRECT_MAX_TIMES<a class="headerlink" href="#redirect-max-times" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">20</span></code></p>
<p>Defines the maximum times a request can be redirected. After this maximum the
request’s response is returned as is. We used Firefox default value for the
same task.</p>
</div>
<div class="section" id="redirect-priority-adjust">
<span id="std:setting-REDIRECT_PRIORITY_ADJUST"></span><h5>REDIRECT_PRIORITY_ADJUST<a class="headerlink" href="#redirect-priority-adjust" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">+2</span></code></p>
<p>Scope: <code class="docutils literal notranslate"><span class="pre">scrapy.downloadermiddlewares.redirect.RedirectMiddleware</span></code></p>
<p>Adjust redirect request priority relative to original request:</p>
<ul class="simple">
<li><strong>a positive priority adjust (default) means higher priority.</strong></li>
<li>a negative priority adjust means lower priority.</li>
</ul>
</div>
<div class="section" id="retry-priority-adjust">
<span id="std:setting-RETRY_PRIORITY_ADJUST"></span><h5>RETRY_PRIORITY_ADJUST<a class="headerlink" href="#retry-priority-adjust" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">-1</span></code></p>
<p>Scope: <code class="docutils literal notranslate"><span class="pre">scrapy.downloadermiddlewares.retry.RetryMiddleware</span></code></p>
<p>Adjust retry request priority relative to original request:</p>
<ul class="simple">
<li>a positive priority adjust means higher priority.</li>
<li><strong>a negative priority adjust (default) means lower priority.</strong></li>
</ul>
</div>
<div class="section" id="robotstxt-obey">
<span id="std:setting-ROBOTSTXT_OBEY"></span><h5>ROBOTSTXT_OBEY<a class="headerlink" href="#robotstxt-obey" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>Scope: <code class="docutils literal notranslate"><span class="pre">scrapy.downloadermiddlewares.robotstxt</span></code></p>
<p>If enabled, Scrapy will respect robots.txt policies. For more information see
<a class="reference internal" href="index.html#topics-dlmw-robots"><span class="std std-ref">RobotsTxtMiddleware</span></a>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">While the default value is <code class="docutils literal notranslate"><span class="pre">False</span></code> for historical reasons,
this option is enabled by default in settings.py file generated
by <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">startproject</span></code> command.</p>
</div>
</div>
<div class="section" id="scheduler">
<span id="std:setting-SCHEDULER"></span><h5>SCHEDULER<a class="headerlink" href="#scheduler" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapy.core.scheduler.Scheduler'</span></code></p>
<p>The scheduler to use for crawling.</p>
</div>
<div class="section" id="scheduler-debug">
<span id="std:setting-SCHEDULER_DEBUG"></span><h5>SCHEDULER_DEBUG<a class="headerlink" href="#scheduler-debug" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>Setting to <code class="docutils literal notranslate"><span class="pre">True</span></code> will log debug information about the requests scheduler.
This currently logs (only once) if the requests cannot be serialized to disk.
Stats counter (<code class="docutils literal notranslate"><span class="pre">scheduler/unserializable</span></code>) tracks the number of times this happens.</p>
<p>Example entry in logs:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="mi">1956</span><span class="o">-</span><span class="mo">01</span><span class="o">-</span><span class="mi">31</span> <span class="mo">00</span><span class="p">:</span><span class="mo">00</span><span class="p">:</span><span class="mo">00</span><span class="o">+</span><span class="mi">0800</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">scheduler</span><span class="p">]</span> <span class="n">ERROR</span><span class="p">:</span> <span class="n">Unable</span> <span class="n">to</span> <span class="n">serialize</span> <span class="n">request</span><span class="p">:</span>
<span class="o">&lt;</span><span class="n">GET</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">example</span><span class="o">.</span><span class="n">com</span><span class="o">&gt;</span> <span class="o">-</span> <span class="n">reason</span><span class="p">:</span> <span class="n">cannot</span> <span class="n">serialize</span> <span class="o">&lt;</span><span class="n">Request</span> <span class="n">at</span> <span class="mh">0x9a7c7ec</span><span class="o">&gt;</span>
<span class="p">(</span><span class="nb">type</span> <span class="n">Request</span><span class="p">)</span><span class="o">&gt;</span> <span class="o">-</span> <span class="n">no</span> <span class="n">more</span> <span class="n">unserializable</span> <span class="n">requests</span> <span class="n">will</span> <span class="n">be</span> <span class="n">logged</span>
<span class="p">(</span><span class="n">see</span> <span class="s1">&#39;scheduler/unserializable&#39;</span> <span class="n">stats</span> <span class="n">counter</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="scheduler-disk-queue">
<span id="std:setting-SCHEDULER_DISK_QUEUE"></span><h5>SCHEDULER_DISK_QUEUE<a class="headerlink" href="#scheduler-disk-queue" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapy.squeues.PickleLifoDiskQueue'</span></code></p>
<p>Type of disk queue that will be used by scheduler. Other available types are
<code class="docutils literal notranslate"><span class="pre">scrapy.squeues.PickleFifoDiskQueue</span></code>, <code class="docutils literal notranslate"><span class="pre">scrapy.squeues.MarshalFifoDiskQueue</span></code>,
<code class="docutils literal notranslate"><span class="pre">scrapy.squeues.MarshalLifoDiskQueue</span></code>.</p>
</div>
<div class="section" id="scheduler-memory-queue">
<span id="std:setting-SCHEDULER_MEMORY_QUEUE"></span><h5>SCHEDULER_MEMORY_QUEUE<a class="headerlink" href="#scheduler-memory-queue" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapy.squeues.LifoMemoryQueue'</span></code></p>
<p>Type of in-memory queue used by scheduler. Other available type is:
<code class="docutils literal notranslate"><span class="pre">scrapy.squeues.FifoMemoryQueue</span></code>.</p>
</div>
<div class="section" id="scheduler-priority-queue">
<span id="std:setting-SCHEDULER_PRIORITY_QUEUE"></span><h5>SCHEDULER_PRIORITY_QUEUE<a class="headerlink" href="#scheduler-priority-queue" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapy.pqueues.ScrapyPriorityQueue'</span></code></p>
<p>Type of priority queue used by the scheduler. Another available type is
<code class="docutils literal notranslate"><span class="pre">scrapy.pqueues.DownloaderAwarePriorityQueue</span></code>.
<code class="docutils literal notranslate"><span class="pre">scrapy.pqueues.DownloaderAwarePriorityQueue</span></code> works better than
<code class="docutils literal notranslate"><span class="pre">scrapy.pqueues.ScrapyPriorityQueue</span></code> when you crawl many different
domains in parallel. But currently <code class="docutils literal notranslate"><span class="pre">scrapy.pqueues.DownloaderAwarePriorityQueue</span></code>
does not work together with <a class="reference internal" href="#std:setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a>.</p>
</div>
<div class="section" id="spider-contracts">
<span id="std:setting-SPIDER_CONTRACTS"></span><h5>SPIDER_CONTRACTS<a class="headerlink" href="#spider-contracts" title="Permalink to this headline">¶</a></h5>
<p>Default:: <code class="docutils literal notranslate"><span class="pre">{}</span></code></p>
<p>A dict containing the spider contracts enabled in your project, used for
testing spiders. For more info see <a class="reference internal" href="index.html#topics-contracts"><span class="std std-ref">Spiders Contracts</span></a>.</p>
</div>
<div class="section" id="spider-contracts-base">
<span id="std:setting-SPIDER_CONTRACTS_BASE"></span><h5>SPIDER_CONTRACTS_BASE<a class="headerlink" href="#spider-contracts-base" title="Permalink to this headline">¶</a></h5>
<p>Default:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;scrapy.contracts.default.UrlContract&#39;</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.contracts.default.ReturnsContract&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.contracts.default.ScrapesContract&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>A dict containing the scrapy contracts enabled by default in Scrapy. You should
never modify this setting in your project, modify <a class="reference internal" href="#std:setting-SPIDER_CONTRACTS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_CONTRACTS</span></code></a>
instead. For more info see <a class="reference internal" href="index.html#topics-contracts"><span class="std std-ref">Spiders Contracts</span></a>.</p>
<p>You can disable any of these contracts by assigning <code class="docutils literal notranslate"><span class="pre">None</span></code> to their class
path in <a class="reference internal" href="#std:setting-SPIDER_CONTRACTS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_CONTRACTS</span></code></a>. E.g., to disable the built-in
<code class="docutils literal notranslate"><span class="pre">ScrapesContract</span></code>, place this in your <code class="docutils literal notranslate"><span class="pre">settings.py</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">SPIDER_CONTRACTS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;scrapy.contracts.default.ScrapesContract&#39;</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="spider-loader-class">
<span id="std:setting-SPIDER_LOADER_CLASS"></span><h5>SPIDER_LOADER_CLASS<a class="headerlink" href="#spider-loader-class" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapy.spiderloader.SpiderLoader'</span></code></p>
<p>The class that will be used for loading spiders, which must implement the
<a class="reference internal" href="index.html#topics-api-spiderloader"><span class="std std-ref">SpiderLoader API</span></a>.</p>
</div>
<div class="section" id="spider-loader-warn-only">
<span id="std:setting-SPIDER_LOADER_WARN_ONLY"></span><h5>SPIDER_LOADER_WARN_ONLY<a class="headerlink" href="#spider-loader-warn-only" title="Permalink to this headline">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.3.3.</span></p>
</div>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>By default, when scrapy tries to import spider classes from <a class="reference internal" href="#std:setting-SPIDER_MODULES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_MODULES</span></code></a>,
it will fail loudly if there is any <code class="docutils literal notranslate"><span class="pre">ImportError</span></code> exception.
But you can choose to silence this exception and turn it into a simple
warning by setting <code class="docutils literal notranslate"><span class="pre">SPIDER_LOADER_WARN_ONLY</span> <span class="pre">=</span> <span class="pre">True</span></code>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Some <a class="reference internal" href="index.html#topics-commands"><span class="std std-ref">scrapy commands</span></a> run with this setting to <code class="docutils literal notranslate"><span class="pre">True</span></code>
already (i.e. they will only issue a warning and will not fail)
since they do not actually need to load spider classes to work:
<a class="reference internal" href="index.html#std:command-runspider"><code class="xref std std-command docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">runspider</span></code></a>,
<a class="reference internal" href="index.html#std:command-settings"><code class="xref std std-command docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">settings</span></code></a>,
<a class="reference internal" href="index.html#std:command-startproject"><code class="xref std std-command docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">startproject</span></code></a>,
<a class="reference internal" href="index.html#std:command-version"><code class="xref std std-command docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">version</span></code></a>.</p>
</div>
</div>
<div class="section" id="spider-middlewares">
<span id="std:setting-SPIDER_MIDDLEWARES"></span><h5>SPIDER_MIDDLEWARES<a class="headerlink" href="#spider-middlewares" title="Permalink to this headline">¶</a></h5>
<p>Default:: <code class="docutils literal notranslate"><span class="pre">{}</span></code></p>
<p>A dict containing the spider middlewares enabled in your project, and their
orders. For more info see <a class="reference internal" href="index.html#topics-spider-middleware-setting"><span class="std std-ref">Activating a spider middleware</span></a>.</p>
</div>
<div class="section" id="spider-middlewares-base">
<span id="std:setting-SPIDER_MIDDLEWARES_BASE"></span><h5>SPIDER_MIDDLEWARES_BASE<a class="headerlink" href="#spider-middlewares-base" title="Permalink to this headline">¶</a></h5>
<p>Default:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;scrapy.spidermiddlewares.httperror.HttpErrorMiddleware&#39;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.spidermiddlewares.offsite.OffsiteMiddleware&#39;</span><span class="p">:</span> <span class="mi">500</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.spidermiddlewares.referer.RefererMiddleware&#39;</span><span class="p">:</span> <span class="mi">700</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.spidermiddlewares.urllength.UrlLengthMiddleware&#39;</span><span class="p">:</span> <span class="mi">800</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.spidermiddlewares.depth.DepthMiddleware&#39;</span><span class="p">:</span> <span class="mi">900</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>A dict containing the spider middlewares enabled by default in Scrapy, and
their orders. Low orders are closer to the engine, high orders are closer to
the spider. For more info see <a class="reference internal" href="index.html#topics-spider-middleware-setting"><span class="std std-ref">Activating a spider middleware</span></a>.</p>
</div>
<div class="section" id="spider-modules">
<span id="std:setting-SPIDER_MODULES"></span><h5>SPIDER_MODULES<a class="headerlink" href="#spider-modules" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">[]</span></code></p>
<p>A list of modules where Scrapy will look for spiders.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">SPIDER_MODULES</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;mybot.spiders_prod&#39;</span><span class="p">,</span> <span class="s1">&#39;mybot.spiders_dev&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="section" id="stats-class">
<span id="std:setting-STATS_CLASS"></span><h5>STATS_CLASS<a class="headerlink" href="#stats-class" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapy.statscollectors.MemoryStatsCollector'</span></code></p>
<p>The class to use for collecting stats, who must implement the
<a class="reference internal" href="index.html#topics-api-stats"><span class="std std-ref">Stats Collector API</span></a>.</p>
</div>
<div class="section" id="stats-dump">
<span id="std:setting-STATS_DUMP"></span><h5>STATS_DUMP<a class="headerlink" href="#stats-dump" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>Dump the <a class="reference internal" href="index.html#topics-stats"><span class="std std-ref">Scrapy stats</span></a> (to the Scrapy log) once the spider
finishes.</p>
<p>For more info see: <a class="reference internal" href="index.html#topics-stats"><span class="std std-ref">Stats Collection</span></a>.</p>
</div>
<div class="section" id="statsmailer-rcpts">
<span id="std:setting-STATSMAILER_RCPTS"></span><h5>STATSMAILER_RCPTS<a class="headerlink" href="#statsmailer-rcpts" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">[]</span></code> (empty list)</p>
<p>Send Scrapy stats after spiders finish scraping. See
<code class="xref py py-class docutils literal notranslate"><span class="pre">StatsMailer</span></code> for more info.</p>
</div>
<div class="section" id="telnetconsole-enabled">
<span id="std:setting-TELNETCONSOLE_ENABLED"></span><h5>TELNETCONSOLE_ENABLED<a class="headerlink" href="#telnetconsole-enabled" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>A boolean which specifies if the <a class="reference internal" href="index.html#topics-telnetconsole"><span class="std std-ref">telnet console</span></a>
will be enabled (provided its extension is also enabled).</p>
</div>
<div class="section" id="telnetconsole-port">
<span id="std:setting-TELNETCONSOLE_PORT"></span><h5>TELNETCONSOLE_PORT<a class="headerlink" href="#telnetconsole-port" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">[6023,</span> <span class="pre">6073]</span></code></p>
<p>The port range to use for the telnet console. If set to <code class="docutils literal notranslate"><span class="pre">None</span></code> or <code class="docutils literal notranslate"><span class="pre">0</span></code>, a
dynamically assigned port is used. For more info see
<a class="reference internal" href="index.html#topics-telnetconsole"><span class="std std-ref">Telnet Console</span></a>.</p>
</div>
<div class="section" id="templates-dir">
<span id="std:setting-TEMPLATES_DIR"></span><h5>TEMPLATES_DIR<a class="headerlink" href="#templates-dir" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">templates</span></code> dir inside scrapy module</p>
<p>The directory where to look for templates when creating new projects with
<a class="reference internal" href="index.html#std:command-startproject"><code class="xref std std-command docutils literal notranslate"><span class="pre">startproject</span></code></a> command and new spiders with <a class="reference internal" href="index.html#std:command-genspider"><code class="xref std std-command docutils literal notranslate"><span class="pre">genspider</span></code></a>
command.</p>
<p>The project name must not conflict with the name of custom files or directories
in the <code class="docutils literal notranslate"><span class="pre">project</span></code> subdirectory.</p>
</div>
<div class="section" id="urllength-limit">
<span id="std:setting-URLLENGTH_LIMIT"></span><h5>URLLENGTH_LIMIT<a class="headerlink" href="#urllength-limit" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">2083</span></code></p>
<p>Scope: <code class="docutils literal notranslate"><span class="pre">spidermiddlewares.urllength</span></code></p>
<p>The maximum URL length to allow for crawled URLs. For more information about
the default value for this setting see: <a class="reference external" href="https://boutell.com/newfaq/misc/urllength.html">https://boutell.com/newfaq/misc/urllength.html</a></p>
</div>
<div class="section" id="user-agent">
<span id="std:setting-USER_AGENT"></span><h5>USER_AGENT<a class="headerlink" href="#user-agent" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">&quot;Scrapy/VERSION</span> <span class="pre">(+https://scrapy.org)&quot;</span></code></p>
<p>The default User-Agent to use when crawling, unless overridden.</p>
</div>
<div class="section" id="settings-documented-elsewhere">
<h5>Settings documented elsewhere:<a class="headerlink" href="#settings-documented-elsewhere" title="Permalink to this headline">¶</a></h5>
<p>The following settings are documented elsewhere, please check each specific
case to see how to enable and use them.</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-AJAXCRAWL_ENABLED">AJAXCRAWL_ENABLED</a></li>
<li><a class="reference internal" href="index.html#std:setting-AUTOTHROTTLE_DEBUG">AUTOTHROTTLE_DEBUG</a></li>
<li><a class="reference internal" href="index.html#std:setting-AUTOTHROTTLE_ENABLED">AUTOTHROTTLE_ENABLED</a></li>
<li><a class="reference internal" href="index.html#std:setting-AUTOTHROTTLE_MAX_DELAY">AUTOTHROTTLE_MAX_DELAY</a></li>
<li><a class="reference internal" href="index.html#std:setting-AUTOTHROTTLE_START_DELAY">AUTOTHROTTLE_START_DELAY</a></li>
<li><a class="reference internal" href="index.html#std:setting-AUTOTHROTTLE_TARGET_CONCURRENCY">AUTOTHROTTLE_TARGET_CONCURRENCY</a></li>
<li><a class="reference internal" href="index.html#std:setting-AWS_ACCESS_KEY_ID">AWS_ACCESS_KEY_ID</a></li>
<li><a class="reference internal" href="index.html#std:setting-AWS_ENDPOINT_URL">AWS_ENDPOINT_URL</a></li>
<li><a class="reference internal" href="index.html#std:setting-AWS_REGION_NAME">AWS_REGION_NAME</a></li>
<li><a class="reference internal" href="index.html#std:setting-AWS_SECRET_ACCESS_KEY">AWS_SECRET_ACCESS_KEY</a></li>
<li><a class="reference internal" href="index.html#std:setting-AWS_USE_SSL">AWS_USE_SSL</a></li>
<li><a class="reference internal" href="index.html#std:setting-AWS_VERIFY">AWS_VERIFY</a></li>
<li><a class="reference internal" href="index.html#std:setting-BOT_NAME">BOT_NAME</a></li>
<li><a class="reference internal" href="index.html#std:setting-CLOSESPIDER_ERRORCOUNT">CLOSESPIDER_ERRORCOUNT</a></li>
<li><a class="reference internal" href="index.html#std:setting-CLOSESPIDER_ITEMCOUNT">CLOSESPIDER_ITEMCOUNT</a></li>
<li><a class="reference internal" href="index.html#std:setting-CLOSESPIDER_PAGECOUNT">CLOSESPIDER_PAGECOUNT</a></li>
<li><a class="reference internal" href="index.html#std:setting-CLOSESPIDER_TIMEOUT">CLOSESPIDER_TIMEOUT</a></li>
<li><a class="reference internal" href="index.html#std:setting-COMMANDS_MODULE">COMMANDS_MODULE</a></li>
<li><a class="reference internal" href="index.html#std:setting-COMPRESSION_ENABLED">COMPRESSION_ENABLED</a></li>
<li><a class="reference internal" href="index.html#std:setting-CONCURRENT_ITEMS">CONCURRENT_ITEMS</a></li>
<li><a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS">CONCURRENT_REQUESTS</a></li>
<li><a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN">CONCURRENT_REQUESTS_PER_DOMAIN</a></li>
<li><a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_IP">CONCURRENT_REQUESTS_PER_IP</a></li>
<li><a class="reference internal" href="index.html#std:setting-COOKIES_DEBUG">COOKIES_DEBUG</a></li>
<li><a class="reference internal" href="index.html#std:setting-COOKIES_ENABLED">COOKIES_ENABLED</a></li>
<li><a class="reference internal" href="index.html#std:setting-DEFAULT_ITEM_CLASS">DEFAULT_ITEM_CLASS</a></li>
<li><a class="reference internal" href="index.html#std:setting-DEFAULT_REQUEST_HEADERS">DEFAULT_REQUEST_HEADERS</a></li>
<li><a class="reference internal" href="index.html#std:setting-DEPTH_LIMIT">DEPTH_LIMIT</a></li>
<li><a class="reference internal" href="index.html#std:setting-DEPTH_PRIORITY">DEPTH_PRIORITY</a></li>
<li><a class="reference internal" href="index.html#std:setting-DEPTH_STATS_VERBOSE">DEPTH_STATS_VERBOSE</a></li>
<li><a class="reference internal" href="index.html#std:setting-DNSCACHE_ENABLED">DNSCACHE_ENABLED</a></li>
<li><a class="reference internal" href="index.html#std:setting-DNSCACHE_SIZE">DNSCACHE_SIZE</a></li>
<li><a class="reference internal" href="index.html#std:setting-DNS_TIMEOUT">DNS_TIMEOUT</a></li>
<li><a class="reference internal" href="index.html#std:setting-DOWNLOADER">DOWNLOADER</a></li>
<li><a class="reference internal" href="index.html#std:setting-DOWNLOADER_CLIENTCONTEXTFACTORY">DOWNLOADER_CLIENTCONTEXTFACTORY</a></li>
<li><a class="reference internal" href="index.html#std:setting-DOWNLOADER_CLIENT_TLS_METHOD">DOWNLOADER_CLIENT_TLS_METHOD</a></li>
<li><a class="reference internal" href="index.html#std:setting-DOWNLOADER_HTTPCLIENTFACTORY">DOWNLOADER_HTTPCLIENTFACTORY</a></li>
<li><a class="reference internal" href="index.html#std:setting-DOWNLOADER_MIDDLEWARES">DOWNLOADER_MIDDLEWARES</a></li>
<li><a class="reference internal" href="index.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE">DOWNLOADER_MIDDLEWARES_BASE</a></li>
<li><a class="reference internal" href="index.html#std:setting-DOWNLOADER_STATS">DOWNLOADER_STATS</a></li>
<li><a class="reference internal" href="index.html#std:setting-DOWNLOAD_DELAY">DOWNLOAD_DELAY</a></li>
<li><a class="reference internal" href="index.html#std:setting-DOWNLOAD_FAIL_ON_DATALOSS">DOWNLOAD_FAIL_ON_DATALOSS</a></li>
<li><a class="reference internal" href="index.html#std:setting-DOWNLOAD_HANDLERS">DOWNLOAD_HANDLERS</a></li>
<li><a class="reference internal" href="index.html#std:setting-DOWNLOAD_HANDLERS_BASE">DOWNLOAD_HANDLERS_BASE</a></li>
<li><a class="reference internal" href="index.html#std:setting-DOWNLOAD_MAXSIZE">DOWNLOAD_MAXSIZE</a></li>
<li><a class="reference internal" href="index.html#std:setting-DOWNLOAD_TIMEOUT">DOWNLOAD_TIMEOUT</a></li>
<li><a class="reference internal" href="index.html#std:setting-DOWNLOAD_WARNSIZE">DOWNLOAD_WARNSIZE</a></li>
<li><a class="reference internal" href="index.html#std:setting-DUPEFILTER_CLASS">DUPEFILTER_CLASS</a></li>
<li><a class="reference internal" href="index.html#std:setting-DUPEFILTER_DEBUG">DUPEFILTER_DEBUG</a></li>
<li><a class="reference internal" href="index.html#std:setting-EDITOR">EDITOR</a></li>
<li><a class="reference internal" href="index.html#std:setting-EXTENSIONS">EXTENSIONS</a></li>
<li><a class="reference internal" href="index.html#std:setting-EXTENSIONS_BASE">EXTENSIONS_BASE</a></li>
<li><a class="reference internal" href="index.html#std:setting-FEED_EXPORTERS">FEED_EXPORTERS</a></li>
<li><a class="reference internal" href="index.html#std:setting-FEED_EXPORTERS_BASE">FEED_EXPORTERS_BASE</a></li>
<li><a class="reference internal" href="index.html#std:setting-FEED_EXPORT_ENCODING">FEED_EXPORT_ENCODING</a></li>
<li><a class="reference internal" href="index.html#std:setting-FEED_EXPORT_FIELDS">FEED_EXPORT_FIELDS</a></li>
<li><a class="reference internal" href="index.html#std:setting-FEED_EXPORT_INDENT">FEED_EXPORT_INDENT</a></li>
<li><a class="reference internal" href="index.html#std:setting-FEED_FORMAT">FEED_FORMAT</a></li>
<li><a class="reference internal" href="index.html#std:setting-FEED_STORAGES">FEED_STORAGES</a></li>
<li><a class="reference internal" href="index.html#std:setting-FEED_STORAGES_BASE">FEED_STORAGES_BASE</a></li>
<li><a class="reference internal" href="index.html#std:setting-FEED_STORAGE_FTP_ACTIVE">FEED_STORAGE_FTP_ACTIVE</a></li>
<li><a class="reference internal" href="index.html#std:setting-FEED_STORAGE_S3_ACL">FEED_STORAGE_S3_ACL</a></li>
<li><a class="reference internal" href="index.html#std:setting-FEED_STORE_EMPTY">FEED_STORE_EMPTY</a></li>
<li><a class="reference internal" href="index.html#std:setting-FEED_TEMPDIR">FEED_TEMPDIR</a></li>
<li><a class="reference internal" href="index.html#std:setting-FEED_URI">FEED_URI</a></li>
<li><a class="reference internal" href="index.html#std:setting-FILES_EXPIRES">FILES_EXPIRES</a></li>
<li><a class="reference internal" href="index.html#std:setting-FILES_RESULT_FIELD">FILES_RESULT_FIELD</a></li>
<li><a class="reference internal" href="index.html#std:setting-FILES_STORE">FILES_STORE</a></li>
<li><a class="reference internal" href="index.html#std:setting-FILES_STORE_GCS_ACL">FILES_STORE_GCS_ACL</a></li>
<li><a class="reference internal" href="index.html#std:setting-FILES_STORE_S3_ACL">FILES_STORE_S3_ACL</a></li>
<li><a class="reference internal" href="index.html#std:setting-FILES_URLS_FIELD">FILES_URLS_FIELD</a></li>
<li><a class="reference internal" href="index.html#std:setting-FTP_PASSIVE_MODE">FTP_PASSIVE_MODE</a></li>
<li><a class="reference internal" href="index.html#std:setting-FTP_PASSWORD">FTP_PASSWORD</a></li>
<li><a class="reference internal" href="index.html#std:setting-FTP_USER">FTP_USER</a></li>
<li><a class="reference internal" href="index.html#std:setting-GCS_PROJECT_ID">GCS_PROJECT_ID</a></li>
<li><a class="reference internal" href="index.html#std:setting-HTTPCACHE_ALWAYS_STORE">HTTPCACHE_ALWAYS_STORE</a></li>
<li><a class="reference internal" href="index.html#std:setting-HTTPCACHE_DBM_MODULE">HTTPCACHE_DBM_MODULE</a></li>
<li><a class="reference internal" href="index.html#std:setting-HTTPCACHE_DIR">HTTPCACHE_DIR</a></li>
<li><a class="reference internal" href="index.html#std:setting-HTTPCACHE_ENABLED">HTTPCACHE_ENABLED</a></li>
<li><a class="reference internal" href="index.html#std:setting-HTTPCACHE_EXPIRATION_SECS">HTTPCACHE_EXPIRATION_SECS</a></li>
<li><a class="reference internal" href="index.html#std:setting-HTTPCACHE_GZIP">HTTPCACHE_GZIP</a></li>
<li><a class="reference internal" href="index.html#std:setting-HTTPCACHE_IGNORE_HTTP_CODES">HTTPCACHE_IGNORE_HTTP_CODES</a></li>
<li><a class="reference internal" href="index.html#std:setting-HTTPCACHE_IGNORE_MISSING">HTTPCACHE_IGNORE_MISSING</a></li>
<li><a class="reference internal" href="index.html#std:setting-HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS">HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS</a></li>
<li><a class="reference internal" href="index.html#std:setting-HTTPCACHE_IGNORE_SCHEMES">HTTPCACHE_IGNORE_SCHEMES</a></li>
<li><a class="reference internal" href="index.html#std:setting-HTTPCACHE_POLICY">HTTPCACHE_POLICY</a></li>
<li><a class="reference internal" href="index.html#std:setting-HTTPCACHE_STORAGE">HTTPCACHE_STORAGE</a></li>
<li><a class="reference internal" href="index.html#std:setting-HTTPERROR_ALLOWED_CODES">HTTPERROR_ALLOWED_CODES</a></li>
<li><a class="reference internal" href="index.html#std:setting-HTTPERROR_ALLOW_ALL">HTTPERROR_ALLOW_ALL</a></li>
<li><a class="reference internal" href="index.html#std:setting-HTTPPROXY_AUTH_ENCODING">HTTPPROXY_AUTH_ENCODING</a></li>
<li><a class="reference internal" href="index.html#std:setting-HTTPPROXY_ENABLED">HTTPPROXY_ENABLED</a></li>
<li><a class="reference internal" href="index.html#std:setting-IMAGES_EXPIRES">IMAGES_EXPIRES</a></li>
<li><a class="reference internal" href="index.html#std:setting-IMAGES_MIN_HEIGHT">IMAGES_MIN_HEIGHT</a></li>
<li><a class="reference internal" href="index.html#std:setting-IMAGES_MIN_WIDTH">IMAGES_MIN_WIDTH</a></li>
<li><a class="reference internal" href="index.html#std:setting-IMAGES_RESULT_FIELD">IMAGES_RESULT_FIELD</a></li>
<li><a class="reference internal" href="index.html#std:setting-IMAGES_STORE">IMAGES_STORE</a></li>
<li><a class="reference internal" href="index.html#std:setting-IMAGES_STORE_GCS_ACL">IMAGES_STORE_GCS_ACL</a></li>
<li><a class="reference internal" href="index.html#std:setting-IMAGES_STORE_S3_ACL">IMAGES_STORE_S3_ACL</a></li>
<li><a class="reference internal" href="index.html#std:setting-IMAGES_THUMBS">IMAGES_THUMBS</a></li>
<li><a class="reference internal" href="index.html#std:setting-IMAGES_URLS_FIELD">IMAGES_URLS_FIELD</a></li>
<li><a class="reference internal" href="index.html#std:setting-ITEM_PIPELINES">ITEM_PIPELINES</a></li>
<li><a class="reference internal" href="index.html#std:setting-ITEM_PIPELINES_BASE">ITEM_PIPELINES_BASE</a></li>
<li><a class="reference internal" href="index.html#std:setting-LOGSTATS_INTERVAL">LOGSTATS_INTERVAL</a></li>
<li><a class="reference internal" href="index.html#std:setting-LOG_DATEFORMAT">LOG_DATEFORMAT</a></li>
<li><a class="reference internal" href="index.html#std:setting-LOG_ENABLED">LOG_ENABLED</a></li>
<li><a class="reference internal" href="index.html#std:setting-LOG_ENCODING">LOG_ENCODING</a></li>
<li><a class="reference internal" href="index.html#std:setting-LOG_FILE">LOG_FILE</a></li>
<li><a class="reference internal" href="index.html#std:setting-LOG_FORMAT">LOG_FORMAT</a></li>
<li><a class="reference internal" href="index.html#std:setting-LOG_LEVEL">LOG_LEVEL</a></li>
<li><a class="reference internal" href="index.html#std:setting-LOG_SHORT_NAMES">LOG_SHORT_NAMES</a></li>
<li><a class="reference internal" href="index.html#std:setting-LOG_STDOUT">LOG_STDOUT</a></li>
<li><a class="reference internal" href="index.html#std:setting-MAIL_FROM">MAIL_FROM</a></li>
<li><a class="reference internal" href="index.html#std:setting-MAIL_HOST">MAIL_HOST</a></li>
<li><a class="reference internal" href="index.html#std:setting-MAIL_PASS">MAIL_PASS</a></li>
<li><a class="reference internal" href="index.html#std:setting-MAIL_PORT">MAIL_PORT</a></li>
<li><a class="reference internal" href="index.html#std:setting-MAIL_SSL">MAIL_SSL</a></li>
<li><a class="reference internal" href="index.html#std:setting-MAIL_TLS">MAIL_TLS</a></li>
<li><a class="reference internal" href="index.html#std:setting-MAIL_USER">MAIL_USER</a></li>
<li><a class="reference internal" href="index.html#std:setting-MEDIA_ALLOW_REDIRECTS">MEDIA_ALLOW_REDIRECTS</a></li>
<li><a class="reference internal" href="index.html#std:setting-MEMDEBUG_ENABLED">MEMDEBUG_ENABLED</a></li>
<li><a class="reference internal" href="index.html#std:setting-MEMDEBUG_NOTIFY">MEMDEBUG_NOTIFY</a></li>
<li><a class="reference internal" href="index.html#std:setting-MEMUSAGE_CHECK_INTERVAL_SECONDS">MEMUSAGE_CHECK_INTERVAL_SECONDS</a></li>
<li><a class="reference internal" href="index.html#std:setting-MEMUSAGE_ENABLED">MEMUSAGE_ENABLED</a></li>
<li><a class="reference internal" href="index.html#std:setting-MEMUSAGE_LIMIT_MB">MEMUSAGE_LIMIT_MB</a></li>
<li><a class="reference internal" href="index.html#std:setting-MEMUSAGE_NOTIFY_MAIL">MEMUSAGE_NOTIFY_MAIL</a></li>
<li><a class="reference internal" href="index.html#std:setting-MEMUSAGE_WARNING_MB">MEMUSAGE_WARNING_MB</a></li>
<li><a class="reference internal" href="index.html#std:setting-METAREFRESH_ENABLED">METAREFRESH_ENABLED</a></li>
<li><a class="reference internal" href="index.html#std:setting-METAREFRESH_IGNORE_TAGS">METAREFRESH_IGNORE_TAGS</a></li>
<li><a class="reference internal" href="index.html#std:setting-METAREFRESH_MAXDELAY">METAREFRESH_MAXDELAY</a></li>
<li><a class="reference internal" href="index.html#std:setting-NEWSPIDER_MODULE">NEWSPIDER_MODULE</a></li>
<li><a class="reference internal" href="index.html#std:setting-RANDOMIZE_DOWNLOAD_DELAY">RANDOMIZE_DOWNLOAD_DELAY</a></li>
<li><a class="reference internal" href="index.html#std:setting-REACTOR_THREADPOOL_MAXSIZE">REACTOR_THREADPOOL_MAXSIZE</a></li>
<li><a class="reference internal" href="index.html#std:setting-REDIRECT_ENABLED">REDIRECT_ENABLED</a></li>
<li><a class="reference internal" href="index.html#std:setting-REDIRECT_MAX_TIMES">REDIRECT_MAX_TIMES</a></li>
<li><a class="reference internal" href="index.html#std:setting-REDIRECT_MAX_TIMES">REDIRECT_MAX_TIMES</a></li>
<li><a class="reference internal" href="index.html#std:setting-REDIRECT_PRIORITY_ADJUST">REDIRECT_PRIORITY_ADJUST</a></li>
<li><a class="reference internal" href="index.html#std:setting-REFERER_ENABLED">REFERER_ENABLED</a></li>
<li><a class="reference internal" href="index.html#std:setting-REFERRER_POLICY">REFERRER_POLICY</a></li>
<li><a class="reference internal" href="index.html#std:setting-RETRY_ENABLED">RETRY_ENABLED</a></li>
<li><a class="reference internal" href="index.html#std:setting-RETRY_HTTP_CODES">RETRY_HTTP_CODES</a></li>
<li><a class="reference internal" href="index.html#std:setting-RETRY_PRIORITY_ADJUST">RETRY_PRIORITY_ADJUST</a></li>
<li><a class="reference internal" href="index.html#std:setting-RETRY_TIMES">RETRY_TIMES</a></li>
<li><a class="reference internal" href="index.html#std:setting-ROBOTSTXT_OBEY">ROBOTSTXT_OBEY</a></li>
<li><a class="reference internal" href="index.html#std:setting-SCHEDULER">SCHEDULER</a></li>
<li><a class="reference internal" href="index.html#std:setting-SCHEDULER_DEBUG">SCHEDULER_DEBUG</a></li>
<li><a class="reference internal" href="index.html#std:setting-SCHEDULER_DISK_QUEUE">SCHEDULER_DISK_QUEUE</a></li>
<li><a class="reference internal" href="index.html#std:setting-SCHEDULER_MEMORY_QUEUE">SCHEDULER_MEMORY_QUEUE</a></li>
<li><a class="reference internal" href="index.html#std:setting-SCHEDULER_PRIORITY_QUEUE">SCHEDULER_PRIORITY_QUEUE</a></li>
<li><a class="reference internal" href="index.html#std:setting-SPIDER_CONTRACTS">SPIDER_CONTRACTS</a></li>
<li><a class="reference internal" href="index.html#std:setting-SPIDER_CONTRACTS_BASE">SPIDER_CONTRACTS_BASE</a></li>
<li><a class="reference internal" href="index.html#std:setting-SPIDER_LOADER_CLASS">SPIDER_LOADER_CLASS</a></li>
<li><a class="reference internal" href="index.html#std:setting-SPIDER_LOADER_WARN_ONLY">SPIDER_LOADER_WARN_ONLY</a></li>
<li><a class="reference internal" href="index.html#std:setting-SPIDER_MIDDLEWARES">SPIDER_MIDDLEWARES</a></li>
<li><a class="reference internal" href="index.html#std:setting-SPIDER_MIDDLEWARES_BASE">SPIDER_MIDDLEWARES_BASE</a></li>
<li><a class="reference internal" href="index.html#std:setting-SPIDER_MODULES">SPIDER_MODULES</a></li>
<li><a class="reference internal" href="index.html#std:setting-STATSMAILER_RCPTS">STATSMAILER_RCPTS</a></li>
<li><a class="reference internal" href="index.html#std:setting-STATS_CLASS">STATS_CLASS</a></li>
<li><a class="reference internal" href="index.html#std:setting-STATS_DUMP">STATS_DUMP</a></li>
<li><a class="reference internal" href="index.html#std:setting-TELNETCONSOLE_ENABLED">TELNETCONSOLE_ENABLED</a></li>
<li><a class="reference internal" href="index.html#std:setting-TELNETCONSOLE_HOST">TELNETCONSOLE_HOST</a></li>
<li><a class="reference internal" href="index.html#std:setting-TELNETCONSOLE_PASSWORD">TELNETCONSOLE_PASSWORD</a></li>
<li><a class="reference internal" href="index.html#std:setting-TELNETCONSOLE_PORT">TELNETCONSOLE_PORT</a></li>
<li><a class="reference internal" href="index.html#std:setting-TELNETCONSOLE_PORT">TELNETCONSOLE_PORT</a></li>
<li><a class="reference internal" href="index.html#std:setting-TELNETCONSOLE_USERNAME">TELNETCONSOLE_USERNAME</a></li>
<li><a class="reference internal" href="index.html#std:setting-TEMPLATES_DIR">TEMPLATES_DIR</a></li>
<li><a class="reference internal" href="index.html#std:setting-URLLENGTH_LIMIT">URLLENGTH_LIMIT</a></li>
<li><a class="reference internal" href="index.html#std:setting-USER_AGENT">USER_AGENT</a></li>
</ul>
</div>
</div>
</div>
<span id="document-topics/exceptions"></span><div class="section" id="module-scrapy.exceptions">
<span id="exceptions"></span><span id="topics-exceptions"></span><h3>Exceptions<a class="headerlink" href="#module-scrapy.exceptions" title="Permalink to this headline">¶</a></h3>
<div class="section" id="built-in-exceptions-reference">
<span id="topics-exceptions-ref"></span><h4>Built-in Exceptions reference<a class="headerlink" href="#built-in-exceptions-reference" title="Permalink to this headline">¶</a></h4>
<p>Here’s a list of all exceptions included in Scrapy and their usage.</p>
<div class="section" id="dropitem">
<h5>DropItem<a class="headerlink" href="#dropitem" title="Permalink to this headline">¶</a></h5>
<dl class="exception">
<dt id="scrapy.exceptions.DropItem">
<em class="property">exception </em><code class="descclassname">scrapy.exceptions.</code><code class="descname">DropItem</code><a class="headerlink" href="#scrapy.exceptions.DropItem" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>The exception that must be raised by item pipeline stages to stop processing an
Item. For more information see <a class="reference internal" href="index.html#topics-item-pipeline"><span class="std std-ref">Item Pipeline</span></a>.</p>
</div>
<div class="section" id="closespider">
<h5>CloseSpider<a class="headerlink" href="#closespider" title="Permalink to this headline">¶</a></h5>
<dl class="exception">
<dt id="scrapy.exceptions.CloseSpider">
<em class="property">exception </em><code class="descclassname">scrapy.exceptions.</code><code class="descname">CloseSpider</code><span class="sig-paren">(</span><em>reason='cancelled'</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exceptions.CloseSpider" title="Permalink to this definition">¶</a></dt>
<dd><p>This exception can be raised from a spider callback to request the spider to be
closed/stopped. Supported arguments:</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>reason</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – the reason for closing</td>
</tr>
</tbody>
</table>
</dd></dl>

<p>For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">parse_page</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="k">if</span> <span class="s1">&#39;Bandwidth exceeded&#39;</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">body</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">CloseSpider</span><span class="p">(</span><span class="s1">&#39;bandwidth_exceeded&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="dontclosespider">
<h5>DontCloseSpider<a class="headerlink" href="#dontclosespider" title="Permalink to this headline">¶</a></h5>
<dl class="exception">
<dt id="scrapy.exceptions.DontCloseSpider">
<em class="property">exception </em><code class="descclassname">scrapy.exceptions.</code><code class="descname">DontCloseSpider</code><a class="headerlink" href="#scrapy.exceptions.DontCloseSpider" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>This exception can be raised in a <a class="reference internal" href="index.html#std:signal-spider_idle"><code class="xref std std-signal docutils literal notranslate"><span class="pre">spider_idle</span></code></a> signal handler to
prevent the spider from being closed.</p>
</div>
<div class="section" id="ignorerequest">
<h5>IgnoreRequest<a class="headerlink" href="#ignorerequest" title="Permalink to this headline">¶</a></h5>
<dl class="exception">
<dt id="scrapy.exceptions.IgnoreRequest">
<em class="property">exception </em><code class="descclassname">scrapy.exceptions.</code><code class="descname">IgnoreRequest</code><a class="headerlink" href="#scrapy.exceptions.IgnoreRequest" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>This exception can be raised by the Scheduler or any downloader middleware to
indicate that the request should be ignored.</p>
</div>
<div class="section" id="notconfigured">
<h5>NotConfigured<a class="headerlink" href="#notconfigured" title="Permalink to this headline">¶</a></h5>
<dl class="exception">
<dt id="scrapy.exceptions.NotConfigured">
<em class="property">exception </em><code class="descclassname">scrapy.exceptions.</code><code class="descname">NotConfigured</code><a class="headerlink" href="#scrapy.exceptions.NotConfigured" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>This exception can be raised by some components to indicate that they will
remain disabled. Those components include:</p>
<blockquote>
<div><ul class="simple">
<li>Extensions</li>
<li>Item pipelines</li>
<li>Downloader middlewares</li>
<li>Spider middlewares</li>
</ul>
</div></blockquote>
<p>The exception must be raised in the component’s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method.</p>
</div>
<div class="section" id="notsupported">
<h5>NotSupported<a class="headerlink" href="#notsupported" title="Permalink to this headline">¶</a></h5>
<dl class="exception">
<dt id="scrapy.exceptions.NotSupported">
<em class="property">exception </em><code class="descclassname">scrapy.exceptions.</code><code class="descname">NotSupported</code><a class="headerlink" href="#scrapy.exceptions.NotSupported" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>This exception is raised to indicate an unsupported feature.</p>
</div>
</div>
</div>
</div>
<dl class="docutils">
<dt><a class="reference internal" href="index.html#document-topics/commands"><span class="doc">Command line tool</span></a></dt>
<dd>Learn about the command-line tool used to manage your Scrapy project.</dd>
<dt><a class="reference internal" href="index.html#document-topics/spiders"><span class="doc">Spiders</span></a></dt>
<dd>Write the rules to crawl your websites.</dd>
<dt><a class="reference internal" href="index.html#document-topics/selectors"><span class="doc">Selectors</span></a></dt>
<dd>Extract the data from web pages using XPath.</dd>
<dt><a class="reference internal" href="index.html#document-topics/shell"><span class="doc">Scrapy shell</span></a></dt>
<dd>Test your extraction code in an interactive environment.</dd>
<dt><a class="reference internal" href="index.html#document-topics/items"><span class="doc">Items</span></a></dt>
<dd>Define the data you want to scrape.</dd>
<dt><a class="reference internal" href="index.html#document-topics/loaders"><span class="doc">Item Loaders</span></a></dt>
<dd>Populate your items with the extracted data.</dd>
<dt><a class="reference internal" href="index.html#document-topics/item-pipeline"><span class="doc">Item Pipeline</span></a></dt>
<dd>Post-process and store your scraped data.</dd>
<dt><a class="reference internal" href="index.html#document-topics/feed-exports"><span class="doc">Feed exports</span></a></dt>
<dd>Output your scraped data using different formats and storages.</dd>
<dt><a class="reference internal" href="index.html#document-topics/request-response"><span class="doc">Requests and Responses</span></a></dt>
<dd>Understand the classes used to represent HTTP requests and responses.</dd>
<dt><a class="reference internal" href="index.html#document-topics/link-extractors"><span class="doc">Link Extractors</span></a></dt>
<dd>Convenient classes to extract links to follow from pages.</dd>
<dt><a class="reference internal" href="index.html#document-topics/settings"><span class="doc">Settings</span></a></dt>
<dd>Learn how to configure Scrapy and see all <a class="reference internal" href="index.html#topics-settings-ref"><span class="std std-ref">available settings</span></a>.</dd>
<dt><a class="reference internal" href="index.html#document-topics/exceptions"><span class="doc">Exceptions</span></a></dt>
<dd>See all available exceptions and their meaning.</dd>
</dl>
</div>
<div class="section" id="built-in-services">
<h2>Built-in services<a class="headerlink" href="#built-in-services" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-topics/logging"></span><div class="section" id="logging">
<span id="topics-logging"></span><h3>Logging<a class="headerlink" href="#logging" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><code class="xref py py-mod docutils literal notranslate"><span class="pre">scrapy.log</span></code> has been deprecated alongside its functions in favor of
explicit calls to the Python standard logging. Keep reading to learn more
about the new logging system.</p>
</div>
<p>Scrapy uses <a class="reference external" href="https://docs.python.org/3/library/logging.html">Python’s builtin logging system</a> for event logging. We’ll
provide some simple examples to get you started, but for more advanced
use-cases it’s strongly suggested to read thoroughly its documentation.</p>
<p>Logging works out of the box, and can be configured to some extent with the
Scrapy settings listed in <a class="reference internal" href="#topics-logging-settings"><span class="std std-ref">Logging settings</span></a>.</p>
<p>Scrapy calls <a class="reference internal" href="#scrapy.utils.log.configure_logging" title="scrapy.utils.log.configure_logging"><code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.utils.log.configure_logging()</span></code></a> to set some reasonable
defaults and handle those settings in <a class="reference internal" href="#topics-logging-settings"><span class="std std-ref">Logging settings</span></a> when
running commands, so it’s recommended to manually call it if you’re running
Scrapy from scripts as described in <a class="reference internal" href="index.html#run-from-script"><span class="std std-ref">Run Scrapy from a script</span></a>.</p>
<div class="section" id="log-levels">
<span id="topics-logging-levels"></span><h4>Log levels<a class="headerlink" href="#log-levels" title="Permalink to this headline">¶</a></h4>
<p>Python’s builtin logging defines 5 different levels to indicate the severity of a
given log message. Here are the standard ones, listed in decreasing order:</p>
<ol class="arabic simple">
<li><code class="docutils literal notranslate"><span class="pre">logging.CRITICAL</span></code> - for critical errors (highest severity)</li>
<li><code class="docutils literal notranslate"><span class="pre">logging.ERROR</span></code> - for regular errors</li>
<li><code class="docutils literal notranslate"><span class="pre">logging.WARNING</span></code> - for warning messages</li>
<li><code class="docutils literal notranslate"><span class="pre">logging.INFO</span></code> - for informational messages</li>
<li><code class="docutils literal notranslate"><span class="pre">logging.DEBUG</span></code> - for debugging messages (lowest severity)</li>
</ol>
</div>
<div class="section" id="how-to-log-messages">
<h4>How to log messages<a class="headerlink" href="#how-to-log-messages" title="Permalink to this headline">¶</a></h4>
<p>Here’s a quick example of how to log a message using the <code class="docutils literal notranslate"><span class="pre">logging.WARNING</span></code>
level:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="n">logging</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;This is a warning&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>There are shortcuts for issuing log messages on any of the standard 5 levels,
and there’s also a general <code class="docutils literal notranslate"><span class="pre">logging.log</span></code> method which takes a given level as
argument.  If needed, the last example could be rewritten as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="n">logging</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">WARNING</span><span class="p">,</span> <span class="s2">&quot;This is a warning&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>On top of that, you can create different “loggers” to encapsulate messages. (For
example, a common practice is to create different loggers for every module).
These loggers can be configured independently, and they allow hierarchical
constructions.</p>
<p>The previous examples use the root logger behind the scenes, which is a top level
logger where all messages are propagated to (unless otherwise specified). Using
<code class="docutils literal notranslate"><span class="pre">logging</span></code> helpers is merely a shortcut for getting the root logger
explicitly, so this is also an equivalent of the last snippets:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">()</span>
<span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;This is a warning&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>You can use a different logger just by getting its name with the
<code class="docutils literal notranslate"><span class="pre">logging.getLogger</span></code> function:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s1">&#39;mycustomlogger&#39;</span><span class="p">)</span>
<span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;This is a warning&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Finally, you can ensure having a custom logger for any module you’re working on
by using the <code class="docutils literal notranslate"><span class="pre">__name__</span></code> variable, which is populated with current module’s
path:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>
<span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;This is a warning&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<dl class="last docutils">
<dt>Module logging, <a class="reference external" href="https://docs.python.org/2/howto/logging.html">HowTo</a></dt>
<dd>Basic Logging Tutorial</dd>
<dt>Module logging, <a class="reference external" href="https://docs.python.org/2/library/logging.html#logger-objects">Loggers</a></dt>
<dd>Further documentation on loggers</dd>
</dl>
</div>
</div>
<div class="section" id="logging-from-spiders">
<span id="topics-logging-from-spiders"></span><h4>Logging from Spiders<a class="headerlink" href="#logging-from-spiders" title="Permalink to this headline">¶</a></h4>
<p>Scrapy provides a <a class="reference internal" href="index.html#scrapy.spiders.Spider.logger" title="scrapy.spiders.Spider.logger"><code class="xref py py-data docutils literal notranslate"><span class="pre">logger</span></code></a> within each Spider
instance, which can be accessed and used like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;myspider&#39;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;https://scrapinghub.com&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Parse function called on </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>
</pre></div>
</div>
<p>That logger is created using the Spider’s name, but you can use any custom
Python logger you want. For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">scrapy</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s1">&#39;mycustomlogger&#39;</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;myspider&#39;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;https://scrapinghub.com&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Parse function called on </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="logging-configuration">
<span id="topics-logging-configuration"></span><h4>Logging configuration<a class="headerlink" href="#logging-configuration" title="Permalink to this headline">¶</a></h4>
<p>Loggers on their own don’t manage how messages sent through them are displayed.
For this task, different “handlers” can be attached to any logger instance and
they will redirect those messages to appropriate destinations, such as the
standard output, files, emails, etc.</p>
<p>By default, Scrapy sets and configures a handler for the root logger, based on
the settings below.</p>
<div class="section" id="logging-settings">
<span id="topics-logging-settings"></span><h5>Logging settings<a class="headerlink" href="#logging-settings" title="Permalink to this headline">¶</a></h5>
<p>These settings can be used to configure the logging:</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-LOG_FILE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_FILE</span></code></a></li>
<li><a class="reference internal" href="index.html#std:setting-LOG_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_ENABLED</span></code></a></li>
<li><a class="reference internal" href="index.html#std:setting-LOG_ENCODING"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_ENCODING</span></code></a></li>
<li><a class="reference internal" href="index.html#std:setting-LOG_LEVEL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_LEVEL</span></code></a></li>
<li><a class="reference internal" href="index.html#std:setting-LOG_FORMAT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_FORMAT</span></code></a></li>
<li><a class="reference internal" href="index.html#std:setting-LOG_DATEFORMAT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_DATEFORMAT</span></code></a></li>
<li><a class="reference internal" href="index.html#std:setting-LOG_STDOUT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_STDOUT</span></code></a></li>
<li><a class="reference internal" href="index.html#std:setting-LOG_SHORT_NAMES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_SHORT_NAMES</span></code></a></li>
</ul>
<p>The first couple of settings define a destination for log messages. If
<a class="reference internal" href="index.html#std:setting-LOG_FILE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_FILE</span></code></a> is set, messages sent through the root logger will be
redirected to a file named <a class="reference internal" href="index.html#std:setting-LOG_FILE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_FILE</span></code></a> with encoding
<a class="reference internal" href="index.html#std:setting-LOG_ENCODING"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_ENCODING</span></code></a>. If unset and <a class="reference internal" href="index.html#std:setting-LOG_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_ENABLED</span></code></a> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, log
messages will be displayed on the standard error. Lastly, if
<a class="reference internal" href="index.html#std:setting-LOG_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_ENABLED</span></code></a> is <code class="docutils literal notranslate"><span class="pre">False</span></code>, there won’t be any visible log output.</p>
<p><a class="reference internal" href="index.html#std:setting-LOG_LEVEL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_LEVEL</span></code></a> determines the minimum level of severity to display, those
messages with lower severity will be filtered out. It ranges through the
possible levels listed in <a class="reference internal" href="#topics-logging-levels"><span class="std std-ref">Log levels</span></a>.</p>
<p><a class="reference internal" href="index.html#std:setting-LOG_FORMAT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_FORMAT</span></code></a> and <a class="reference internal" href="index.html#std:setting-LOG_DATEFORMAT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_DATEFORMAT</span></code></a> specify formatting strings
used as layouts for all messages. Those strings can contain any placeholders
listed in <a class="reference external" href="https://docs.python.org/2/library/logging.html#logrecord-attributes">logging’s logrecord attributes docs</a> and
<a class="reference external" href="https://docs.python.org/2/library/datetime.html#strftime-and-strptime-behavior">datetime’s strftime and strptime directives</a>
respectively.</p>
<p>If <a class="reference internal" href="index.html#std:setting-LOG_SHORT_NAMES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_SHORT_NAMES</span></code></a> is set, then the logs will not display the scrapy
component that prints the log. It is unset by default, hence logs contain the
scrapy component responsible for that log output.</p>
</div>
<div class="section" id="command-line-options">
<h5>Command-line options<a class="headerlink" href="#command-line-options" title="Permalink to this headline">¶</a></h5>
<p>There are command-line arguments, available for all commands, that you can use
to override some of the Scrapy settings regarding logging.</p>
<ul class="simple">
<li><dl class="first docutils">
<dt><code class="docutils literal notranslate"><span class="pre">--logfile</span> <span class="pre">FILE</span></code></dt>
<dd>Overrides <a class="reference internal" href="index.html#std:setting-LOG_FILE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_FILE</span></code></a></dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><code class="docutils literal notranslate"><span class="pre">--loglevel/-L</span> <span class="pre">LEVEL</span></code></dt>
<dd>Overrides <a class="reference internal" href="index.html#std:setting-LOG_LEVEL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_LEVEL</span></code></a></dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><code class="docutils literal notranslate"><span class="pre">--nolog</span></code></dt>
<dd>Sets <a class="reference internal" href="index.html#std:setting-LOG_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_ENABLED</span></code></a> to <code class="docutils literal notranslate"><span class="pre">False</span></code></dd>
</dl>
</li>
</ul>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<dl class="last docutils">
<dt>Module <a class="reference external" href="https://docs.python.org/2/library/logging.handlers.html">logging.handlers</a></dt>
<dd>Further documentation on available handlers</dd>
</dl>
</div>
</div>
<div class="section" id="advanced-customization">
<h5>Advanced customization<a class="headerlink" href="#advanced-customization" title="Permalink to this headline">¶</a></h5>
<p>Because Scrapy uses stdlib logging module, you can customize logging using
all features of stdlib logging.</p>
<p>For example, let’s say you’re scraping a website which returns many
HTTP 404 and 500 responses, and you want to hide all messages like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">22</span><span class="p">:</span><span class="mi">00</span><span class="p">:</span><span class="mi">06</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">spidermiddlewares</span><span class="o">.</span><span class="n">httperror</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Ignoring</span>
<span class="n">response</span> <span class="o">&lt;</span><span class="mi">500</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">quotes</span><span class="o">.</span><span class="n">toscrape</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">page</span><span class="o">/</span><span class="mi">1</span><span class="o">-</span><span class="mi">34</span><span class="o">/&gt;</span><span class="p">:</span> <span class="n">HTTP</span> <span class="n">status</span> <span class="n">code</span>
<span class="ow">is</span> <span class="ow">not</span> <span class="n">handled</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">allowed</span>
</pre></div>
</div>
<p>The first thing to note is a logger name - it is in brackets:
<code class="docutils literal notranslate"><span class="pre">[scrapy.spidermiddlewares.httperror]</span></code>. If you get just <code class="docutils literal notranslate"><span class="pre">[scrapy]</span></code> then
<a class="reference internal" href="index.html#std:setting-LOG_SHORT_NAMES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_SHORT_NAMES</span></code></a> is likely set to True; set it to False and re-run
the crawl.</p>
<p>Next, we can see that the message has INFO level. To hide it
we should set logging level for <code class="docutils literal notranslate"><span class="pre">scrapy.spidermiddlewares.httperror</span></code>
higher than INFO; next level after INFO is WARNING. It could be done
e.g. in the spider’s <code class="docutils literal notranslate"><span class="pre">__init__</span></code> method:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="c1"># ...</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s1">&#39;scrapy.spidermiddlewares.httperror&#39;</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">WARNING</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
<p>If you run this spider again then INFO messages from
<code class="docutils literal notranslate"><span class="pre">scrapy.spidermiddlewares.httperror</span></code> logger will be gone.</p>
</div>
</div>
<div class="section" id="module-scrapy.utils.log">
<span id="scrapy-utils-log-module"></span><h4>scrapy.utils.log module<a class="headerlink" href="#module-scrapy.utils.log" title="Permalink to this headline">¶</a></h4>
<dl class="function">
<dt id="scrapy.utils.log.configure_logging">
<code class="descclassname">scrapy.utils.log.</code><code class="descname">configure_logging</code><span class="sig-paren">(</span><em>settings=None</em>, <em>install_root_handler=True</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.utils.log.configure_logging" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize logging defaults for Scrapy.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>settings</strong> (dict, <a class="reference internal" href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">Settings</span></code></a> object or <code class="docutils literal notranslate"><span class="pre">None</span></code>) – settings used to create and configure a handler for the
root logger (default: None).</li>
<li><strong>install_root_handler</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.7)"><em>bool</em></a>) – whether to install root logging handler
(default: True)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>This function does:</p>
<ul class="simple">
<li>Route warnings and twisted logging through Python standard logging</li>
<li>Assign DEBUG and ERROR level to Scrapy and Twisted loggers respectively</li>
<li>Route stdout to log if LOG_STDOUT setting is True</li>
</ul>
<p>When <code class="docutils literal notranslate"><span class="pre">install_root_handler</span></code> is True (default), this function also
creates a handler for the root logger according to given settings
(see <a class="reference internal" href="#topics-logging-settings"><span class="std std-ref">Logging settings</span></a>). You can override default options
using <code class="docutils literal notranslate"><span class="pre">settings</span></code> argument. When <code class="docutils literal notranslate"><span class="pre">settings</span></code> is empty or None, defaults
are used.</p>
<p><code class="docutils literal notranslate"><span class="pre">configure_logging</span></code> is automatically called when using Scrapy commands
or <a class="reference internal" href="index.html#scrapy.crawler.CrawlerProcess" title="scrapy.crawler.CrawlerProcess"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerProcess</span></code></a>, but needs to be called explicitly
when running custom scripts using <a class="reference internal" href="index.html#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerRunner</span></code></a>.
In that case, its usage is not required but it’s recommended.</p>
<p>If you plan on configuring the handlers yourself is still recommended you
call this function, passing <code class="docutils literal notranslate"><span class="pre">install_root_handler=False</span></code>. Bear in mind
there won’t be any log output set by default in that case.</p>
<p>To get you started on manually configuring logging’s output, you can use
<a class="reference external" href="https://docs.python.org/2/library/logging.html#logging.basicConfig">logging.basicConfig()</a> to set a basic root handler. This is an example
on how to redirect <code class="docutils literal notranslate"><span class="pre">INFO</span></code> or higher messages to a file:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">from</span> <span class="nn">scrapy.utils.log</span> <span class="k">import</span> <span class="n">configure_logging</span>

<span class="n">configure_logging</span><span class="p">(</span><span class="n">install_root_handler</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span>
    <span class="n">filename</span><span class="o">=</span><span class="s1">&#39;log.txt&#39;</span><span class="p">,</span>
    <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;</span><span class="si">%(levelname)s</span><span class="s1">: </span><span class="si">%(message)s</span><span class="s1">&#39;</span><span class="p">,</span>
    <span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Refer to <a class="reference internal" href="index.html#run-from-script"><span class="std std-ref">Run Scrapy from a script</span></a> for more details about using Scrapy this
way.</p>
</dd></dl>

</div>
</div>
<span id="document-topics/stats"></span><div class="section" id="stats-collection">
<span id="topics-stats"></span><h3>Stats Collection<a class="headerlink" href="#stats-collection" title="Permalink to this headline">¶</a></h3>
<p>Scrapy provides a convenient facility for collecting stats in the form of
key/values, where values are often counters. The facility is called the Stats
Collector, and can be accessed through the <a class="reference internal" href="index.html#scrapy.crawler.Crawler.stats" title="scrapy.crawler.Crawler.stats"><code class="xref py py-attr docutils literal notranslate"><span class="pre">stats</span></code></a>
attribute of the <a class="reference internal" href="index.html#topics-api-crawler"><span class="std std-ref">Crawler API</span></a>, as illustrated by the examples in
the <a class="reference internal" href="#topics-stats-usecases"><span class="std std-ref">Common Stats Collector uses</span></a> section below.</p>
<p>However, the Stats Collector is always available, so you can always import it
in your module and use its API (to increment or set new stat keys), regardless
of whether the stats collection is enabled or not. If it’s disabled, the API
will still work but it won’t collect anything. This is aimed at simplifying the
stats collector usage: you should spend no more than one line of code for
collecting stats in your spider, Scrapy extension, or whatever code you’re
using the Stats Collector from.</p>
<p>Another feature of the Stats Collector is that it’s very efficient (when
enabled) and extremely efficient (almost unnoticeable) when disabled.</p>
<p>The Stats Collector keeps a stats table per open spider which is automatically
opened when the spider is opened, and closed when the spider is closed.</p>
<div class="section" id="common-stats-collector-uses">
<span id="topics-stats-usecases"></span><h4>Common Stats Collector uses<a class="headerlink" href="#common-stats-collector-uses" title="Permalink to this headline">¶</a></h4>
<p>Access the stats collector through the <a class="reference internal" href="index.html#scrapy.crawler.Crawler.stats" title="scrapy.crawler.Crawler.stats"><code class="xref py py-attr docutils literal notranslate"><span class="pre">stats</span></code></a>
attribute. Here is an example of an extension that access stats:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ExtensionThatAccessStats</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stats</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stats</span> <span class="o">=</span> <span class="n">stats</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_crawler</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">crawler</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">crawler</span><span class="o">.</span><span class="n">stats</span><span class="p">)</span>
</pre></div>
</div>
<p>Set stat value:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">stats</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="s1">&#39;hostname&#39;</span><span class="p">,</span> <span class="n">socket</span><span class="o">.</span><span class="n">gethostname</span><span class="p">())</span>
</pre></div>
</div>
<p>Increment stat value:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">stats</span><span class="o">.</span><span class="n">inc_value</span><span class="p">(</span><span class="s1">&#39;custom_count&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Set stat value only if greater than previous:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">stats</span><span class="o">.</span><span class="n">max_value</span><span class="p">(</span><span class="s1">&#39;max_items_scraped&#39;</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
<p>Set stat value only if lower than previous:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">stats</span><span class="o">.</span><span class="n">min_value</span><span class="p">(</span><span class="s1">&#39;min_free_memory_percent&#39;</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
<p>Get stat value:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">stats</span><span class="o">.</span><span class="n">get_value</span><span class="p">(</span><span class="s1">&#39;custom_count&#39;</span><span class="p">)</span>
<span class="go">1</span>
</pre></div>
</div>
<p>Get all stats:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">stats</span><span class="o">.</span><span class="n">get_stats</span><span class="p">()</span>
<span class="go">{&#39;custom_count&#39;: 1, &#39;start_time&#39;: datetime.datetime(2009, 7, 14, 21, 47, 28, 977139)}</span>
</pre></div>
</div>
</div>
<div class="section" id="available-stats-collectors">
<h4>Available Stats Collectors<a class="headerlink" href="#available-stats-collectors" title="Permalink to this headline">¶</a></h4>
<p>Besides the basic <code class="xref py py-class docutils literal notranslate"><span class="pre">StatsCollector</span></code> there are other Stats Collectors
available in Scrapy which extend the basic Stats Collector. You can select
which Stats Collector to use through the <a class="reference internal" href="index.html#std:setting-STATS_CLASS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">STATS_CLASS</span></code></a> setting. The
default Stats Collector used is the <code class="xref py py-class docutils literal notranslate"><span class="pre">MemoryStatsCollector</span></code>.</p>
<div class="section" id="memorystatscollector">
<h5>MemoryStatsCollector<a class="headerlink" href="#memorystatscollector" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.statscollectors.MemoryStatsCollector">
<em class="property">class </em><code class="descclassname">scrapy.statscollectors.</code><code class="descname">MemoryStatsCollector</code><a class="headerlink" href="#scrapy.statscollectors.MemoryStatsCollector" title="Permalink to this definition">¶</a></dt>
<dd><p>A simple stats collector that keeps the stats of the last scraping run (for
each spider) in memory, after they’re closed. The stats can be accessed
through the <a class="reference internal" href="#scrapy.statscollectors.MemoryStatsCollector.spider_stats" title="scrapy.statscollectors.MemoryStatsCollector.spider_stats"><code class="xref py py-attr docutils literal notranslate"><span class="pre">spider_stats</span></code></a> attribute, which is a dict keyed by spider
domain name.</p>
<p>This is the default Stats Collector used in Scrapy.</p>
<dl class="attribute">
<dt id="scrapy.statscollectors.MemoryStatsCollector.spider_stats">
<code class="descname">spider_stats</code><a class="headerlink" href="#scrapy.statscollectors.MemoryStatsCollector.spider_stats" title="Permalink to this definition">¶</a></dt>
<dd><p>A dict of dicts (keyed by spider name) containing the stats of the last
scraping run for each spider.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="dummystatscollector">
<h5>DummyStatsCollector<a class="headerlink" href="#dummystatscollector" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.statscollectors.DummyStatsCollector">
<em class="property">class </em><code class="descclassname">scrapy.statscollectors.</code><code class="descname">DummyStatsCollector</code><a class="headerlink" href="#scrapy.statscollectors.DummyStatsCollector" title="Permalink to this definition">¶</a></dt>
<dd><p>A Stats collector which does nothing but is very efficient (because it does
nothing). This stats collector can be set via the <a class="reference internal" href="index.html#std:setting-STATS_CLASS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">STATS_CLASS</span></code></a>
setting, to disable stats collect in order to improve performance. However,
the performance penalty of stats collection is usually marginal compared to
other Scrapy workload like parsing pages.</p>
</dd></dl>

</div>
</div>
</div>
<span id="document-topics/email"></span><div class="section" id="module-scrapy.mail">
<span id="sending-e-mail"></span><span id="topics-email"></span><h3>Sending e-mail<a class="headerlink" href="#module-scrapy.mail" title="Permalink to this headline">¶</a></h3>
<p>Although Python makes sending e-mails relatively easy via the <a class="reference external" href="https://docs.python.org/2/library/smtplib.html">smtplib</a>
library, Scrapy provides its own facility for sending e-mails which is very
easy to use and it’s implemented using <a class="reference external" href="https://twistedmatrix.com/documents/current/core/howto/defer-intro.html">Twisted non-blocking IO</a>, to avoid
interfering with the non-blocking IO of the crawler. It also provides a
simple API for sending attachments and it’s very easy to configure, with a few
<a class="reference internal" href="#topics-email-settings"><span class="std std-ref">settings</span></a>.</p>
<div class="section" id="quick-example">
<h4>Quick example<a class="headerlink" href="#quick-example" title="Permalink to this headline">¶</a></h4>
<p>There are two ways to instantiate the mail sender. You can instantiate it using
the standard constructor:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.mail</span> <span class="k">import</span> <span class="n">MailSender</span>
<span class="n">mailer</span> <span class="o">=</span> <span class="n">MailSender</span><span class="p">()</span>
</pre></div>
</div>
<p>Or you can instantiate it passing a Scrapy settings object, which will respect
the <a class="reference internal" href="#topics-email-settings"><span class="std std-ref">settings</span></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">mailer</span> <span class="o">=</span> <span class="n">MailSender</span><span class="o">.</span><span class="n">from_settings</span><span class="p">(</span><span class="n">settings</span><span class="p">)</span>
</pre></div>
</div>
<p>And here is how to use it to send an e-mail (without attachments):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">mailer</span><span class="o">.</span><span class="n">send</span><span class="p">(</span><span class="n">to</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;someone@example.com&quot;</span><span class="p">],</span> <span class="n">subject</span><span class="o">=</span><span class="s2">&quot;Some subject&quot;</span><span class="p">,</span> <span class="n">body</span><span class="o">=</span><span class="s2">&quot;Some body&quot;</span><span class="p">,</span> <span class="n">cc</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;another@example.com&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="section" id="mailsender-class-reference">
<h4>MailSender class reference<a class="headerlink" href="#mailsender-class-reference" title="Permalink to this headline">¶</a></h4>
<p>MailSender is the preferred class to use for sending emails from Scrapy, as it
uses <a class="reference external" href="https://twistedmatrix.com/documents/current/core/howto/defer-intro.html">Twisted non-blocking IO</a>, like the rest of the framework.</p>
<dl class="class">
<dt id="scrapy.mail.MailSender">
<em class="property">class </em><code class="descclassname">scrapy.mail.</code><code class="descname">MailSender</code><span class="sig-paren">(</span><em>smtphost=None</em>, <em>mailfrom=None</em>, <em>smtpuser=None</em>, <em>smtppass=None</em>, <em>smtpport=None</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.mail.MailSender" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>smtphost</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.7)"><em>bytes</em></a>) – the SMTP host to use for sending the emails. If omitted, the
<a class="reference internal" href="#std:setting-MAIL_HOST"><code class="xref std std-setting docutils literal notranslate"><span class="pre">MAIL_HOST</span></code></a> setting will be used.</li>
<li><strong>mailfrom</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – the address used to send emails (in the <code class="docutils literal notranslate"><span class="pre">From:</span></code> header).
If omitted, the <a class="reference internal" href="#std:setting-MAIL_FROM"><code class="xref std std-setting docutils literal notranslate"><span class="pre">MAIL_FROM</span></code></a> setting will be used.</li>
<li><strong>smtpuser</strong> – the SMTP user. If omitted, the <a class="reference internal" href="#std:setting-MAIL_USER"><code class="xref std std-setting docutils literal notranslate"><span class="pre">MAIL_USER</span></code></a>
setting will be used. If not given, no SMTP authentication will be
performed.</li>
<li><strong>smtppass</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.7)"><em>bytes</em></a>) – the SMTP pass for authentication.</li>
<li><strong>smtpport</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the SMTP port to connect to</li>
<li><strong>smtptls</strong> (<em>boolean</em>) – enforce using SMTP STARTTLS</li>
<li><strong>smtpssl</strong> (<em>boolean</em>) – enforce using a secure SSL connection</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="classmethod">
<dt id="scrapy.mail.MailSender.from_settings">
<em class="property">classmethod </em><code class="descname">from_settings</code><span class="sig-paren">(</span><em>settings</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.mail.MailSender.from_settings" title="Permalink to this definition">¶</a></dt>
<dd><p>Instantiate using a Scrapy settings object, which will respect
<a class="reference internal" href="#topics-email-settings"><span class="std std-ref">these Scrapy settings</span></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>settings</strong> (<a class="reference internal" href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.settings.Settings</span></code></a> object) – the e-mail recipients</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.mail.MailSender.send">
<code class="descname">send</code><span class="sig-paren">(</span><em>to</em>, <em>subject</em>, <em>body</em>, <em>cc=None</em>, <em>attachs=()</em>, <em>mimetype='text/plain'</em>, <em>charset=None</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.mail.MailSender.send" title="Permalink to this definition">¶</a></dt>
<dd><p>Send email to the given recipients.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>to</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a><em> or </em><em>list of str</em>) – the e-mail recipients</li>
<li><strong>subject</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – the subject of the e-mail</li>
<li><strong>cc</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a><em> or </em><em>list of str</em>) – the e-mails to CC</li>
<li><strong>body</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – the e-mail body</li>
<li><strong>attachs</strong> (<em>iterable</em>) – an iterable of tuples <code class="docutils literal notranslate"><span class="pre">(attach_name,</span> <span class="pre">mimetype,</span>
<span class="pre">file_object)</span></code> where  <code class="docutils literal notranslate"><span class="pre">attach_name</span></code> is a string with the name that will
appear on the e-mail’s attachment, <code class="docutils literal notranslate"><span class="pre">mimetype</span></code> is the mimetype of the
attachment and <code class="docutils literal notranslate"><span class="pre">file_object</span></code> is a readable file object with the
contents of the attachment</li>
<li><strong>mimetype</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – the MIME type of the e-mail</li>
<li><strong>charset</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – the character encoding to use for the e-mail contents</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="mail-settings">
<span id="topics-email-settings"></span><h4>Mail settings<a class="headerlink" href="#mail-settings" title="Permalink to this headline">¶</a></h4>
<p>These settings define the default constructor values of the <a class="reference internal" href="#scrapy.mail.MailSender" title="scrapy.mail.MailSender"><code class="xref py py-class docutils literal notranslate"><span class="pre">MailSender</span></code></a>
class, and can be used to configure e-mail notifications in your project without
writing any code (for those extensions and code that uses <a class="reference internal" href="#scrapy.mail.MailSender" title="scrapy.mail.MailSender"><code class="xref py py-class docutils literal notranslate"><span class="pre">MailSender</span></code></a>).</p>
<div class="section" id="mail-from">
<span id="std:setting-MAIL_FROM"></span><h5>MAIL_FROM<a class="headerlink" href="#mail-from" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapy&#64;localhost'</span></code></p>
<p>Sender email to use (<code class="docutils literal notranslate"><span class="pre">From:</span></code> header) for sending emails.</p>
</div>
<div class="section" id="mail-host">
<span id="std:setting-MAIL_HOST"></span><h5>MAIL_HOST<a class="headerlink" href="#mail-host" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'localhost'</span></code></p>
<p>SMTP host to use for sending emails.</p>
</div>
<div class="section" id="mail-port">
<span id="std:setting-MAIL_PORT"></span><h5>MAIL_PORT<a class="headerlink" href="#mail-port" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">25</span></code></p>
<p>SMTP port to use for sending emails.</p>
</div>
<div class="section" id="mail-user">
<span id="std:setting-MAIL_USER"></span><h5>MAIL_USER<a class="headerlink" href="#mail-user" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p>
<p>User to use for SMTP authentication. If disabled no SMTP authentication will be
performed.</p>
</div>
<div class="section" id="mail-pass">
<span id="std:setting-MAIL_PASS"></span><h5>MAIL_PASS<a class="headerlink" href="#mail-pass" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p>
<p>Password to use for SMTP authentication, along with <a class="reference internal" href="#std:setting-MAIL_USER"><code class="xref std std-setting docutils literal notranslate"><span class="pre">MAIL_USER</span></code></a>.</p>
</div>
<div class="section" id="mail-tls">
<span id="std:setting-MAIL_TLS"></span><h5>MAIL_TLS<a class="headerlink" href="#mail-tls" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>Enforce using STARTTLS. STARTTLS is a way to take an existing insecure connection, and upgrade it to a secure connection using SSL/TLS.</p>
</div>
<div class="section" id="mail-ssl">
<span id="std:setting-MAIL_SSL"></span><h5>MAIL_SSL<a class="headerlink" href="#mail-ssl" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>Enforce connecting using an SSL encrypted connection</p>
</div>
</div>
</div>
<span id="document-topics/telnetconsole"></span><div class="section" id="telnet-console">
<span id="topics-telnetconsole"></span><h3>Telnet Console<a class="headerlink" href="#telnet-console" title="Permalink to this headline">¶</a></h3>
<p>Scrapy comes with a built-in telnet console for inspecting and controlling a
Scrapy running process. The telnet console is just a regular python shell
running inside the Scrapy process, so you can do literally anything from it.</p>
<p>The telnet console is a <a class="reference internal" href="index.html#topics-extensions-ref"><span class="std std-ref">built-in Scrapy extension</span></a> which comes enabled by default, but you can also
disable it if you want. For more information about the extension itself see
<a class="reference internal" href="index.html#topics-extensions-ref-telnetconsole"><span class="std std-ref">Telnet console extension</span></a>.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p>It is not secure to use telnet console via public networks, as telnet
doesn’t provide any transport-layer security. Having username/password
authentication doesn’t change that.</p>
<p class="last">Intended usage is connecting to a running Scrapy spider locally
(spider process and telnet client are on the same machine)
or over a secure connection (VPN, SSH tunnel).
Please avoid using telnet console over insecure connections,
or disable it completely using <a class="reference internal" href="index.html#std:setting-TELNETCONSOLE_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">TELNETCONSOLE_ENABLED</span></code></a> option.</p>
</div>
<div class="section" id="how-to-access-the-telnet-console">
<h4>How to access the telnet console<a class="headerlink" href="#how-to-access-the-telnet-console" title="Permalink to this headline">¶</a></h4>
<p>The telnet console listens in the TCP port defined in the
<a class="reference internal" href="#std:setting-TELNETCONSOLE_PORT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">TELNETCONSOLE_PORT</span></code></a> setting, which defaults to <code class="docutils literal notranslate"><span class="pre">6023</span></code>. To access
the console you need to type:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>telnet localhost 6023
Trying localhost...
Connected to localhost.
Escape character is &#39;^]&#39;.
Username:
Password:
&gt;&gt;&gt;
</pre></div>
</div>
<p>By default Username is <code class="docutils literal notranslate"><span class="pre">scrapy</span></code> and Password is autogenerated. The
autogenerated Password can be seen on scrapy logs like the example below:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>2018-10-16 14:35:21 [scrapy.extensions.telnet] INFO: Telnet Password: 16f92501e8a59326
</pre></div>
</div>
<p>Default Username and Password can be overriden by the settings
<a class="reference internal" href="#std:setting-TELNETCONSOLE_USERNAME"><code class="xref std std-setting docutils literal notranslate"><span class="pre">TELNETCONSOLE_USERNAME</span></code></a> and <a class="reference internal" href="#std:setting-TELNETCONSOLE_PASSWORD"><code class="xref std std-setting docutils literal notranslate"><span class="pre">TELNETCONSOLE_PASSWORD</span></code></a>.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Username and password provide only a limited protection, as telnet
is not using secure transport - by default traffic is not encrypted
even if username and password are set.</p>
</div>
<p>You need the telnet program which comes installed by default in Windows, and
most Linux distros.</p>
</div>
<div class="section" id="available-variables-in-the-telnet-console">
<h4>Available variables in the telnet console<a class="headerlink" href="#available-variables-in-the-telnet-console" title="Permalink to this headline">¶</a></h4>
<p>The telnet console is like a regular Python shell running inside the Scrapy
process, so you can do anything from it including importing new modules, etc.</p>
<p>However, the telnet console comes with some default variables defined for
convenience:</p>
<table border="1" class="docutils">
<colgroup>
<col width="19%" />
<col width="81%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Shortcut</th>
<th class="head">Description</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">crawler</span></code></td>
<td>the Scrapy Crawler (<a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.crawler.Crawler</span></code></a> object)</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">engine</span></code></td>
<td>Crawler.engine attribute</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">spider</span></code></td>
<td>the active spider</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">slot</span></code></td>
<td>the engine slot</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">extensions</span></code></td>
<td>the Extension Manager (Crawler.extensions attribute)</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">stats</span></code></td>
<td>the Stats Collector (Crawler.stats attribute)</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">settings</span></code></td>
<td>the Scrapy settings object (Crawler.settings attribute)</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">est</span></code></td>
<td>print a report of the engine status</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">prefs</span></code></td>
<td>for memory debugging (see <a class="reference internal" href="index.html#topics-leaks"><span class="std std-ref">Debugging memory leaks</span></a>)</td>
</tr>
<tr class="row-odd"><td><code class="docutils literal notranslate"><span class="pre">p</span></code></td>
<td>a shortcut to the <a class="reference external" href="https://docs.python.org/library/pprint.html#pprint.pprint">pprint.pprint</a> function</td>
</tr>
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">hpy</span></code></td>
<td>for memory debugging (see <a class="reference internal" href="index.html#topics-leaks"><span class="std std-ref">Debugging memory leaks</span></a>)</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="telnet-console-usage-examples">
<h4>Telnet console usage examples<a class="headerlink" href="#telnet-console-usage-examples" title="Permalink to this headline">¶</a></h4>
<p>Here are some example tasks you can do with the telnet console:</p>
<div class="section" id="view-engine-status">
<h5>View engine status<a class="headerlink" href="#view-engine-status" title="Permalink to this headline">¶</a></h5>
<p>You can use the <code class="docutils literal notranslate"><span class="pre">est()</span></code> method of the Scrapy engine to quickly show its state
using the telnet console:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>telnet localhost 6023
&gt;&gt;&gt; est()
Execution engine status

time()-engine.start_time                        : 8.62972998619
engine.has_capacity()                           : False
len(engine.downloader.active)                   : 16
engine.scraper.is_idle()                        : False
engine.spider.name                              : followall
engine.spider_is_idle(engine.spider)            : False
engine.slot.closing                             : False
len(engine.slot.inprogress)                     : 16
len(engine.slot.scheduler.dqs or [])            : 0
len(engine.slot.scheduler.mqs)                  : 92
len(engine.scraper.slot.queue)                  : 0
len(engine.scraper.slot.active)                 : 0
engine.scraper.slot.active_size                 : 0
engine.scraper.slot.itemproc_size               : 0
engine.scraper.slot.needs_backout()             : False
</pre></div>
</div>
</div>
<div class="section" id="pause-resume-and-stop-the-scrapy-engine">
<h5>Pause, resume and stop the Scrapy engine<a class="headerlink" href="#pause-resume-and-stop-the-scrapy-engine" title="Permalink to this headline">¶</a></h5>
<p>To pause:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>telnet localhost 6023
&gt;&gt;&gt; engine.pause()
&gt;&gt;&gt;
</pre></div>
</div>
<p>To resume:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>telnet localhost 6023
&gt;&gt;&gt; engine.unpause()
&gt;&gt;&gt;
</pre></div>
</div>
<p>To stop:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>telnet localhost 6023
&gt;&gt;&gt; engine.stop()
Connection closed by foreign host.
</pre></div>
</div>
</div>
</div>
<div class="section" id="telnet-console-signals">
<h4>Telnet Console signals<a class="headerlink" href="#telnet-console-signals" title="Permalink to this headline">¶</a></h4>
<span class="target" id="std:signal-update_telnet_vars"></span><dl class="function">
<dt id="scrapy.extensions.telnet.update_telnet_vars">
<code class="descclassname">scrapy.extensions.telnet.</code><code class="descname">update_telnet_vars</code><span class="sig-paren">(</span><em>telnet_vars</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.extensions.telnet.update_telnet_vars" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent just before the telnet console is opened. You can hook up to this
signal to add, remove or update the variables that will be available in the
telnet local namespace. In order to do that, you need to update the
<code class="docutils literal notranslate"><span class="pre">telnet_vars</span></code> dict in your handler.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>telnet_vars</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.7)"><em>dict</em></a>) – the dict of telnet variables</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="telnet-settings">
<h4>Telnet settings<a class="headerlink" href="#telnet-settings" title="Permalink to this headline">¶</a></h4>
<p>These are the settings that control the telnet console’s behaviour:</p>
<div class="section" id="telnetconsole-port">
<span id="std:setting-TELNETCONSOLE_PORT"></span><h5>TELNETCONSOLE_PORT<a class="headerlink" href="#telnetconsole-port" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">[6023,</span> <span class="pre">6073]</span></code></p>
<p>The port range to use for the telnet console. If set to <code class="docutils literal notranslate"><span class="pre">None</span></code> or <code class="docutils literal notranslate"><span class="pre">0</span></code>, a
dynamically assigned port is used.</p>
</div>
<div class="section" id="telnetconsole-host">
<span id="std:setting-TELNETCONSOLE_HOST"></span><h5>TELNETCONSOLE_HOST<a class="headerlink" href="#telnetconsole-host" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'127.0.0.1'</span></code></p>
<p>The interface the telnet console should listen on</p>
</div>
<div class="section" id="telnetconsole-username">
<span id="std:setting-TELNETCONSOLE_USERNAME"></span><h5>TELNETCONSOLE_USERNAME<a class="headerlink" href="#telnetconsole-username" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapy'</span></code></p>
<p>The username used for the telnet console</p>
</div>
<div class="section" id="telnetconsole-password">
<span id="std:setting-TELNETCONSOLE_PASSWORD"></span><h5>TELNETCONSOLE_PASSWORD<a class="headerlink" href="#telnetconsole-password" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">None</span></code></p>
<p>The password used for the telnet console, default behaviour is to have it
autogenerated</p>
</div>
</div>
</div>
<span id="document-topics/webservice"></span><div class="section" id="web-service">
<span id="topics-webservice"></span><h3>Web Service<a class="headerlink" href="#web-service" title="Permalink to this headline">¶</a></h3>
<p>webservice has been moved into a separate project.</p>
<p>It is hosted at:</p>
<blockquote>
<div><a class="reference external" href="https://github.com/scrapy-plugins/scrapy-jsonrpc">https://github.com/scrapy-plugins/scrapy-jsonrpc</a></div></blockquote>
</div>
</div>
<dl class="docutils">
<dt><a class="reference internal" href="index.html#document-topics/logging"><span class="doc">Logging</span></a></dt>
<dd>Learn how to use Python’s builtin logging on Scrapy.</dd>
<dt><a class="reference internal" href="index.html#document-topics/stats"><span class="doc">Stats Collection</span></a></dt>
<dd>Collect statistics about your scraping crawler.</dd>
<dt><a class="reference internal" href="index.html#document-topics/email"><span class="doc">Sending e-mail</span></a></dt>
<dd>Send email notifications when certain events occur.</dd>
<dt><a class="reference internal" href="index.html#document-topics/telnetconsole"><span class="doc">Telnet Console</span></a></dt>
<dd>Inspect a running crawler using a built-in Python console.</dd>
<dt><a class="reference internal" href="index.html#document-topics/webservice"><span class="doc">Web Service</span></a></dt>
<dd>Monitor and control a crawler using a web service.</dd>
</dl>
</div>
<div class="section" id="solving-specific-problems">
<h2>Solving specific problems<a class="headerlink" href="#solving-specific-problems" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-faq"></span><div class="section" id="frequently-asked-questions">
<span id="faq"></span><h3>Frequently Asked Questions<a class="headerlink" href="#frequently-asked-questions" title="Permalink to this headline">¶</a></h3>
<div class="section" id="how-does-scrapy-compare-to-beautifulsoup-or-lxml">
<span id="faq-scrapy-bs-cmp"></span><h4>How does Scrapy compare to BeautifulSoup or lxml?<a class="headerlink" href="#how-does-scrapy-compare-to-beautifulsoup-or-lxml" title="Permalink to this headline">¶</a></h4>
<p><a class="reference external" href="https://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a> and <a class="reference external" href="http://lxml.de/">lxml</a> are libraries for parsing HTML and XML. Scrapy is
an application framework for writing web spiders that crawl web sites and
extract data from them.</p>
<p>Scrapy provides a built-in mechanism for extracting data (called
<a class="reference internal" href="index.html#topics-selectors"><span class="std std-ref">selectors</span></a>) but you can easily use <a class="reference external" href="https://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a>
(or <a class="reference external" href="http://lxml.de/">lxml</a>) instead, if you feel more comfortable working with them. After
all, they’re just parsing libraries which can be imported and used from any
Python code.</p>
<p>In other words, comparing <a class="reference external" href="https://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a> (or <a class="reference external" href="http://lxml.de/">lxml</a>) to Scrapy is like
comparing <a class="reference external" href="http://jinja.pocoo.org/">jinja2</a> to <a class="reference external" href="https://www.djangoproject.com/">Django</a>.</p>
</div>
<div class="section" id="can-i-use-scrapy-with-beautifulsoup">
<h4>Can I use Scrapy with BeautifulSoup?<a class="headerlink" href="#can-i-use-scrapy-with-beautifulsoup" title="Permalink to this headline">¶</a></h4>
<p>Yes, you can.
As mentioned <a class="reference internal" href="#faq-scrapy-bs-cmp"><span class="std std-ref">above</span></a>, <a class="reference external" href="https://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a> can be used
for parsing HTML responses in Scrapy callbacks.
You just have to feed the response’s body into a <code class="docutils literal notranslate"><span class="pre">BeautifulSoup</span></code> object
and extract whatever data you need from it.</p>
<p>Here’s an example spider using BeautifulSoup API, with <code class="docutils literal notranslate"><span class="pre">lxml</span></code> as the HTML parser:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">bs4</span> <span class="k">import</span> <span class="n">BeautifulSoup</span>
<span class="kn">import</span> <span class="nn">scrapy</span>


<span class="k">class</span> <span class="nc">ExampleSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;example&quot;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;example.com&quot;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s1">&#39;http://www.example.com/&#39;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="c1"># use lxml to get decent HTML parsing speed</span>
        <span class="n">soup</span> <span class="o">=</span> <span class="n">BeautifulSoup</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">text</span><span class="p">,</span> <span class="s1">&#39;lxml&#39;</span><span class="p">)</span>
        <span class="k">yield</span> <span class="p">{</span>
            <span class="s2">&quot;url&quot;</span><span class="p">:</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">,</span>
            <span class="s2">&quot;title&quot;</span><span class="p">:</span> <span class="n">soup</span><span class="o">.</span><span class="n">h1</span><span class="o">.</span><span class="n">string</span>
        <span class="p">}</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><code class="docutils literal notranslate"><span class="pre">BeautifulSoup</span></code> supports several HTML/XML parsers.
See <a class="reference external" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/#specifying-the-parser-to-use">BeautifulSoup’s official documentation</a> on which ones are available.</p>
</div>
</div>
<div class="section" id="what-python-versions-does-scrapy-support">
<span id="faq-python-versions"></span><h4>What Python versions does Scrapy support?<a class="headerlink" href="#what-python-versions-does-scrapy-support" title="Permalink to this headline">¶</a></h4>
<p>Scrapy is supported under Python 2.7 and Python 3.4+
under CPython (default Python implementation) and PyPy (starting with PyPy 5.9).
Python 2.6 support was dropped starting at Scrapy 0.20.
Python 3 support was added in Scrapy 1.1.
PyPy support was added in Scrapy 1.4, PyPy3 support was added in Scrapy 1.5.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">For Python 3 support on Windows, it is recommended to use
Anaconda/Miniconda as <a class="reference internal" href="index.html#intro-install-windows"><span class="std std-ref">outlined in the installation guide</span></a>.</p>
</div>
</div>
<div class="section" id="did-scrapy-steal-x-from-django">
<h4>Did Scrapy “steal” X from Django?<a class="headerlink" href="#did-scrapy-steal-x-from-django" title="Permalink to this headline">¶</a></h4>
<p>Probably, but we don’t like that word. We think <a class="reference external" href="https://www.djangoproject.com/">Django</a> is a great open source
project and an example to follow, so we’ve used it as an inspiration for
Scrapy.</p>
<p>We believe that, if something is already done well, there’s no need to reinvent
it. This concept, besides being one of the foundations for open source and free
software, not only applies to software but also to documentation, procedures,
policies, etc. So, instead of going through each problem ourselves, we choose
to copy ideas from those projects that have already solved them properly, and
focus on the real problems we need to solve.</p>
<p>We’d be proud if Scrapy serves as an inspiration for other projects. Feel free
to steal from us!</p>
</div>
<div class="section" id="does-scrapy-work-with-http-proxies">
<h4>Does Scrapy work with HTTP proxies?<a class="headerlink" href="#does-scrapy-work-with-http-proxies" title="Permalink to this headline">¶</a></h4>
<p>Yes. Support for HTTP proxies is provided (since Scrapy 0.8) through the HTTP
Proxy downloader middleware. See
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">HttpProxyMiddleware</span></code></a>.</p>
</div>
<div class="section" id="how-can-i-scrape-an-item-with-attributes-in-different-pages">
<h4>How can I scrape an item with attributes in different pages?<a class="headerlink" href="#how-can-i-scrape-an-item-with-attributes-in-different-pages" title="Permalink to this headline">¶</a></h4>
<p>See <a class="reference internal" href="index.html#topics-request-response-ref-request-callback-arguments"><span class="std std-ref">Passing additional data to callback functions</span></a>.</p>
</div>
<div class="section" id="scrapy-crashes-with-importerror-no-module-named-win32api">
<h4>Scrapy crashes with: ImportError: No module named win32api<a class="headerlink" href="#scrapy-crashes-with-importerror-no-module-named-win32api" title="Permalink to this headline">¶</a></h4>
<p>You need to install <a class="reference external" href="https://sourceforge.net/projects/pywin32/">pywin32</a> because of <a class="reference external" href="https://twistedmatrix.com/trac/ticket/3707">this Twisted bug</a>.</p>
</div>
<div class="section" id="how-can-i-simulate-a-user-login-in-my-spider">
<h4>How can I simulate a user login in my spider?<a class="headerlink" href="#how-can-i-simulate-a-user-login-in-my-spider" title="Permalink to this headline">¶</a></h4>
<p>See <a class="reference internal" href="index.html#topics-request-response-ref-request-userlogin"><span class="std std-ref">Using FormRequest.from_response() to simulate a user login</span></a>.</p>
</div>
<div class="section" id="does-scrapy-crawl-in-breadth-first-or-depth-first-order">
<span id="faq-bfo-dfo"></span><h4>Does Scrapy crawl in breadth-first or depth-first order?<a class="headerlink" href="#does-scrapy-crawl-in-breadth-first-or-depth-first-order" title="Permalink to this headline">¶</a></h4>
<p>By default, Scrapy uses a <a class="reference external" href="https://en.wikipedia.org/wiki/Stack_(abstract_data_type)">LIFO</a> queue for storing pending requests, which
basically means that it crawls in <a class="reference external" href="https://en.wikipedia.org/wiki/Depth-first_search">DFO order</a>. This order is more convenient
in most cases.</p>
<p>If you do want to crawl in true <a class="reference external" href="https://en.wikipedia.org/wiki/Breadth-first_search">BFO order</a>, you can do it by
setting the following settings:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">DEPTH_PRIORITY</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">SCHEDULER_DISK_QUEUE</span> <span class="o">=</span> <span class="s1">&#39;scrapy.squeues.PickleFifoDiskQueue&#39;</span>
<span class="n">SCHEDULER_MEMORY_QUEUE</span> <span class="o">=</span> <span class="s1">&#39;scrapy.squeues.FifoMemoryQueue&#39;</span>
</pre></div>
</div>
<p>While pending requests are below the configured values of
<a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS</span></code></a>, <a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a> or
<a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a>, those requests are sent
concurrently. As a result, the first few requests of a crawl rarely follow the
desired order. Lowering those settings to <code class="docutils literal notranslate"><span class="pre">1</span></code> enforces the desired order, but
it significantly slows down the crawl as a whole.</p>
</div>
<div class="section" id="my-scrapy-crawler-has-memory-leaks-what-can-i-do">
<h4>My Scrapy crawler has memory leaks. What can I do?<a class="headerlink" href="#my-scrapy-crawler-has-memory-leaks-what-can-i-do" title="Permalink to this headline">¶</a></h4>
<p>See <a class="reference internal" href="index.html#topics-leaks"><span class="std std-ref">Debugging memory leaks</span></a>.</p>
<p>Also, Python has a builtin memory leak issue which is described in
<a class="reference internal" href="index.html#topics-leaks-without-leaks"><span class="std std-ref">Leaks without leaks</span></a>.</p>
</div>
<div class="section" id="how-can-i-make-scrapy-consume-less-memory">
<h4>How can I make Scrapy consume less memory?<a class="headerlink" href="#how-can-i-make-scrapy-consume-less-memory" title="Permalink to this headline">¶</a></h4>
<p>See previous question.</p>
</div>
<div class="section" id="can-i-use-basic-http-authentication-in-my-spiders">
<h4>Can I use Basic HTTP Authentication in my spiders?<a class="headerlink" href="#can-i-use-basic-http-authentication-in-my-spiders" title="Permalink to this headline">¶</a></h4>
<p>Yes, see <a class="reference internal" href="index.html#scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware" title="scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">HttpAuthMiddleware</span></code></a>.</p>
</div>
<div class="section" id="why-does-scrapy-download-pages-in-english-instead-of-my-native-language">
<h4>Why does Scrapy download pages in English instead of my native language?<a class="headerlink" href="#why-does-scrapy-download-pages-in-english-instead-of-my-native-language" title="Permalink to this headline">¶</a></h4>
<p>Try changing the default <a class="reference external" href="https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.4">Accept-Language</a> request header by overriding the
<a class="reference internal" href="index.html#std:setting-DEFAULT_REQUEST_HEADERS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DEFAULT_REQUEST_HEADERS</span></code></a> setting.</p>
</div>
<div class="section" id="where-can-i-find-some-example-scrapy-projects">
<h4>Where can I find some example Scrapy projects?<a class="headerlink" href="#where-can-i-find-some-example-scrapy-projects" title="Permalink to this headline">¶</a></h4>
<p>See <a class="reference internal" href="index.html#intro-examples"><span class="std std-ref">Examples</span></a>.</p>
</div>
<div class="section" id="can-i-run-a-spider-without-creating-a-project">
<h4>Can I run a spider without creating a project?<a class="headerlink" href="#can-i-run-a-spider-without-creating-a-project" title="Permalink to this headline">¶</a></h4>
<p>Yes. You can use the <a class="reference internal" href="index.html#std:command-runspider"><code class="xref std std-command docutils literal notranslate"><span class="pre">runspider</span></code></a> command. For example, if you have a
spider written in a <code class="docutils literal notranslate"><span class="pre">my_spider.py</span></code> file you can run it with:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">runspider</span> <span class="n">my_spider</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
<p>See <a class="reference internal" href="index.html#std:command-runspider"><code class="xref std std-command docutils literal notranslate"><span class="pre">runspider</span></code></a> command for more info.</p>
</div>
<div class="section" id="i-get-filtered-offsite-request-messages-how-can-i-fix-them">
<h4>I get “Filtered offsite request” messages. How can I fix them?<a class="headerlink" href="#i-get-filtered-offsite-request-messages-how-can-i-fix-them" title="Permalink to this headline">¶</a></h4>
<p>Those messages (logged with <code class="docutils literal notranslate"><span class="pre">DEBUG</span></code> level) don’t necessarily mean there is a
problem, so you may not need to fix them.</p>
<p>Those messages are thrown by the Offsite Spider Middleware, which is a spider
middleware (enabled by default) whose purpose is to filter out requests to
domains outside the ones covered by the spider.</p>
<p>For more info see:
<a class="reference internal" href="index.html#scrapy.spidermiddlewares.offsite.OffsiteMiddleware" title="scrapy.spidermiddlewares.offsite.OffsiteMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">OffsiteMiddleware</span></code></a>.</p>
</div>
<div class="section" id="what-is-the-recommended-way-to-deploy-a-scrapy-crawler-in-production">
<h4>What is the recommended way to deploy a Scrapy crawler in production?<a class="headerlink" href="#what-is-the-recommended-way-to-deploy-a-scrapy-crawler-in-production" title="Permalink to this headline">¶</a></h4>
<p>See <a class="reference internal" href="index.html#topics-deploy"><span class="std std-ref">Deploying Spiders</span></a>.</p>
</div>
<div class="section" id="can-i-use-json-for-large-exports">
<h4>Can I use JSON for large exports?<a class="headerlink" href="#can-i-use-json-for-large-exports" title="Permalink to this headline">¶</a></h4>
<p>It’ll depend on how large your output is. See <a class="reference internal" href="index.html#json-with-large-data"><span class="std std-ref">this warning</span></a> in <a class="reference internal" href="index.html#scrapy.exporters.JsonItemExporter" title="scrapy.exporters.JsonItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">JsonItemExporter</span></code></a>
documentation.</p>
</div>
<div class="section" id="can-i-return-twisted-deferreds-from-signal-handlers">
<h4>Can I return (Twisted) deferreds from signal handlers?<a class="headerlink" href="#can-i-return-twisted-deferreds-from-signal-handlers" title="Permalink to this headline">¶</a></h4>
<p>Some signals support returning deferreds from their handlers, others don’t. See
the <a class="reference internal" href="index.html#topics-signals-ref"><span class="std std-ref">Built-in signals reference</span></a> to know which ones.</p>
</div>
<div class="section" id="what-does-the-response-status-code-999-means">
<h4>What does the response status code 999 means?<a class="headerlink" href="#what-does-the-response-status-code-999-means" title="Permalink to this headline">¶</a></h4>
<p>999 is a custom response status code used by Yahoo sites to throttle requests.
Try slowing down the crawling speed by using a download delay of <code class="docutils literal notranslate"><span class="pre">2</span></code> (or
higher) in your spider:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">CrawlSpider</span><span class="p">):</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;myspider&#39;</span>

    <span class="n">download_delay</span> <span class="o">=</span> <span class="mi">2</span>

    <span class="c1"># [ ... rest of the spider code ... ]</span>
</pre></div>
</div>
<p>Or by setting a global download delay in your project with the
<a class="reference internal" href="index.html#std:setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_DELAY</span></code></a> setting.</p>
</div>
<div class="section" id="can-i-call-pdb-set-trace-from-my-spiders-to-debug-them">
<h4>Can I call <code class="docutils literal notranslate"><span class="pre">pdb.set_trace()</span></code> from my spiders to debug them?<a class="headerlink" href="#can-i-call-pdb-set-trace-from-my-spiders-to-debug-them" title="Permalink to this headline">¶</a></h4>
<p>Yes, but you can also use the Scrapy shell which allows you to quickly analyze
(and even modify) the response being processed by your spider, which is, quite
often, more useful than plain old <code class="docutils literal notranslate"><span class="pre">pdb.set_trace()</span></code>.</p>
<p>For more info see <a class="reference internal" href="index.html#topics-shell-inspect-response"><span class="std std-ref">Invoking the shell from spiders to inspect responses</span></a>.</p>
</div>
<div class="section" id="simplest-way-to-dump-all-my-scraped-items-into-a-json-csv-xml-file">
<h4>Simplest way to dump all my scraped items into a JSON/CSV/XML file?<a class="headerlink" href="#simplest-way-to-dump-all-my-scraped-items-into-a-json-csv-xml-file" title="Permalink to this headline">¶</a></h4>
<p>To dump into a JSON file:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">crawl</span> <span class="n">myspider</span> <span class="o">-</span><span class="n">o</span> <span class="n">items</span><span class="o">.</span><span class="n">json</span>
</pre></div>
</div>
<p>To dump into a CSV file:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">crawl</span> <span class="n">myspider</span> <span class="o">-</span><span class="n">o</span> <span class="n">items</span><span class="o">.</span><span class="n">csv</span>
</pre></div>
</div>
<p>To dump into a XML file:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">crawl</span> <span class="n">myspider</span> <span class="o">-</span><span class="n">o</span> <span class="n">items</span><span class="o">.</span><span class="n">xml</span>
</pre></div>
</div>
<p>For more information see <a class="reference internal" href="index.html#topics-feed-exports"><span class="std std-ref">Feed exports</span></a></p>
</div>
<div class="section" id="what-s-this-huge-cryptic-viewstate-parameter-used-in-some-forms">
<h4>What’s this huge cryptic <code class="docutils literal notranslate"><span class="pre">__VIEWSTATE</span></code> parameter used in some forms?<a class="headerlink" href="#what-s-this-huge-cryptic-viewstate-parameter-used-in-some-forms" title="Permalink to this headline">¶</a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">__VIEWSTATE</span></code> parameter is used in sites built with ASP.NET/VB.NET. For
more info on how it works see <a class="reference external" href="http://search.cpan.org/~ecarroll/HTML-TreeBuilderX-ASP_NET-0.09/lib/HTML/TreeBuilderX/ASP_NET.pm">this page</a>. Also, here’s an <a class="reference external" href="https://github.com/AmbientLighter/rpn-fas/blob/master/fas/spiders/rnp.py">example spider</a>
which scrapes one of these sites.</p>
</div>
<div class="section" id="what-s-the-best-way-to-parse-big-xml-csv-data-feeds">
<h4>What’s the best way to parse big XML/CSV data feeds?<a class="headerlink" href="#what-s-the-best-way-to-parse-big-xml-csv-data-feeds" title="Permalink to this headline">¶</a></h4>
<p>Parsing big feeds with XPath selectors can be problematic since they need to
build the DOM of the entire feed in memory, and this can be quite slow and
consume a lot of memory.</p>
<p>In order to avoid parsing all the entire feed at once in memory, you can use
the functions <code class="docutils literal notranslate"><span class="pre">xmliter</span></code> and <code class="docutils literal notranslate"><span class="pre">csviter</span></code> from <code class="docutils literal notranslate"><span class="pre">scrapy.utils.iterators</span></code>
module. In fact, this is what the feed spiders (see <a class="reference internal" href="index.html#topics-spiders"><span class="std std-ref">Spiders</span></a>) use
under the cover.</p>
</div>
<div class="section" id="does-scrapy-manage-cookies-automatically">
<h4>Does Scrapy manage cookies automatically?<a class="headerlink" href="#does-scrapy-manage-cookies-automatically" title="Permalink to this headline">¶</a></h4>
<p>Yes, Scrapy receives and keeps track of cookies sent by servers, and sends them
back on subsequent requests, like any regular web browser does.</p>
<p>For more info see <a class="reference internal" href="index.html#topics-request-response"><span class="std std-ref">Requests and Responses</span></a> and <a class="reference internal" href="index.html#cookies-mw"><span class="std std-ref">CookiesMiddleware</span></a>.</p>
</div>
<div class="section" id="how-can-i-see-the-cookies-being-sent-and-received-from-scrapy">
<h4>How can I see the cookies being sent and received from Scrapy?<a class="headerlink" href="#how-can-i-see-the-cookies-being-sent-and-received-from-scrapy" title="Permalink to this headline">¶</a></h4>
<p>Enable the <a class="reference internal" href="index.html#std:setting-COOKIES_DEBUG"><code class="xref std std-setting docutils literal notranslate"><span class="pre">COOKIES_DEBUG</span></code></a> setting.</p>
</div>
<div class="section" id="how-can-i-instruct-a-spider-to-stop-itself">
<h4>How can I instruct a spider to stop itself?<a class="headerlink" href="#how-can-i-instruct-a-spider-to-stop-itself" title="Permalink to this headline">¶</a></h4>
<p>Raise the <a class="reference internal" href="index.html#scrapy.exceptions.CloseSpider" title="scrapy.exceptions.CloseSpider"><code class="xref py py-exc docutils literal notranslate"><span class="pre">CloseSpider</span></code></a> exception from a callback. For
more info see: <a class="reference internal" href="index.html#scrapy.exceptions.CloseSpider" title="scrapy.exceptions.CloseSpider"><code class="xref py py-exc docutils literal notranslate"><span class="pre">CloseSpider</span></code></a>.</p>
</div>
<div class="section" id="how-can-i-prevent-my-scrapy-bot-from-getting-banned">
<h4>How can I prevent my Scrapy bot from getting banned?<a class="headerlink" href="#how-can-i-prevent-my-scrapy-bot-from-getting-banned" title="Permalink to this headline">¶</a></h4>
<p>See <a class="reference internal" href="index.html#bans"><span class="std std-ref">Avoiding getting banned</span></a>.</p>
</div>
<div class="section" id="should-i-use-spider-arguments-or-settings-to-configure-my-spider">
<h4>Should I use spider arguments or settings to configure my spider?<a class="headerlink" href="#should-i-use-spider-arguments-or-settings-to-configure-my-spider" title="Permalink to this headline">¶</a></h4>
<p>Both <a class="reference internal" href="index.html#spiderargs"><span class="std std-ref">spider arguments</span></a> and <a class="reference internal" href="index.html#topics-settings"><span class="std std-ref">settings</span></a>
can be used to configure your spider. There is no strict rule that mandates to
use one or the other, but settings are more suited for parameters that, once
set, don’t change much, while spider arguments are meant to change more often,
even on each spider run and sometimes are required for the spider to run at all
(for example, to set the start url of a spider).</p>
<p>To illustrate with an example, assuming you have a spider that needs to log
into a site to scrape data, and you only want to scrape data from a certain
section of the site (which varies each time). In that case, the credentials to
log in would be settings, while the url of the section to scrape would be a
spider argument.</p>
</div>
<div class="section" id="i-m-scraping-a-xml-document-and-my-xpath-selector-doesn-t-return-any-items">
<h4>I’m scraping a XML document and my XPath selector doesn’t return any items<a class="headerlink" href="#i-m-scraping-a-xml-document-and-my-xpath-selector-doesn-t-return-any-items" title="Permalink to this headline">¶</a></h4>
<p>You may need to remove namespaces. See <a class="reference internal" href="index.html#removing-namespaces"><span class="std std-ref">Removing namespaces</span></a>.</p>
</div>
<div class="section" id="how-to-split-an-item-into-multiple-items-in-an-item-pipeline">
<span id="faq-split-item"></span><h4>How to split an item into multiple items in an item pipeline?<a class="headerlink" href="#how-to-split-an-item-into-multiple-items-in-an-item-pipeline" title="Permalink to this headline">¶</a></h4>
<p><a class="reference internal" href="index.html#topics-item-pipeline"><span class="std std-ref">Item pipelines</span></a> cannot yield multiple items per
input item. <a class="reference internal" href="index.html#custom-spider-middleware"><span class="std std-ref">Create a spider middleware</span></a>
instead, and use its
<a class="reference internal" href="index.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_output()</span></code></a>
method for this puspose. For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">copy</span> <span class="k">import</span> <span class="n">deepcopy</span>

<span class="kn">from</span> <span class="nn">scrapy.item</span> <span class="k">import</span> <span class="n">BaseItem</span>


<span class="k">class</span> <span class="nc">MultiplyItemsMiddleware</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">process_spider_output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">result</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">item</span><span class="p">,</span> <span class="p">(</span><span class="n">BaseItem</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)):</span>
                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">item</span><span class="p">[</span><span class="s1">&#39;multiply_by&#39;</span><span class="p">]):</span>
                    <span class="k">yield</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<span id="document-topics/debug"></span><div class="section" id="debugging-spiders">
<span id="topics-debug"></span><h3>Debugging Spiders<a class="headerlink" href="#debugging-spiders" title="Permalink to this headline">¶</a></h3>
<p>This document explains the most common techniques for debugging spiders.
Consider the following scrapy spider below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">from</span> <span class="nn">myproject.items</span> <span class="k">import</span> <span class="n">MyItem</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;myspider&#39;</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">(</span>
        <span class="s1">&#39;http://example.com/page1&#39;</span><span class="p">,</span>
        <span class="s1">&#39;http://example.com/page2&#39;</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="c1"># &lt;processing code not shown&gt;</span>
        <span class="c1"># collect `item_urls`</span>
        <span class="k">for</span> <span class="n">item_url</span> <span class="ow">in</span> <span class="n">item_urls</span><span class="p">:</span>
            <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">item_url</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse_item</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">parse_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="c1"># &lt;processing code not shown&gt;</span>
        <span class="n">item</span> <span class="o">=</span> <span class="n">MyItem</span><span class="p">()</span>
        <span class="c1"># populate `item` fields</span>
        <span class="c1"># and extract item_details_url</span>
        <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">item_details_url</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse_details</span><span class="p">,</span> <span class="n">cb_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;item&#39;</span><span class="p">:</span> <span class="n">item</span><span class="p">})</span>

    <span class="k">def</span> <span class="nf">parse_details</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
        <span class="c1"># populate more `item` fields</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
<p>Basically this is a simple spider which parses two pages of items (the
start_urls). Items also have a details page with additional information, so we
use the <code class="docutils literal notranslate"><span class="pre">cb_kwargs</span></code> functionality of <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> to pass a
partially populated item.</p>
<div class="section" id="parse-command">
<h4>Parse Command<a class="headerlink" href="#parse-command" title="Permalink to this headline">¶</a></h4>
<p>The most basic way of checking the output of your spider is to use the
<a class="reference internal" href="index.html#std:command-parse"><code class="xref std std-command docutils literal notranslate"><span class="pre">parse</span></code></a> command. It allows to check the behaviour of different parts
of the spider at the method level. It has the advantage of being flexible and
simple to use, but does not allow debugging code inside a method.</p>
<p>In order to see the item scraped from a specific url:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ scrapy parse --spider=myspider -c parse_item -d 2 &lt;item_url&gt;
[ ... scrapy log lines crawling example.com spider ... ]

&gt;&gt;&gt; STATUS DEPTH LEVEL 2 &lt;&lt;&lt;
# Scraped Items  ------------------------------------------------------------
[{&#39;url&#39;: &lt;item_url&gt;}]

# Requests  -----------------------------------------------------------------
[]
</pre></div>
</div>
<p>Using the <code class="docutils literal notranslate"><span class="pre">--verbose</span></code> or <code class="docutils literal notranslate"><span class="pre">-v</span></code> option we can see the status at each depth level:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ scrapy parse --spider=myspider -c parse_item -d 2 -v &lt;item_url&gt;
[ ... scrapy log lines crawling example.com spider ... ]

&gt;&gt;&gt; DEPTH LEVEL: 1 &lt;&lt;&lt;
# Scraped Items  ------------------------------------------------------------
[]

# Requests  -----------------------------------------------------------------
[&lt;GET item_details_url&gt;]


&gt;&gt;&gt; DEPTH LEVEL: 2 &lt;&lt;&lt;
# Scraped Items  ------------------------------------------------------------
[{&#39;url&#39;: &lt;item_url&gt;}]

# Requests  -----------------------------------------------------------------
[]
</pre></div>
</div>
<p>Checking items scraped from a single start_url, can also be easily achieved
using:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ scrapy parse --spider=myspider -d 3 &#39;http://example.com/page1&#39;
</pre></div>
</div>
</div>
<div class="section" id="scrapy-shell">
<h4>Scrapy Shell<a class="headerlink" href="#scrapy-shell" title="Permalink to this headline">¶</a></h4>
<p>While the <a class="reference internal" href="index.html#std:command-parse"><code class="xref std std-command docutils literal notranslate"><span class="pre">parse</span></code></a> command is very useful for checking behaviour of a
spider, it is of little help to check what happens inside a callback, besides
showing the response received and the output. How to debug the situation when
<code class="docutils literal notranslate"><span class="pre">parse_details</span></code> sometimes receives no item?</p>
<p>Fortunately, the <a class="reference internal" href="index.html#std:command-shell"><code class="xref std std-command docutils literal notranslate"><span class="pre">shell</span></code></a> is your bread and butter in this case (see
<a class="reference internal" href="index.html#topics-shell-inspect-response"><span class="std std-ref">Invoking the shell from spiders to inspect responses</span></a>):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.shell</span> <span class="k">import</span> <span class="n">inspect_response</span>

<span class="k">def</span> <span class="nf">parse_details</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">item</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">item</span><span class="p">:</span>
        <span class="c1"># populate more `item` fields</span>
        <span class="k">return</span> <span class="n">item</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">inspect_response</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
</pre></div>
</div>
<p>See also: <a class="reference internal" href="index.html#topics-shell-inspect-response"><span class="std std-ref">Invoking the shell from spiders to inspect responses</span></a>.</p>
</div>
<div class="section" id="open-in-browser">
<h4>Open in browser<a class="headerlink" href="#open-in-browser" title="Permalink to this headline">¶</a></h4>
<p>Sometimes you just want to see how a certain response looks in a browser, you
can use the <code class="docutils literal notranslate"><span class="pre">open_in_browser</span></code> function for that. Here is an example of how
you would use it:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.utils.response</span> <span class="k">import</span> <span class="n">open_in_browser</span>

<span class="k">def</span> <span class="nf">parse_details</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="k">if</span> <span class="s2">&quot;item name&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">body</span><span class="p">:</span>
        <span class="n">open_in_browser</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">open_in_browser</span></code> will open a browser with the response received by Scrapy at
that point, adjusting the <a class="reference external" href="https://www.w3schools.com/tags/tag_base.asp">base tag</a> so that images and styles are displayed
properly.</p>
</div>
<div class="section" id="logging">
<h4>Logging<a class="headerlink" href="#logging" title="Permalink to this headline">¶</a></h4>
<p>Logging is another useful option for getting information about your spider run.
Although not as convenient, it comes with the advantage that the logs will be
available in all future runs should they be necessary again:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">parse_details</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">item</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">item</span><span class="p">:</span>
        <span class="c1"># populate more `item` fields</span>
        <span class="k">return</span> <span class="n">item</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s1">&#39;No item received for </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>
</pre></div>
</div>
<p>For more information, check the <a class="reference internal" href="index.html#topics-logging"><span class="std std-ref">Logging</span></a> section.</p>
</div>
</div>
<span id="document-topics/contracts"></span><div class="section" id="spiders-contracts">
<span id="topics-contracts"></span><h3>Spiders Contracts<a class="headerlink" href="#spiders-contracts" title="Permalink to this headline">¶</a></h3>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.15.</span></p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This is a new feature (introduced in Scrapy 0.15) and may be subject
to minor functionality/API updates. Check the <a class="reference internal" href="index.html#news"><span class="std std-ref">release notes</span></a> to
be notified of updates.</p>
</div>
<p>Testing spiders can get particularly annoying and while nothing prevents you
from writing unit tests the task gets cumbersome quickly. Scrapy offers an
integrated way of testing your spiders by the means of contracts.</p>
<p>This allows you to test each callback of your spider by hardcoding a sample url
and check various constraints for how the callback processes the response. Each
contract is prefixed with an <code class="docutils literal notranslate"><span class="pre">&#64;</span></code> and included in the docstring. See the
following example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; This function parses a sample response. Some contracts are mingled</span>
<span class="sd">    with this docstring.</span>

<span class="sd">    @url http://www.amazon.com/s?field-keywords=selfish+gene</span>
<span class="sd">    @returns items 1 16</span>
<span class="sd">    @returns requests 0 0</span>
<span class="sd">    @scrapes Title Author Year Price</span>
<span class="sd">    &quot;&quot;&quot;</span>
</pre></div>
</div>
<p>This callback is tested using three built-in contracts:</p>
<span class="target" id="module-scrapy.contracts.default"></span><dl class="class">
<dt id="scrapy.contracts.default.UrlContract">
<em class="property">class </em><code class="descclassname">scrapy.contracts.default.</code><code class="descname">UrlContract</code><a class="headerlink" href="#scrapy.contracts.default.UrlContract" title="Permalink to this definition">¶</a></dt>
<dd><p>This contract (<code class="docutils literal notranslate"><span class="pre">&#64;url</span></code>) sets the sample url used when checking other
contract conditions for this spider. This contract is mandatory. All
callbacks lacking this contract are ignored when running the checks:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@url</span> <span class="n">url</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="scrapy.contracts.default.ReturnsContract">
<em class="property">class </em><code class="descclassname">scrapy.contracts.default.</code><code class="descname">ReturnsContract</code><a class="headerlink" href="#scrapy.contracts.default.ReturnsContract" title="Permalink to this definition">¶</a></dt>
<dd><p>This contract (<code class="docutils literal notranslate"><span class="pre">&#64;returns</span></code>) sets lower and upper bounds for the items and
requests returned by the spider. The upper bound is optional:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@returns</span> <span class="n">item</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="o">|</span><span class="n">request</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="p">[</span><span class="nb">min</span> <span class="p">[</span><span class="nb">max</span><span class="p">]]</span>
</pre></div>
</div>
</dd></dl>

<dl class="class">
<dt id="scrapy.contracts.default.ScrapesContract">
<em class="property">class </em><code class="descclassname">scrapy.contracts.default.</code><code class="descname">ScrapesContract</code><a class="headerlink" href="#scrapy.contracts.default.ScrapesContract" title="Permalink to this definition">¶</a></dt>
<dd><p>This contract (<code class="docutils literal notranslate"><span class="pre">&#64;scrapes</span></code>) checks that all the items returned by the
callback have the specified fields:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@scrapes</span> <span class="n">field_1</span> <span class="n">field_2</span> <span class="o">...</span>
</pre></div>
</div>
</dd></dl>

<p>Use the <a class="reference internal" href="index.html#std:command-check"><code class="xref std std-command docutils literal notranslate"><span class="pre">check</span></code></a> command to run the contract checks.</p>
<div class="section" id="custom-contracts">
<h4>Custom Contracts<a class="headerlink" href="#custom-contracts" title="Permalink to this headline">¶</a></h4>
<p>If you find you need more power than the built-in scrapy contracts you can
create and load your own contracts in the project by using the
<a class="reference internal" href="index.html#std:setting-SPIDER_CONTRACTS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_CONTRACTS</span></code></a> setting:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">SPIDER_CONTRACTS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;myproject.contracts.ResponseCheck&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
    <span class="s1">&#39;myproject.contracts.ItemValidate&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Each contract must inherit from <a class="reference internal" href="#scrapy.contracts.Contract" title="scrapy.contracts.Contract"><code class="xref py py-class docutils literal notranslate"><span class="pre">Contract</span></code></a> and can
override three methods:</p>
<span class="target" id="module-scrapy.contracts"></span><dl class="class">
<dt id="scrapy.contracts.Contract">
<em class="property">class </em><code class="descclassname">scrapy.contracts.</code><code class="descname">Contract</code><span class="sig-paren">(</span><em>method</em>, <em>*args</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.contracts.Contract" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>method</strong> (<em>function</em>) – callback function to which the contract is associated</li>
<li><strong>args</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a>) – list of arguments passed into the docstring (whitespace
separated)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="scrapy.contracts.Contract.adjust_request_args">
<code class="descname">adjust_request_args</code><span class="sig-paren">(</span><em>args</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.contracts.Contract.adjust_request_args" title="Permalink to this definition">¶</a></dt>
<dd><p>This receives a <code class="docutils literal notranslate"><span class="pre">dict</span></code> as an argument containing default arguments
for request object. <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> is used by default,
but this can be changed with the <code class="docutils literal notranslate"><span class="pre">request_cls</span></code> attribute.
If multiple contracts in chain have this attribute defined, the last one is used.</p>
<p>Must return the same or a modified version of it.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.contracts.Contract.pre_process">
<code class="descname">pre_process</code><span class="sig-paren">(</span><em>response</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.contracts.Contract.pre_process" title="Permalink to this definition">¶</a></dt>
<dd><p>This allows hooking in various checks on the response received from the
sample request, before it’s being passed to the callback.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.contracts.Contract.post_process">
<code class="descname">post_process</code><span class="sig-paren">(</span><em>output</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.contracts.Contract.post_process" title="Permalink to this definition">¶</a></dt>
<dd><p>This allows processing the output of the callback. Iterators are
converted listified before being passed to this hook.</p>
</dd></dl>

</dd></dl>

<p>Raise <a class="reference internal" href="#scrapy.exceptions.ContractFail" title="scrapy.exceptions.ContractFail"><code class="xref py py-class docutils literal notranslate"><span class="pre">ContractFail</span></code></a> from
<a class="reference internal" href="#scrapy.contracts.Contract.pre_process" title="scrapy.contracts.Contract.pre_process"><code class="xref py py-class docutils literal notranslate"><span class="pre">pre_process</span></code></a> or
<a class="reference internal" href="#scrapy.contracts.Contract.post_process" title="scrapy.contracts.Contract.post_process"><code class="xref py py-class docutils literal notranslate"><span class="pre">post_process</span></code></a> if expectations are not met:</p>
<dl class="class">
<dt id="scrapy.exceptions.ContractFail">
<em class="property">class </em><code class="descclassname">scrapy.exceptions.</code><code class="descname">ContractFail</code><a class="headerlink" href="#scrapy.exceptions.ContractFail" title="Permalink to this definition">¶</a></dt>
<dd><p>Error raised in case of a failing contract</p>
</dd></dl>

<p>Here is a demo contract which checks the presence of a custom header in the
response received:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.contracts</span> <span class="k">import</span> <span class="n">Contract</span>
<span class="kn">from</span> <span class="nn">scrapy.exceptions</span> <span class="k">import</span> <span class="n">ContractFail</span>

<span class="k">class</span> <span class="nc">HasHeaderContract</span><span class="p">(</span><span class="n">Contract</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Demo contract which checks the presence of a custom header</span>
<span class="sd">        @has_header X-CustomHeader</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;has_header&#39;</span>

    <span class="k">def</span> <span class="nf">pre_process</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">header</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">header</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">headers</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">ContractFail</span><span class="p">(</span><span class="s1">&#39;X-CustomHeader not present&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="detecting-check-runs">
<span id="detecting-contract-check-runs"></span><h4>Detecting check runs<a class="headerlink" href="#detecting-check-runs" title="Permalink to this headline">¶</a></h4>
<p>When <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">check</span></code> is running, the <code class="docutils literal notranslate"><span class="pre">SCRAPY_CHECK</span></code> environment variable is
set to the <code class="docutils literal notranslate"><span class="pre">true</span></code> string. You can use <a class="reference external" href="https://docs.python.org/3/library/os.html#os.environ">os.environ</a> to perform any change to
your spiders or your settings when <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">check</span></code> is used:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">ExampleSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;example&#39;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;SCRAPY_CHECK&#39;</span><span class="p">):</span>
            <span class="k">pass</span>  <span class="c1"># Do some scraper adjustments when a check is running</span>
</pre></div>
</div>
</div>
</div>
<span id="document-topics/practices"></span><div class="section" id="common-practices">
<span id="topics-practices"></span><h3>Common Practices<a class="headerlink" href="#common-practices" title="Permalink to this headline">¶</a></h3>
<p>This section documents common practices when using Scrapy. These are things
that cover many topics and don’t often fall into any other specific section.</p>
<div class="section" id="run-scrapy-from-a-script">
<span id="run-from-script"></span><h4>Run Scrapy from a script<a class="headerlink" href="#run-scrapy-from-a-script" title="Permalink to this headline">¶</a></h4>
<p>You can use the <a class="reference internal" href="index.html#topics-api"><span class="std std-ref">API</span></a> to run Scrapy from a script, instead of
the typical way of running Scrapy via <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">crawl</span></code>.</p>
<p>Remember that Scrapy is built on top of the Twisted
asynchronous networking library, so you need to run it inside the Twisted reactor.</p>
<p>The first utility you can use to run your spiders is
<a class="reference internal" href="index.html#scrapy.crawler.CrawlerProcess" title="scrapy.crawler.CrawlerProcess"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.crawler.CrawlerProcess</span></code></a>. This class will start a Twisted reactor
for you, configuring the logging and setting shutdown handlers. This class is
the one used by all Scrapy commands.</p>
<p>Here’s an example showing how to run a single spider with it.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">from</span> <span class="nn">scrapy.crawler</span> <span class="k">import</span> <span class="n">CrawlerProcess</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="c1"># Your spider definition</span>
    <span class="o">...</span>

<span class="n">process</span> <span class="o">=</span> <span class="n">CrawlerProcess</span><span class="p">(</span><span class="n">settings</span><span class="o">=</span><span class="p">{</span>
    <span class="s1">&#39;FEED_FORMAT&#39;</span><span class="p">:</span> <span class="s1">&#39;json&#39;</span><span class="p">,</span>
    <span class="s1">&#39;FEED_URI&#39;</span><span class="p">:</span> <span class="s1">&#39;items.json&#39;</span>
<span class="p">})</span>

<span class="n">process</span><span class="o">.</span><span class="n">crawl</span><span class="p">(</span><span class="n">MySpider</span><span class="p">)</span>
<span class="n">process</span><span class="o">.</span><span class="n">start</span><span class="p">()</span> <span class="c1"># the script will block here until the crawling is finished</span>
</pre></div>
</div>
<p>Define settings within dictionary in CrawlerProcess. Make sure to check <a class="reference internal" href="index.html#scrapy.crawler.CrawlerProcess" title="scrapy.crawler.CrawlerProcess"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerProcess</span></code></a>
documentation to get acquainted with its usage details.</p>
<p>If you are inside a Scrapy project there are some additional helpers you can
use to import those components within the project. You can automatically import
your spiders passing their name to <a class="reference internal" href="index.html#scrapy.crawler.CrawlerProcess" title="scrapy.crawler.CrawlerProcess"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerProcess</span></code></a>, and
use <code class="docutils literal notranslate"><span class="pre">get_project_settings</span></code> to get a <a class="reference internal" href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">Settings</span></code></a>
instance with your project settings.</p>
<p>What follows is a working example of how to do that, using the <a class="reference external" href="https://github.com/scrapinghub/testspiders">testspiders</a>
project as example.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.crawler</span> <span class="k">import</span> <span class="n">CrawlerProcess</span>
<span class="kn">from</span> <span class="nn">scrapy.utils.project</span> <span class="k">import</span> <span class="n">get_project_settings</span>

<span class="n">process</span> <span class="o">=</span> <span class="n">CrawlerProcess</span><span class="p">(</span><span class="n">get_project_settings</span><span class="p">())</span>

<span class="c1"># &#39;followall&#39; is the name of one of the spiders of the project.</span>
<span class="n">process</span><span class="o">.</span><span class="n">crawl</span><span class="p">(</span><span class="s1">&#39;followall&#39;</span><span class="p">,</span> <span class="n">domain</span><span class="o">=</span><span class="s1">&#39;scrapinghub.com&#39;</span><span class="p">)</span>
<span class="n">process</span><span class="o">.</span><span class="n">start</span><span class="p">()</span> <span class="c1"># the script will block here until the crawling is finished</span>
</pre></div>
</div>
<p>There’s another Scrapy utility that provides more control over the crawling
process: <a class="reference internal" href="index.html#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.crawler.CrawlerRunner</span></code></a>. This class is a thin wrapper
that encapsulates some simple helpers to run multiple crawlers, but it won’t
start or interfere with existing reactors in any way.</p>
<p>Using this class the reactor should be explicitly run after scheduling your
spiders. It’s recommended you use <a class="reference internal" href="index.html#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerRunner</span></code></a>
instead of <a class="reference internal" href="index.html#scrapy.crawler.CrawlerProcess" title="scrapy.crawler.CrawlerProcess"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerProcess</span></code></a> if your application is
already using Twisted and you want to run Scrapy in the same reactor.</p>
<p>Note that you will also have to shutdown the Twisted reactor yourself after the
spider is finished. This can be achieved by adding callbacks to the deferred
returned by the <a class="reference internal" href="index.html#scrapy.crawler.CrawlerRunner.crawl" title="scrapy.crawler.CrawlerRunner.crawl"><code class="xref py py-meth docutils literal notranslate"><span class="pre">CrawlerRunner.crawl</span></code></a> method.</p>
<p>Here’s an example of its usage, along with a callback to manually stop the
reactor after <code class="docutils literal notranslate"><span class="pre">MySpider</span></code> has finished running.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">twisted.internet</span> <span class="k">import</span> <span class="n">reactor</span>
<span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">from</span> <span class="nn">scrapy.crawler</span> <span class="k">import</span> <span class="n">CrawlerRunner</span>
<span class="kn">from</span> <span class="nn">scrapy.utils.log</span> <span class="k">import</span> <span class="n">configure_logging</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="c1"># Your spider definition</span>
    <span class="o">...</span>

<span class="n">configure_logging</span><span class="p">({</span><span class="s1">&#39;LOG_FORMAT&#39;</span><span class="p">:</span> <span class="s1">&#39;</span><span class="si">%(levelname)s</span><span class="s1">: </span><span class="si">%(message)s</span><span class="s1">&#39;</span><span class="p">})</span>
<span class="n">runner</span> <span class="o">=</span> <span class="n">CrawlerRunner</span><span class="p">()</span>

<span class="n">d</span> <span class="o">=</span> <span class="n">runner</span><span class="o">.</span><span class="n">crawl</span><span class="p">(</span><span class="n">MySpider</span><span class="p">)</span>
<span class="n">d</span><span class="o">.</span><span class="n">addBoth</span><span class="p">(</span><span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="n">reactor</span><span class="o">.</span><span class="n">stop</span><span class="p">())</span>
<span class="n">reactor</span><span class="o">.</span><span class="n">run</span><span class="p">()</span> <span class="c1"># the script will block here until the crawling is finished</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference external" href="https://twistedmatrix.com/documents/current/core/howto/reactor-basics.html">Twisted Reactor Overview</a>.</p>
</div>
</div>
<div class="section" id="running-multiple-spiders-in-the-same-process">
<span id="run-multiple-spiders"></span><h4>Running multiple spiders in the same process<a class="headerlink" href="#running-multiple-spiders-in-the-same-process" title="Permalink to this headline">¶</a></h4>
<p>By default, Scrapy runs a single spider per process when you run <code class="docutils literal notranslate"><span class="pre">scrapy</span>
<span class="pre">crawl</span></code>. However, Scrapy supports running multiple spiders per process using
the <a class="reference internal" href="index.html#topics-api"><span class="std std-ref">internal API</span></a>.</p>
<p>Here is an example that runs multiple spiders simultaneously:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">from</span> <span class="nn">scrapy.crawler</span> <span class="k">import</span> <span class="n">CrawlerProcess</span>

<span class="k">class</span> <span class="nc">MySpider1</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="c1"># Your first spider definition</span>
    <span class="o">...</span>

<span class="k">class</span> <span class="nc">MySpider2</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="c1"># Your second spider definition</span>
    <span class="o">...</span>

<span class="n">process</span> <span class="o">=</span> <span class="n">CrawlerProcess</span><span class="p">()</span>
<span class="n">process</span><span class="o">.</span><span class="n">crawl</span><span class="p">(</span><span class="n">MySpider1</span><span class="p">)</span>
<span class="n">process</span><span class="o">.</span><span class="n">crawl</span><span class="p">(</span><span class="n">MySpider2</span><span class="p">)</span>
<span class="n">process</span><span class="o">.</span><span class="n">start</span><span class="p">()</span> <span class="c1"># the script will block here until all crawling jobs are finished</span>
</pre></div>
</div>
<p>Same example using <a class="reference internal" href="index.html#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerRunner</span></code></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">from</span> <span class="nn">twisted.internet</span> <span class="k">import</span> <span class="n">reactor</span>
<span class="kn">from</span> <span class="nn">scrapy.crawler</span> <span class="k">import</span> <span class="n">CrawlerRunner</span>
<span class="kn">from</span> <span class="nn">scrapy.utils.log</span> <span class="k">import</span> <span class="n">configure_logging</span>

<span class="k">class</span> <span class="nc">MySpider1</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="c1"># Your first spider definition</span>
    <span class="o">...</span>

<span class="k">class</span> <span class="nc">MySpider2</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="c1"># Your second spider definition</span>
    <span class="o">...</span>

<span class="n">configure_logging</span><span class="p">()</span>
<span class="n">runner</span> <span class="o">=</span> <span class="n">CrawlerRunner</span><span class="p">()</span>
<span class="n">runner</span><span class="o">.</span><span class="n">crawl</span><span class="p">(</span><span class="n">MySpider1</span><span class="p">)</span>
<span class="n">runner</span><span class="o">.</span><span class="n">crawl</span><span class="p">(</span><span class="n">MySpider2</span><span class="p">)</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">runner</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
<span class="n">d</span><span class="o">.</span><span class="n">addBoth</span><span class="p">(</span><span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="n">reactor</span><span class="o">.</span><span class="n">stop</span><span class="p">())</span>

<span class="n">reactor</span><span class="o">.</span><span class="n">run</span><span class="p">()</span> <span class="c1"># the script will block here until all crawling jobs are finished</span>
</pre></div>
</div>
<p>Same example but running the spiders sequentially by chaining the deferreds:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">twisted.internet</span> <span class="k">import</span> <span class="n">reactor</span><span class="p">,</span> <span class="n">defer</span>
<span class="kn">from</span> <span class="nn">scrapy.crawler</span> <span class="k">import</span> <span class="n">CrawlerRunner</span>
<span class="kn">from</span> <span class="nn">scrapy.utils.log</span> <span class="k">import</span> <span class="n">configure_logging</span>

<span class="k">class</span> <span class="nc">MySpider1</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="c1"># Your first spider definition</span>
    <span class="o">...</span>

<span class="k">class</span> <span class="nc">MySpider2</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="c1"># Your second spider definition</span>
    <span class="o">...</span>

<span class="n">configure_logging</span><span class="p">()</span>
<span class="n">runner</span> <span class="o">=</span> <span class="n">CrawlerRunner</span><span class="p">()</span>

<span class="nd">@defer</span><span class="o">.</span><span class="n">inlineCallbacks</span>
<span class="k">def</span> <span class="nf">crawl</span><span class="p">():</span>
    <span class="k">yield</span> <span class="n">runner</span><span class="o">.</span><span class="n">crawl</span><span class="p">(</span><span class="n">MySpider1</span><span class="p">)</span>
    <span class="k">yield</span> <span class="n">runner</span><span class="o">.</span><span class="n">crawl</span><span class="p">(</span><span class="n">MySpider2</span><span class="p">)</span>
    <span class="n">reactor</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>

<span class="n">crawl</span><span class="p">()</span>
<span class="n">reactor</span><span class="o">.</span><span class="n">run</span><span class="p">()</span> <span class="c1"># the script will block here until the last crawl call is finished</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference internal" href="#run-from-script"><span class="std std-ref">Run Scrapy from a script</span></a>.</p>
</div>
</div>
<div class="section" id="distributed-crawls">
<span id="id1"></span><h4>Distributed crawls<a class="headerlink" href="#distributed-crawls" title="Permalink to this headline">¶</a></h4>
<p>Scrapy doesn’t provide any built-in facility for running crawls in a distribute
(multi-server) manner. However, there are some ways to distribute crawls, which
vary depending on how you plan to distribute them.</p>
<p>If you have many spiders, the obvious way to distribute the load is to setup
many Scrapyd instances and distribute spider runs among those.</p>
<p>If you instead want to run a single (big) spider through many machines, what
you usually do is partition the urls to crawl and send them to each separate
spider. Here is a concrete example:</p>
<p>First, you prepare the list of urls to crawl and put them into separate
files/urls:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">somedomain</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">urls</span><span class="o">-</span><span class="n">to</span><span class="o">-</span><span class="n">crawl</span><span class="o">/</span><span class="n">spider1</span><span class="o">/</span><span class="n">part1</span><span class="o">.</span><span class="n">list</span>
<span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">somedomain</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">urls</span><span class="o">-</span><span class="n">to</span><span class="o">-</span><span class="n">crawl</span><span class="o">/</span><span class="n">spider1</span><span class="o">/</span><span class="n">part2</span><span class="o">.</span><span class="n">list</span>
<span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">somedomain</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">urls</span><span class="o">-</span><span class="n">to</span><span class="o">-</span><span class="n">crawl</span><span class="o">/</span><span class="n">spider1</span><span class="o">/</span><span class="n">part3</span><span class="o">.</span><span class="n">list</span>
</pre></div>
</div>
<p>Then you fire a spider run on 3 different Scrapyd servers. The spider would
receive a (spider) argument <code class="docutils literal notranslate"><span class="pre">part</span></code> with the number of the partition to
crawl:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">curl</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">scrapy1</span><span class="o">.</span><span class="n">mycompany</span><span class="o">.</span><span class="n">com</span><span class="p">:</span><span class="mi">6800</span><span class="o">/</span><span class="n">schedule</span><span class="o">.</span><span class="n">json</span> <span class="o">-</span><span class="n">d</span> <span class="n">project</span><span class="o">=</span><span class="n">myproject</span> <span class="o">-</span><span class="n">d</span> <span class="n">spider</span><span class="o">=</span><span class="n">spider1</span> <span class="o">-</span><span class="n">d</span> <span class="n">part</span><span class="o">=</span><span class="mi">1</span>
<span class="n">curl</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">scrapy2</span><span class="o">.</span><span class="n">mycompany</span><span class="o">.</span><span class="n">com</span><span class="p">:</span><span class="mi">6800</span><span class="o">/</span><span class="n">schedule</span><span class="o">.</span><span class="n">json</span> <span class="o">-</span><span class="n">d</span> <span class="n">project</span><span class="o">=</span><span class="n">myproject</span> <span class="o">-</span><span class="n">d</span> <span class="n">spider</span><span class="o">=</span><span class="n">spider1</span> <span class="o">-</span><span class="n">d</span> <span class="n">part</span><span class="o">=</span><span class="mi">2</span>
<span class="n">curl</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">scrapy3</span><span class="o">.</span><span class="n">mycompany</span><span class="o">.</span><span class="n">com</span><span class="p">:</span><span class="mi">6800</span><span class="o">/</span><span class="n">schedule</span><span class="o">.</span><span class="n">json</span> <span class="o">-</span><span class="n">d</span> <span class="n">project</span><span class="o">=</span><span class="n">myproject</span> <span class="o">-</span><span class="n">d</span> <span class="n">spider</span><span class="o">=</span><span class="n">spider1</span> <span class="o">-</span><span class="n">d</span> <span class="n">part</span><span class="o">=</span><span class="mi">3</span>
</pre></div>
</div>
</div>
<div class="section" id="avoiding-getting-banned">
<span id="bans"></span><h4>Avoiding getting banned<a class="headerlink" href="#avoiding-getting-banned" title="Permalink to this headline">¶</a></h4>
<p>Some websites implement certain measures to prevent bots from crawling them,
with varying degrees of sophistication. Getting around those measures can be
difficult and tricky, and may sometimes require special infrastructure. Please
consider contacting <a class="reference external" href="https://scrapy.org/support/">commercial support</a> if in doubt.</p>
<p>Here are some tips to keep in mind when dealing with these kinds of sites:</p>
<ul class="simple">
<li>rotate your user agent from a pool of well-known ones from browsers (google
around to get a list of them)</li>
<li>disable cookies (see <a class="reference internal" href="index.html#std:setting-COOKIES_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">COOKIES_ENABLED</span></code></a>) as some sites may use
cookies to spot bot behaviour</li>
<li>use download delays (2 or higher). See <a class="reference internal" href="index.html#std:setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_DELAY</span></code></a> setting.</li>
<li>if possible, use <a class="reference external" href="http://www.googleguide.com/cached_pages.html">Google cache</a> to fetch pages, instead of hitting the sites
directly</li>
<li>use a pool of rotating IPs. For example, the free <a class="reference external" href="https://www.torproject.org/">Tor project</a> or paid
services like <a class="reference external" href="https://proxymesh.com/">ProxyMesh</a>. An open source alternative is <a class="reference external" href="https://scrapoxy.io/">scrapoxy</a>, a
super proxy that you can attach your own proxies to.</li>
<li>use a highly distributed downloader that circumvents bans internally, so you
can just focus on parsing clean pages. One example of such downloaders is
<a class="reference external" href="https://scrapinghub.com/crawlera">Crawlera</a></li>
</ul>
<p>If you are still unable to prevent your bot getting banned, consider contacting
<a class="reference external" href="https://scrapy.org/support/">commercial support</a>.</p>
</div>
</div>
<span id="document-topics/broad-crawls"></span><div class="section" id="broad-crawls">
<span id="topics-broad-crawls"></span><h3>Broad Crawls<a class="headerlink" href="#broad-crawls" title="Permalink to this headline">¶</a></h3>
<p>Scrapy defaults are optimized for crawling specific sites. These sites are
often handled by a single Scrapy spider, although this is not necessary or
required (for example, there are generic spiders that handle any given site
thrown at them).</p>
<p>In addition to this “focused crawl”, there is another common type of crawling
which covers a large (potentially unlimited) number of domains, and is only
limited by time or other arbitrary constraint, rather than stopping when the
domain was crawled to completion or when there are no more requests to perform.
These are called “broad crawls” and is the typical crawlers employed by search
engines.</p>
<p>These are some common properties often found in broad crawls:</p>
<ul class="simple">
<li>they crawl many domains (often, unbounded) instead of a specific set of sites</li>
<li>they don’t necessarily crawl domains to completion, because it would be
impractical (or impossible) to do so, and instead limit the crawl by time or
number of pages crawled</li>
<li>they are simpler in logic (as opposed to very complex spiders with many
extraction rules) because data is often post-processed in a separate stage</li>
<li>they crawl many domains concurrently, which allows them to achieve faster
crawl speeds by not being limited by any particular site constraint (each site
is crawled slowly to respect politeness, but many sites are crawled in
parallel)</li>
</ul>
<p>As said above, Scrapy default settings are optimized for focused crawls, not
broad crawls. However, due to its asynchronous architecture, Scrapy is very
well suited for performing fast broad crawls. This page summarizes some things
you need to keep in mind when using Scrapy for doing broad crawls, along with
concrete suggestions of Scrapy settings to tune in order to achieve an
efficient broad crawl.</p>
<div class="section" id="use-the-right-scheduler-priority-queue">
<span id="broad-crawls-scheduler-priority-queue"></span><h4>Use the right <a class="reference internal" href="index.html#std:setting-SCHEDULER_PRIORITY_QUEUE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SCHEDULER_PRIORITY_QUEUE</span></code></a><a class="headerlink" href="#use-the-right-scheduler-priority-queue" title="Permalink to this headline">¶</a></h4>
<p>Scrapy’s default scheduler priority queue is <code class="docutils literal notranslate"><span class="pre">'scrapy.pqueues.ScrapyPriorityQueue'</span></code>.
It works best during single-domain crawl. It does not work well with crawling
many different domains in parallel</p>
<p>To apply the recommended priority queue use:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">SCHEDULER_PRIORITY_QUEUE</span> <span class="o">=</span> <span class="s1">&#39;scrapy.pqueues.DownloaderAwarePriorityQueue&#39;</span>
</pre></div>
</div>
</div>
<div class="section" id="increase-concurrency">
<span id="broad-crawls-concurrency"></span><h4>Increase concurrency<a class="headerlink" href="#increase-concurrency" title="Permalink to this headline">¶</a></h4>
<p>Concurrency is the number of requests that are processed in parallel. There is
a global limit (<a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS</span></code></a>) and an additional limit that
can be set either per domain (<a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a>) or per
IP (<a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a>).</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The scheduler priority queue <a class="reference internal" href="#broad-crawls-scheduler-priority-queue"><span class="std std-ref">recommended for broad crawls</span></a> does not support
<a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a>.</p>
</div>
<p>The default global concurrency limit in Scrapy is not suitable for crawling
many different domains in parallel, so you will want to increase it. How much
to increase it will depend on how much CPU and memory you crawler will have
available.</p>
<p>A good starting point is <code class="docutils literal notranslate"><span class="pre">100</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">CONCURRENT_REQUESTS</span> <span class="o">=</span> <span class="mi">100</span>
</pre></div>
</div>
<p>But the best way to find out is by doing some trials and identifying at what
concurrency your Scrapy process gets CPU bounded. For optimum performance, you
should pick a concurrency where CPU usage is at 80-90%.</p>
<p>Increasing concurrency also increases memory usage. If memory usage is a
concern, you might need to lower your global concurrency limit accordingly.</p>
</div>
<div class="section" id="increase-twisted-io-thread-pool-maximum-size">
<h4>Increase Twisted IO thread pool maximum size<a class="headerlink" href="#increase-twisted-io-thread-pool-maximum-size" title="Permalink to this headline">¶</a></h4>
<p>Currently Scrapy does DNS resolution in a blocking way with usage of thread
pool. With higher concurrency levels the crawling could be slow or even fail
hitting DNS resolver timeouts. Possible solution to increase the number of
threads handling DNS queries. The DNS queue will be processed faster speeding
up establishing of connection and crawling overall.</p>
<p>To increase maximum thread pool size use:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">REACTOR_THREADPOOL_MAXSIZE</span> <span class="o">=</span> <span class="mi">20</span>
</pre></div>
</div>
</div>
<div class="section" id="setup-your-own-dns">
<h4>Setup your own DNS<a class="headerlink" href="#setup-your-own-dns" title="Permalink to this headline">¶</a></h4>
<p>If you have multiple crawling processes and single central DNS, it can act
like DoS attack on the DNS server resulting to slow down of entire network or
even blocking your machines. To avoid this setup your own DNS server with
local cache and upstream to some large DNS like OpenDNS or Verizon.</p>
</div>
<div class="section" id="reduce-log-level">
<h4>Reduce log level<a class="headerlink" href="#reduce-log-level" title="Permalink to this headline">¶</a></h4>
<p>When doing broad crawls you are often only interested in the crawl rates you
get and any errors found. These stats are reported by Scrapy when using the
<code class="docutils literal notranslate"><span class="pre">INFO</span></code> log level. In order to save CPU (and log storage requirements) you
should not use <code class="docutils literal notranslate"><span class="pre">DEBUG</span></code> log level when preforming large broad crawls in
production. Using <code class="docutils literal notranslate"><span class="pre">DEBUG</span></code> level when developing your (broad) crawler may be
fine though.</p>
<p>To set the log level use:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">LOG_LEVEL</span> <span class="o">=</span> <span class="s1">&#39;INFO&#39;</span>
</pre></div>
</div>
</div>
<div class="section" id="disable-cookies">
<h4>Disable cookies<a class="headerlink" href="#disable-cookies" title="Permalink to this headline">¶</a></h4>
<p>Disable cookies unless you <em>really</em> need. Cookies are often not needed when
doing broad crawls (search engine crawlers ignore them), and they improve
performance by saving some CPU cycles and reducing the memory footprint of your
Scrapy crawler.</p>
<p>To disable cookies use:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">COOKIES_ENABLED</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
</div>
<div class="section" id="disable-retries">
<h4>Disable retries<a class="headerlink" href="#disable-retries" title="Permalink to this headline">¶</a></h4>
<p>Retrying failed HTTP requests can slow down the crawls substantially, specially
when sites causes are very slow (or fail) to respond, thus causing a timeout
error which gets retried many times, unnecessarily, preventing crawler capacity
to be reused for other domains.</p>
<p>To disable retries use:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">RETRY_ENABLED</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
</div>
<div class="section" id="reduce-download-timeout">
<h4>Reduce download timeout<a class="headerlink" href="#reduce-download-timeout" title="Permalink to this headline">¶</a></h4>
<p>Unless you are crawling from a very slow connection (which shouldn’t be the
case for broad crawls) reduce the download timeout so that stuck requests are
discarded quickly and free up capacity to process the next ones.</p>
<p>To reduce the download timeout use:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">DOWNLOAD_TIMEOUT</span> <span class="o">=</span> <span class="mi">15</span>
</pre></div>
</div>
</div>
<div class="section" id="disable-redirects">
<h4>Disable redirects<a class="headerlink" href="#disable-redirects" title="Permalink to this headline">¶</a></h4>
<p>Consider disabling redirects, unless you are interested in following them. When
doing broad crawls it’s common to save redirects and resolve them when
revisiting the site at a later crawl. This also help to keep the number of
request constant per crawl batch, otherwise redirect loops may cause the
crawler to dedicate too many resources on any specific domain.</p>
<p>To disable redirects use:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">REDIRECT_ENABLED</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
</div>
<div class="section" id="enable-crawling-of-ajax-crawlable-pages">
<h4>Enable crawling of “Ajax Crawlable Pages”<a class="headerlink" href="#enable-crawling-of-ajax-crawlable-pages" title="Permalink to this headline">¶</a></h4>
<p>Some pages (up to 1%, based on empirical data from year 2013) declare
themselves as <a class="reference external" href="https://developers.google.com/webmasters/ajax-crawling/docs/getting-started">ajax crawlable</a>. This means they provide plain HTML
version of content that is usually available only via AJAX.
Pages can indicate it in two ways:</p>
<ol class="arabic simple">
<li>by using <code class="docutils literal notranslate"><span class="pre">#!</span></code> in URL - this is the default way;</li>
<li>by using a special meta tag - this way is used on
“main”, “index” website pages.</li>
</ol>
<p>Scrapy handles (1) automatically; to handle (2) enable
<a class="reference internal" href="index.html#ajaxcrawl-middleware"><span class="std std-ref">AjaxCrawlMiddleware</span></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">AJAXCRAWL_ENABLED</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
<p>When doing broad crawls it’s common to crawl a lot of “index” web pages;
AjaxCrawlMiddleware helps to crawl them correctly.
It is turned OFF by default because it has some performance overhead,
and enabling it for focused crawls doesn’t make much sense.</p>
</div>
<div class="section" id="crawl-in-bfo-order">
<span id="broad-crawls-bfo"></span><h4>Crawl in BFO order<a class="headerlink" href="#crawl-in-bfo-order" title="Permalink to this headline">¶</a></h4>
<p><a class="reference internal" href="index.html#faq-bfo-dfo"><span class="std std-ref">Scrapy crawls in DFO order by default</span></a>.</p>
<p>In broad crawls, however, page crawling tends to be faster than page
processing. As a result, unprocessed early requests stay in memory until the
final depth is reached, which can significantly increase memory usage.</p>
<p><a class="reference internal" href="index.html#faq-bfo-dfo"><span class="std std-ref">Crawl in BFO order</span></a> instead to save memory.</p>
</div>
<div class="section" id="be-mindful-of-memory-leaks">
<h4>Be mindful of memory leaks<a class="headerlink" href="#be-mindful-of-memory-leaks" title="Permalink to this headline">¶</a></h4>
<p>If your broad crawl shows a high memory usage, in addition to <a class="reference internal" href="#broad-crawls-bfo"><span class="std std-ref">crawling in
BFO order</span></a> and <a class="reference internal" href="#broad-crawls-concurrency"><span class="std std-ref">lowering concurrency</span></a> you should <a class="reference internal" href="index.html#topics-leaks"><span class="std std-ref">debug your memory leaks</span></a>.</p>
</div>
</div>
<span id="document-topics/developer-tools"></span><div class="section" id="using-your-browser-s-developer-tools-for-scraping">
<span id="topics-developer-tools"></span><h3>Using your browser’s Developer Tools for scraping<a class="headerlink" href="#using-your-browser-s-developer-tools-for-scraping" title="Permalink to this headline">¶</a></h3>
<p>Here is a general guide on how to use your browser’s Developer Tools
to ease the scraping process. Today almost all browsers come with
built in <a class="reference external" href="https://en.wikipedia.org/wiki/Web_development_tools">Developer Tools</a> and although we will use Firefox in this
guide, the concepts are applicable to any other browser.</p>
<p>In this guide we’ll introduce the basic tools to use from a browser’s
Developer Tools by scraping <a class="reference external" href="http://quotes.toscrape.com">quotes.toscrape.com</a>.</p>
<div class="section" id="caveats-with-inspecting-the-live-browser-dom">
<span id="topics-livedom"></span><h4>Caveats with inspecting the live browser DOM<a class="headerlink" href="#caveats-with-inspecting-the-live-browser-dom" title="Permalink to this headline">¶</a></h4>
<p>Since Developer Tools operate on a live browser DOM, what you’ll actually see
when inspecting the page source is not the original HTML, but a modified one
after applying some browser clean up and executing Javascript code.  Firefox,
in particular, is known for adding <code class="docutils literal notranslate"><span class="pre">&lt;tbody&gt;</span></code> elements to tables.  Scrapy, on
the other hand, does not modify the original page HTML, so you won’t be able to
extract any data if you use <code class="docutils literal notranslate"><span class="pre">&lt;tbody&gt;</span></code> in your XPath expressions.</p>
<p>Therefore, you should keep in mind the following things:</p>
<ul class="simple">
<li>Disable Javascript while inspecting the DOM looking for XPaths to be
used in Scrapy (in the Developer Tools settings click <cite>Disable JavaScript</cite>)</li>
<li>Never use full XPath paths, use relative and clever ones based on attributes
(such as <code class="docutils literal notranslate"><span class="pre">id</span></code>, <code class="docutils literal notranslate"><span class="pre">class</span></code>, <code class="docutils literal notranslate"><span class="pre">width</span></code>, etc) or any identifying features like
<code class="docutils literal notranslate"><span class="pre">contains(&#64;href,</span> <span class="pre">'image')</span></code>.</li>
<li>Never include <code class="docutils literal notranslate"><span class="pre">&lt;tbody&gt;</span></code> elements in your XPath expressions unless you
really know what you’re doing</li>
</ul>
</div>
<div class="section" id="inspecting-a-website">
<span id="topics-inspector"></span><h4>Inspecting a website<a class="headerlink" href="#inspecting-a-website" title="Permalink to this headline">¶</a></h4>
<p>By far the most handy feature of the Developer Tools is the <cite>Inspector</cite>
feature, which allows you to inspect the underlying HTML code of
any webpage. To demonstrate the Inspector, let’s look at the
<a class="reference external" href="http://quotes.toscrape.com">quotes.toscrape.com</a>-site.</p>
<p>On the site we have a total of ten quotes from various authors with specific
tags, as well as the Top Ten Tags. Let’s say we want to extract all the quotes
on this page, without any meta-information about authors, tags, etc.</p>
<p>Instead of viewing the whole source code for the page, we can simply right click
on a quote and select <code class="docutils literal notranslate"><span class="pre">Inspect</span> <span class="pre">Element</span> <span class="pre">(Q)</span></code>, which opens up the <cite>Inspector</cite>.
In it you should see something like this:</p>
<a class="reference internal image-reference" href="_images/inspector_01.png"><img alt="Firefox's Inspector-tool" src="_images/inspector_01.png" style="width: 777px; height: 469px;" /></a>
<p>The interesting part for us is this:</p>
<div class="highlight-html notranslate"><div class="highlight"><pre><span></span><span class="p">&lt;</span><span class="nt">div</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;quote&quot;</span> <span class="na">itemscope</span><span class="o">=</span><span class="s">&quot;&quot;</span> <span class="na">itemtype</span><span class="o">=</span><span class="s">&quot;http://schema.org/CreativeWork&quot;</span><span class="p">&gt;</span>
  <span class="p">&lt;</span><span class="nt">span</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;text&quot;</span> <span class="na">itemprop</span><span class="o">=</span><span class="s">&quot;text&quot;</span><span class="p">&gt;</span>(...)<span class="p">&lt;/</span><span class="nt">span</span><span class="p">&gt;</span>
  <span class="p">&lt;</span><span class="nt">span</span><span class="p">&gt;</span>(...)<span class="p">&lt;/</span><span class="nt">span</span><span class="p">&gt;</span>
  <span class="p">&lt;</span><span class="nt">div</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;tags&quot;</span><span class="p">&gt;</span>(...)<span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
<span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
</pre></div>
</div>
<p>If you hover over the first <code class="docutils literal notranslate"><span class="pre">div</span></code> directly above the <code class="docutils literal notranslate"><span class="pre">span</span></code> tag highlighted
in the screenshot, you’ll see that the corresponding section of the webpage gets
highlighted as well. So now we have a section, but we can’t find our quote text
anywhere.</p>
<p>The advantage of the <cite>Inspector</cite> is that it automatically expands and collapses
sections and tags of a webpage, which greatly improves readability. You can
expand and collapse a tag by clicking on the arrow in front of it or by double
clicking directly on the tag. If we expand the <code class="docutils literal notranslate"><span class="pre">span</span></code> tag with the <code class="docutils literal notranslate"><span class="pre">class=</span>
<span class="pre">&quot;text&quot;</span></code> we will see the quote-text we clicked on. The <cite>Inspector</cite> lets you
copy XPaths to selected elements. Let’s try it out: Right-click on the <code class="docutils literal notranslate"><span class="pre">span</span></code>
tag, select <code class="docutils literal notranslate"><span class="pre">Copy</span> <span class="pre">&gt;</span> <span class="pre">XPath</span></code> and paste it in the scrapy shell like so:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ scrapy shell &quot;http://quotes.toscrape.com/&quot;
(...)
&gt;&gt;&gt; response.xpath(&#39;/html/body/div/div[2]/div[1]/div[1]/span[1]/text()&#39;).getall()
[&#39;&quot;The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”]
</pre></div>
</div>
<p>Adding <code class="docutils literal notranslate"><span class="pre">text()</span></code> at the end we are able to extract the first quote with this
basic selector. But this XPath is not really that clever. All it does is
go down a desired path in the source code starting from <code class="docutils literal notranslate"><span class="pre">html</span></code>. So let’s
see if we can refine our XPath a bit:</p>
<p>If we check the <cite>Inspector</cite> again we’ll see that directly beneath our
expanded <code class="docutils literal notranslate"><span class="pre">div</span></code> tag we have nine identical <code class="docutils literal notranslate"><span class="pre">div</span></code> tags, each with the
same attributes as our first. If we expand any of them, we’ll see the same
structure as with our first quote: Two <code class="docutils literal notranslate"><span class="pre">span</span></code> tags and one <code class="docutils literal notranslate"><span class="pre">div</span></code> tag. We can
expand each <code class="docutils literal notranslate"><span class="pre">span</span></code> tag with the <code class="docutils literal notranslate"><span class="pre">class=&quot;text&quot;</span></code> inside our <code class="docutils literal notranslate"><span class="pre">div</span></code> tags and
see each quote:</p>
<div class="highlight-html notranslate"><div class="highlight"><pre><span></span><span class="p">&lt;</span><span class="nt">div</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;quote&quot;</span> <span class="na">itemscope</span><span class="o">=</span><span class="s">&quot;&quot;</span> <span class="na">itemtype</span><span class="o">=</span><span class="s">&quot;http://schema.org/CreativeWork&quot;</span><span class="p">&gt;</span>
  <span class="p">&lt;</span><span class="nt">span</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;text&quot;</span> <span class="na">itemprop</span><span class="o">=</span><span class="s">&quot;text&quot;</span><span class="p">&gt;</span>
    “The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”
  <span class="p">&lt;/</span><span class="nt">span</span><span class="p">&gt;</span>
  <span class="p">&lt;</span><span class="nt">span</span><span class="p">&gt;</span>(...)<span class="p">&lt;/</span><span class="nt">span</span><span class="p">&gt;</span>
  <span class="p">&lt;</span><span class="nt">div</span> <span class="na">class</span><span class="o">=</span><span class="s">&quot;tags&quot;</span><span class="p">&gt;</span>(...)<span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
<span class="p">&lt;/</span><span class="nt">div</span><span class="p">&gt;</span>
</pre></div>
</div>
<p>With this knowledge we can refine our XPath: Instead of a path to follow,
we’ll simply select all <code class="docutils literal notranslate"><span class="pre">span</span></code> tags with the <code class="docutils literal notranslate"><span class="pre">class=&quot;text&quot;</span></code> by using
the <a class="reference external" href="https://parsel.readthedocs.io/en/latest/usage.html#other-xpath-extensions">has-class-extension</a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span> <span class="o">&gt;&gt;&gt;</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//span[has-class(&quot;text&quot;)]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">getall</span><span class="p">()</span>
<span class="p">[</span><span class="s1">&#39;&quot;The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”,</span>
 <span class="s1">&#39;“It is our choices, Harry, that show what we truly are, far more than our abilities.”&#39;</span><span class="p">,</span>
 <span class="s1">&#39;“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”&#39;</span><span class="p">,</span>
 <span class="p">(</span><span class="o">...</span><span class="p">)]</span>
</pre></div>
</div>
<p>And with one simple, cleverer XPath we are able to extract all quotes from
the page. We could have constructed a loop over our first XPath to increase
the number of the last <code class="docutils literal notranslate"><span class="pre">div</span></code>, but this would have been unnecessarily
complex and by simply constructing an XPath with <code class="docutils literal notranslate"><span class="pre">has-class(&quot;text&quot;)</span></code>
we were able to extract all quotes in one line.</p>
<p>The <cite>Inspector</cite> has a lot of other helpful features, such as searching in the
source code or directly scrolling to an element you selected. Let’s demonstrate
a use case:</p>
<p>Say you want to find the <code class="docutils literal notranslate"><span class="pre">Next</span></code> button on the page. Type <code class="docutils literal notranslate"><span class="pre">Next</span></code> into the
search bar on the top right of the <cite>Inspector</cite>. You should get two results.
The first is a <code class="docutils literal notranslate"><span class="pre">li</span></code> tag with the <code class="docutils literal notranslate"><span class="pre">class=&quot;text&quot;</span></code>, the second the text
of an <code class="docutils literal notranslate"><span class="pre">a</span></code> tag. Right click on the <code class="docutils literal notranslate"><span class="pre">a</span></code> tag and select <code class="docutils literal notranslate"><span class="pre">Scroll</span> <span class="pre">into</span> <span class="pre">View</span></code>.
If you hover over the tag, you’ll see the button highlighted. From here
we could easily create a <a class="reference internal" href="index.html#topics-link-extractors"><span class="std std-ref">Link Extractor</span></a> to
follow the pagination. On a simple site such as this, there may not be
the need to find an element visually but the <code class="docutils literal notranslate"><span class="pre">Scroll</span> <span class="pre">into</span> <span class="pre">View</span></code> function
can be quite useful on complex sites.</p>
<p>Note that the search bar can also be used to search for and test CSS
selectors. For example, you could search for <code class="docutils literal notranslate"><span class="pre">span.text</span></code> to find
all quote texts. Instead of a full text search, this searches for
exactly the <code class="docutils literal notranslate"><span class="pre">span</span></code> tag with the <code class="docutils literal notranslate"><span class="pre">class=&quot;text&quot;</span></code> in the page.</p>
</div>
<div class="section" id="the-network-tool">
<span id="topics-network-tool"></span><h4>The Network-tool<a class="headerlink" href="#the-network-tool" title="Permalink to this headline">¶</a></h4>
<p>While scraping you may come across dynamic webpages where some parts
of the page are loaded dynamically through multiple requests. While
this can be quite tricky, the <cite>Network</cite>-tool in the Developer Tools
greatly facilitates this task. To demonstrate the Network-tool, let’s
take a look at the page <a class="reference external" href="quotes.toscrape.com/scroll/">quotes.toscrape.com/scroll</a>.</p>
<p>The page is quite similar to the basic <a class="reference external" href="http://quotes.toscrape.com">quotes.toscrape.com</a>-page,
but instead of the above-mentioned <code class="docutils literal notranslate"><span class="pre">Next</span></code> button, the page
automatically loads new quotes when you scroll to the bottom. We
could go ahead and try out different XPaths directly, but instead
we’ll check another quite useful command from the scrapy shell:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ scrapy shell &quot;quotes.toscrape.com/scroll&quot;
(...)
&gt;&gt;&gt; view(response)
</pre></div>
</div>
<p>A browser window should open with the webpage but with one
crucial difference: Instead of the quotes we just see a greenish
bar with the word <code class="docutils literal notranslate"><span class="pre">Loading...</span></code>.</p>
<a class="reference internal image-reference" href="_images/network_01.png"><img alt="Response from quotes.toscrape.com/scroll" src="_images/network_01.png" style="width: 777px; height: 296px;" /></a>
<p>The <code class="docutils literal notranslate"><span class="pre">view(response)</span></code> command let’s us view the response our
shell or later our spider receives from the server. Here we see
that some basic template is loaded which includes the title,
the login-button and the footer, but the quotes are missing. This
tells us that the quotes are being loaded from a different request
than <code class="docutils literal notranslate"><span class="pre">quotes.toscrape/scroll</span></code>.</p>
<p>If you click on the <code class="docutils literal notranslate"><span class="pre">Network</span></code> tab, you will probably only see
two entries. The first thing we do is enable persistent logs by
clicking on <code class="docutils literal notranslate"><span class="pre">Persist</span> <span class="pre">Logs</span></code>. If this option is disabled, the
log is automatically cleared each time you navigate to a different
page. Enabling this option is a good default, since it gives us
control on when to clear the logs.</p>
<p>If we reload the page now, you’ll see the log get populated with six
new requests.</p>
<a class="reference internal image-reference" href="_images/network_02.png"><img alt="Network tab with persistent logs and requests" src="_images/network_02.png" style="width: 777px; height: 241px;" /></a>
<p>Here we see every request that has been made when reloading the page
and can inspect each request and its response. So let’s find out
where our quotes are coming from:</p>
<p>First click on the request with the name <code class="docutils literal notranslate"><span class="pre">scroll</span></code>. On the right
you can now inspect the request. In <code class="docutils literal notranslate"><span class="pre">Headers</span></code> you’ll find details
about the request headers, such as the URL, the method, the IP-address,
and so on. We’ll ignore the other tabs and click directly on <code class="docutils literal notranslate"><span class="pre">Reponse</span></code>.</p>
<p>What you should see in the <code class="docutils literal notranslate"><span class="pre">Preview</span></code> pane is the rendered HTML-code,
that is exactly what we saw when we called <code class="docutils literal notranslate"><span class="pre">view(response)</span></code> in the
shell. Accordingly the <code class="docutils literal notranslate"><span class="pre">type</span></code> of the request in the log is <code class="docutils literal notranslate"><span class="pre">html</span></code>.
The other requests have types like <code class="docutils literal notranslate"><span class="pre">css</span></code> or <code class="docutils literal notranslate"><span class="pre">js</span></code>, but what
interests us is the one request called <code class="docutils literal notranslate"><span class="pre">quotes?page=1</span></code> with the
type <code class="docutils literal notranslate"><span class="pre">json</span></code>.</p>
<p>If we click on this request, we see that the request URL is
<code class="docutils literal notranslate"><span class="pre">http://quotes.toscrape.com/api/quotes?page=1</span></code> and the response
is a JSON-object that contains our quotes. We can also right-click
on the request and open <code class="docutils literal notranslate"><span class="pre">Open</span> <span class="pre">in</span> <span class="pre">new</span> <span class="pre">tab</span></code> to get a better overview.</p>
<a class="reference internal image-reference" href="_images/network_03.png"><img alt="JSON-object returned from the quotes.toscrape API" src="_images/network_03.png" style="width: 777px; height: 375px;" /></a>
<p>With this response we can now easily parse the JSON-object and
also request each page to get every quote on the site:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">import</span> <span class="nn">json</span>


<span class="k">class</span> <span class="nc">QuoteSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;quote&#39;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;quotes.toscrape.com&#39;</span><span class="p">]</span>
    <span class="n">page</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;http://quotes.toscrape.com/api/quotes?page=1&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">quote</span> <span class="ow">in</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;quotes&quot;</span><span class="p">]:</span>
            <span class="k">yield</span> <span class="p">{</span><span class="s2">&quot;quote&quot;</span><span class="p">:</span> <span class="n">quote</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]}</span>
        <span class="k">if</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;has_next&quot;</span><span class="p">]:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">page</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;http://quotes.toscrape.com/api/quotes?page=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">page</span><span class="p">)</span>
            <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="n">url</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
</pre></div>
</div>
<p>This spider starts at the first page of the quotes-API. With each
response, we parse the <code class="docutils literal notranslate"><span class="pre">response.text</span></code> and assign it to <code class="docutils literal notranslate"><span class="pre">data</span></code>.
This lets us operate on the JSON-object like on a Python dictionary.
We iterate through the <code class="docutils literal notranslate"><span class="pre">quotes</span></code> and print out the <code class="docutils literal notranslate"><span class="pre">quote[&quot;text&quot;]</span></code>.
If the handy <code class="docutils literal notranslate"><span class="pre">has_next</span></code> element is <code class="docutils literal notranslate"><span class="pre">true</span></code> (try loading
<a class="reference external" href="http://quotes.toscrape.com/api/quotes?page=10">quotes.toscrape.com/api/quotes?page=10</a> in your browser or a
page-number greater than 10), we increment the <code class="docutils literal notranslate"><span class="pre">page</span></code> attribute
and <code class="docutils literal notranslate"><span class="pre">yield</span></code> a new request, inserting the incremented page-number
into our <code class="docutils literal notranslate"><span class="pre">url</span></code>.</p>
<p>You can see that with a few inspections in the <cite>Network</cite>-tool we
were able to easily replicate the dynamic requests of the scrolling
functionality of the page. Crawling dynamic pages can be quite
daunting and pages can be very complex, but it (mostly) boils down
to identifying the correct request and replicating it in your spider.</p>
</div>
</div>
<span id="document-topics/dynamic-content"></span><div class="section" id="selecting-dynamically-loaded-content">
<span id="topics-dynamic-content"></span><h3>Selecting dynamically-loaded content<a class="headerlink" href="#selecting-dynamically-loaded-content" title="Permalink to this headline">¶</a></h3>
<p>Some webpages show the desired data when you load them in a web browser.
However, when you download them using Scrapy, you cannot reach the desired data
using <a class="reference internal" href="index.html#topics-selectors"><span class="std std-ref">selectors</span></a>.</p>
<p>When this happens, the recommended approach is to
<a class="reference internal" href="#topics-finding-data-source"><span class="std std-ref">find the data source</span></a> and extract the data
from it.</p>
<p>If you fail to do that, and you can nonetheless access the desired data through
the <a class="reference internal" href="index.html#topics-livedom"><span class="std std-ref">DOM</span></a> from your web browser, see
<a class="reference internal" href="#topics-javascript-rendering"><span class="std std-ref">Pre-rendering JavaScript</span></a>.</p>
<div class="section" id="finding-the-data-source">
<span id="topics-finding-data-source"></span><h4>Finding the data source<a class="headerlink" href="#finding-the-data-source" title="Permalink to this headline">¶</a></h4>
<p>To extract the desired data, you must first find its source location.</p>
<p>If the data is in a non-text-based format, such as an image or a PDF document,
use the <a class="reference internal" href="index.html#topics-network-tool"><span class="std std-ref">network tool</span></a> of your web browser to find
the corresponding request, and <a class="reference internal" href="#topics-reproducing-requests"><span class="std std-ref">reproduce it</span></a>.</p>
<p>If your web browser lets you select the desired data as text, the data may be
defined in embedded JavaScript code, or loaded from an external resource in a
text-based format.</p>
<p>In that case, you can use a tool like <a class="reference external" href="https://github.com/stav/wgrep">wgrep</a> to find the URL of that resource.</p>
<p>If the data turns out to come from the original URL itself, you must
<a class="reference internal" href="#topics-inspecting-source"><span class="std std-ref">inspect the source code of the webpage</span></a> to
determine where the data is located.</p>
<p>If the data comes from a different URL, you will need to <a class="reference internal" href="#topics-reproducing-requests"><span class="std std-ref">reproduce the
corresponding request</span></a>.</p>
</div>
<div class="section" id="inspecting-the-source-code-of-a-webpage">
<span id="topics-inspecting-source"></span><h4>Inspecting the source code of a webpage<a class="headerlink" href="#inspecting-the-source-code-of-a-webpage" title="Permalink to this headline">¶</a></h4>
<p>Sometimes you need to inspect the source code of a webpage (not the
<a class="reference internal" href="index.html#topics-livedom"><span class="std std-ref">DOM</span></a>) to determine where some desired data is located.</p>
<p>Use Scrapy’s <a class="reference internal" href="index.html#std:command-fetch"><code class="xref std std-command docutils literal notranslate"><span class="pre">fetch</span></code></a> command to download the webpage contents as seen
by Scrapy:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">fetch</span> <span class="o">--</span><span class="n">nolog</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">example</span><span class="o">.</span><span class="n">com</span> <span class="o">&gt;</span> <span class="n">response</span><span class="o">.</span><span class="n">html</span>
</pre></div>
</div>
<p>If the desired data is in embedded JavaScript code within a <code class="docutils literal notranslate"><span class="pre">&lt;script/&gt;</span></code>
element, see <a class="reference internal" href="#topics-parsing-javascript"><span class="std std-ref">Parsing JavaScript code</span></a>.</p>
<p>If you cannot find the desired data, first make sure it’s not just Scrapy:
download the webpage with an HTTP client like <a class="reference external" href="https://curl.haxx.se/">curl</a> or <a class="reference external" href="https://www.gnu.org/software/wget/">wget</a> and see if the
information can be found in the response they get.</p>
<p>If they get a response with the desired data, modify your Scrapy
<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> to match that of the other HTTP client. For
example, try using the same user-agent string (<a class="reference internal" href="index.html#std:setting-USER_AGENT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">USER_AGENT</span></code></a>) or the
same <a class="reference internal" href="index.html#scrapy.http.Request.headers" title="scrapy.http.Request.headers"><code class="xref py py-attr docutils literal notranslate"><span class="pre">headers</span></code></a>.</p>
<p>If they also get a response without the desired data, you’ll need to take
steps to make your request more similar to that of the web browser. See
<a class="reference internal" href="#topics-reproducing-requests"><span class="std std-ref">Reproducing requests</span></a>.</p>
</div>
<div class="section" id="reproducing-requests">
<span id="topics-reproducing-requests"></span><h4>Reproducing requests<a class="headerlink" href="#reproducing-requests" title="Permalink to this headline">¶</a></h4>
<p>Sometimes we need to reproduce a request the way our web browser performs it.</p>
<p>Use the <a class="reference internal" href="index.html#topics-network-tool"><span class="std std-ref">network tool</span></a> of your web browser to see
how your web browser performs the desired request, and try to reproduce that
request with Scrapy.</p>
<p>It might be enough to yield a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> with the same HTTP
method and URL. However, you may also need to reproduce the body, headers and
form parameters (see <a class="reference internal" href="index.html#scrapy.http.FormRequest" title="scrapy.http.FormRequest"><code class="xref py py-class docutils literal notranslate"><span class="pre">FormRequest</span></code></a>) of that request.</p>
<p>Once you get the expected response, you can <a class="reference internal" href="#topics-handling-response-formats"><span class="std std-ref">extract the desired data from
it</span></a>.</p>
<p>You can reproduce any request with Scrapy. However, some times reproducing all
necessary requests may not seem efficient in developer time. If that is your
case, and crawling speed is not a major concern for you, you can alternatively
consider <a class="reference internal" href="#topics-javascript-rendering"><span class="std std-ref">JavaScript pre-rendering</span></a>.</p>
<p>If you get the expected response <cite>sometimes</cite>, but not always, the issue is
probably not your request, but the target server. The target server might be
buggy, overloaded, or <a class="reference internal" href="index.html#bans"><span class="std std-ref">banning</span></a> some of your requests.</p>
</div>
<div class="section" id="handling-different-response-formats">
<span id="topics-handling-response-formats"></span><h4>Handling different response formats<a class="headerlink" href="#handling-different-response-formats" title="Permalink to this headline">¶</a></h4>
<p>Once you have a response with the desired data, how you extract the desired
data from it depends on the type of response:</p>
<ul>
<li><p class="first">If the response is HTML or XML, use <a class="reference internal" href="index.html#topics-selectors"><span class="std std-ref">selectors</span></a> as usual.</p>
</li>
<li><p class="first">If the response is JSON, use <a class="reference external" href="https://docs.python.org/library/json.html#json.loads">json.loads</a> to load the desired data from
<a class="reference internal" href="index.html#scrapy.http.TextResponse.text" title="scrapy.http.TextResponse.text"><code class="xref py py-attr docutils literal notranslate"><span class="pre">response.text</span></code></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
</pre></div>
</div>
<p>If the desired data is inside HTML or XML code embedded within JSON data,
you can load that HTML or XML code into a
<a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a> and then
<a class="reference internal" href="index.html#topics-selectors"><span class="std std-ref">use it</span></a> as usual:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">selector</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;html&#39;</span><span class="p">])</span>
</pre></div>
</div>
</li>
<li><p class="first">If the response is JavaScript, or HTML with a <code class="docutils literal notranslate"><span class="pre">&lt;script/&gt;</span></code> element
containing the desired data, see <a class="reference internal" href="#topics-parsing-javascript"><span class="std std-ref">Parsing JavaScript code</span></a>.</p>
</li>
<li><p class="first">If the response is CSS, use a <a class="reference external" href="https://docs.python.org/library/re.html">regular expression</a> to extract the desired
data from <a class="reference internal" href="index.html#scrapy.http.TextResponse.text" title="scrapy.http.TextResponse.text"><code class="xref py py-attr docutils literal notranslate"><span class="pre">response.text</span></code></a>.</p>
</li>
</ul>
<ul id="topics-parsing-images">
<li><p class="first">If the response is an image or another format based on images (e.g. PDF),
read the response as bytes from
<code class="xref py py-attr docutils literal notranslate"><span class="pre">response.body</span></code> and use an OCR
solution to extract the desired data as text.</p>
<p>For example, you can use <a class="reference external" href="https://github.com/madmaze/pytesseract">pytesseract</a>. To read a table from a PDF,
<a class="reference external" href="https://github.com/chezou/tabula-py">tabula-py</a> may be a better choice.</p>
</li>
<li><p class="first">If the response is SVG, or HTML with embedded SVG containing the desired
data, you may be able to extract the desired data using
<a class="reference internal" href="index.html#topics-selectors"><span class="std std-ref">selectors</span></a>, since SVG is based on XML.</p>
<p>Otherwise, you might need to convert the SVG code into a raster image, and
<a class="reference internal" href="#topics-parsing-images"><span class="std std-ref">handle that raster image</span></a>.</p>
</li>
</ul>
</div>
<div class="section" id="parsing-javascript-code">
<span id="topics-parsing-javascript"></span><h4>Parsing JavaScript code<a class="headerlink" href="#parsing-javascript-code" title="Permalink to this headline">¶</a></h4>
<p>If the desired data is hardcoded in JavaScript, you first need to get the
JavaScript code:</p>
<ul class="simple">
<li>If the JavaScript code is in a JavaScript file, simply read
<a class="reference internal" href="index.html#scrapy.http.TextResponse.text" title="scrapy.http.TextResponse.text"><code class="xref py py-attr docutils literal notranslate"><span class="pre">response.text</span></code></a>.</li>
<li>If the JavaScript code is within a <code class="docutils literal notranslate"><span class="pre">&lt;script/&gt;</span></code> element of an HTML page,
use <a class="reference internal" href="index.html#topics-selectors"><span class="std std-ref">selectors</span></a> to extract the text within that
<code class="docutils literal notranslate"><span class="pre">&lt;script/&gt;</span></code> element.</li>
</ul>
<p>Once you have a string with the JavaScript code, you can extract the desired
data from it:</p>
<ul>
<li><p class="first">You might be able to use a <a class="reference external" href="https://docs.python.org/library/re.html">regular expression</a> to extract the desired
data in JSON format, which you can then parse with <a class="reference external" href="https://docs.python.org/library/json.html#json.loads">json.loads</a>.</p>
<p>For example, if the JavaScript code contains a separate line like
<code class="docutils literal notranslate"><span class="pre">var</span> <span class="pre">data</span> <span class="pre">=</span> <span class="pre">{&quot;field&quot;:</span> <span class="pre">&quot;value&quot;};</span></code> you can extract that data as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pattern</span> <span class="o">=</span> <span class="sa">r</span><span class="s1">&#39;\bvar\s+data\s*=\s*(\{.*?\})\s*;\s*\n&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">json_data</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;script::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">re_first</span><span class="p">(</span><span class="n">pattern</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">json_data</span><span class="p">)</span>
<span class="go">{&#39;field&#39;: &#39;value&#39;}</span>
</pre></div>
</div>
</li>
<li><p class="first">Otherwise, use <a class="reference external" href="https://github.com/scrapinghub/js2xml">js2xml</a> to convert the JavaScript code into an XML document
that you can parse using <a class="reference internal" href="index.html#topics-selectors"><span class="std std-ref">selectors</span></a>.</p>
<p>For example, if the JavaScript code contains
<code class="docutils literal notranslate"><span class="pre">var</span> <span class="pre">data</span> <span class="pre">=</span> <span class="pre">{field:</span> <span class="pre">&quot;value&quot;};</span></code> you can extract that data as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">js2xml</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">lxml.etree</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">parsel</span> <span class="k">import</span> <span class="n">Selector</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">javascript</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;script::text&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">xml</span> <span class="o">=</span> <span class="n">lxml</span><span class="o">.</span><span class="n">etree</span><span class="o">.</span><span class="n">tostring</span><span class="p">(</span><span class="n">js2xml</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">javascript</span><span class="p">),</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">&#39;unicode&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span> <span class="o">=</span> <span class="n">Selector</span><span class="p">(</span><span class="n">text</span><span class="o">=</span><span class="n">xml</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">selector</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;var[name=&quot;data&quot;]&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>
<span class="go">&#39;&lt;var name=&quot;data&quot;&gt;&lt;object&gt;&lt;property name=&quot;field&quot;&gt;&lt;string&gt;value&lt;/string&gt;&lt;/property&gt;&lt;/object&gt;&lt;/var&gt;&#39;</span>
</pre></div>
</div>
</li>
</ul>
</div>
<div class="section" id="pre-rendering-javascript">
<span id="topics-javascript-rendering"></span><h4>Pre-rendering JavaScript<a class="headerlink" href="#pre-rendering-javascript" title="Permalink to this headline">¶</a></h4>
<p>On webpages that fetch data from additional requests, reproducing those
requests that contain the desired data is the preferred approach. The effort is
often worth the result: structured, complete data with minimum parsing time and
network transfer.</p>
<p>However, sometimes it can be really hard to reproduce certain requests. Or you
may need something that no request can give you, such as a screenshot of a
webpage as seen in a web browser.</p>
<p>In these cases use the <a class="reference external" href="https://github.com/scrapinghub/splash">Splash</a> JavaScript-rendering service, along with
<a class="reference external" href="https://github.com/scrapy-plugins/scrapy-splash">scrapy-splash</a> for seamless integration.</p>
<p>Splash returns as HTML the <a class="reference internal" href="index.html#topics-livedom"><span class="std std-ref">DOM</span></a> of a webpage, so that
you can parse it with <a class="reference internal" href="index.html#topics-selectors"><span class="std std-ref">selectors</span></a>. It provides great
flexibility through <a class="reference external" href="https://splash.readthedocs.io/en/stable/api.html">configuration</a> or <a class="reference external" href="https://splash.readthedocs.io/en/stable/scripting-tutorial.html">scripting</a>.</p>
<p>If you need something beyond what Splash offers, such as interacting with the
DOM on-the-fly from Python code instead of using a previously-written script,
or handling multiple web browser windows, you might need to
<a class="reference internal" href="#topics-headless-browsing"><span class="std std-ref">use a headless browser</span></a> instead.</p>
</div>
<div class="section" id="using-a-headless-browser">
<span id="topics-headless-browsing"></span><h4>Using a headless browser<a class="headerlink" href="#using-a-headless-browser" title="Permalink to this headline">¶</a></h4>
<p>A <a class="reference external" href="https://en.wikipedia.org/wiki/Headless_browser">headless browser</a> is a special web browser that provides an API for
automation.</p>
<p>The easiest way to use a headless browser with Scrapy is to use <a class="reference external" href="https://www.seleniumhq.org/">Selenium</a>,
along with <a class="reference external" href="https://github.com/clemfromspace/scrapy-selenium">scrapy-selenium</a> for seamless integration.</p>
</div>
</div>
<span id="document-topics/leaks"></span><div class="section" id="debugging-memory-leaks">
<span id="topics-leaks"></span><h3>Debugging memory leaks<a class="headerlink" href="#debugging-memory-leaks" title="Permalink to this headline">¶</a></h3>
<p>In Scrapy, objects such as Requests, Responses and Items have a finite
lifetime: they are created, used for a while, and finally destroyed.</p>
<p>From all those objects, the Request is probably the one with the longest
lifetime, as it stays waiting in the Scheduler queue until it’s time to process
it. For more info see <a class="reference internal" href="index.html#topics-architecture"><span class="std std-ref">Architecture overview</span></a>.</p>
<p>As these Scrapy objects have a (rather long) lifetime, there is always the risk
of accumulating them in memory without releasing them properly and thus causing
what is known as a “memory leak”.</p>
<p>To help debugging memory leaks, Scrapy provides a built-in mechanism for
tracking objects references called <a class="reference internal" href="#topics-leaks-trackrefs"><span class="std std-ref">trackref</span></a>,
and you can also use a third-party library called <a class="reference internal" href="#topics-leaks-guppy"><span class="std std-ref">Guppy</span></a> for more advanced memory debugging (see below for more
info). Both mechanisms must be used from the <a class="reference internal" href="index.html#topics-telnetconsole"><span class="std std-ref">Telnet Console</span></a>.</p>
<div class="section" id="common-causes-of-memory-leaks">
<h4>Common causes of memory leaks<a class="headerlink" href="#common-causes-of-memory-leaks" title="Permalink to this headline">¶</a></h4>
<p>It happens quite often (sometimes by accident, sometimes on purpose) that the
Scrapy developer passes objects referenced in Requests (for example, using the
<a class="reference internal" href="index.html#scrapy.http.Request.cb_kwargs" title="scrapy.http.Request.cb_kwargs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">cb_kwargs</span></code></a> or <a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal notranslate"><span class="pre">meta</span></code></a>
attributes or the request callback function) and that effectively bounds the
lifetime of those referenced objects to the lifetime of the Request. This is,
by far, the most common cause of memory leaks in Scrapy projects, and a quite
difficult one to debug for newcomers.</p>
<p>In big projects, the spiders are typically written by different people and some
of those spiders could be “leaking” and thus affecting the rest of the other
(well-written) spiders when they get to run concurrently, which, in turn,
affects the whole crawling process.</p>
<p>The leak could also come from a custom middleware, pipeline or extension that
you have written, if you are not releasing the (previously allocated) resources
properly. For example, allocating resources on <a class="reference internal" href="index.html#std:signal-spider_opened"><code class="xref std std-signal docutils literal notranslate"><span class="pre">spider_opened</span></code></a>
but not releasing them on <a class="reference internal" href="index.html#std:signal-spider_closed"><code class="xref std std-signal docutils literal notranslate"><span class="pre">spider_closed</span></code></a> may cause problems if
you’re running <a class="reference internal" href="index.html#run-multiple-spiders"><span class="std std-ref">multiple spiders per process</span></a>.</p>
<div class="section" id="too-many-requests">
<h5>Too Many Requests?<a class="headerlink" href="#too-many-requests" title="Permalink to this headline">¶</a></h5>
<p>By default Scrapy keeps the request queue in memory; it includes
<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> objects and all objects
referenced in Request attributes (e.g. in <a class="reference internal" href="index.html#scrapy.http.Request.cb_kwargs" title="scrapy.http.Request.cb_kwargs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">cb_kwargs</span></code></a>
and <a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal notranslate"><span class="pre">meta</span></code></a>).
While not necessarily a leak, this can take a lot of memory. Enabling
<a class="reference internal" href="index.html#topics-jobs"><span class="std std-ref">persistent job queue</span></a> could help keeping memory usage
in control.</p>
</div>
</div>
<div class="section" id="debugging-memory-leaks-with-trackref">
<span id="topics-leaks-trackrefs"></span><h4>Debugging memory leaks with <code class="docutils literal notranslate"><span class="pre">trackref</span></code><a class="headerlink" href="#debugging-memory-leaks-with-trackref" title="Permalink to this headline">¶</a></h4>
<p><code class="xref py py-mod docutils literal notranslate"><span class="pre">trackref</span></code> is a module provided by Scrapy to debug the most common cases of
memory leaks. It basically tracks the references to all live Requests,
Responses, Item and Selector objects.</p>
<p>You can enter the telnet console and inspect how many objects (of the classes
mentioned above) are currently alive using the <code class="docutils literal notranslate"><span class="pre">prefs()</span></code> function which is an
alias to the <a class="reference internal" href="#scrapy.utils.trackref.print_live_refs" title="scrapy.utils.trackref.print_live_refs"><code class="xref py py-func docutils literal notranslate"><span class="pre">print_live_refs()</span></code></a> function:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">telnet</span> <span class="n">localhost</span> <span class="mi">6023</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">prefs</span><span class="p">()</span>
<span class="n">Live</span> <span class="n">References</span>

<span class="n">ExampleSpider</span>                       <span class="mi">1</span>   <span class="n">oldest</span><span class="p">:</span> <span class="mi">15</span><span class="n">s</span> <span class="n">ago</span>
<span class="n">HtmlResponse</span>                       <span class="mi">10</span>   <span class="n">oldest</span><span class="p">:</span> <span class="mi">1</span><span class="n">s</span> <span class="n">ago</span>
<span class="n">Selector</span>                            <span class="mi">2</span>   <span class="n">oldest</span><span class="p">:</span> <span class="mi">0</span><span class="n">s</span> <span class="n">ago</span>
<span class="n">FormRequest</span>                       <span class="mi">878</span>   <span class="n">oldest</span><span class="p">:</span> <span class="mi">7</span><span class="n">s</span> <span class="n">ago</span>
</pre></div>
</div>
<p>As you can see, that report also shows the “age” of the oldest object in each
class. If you’re running multiple spiders per process chances are you can
figure out which spider is leaking by looking at the oldest request or response.
You can get the oldest object of each class using the
<a class="reference internal" href="#scrapy.utils.trackref.get_oldest" title="scrapy.utils.trackref.get_oldest"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_oldest()</span></code></a> function (from the telnet console).</p>
<div class="section" id="which-objects-are-tracked">
<h5>Which objects are tracked?<a class="headerlink" href="#which-objects-are-tracked" title="Permalink to this headline">¶</a></h5>
<p>The objects tracked by <code class="docutils literal notranslate"><span class="pre">trackrefs</span></code> are all from these classes (and all its
subclasses):</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.http.Request</span></code></a></li>
<li><a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.http.Response</span></code></a></li>
<li><a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.item.Item</span></code></a></li>
<li><a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.selector.Selector</span></code></a></li>
<li><a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.spiders.Spider</span></code></a></li>
</ul>
</div>
<div class="section" id="a-real-example">
<h5>A real example<a class="headerlink" href="#a-real-example" title="Permalink to this headline">¶</a></h5>
<p>Let’s see a concrete example of a hypothetical case of memory leaks.
Suppose we have some spider with a line similar to this one:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">return</span> <span class="n">Request</span><span class="p">(</span><span class="s2">&quot;http://www.somenastyspider.com/product.php?pid=</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">product_id</span><span class="p">,</span>
               <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">,</span> <span class="n">cb_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;referer&#39;</span><span class="p">:</span> <span class="n">response</span><span class="p">})</span>
</pre></div>
</div>
<p>That line is passing a response reference inside a request which effectively
ties the response lifetime to the requests’ one, and that would definitely
cause memory leaks.</p>
<p>Let’s see how we can discover the cause (without knowing it
a-priori, of course) by using the <code class="docutils literal notranslate"><span class="pre">trackref</span></code> tool.</p>
<p>After the crawler is running for a few minutes and we notice its memory usage
has grown a lot, we can enter its telnet console and check the live
references:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">prefs</span><span class="p">()</span>
<span class="go">Live References</span>

<span class="go">SomenastySpider                     1   oldest: 15s ago</span>
<span class="go">HtmlResponse                     3890   oldest: 265s ago</span>
<span class="go">Selector                            2   oldest: 0s ago</span>
<span class="go">Request                          3878   oldest: 250s ago</span>
</pre></div>
</div>
<p>The fact that there are so many live responses (and that they’re so old) is
definitely suspicious, as responses should have a relatively short lifetime
compared to Requests. The number of responses is similar to the number
of requests, so it looks like they are tied in a some way. We can now go
and check the code of the spider to discover the nasty line that is
generating the leaks (passing response references inside requests).</p>
<p>Sometimes extra information about live objects can be helpful.
Let’s check the oldest response:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.utils.trackref</span> <span class="k">import</span> <span class="n">get_oldest</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r</span> <span class="o">=</span> <span class="n">get_oldest</span><span class="p">(</span><span class="s1">&#39;HtmlResponse&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r</span><span class="o">.</span><span class="n">url</span>
<span class="go">&#39;http://www.somenastyspider.com/product.php?pid=123&#39;</span>
</pre></div>
</div>
<p>If you want to iterate over all objects, instead of getting the oldest one, you
can use the <a class="reference internal" href="#scrapy.utils.trackref.iter_all" title="scrapy.utils.trackref.iter_all"><code class="xref py py-func docutils literal notranslate"><span class="pre">scrapy.utils.trackref.iter_all()</span></code></a> function:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.utils.trackref</span> <span class="k">import</span> <span class="n">iter_all</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="n">r</span><span class="o">.</span><span class="n">url</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">iter_all</span><span class="p">(</span><span class="s1">&#39;HtmlResponse&#39;</span><span class="p">)]</span>
<span class="go">[&#39;http://www.somenastyspider.com/product.php?pid=123&#39;,</span>
<span class="go"> &#39;http://www.somenastyspider.com/product.php?pid=584&#39;,</span>
<span class="gp">...</span>
</pre></div>
</div>
</div>
<div class="section" id="too-many-spiders">
<h5>Too many spiders?<a class="headerlink" href="#too-many-spiders" title="Permalink to this headline">¶</a></h5>
<p>If your project has too many spiders executed in parallel,
the output of <code class="xref py py-func docutils literal notranslate"><span class="pre">prefs()</span></code> can be difficult to read.
For this reason, that function has a <code class="docutils literal notranslate"><span class="pre">ignore</span></code> argument which can be used to
ignore a particular class (and all its subclases). For
example, this won’t show any live references to spiders:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scrapy.spiders</span> <span class="k">import</span> <span class="n">Spider</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">prefs</span><span class="p">(</span><span class="n">ignore</span><span class="o">=</span><span class="n">Spider</span><span class="p">)</span>
</pre></div>
</div>
<span class="target" id="module-scrapy.utils.trackref"></span></div>
<div class="section" id="scrapy-utils-trackref-module">
<h5>scrapy.utils.trackref module<a class="headerlink" href="#scrapy-utils-trackref-module" title="Permalink to this headline">¶</a></h5>
<p>Here are the functions available in the <a class="reference internal" href="#module-scrapy.utils.trackref" title="scrapy.utils.trackref: Track references of live objects"><code class="xref py py-mod docutils literal notranslate"><span class="pre">trackref</span></code></a> module.</p>
<dl class="class">
<dt id="scrapy.utils.trackref.object_ref">
<em class="property">class </em><code class="descclassname">scrapy.utils.trackref.</code><code class="descname">object_ref</code><a class="headerlink" href="#scrapy.utils.trackref.object_ref" title="Permalink to this definition">¶</a></dt>
<dd><p>Inherit from this class (instead of object) if you want to track live
instances with the <code class="docutils literal notranslate"><span class="pre">trackref</span></code> module.</p>
</dd></dl>

<dl class="function">
<dt id="scrapy.utils.trackref.print_live_refs">
<code class="descclassname">scrapy.utils.trackref.</code><code class="descname">print_live_refs</code><span class="sig-paren">(</span><em>class_name</em>, <em>ignore=NoneType</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.utils.trackref.print_live_refs" title="Permalink to this definition">¶</a></dt>
<dd><p>Print a report of live references, grouped by class name.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>ignore</strong> (<em>class</em><em> or </em><em>classes tuple</em>) – if given, all objects from the specified class (or tuple of
classes) will be ignored.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="scrapy.utils.trackref.get_oldest">
<code class="descclassname">scrapy.utils.trackref.</code><code class="descname">get_oldest</code><span class="sig-paren">(</span><em>class_name</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.utils.trackref.get_oldest" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the oldest object alive with the given class name, or <code class="docutils literal notranslate"><span class="pre">None</span></code> if
none is found. Use <a class="reference internal" href="#scrapy.utils.trackref.print_live_refs" title="scrapy.utils.trackref.print_live_refs"><code class="xref py py-func docutils literal notranslate"><span class="pre">print_live_refs()</span></code></a> first to get a list of all
tracked live objects per class name.</p>
</dd></dl>

<dl class="function">
<dt id="scrapy.utils.trackref.iter_all">
<code class="descclassname">scrapy.utils.trackref.</code><code class="descname">iter_all</code><span class="sig-paren">(</span><em>class_name</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.utils.trackref.iter_all" title="Permalink to this definition">¶</a></dt>
<dd><p>Return an iterator over all objects alive with the given class name, or
<code class="docutils literal notranslate"><span class="pre">None</span></code> if none is found. Use <a class="reference internal" href="#scrapy.utils.trackref.print_live_refs" title="scrapy.utils.trackref.print_live_refs"><code class="xref py py-func docutils literal notranslate"><span class="pre">print_live_refs()</span></code></a> first to get a list
of all tracked live objects per class name.</p>
</dd></dl>

</div>
</div>
<div class="section" id="debugging-memory-leaks-with-guppy">
<span id="topics-leaks-guppy"></span><h4>Debugging memory leaks with Guppy<a class="headerlink" href="#debugging-memory-leaks-with-guppy" title="Permalink to this headline">¶</a></h4>
<p><code class="docutils literal notranslate"><span class="pre">trackref</span></code> provides a very convenient mechanism for tracking down memory
leaks, but it only keeps track of the objects that are more likely to cause
memory leaks (Requests, Responses, Items, and Selectors). However, there are
other cases where the memory leaks could come from other (more or less obscure)
objects. If this is your case, and you can’t find your leaks using <code class="docutils literal notranslate"><span class="pre">trackref</span></code>,
you still have another resource: the <a class="reference external" href="https://pypi.python.org/pypi/guppy">Guppy library</a>.
If you’re using Python3, see <a class="reference internal" href="#topics-leaks-muppy"><span class="std std-ref">Debugging memory leaks with muppy</span></a>.</p>
<p>If you use <code class="docutils literal notranslate"><span class="pre">pip</span></code>, you can install Guppy with the following command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">guppy</span>
</pre></div>
</div>
<p>The telnet console also comes with a built-in shortcut (<code class="docutils literal notranslate"><span class="pre">hpy</span></code>) for accessing
Guppy heap objects. Here’s an example to view all Python objects available in
the heap using Guppy:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">hpy</span><span class="o">.</span><span class="n">heap</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">bytype</span>
<span class="go">Partition of a set of 297033 objects. Total size = 52587824 bytes.</span>
<span class="go"> Index  Count   %     Size   % Cumulative  % Type</span>
<span class="go">     0  22307   8 16423880  31  16423880  31 dict</span>
<span class="go">     1 122285  41 12441544  24  28865424  55 str</span>
<span class="go">     2  68346  23  5966696  11  34832120  66 tuple</span>
<span class="go">     3    227   0  5836528  11  40668648  77 unicode</span>
<span class="go">     4   2461   1  2222272   4  42890920  82 type</span>
<span class="go">     5  16870   6  2024400   4  44915320  85 function</span>
<span class="go">     6  13949   5  1673880   3  46589200  89 types.CodeType</span>
<span class="go">     7  13422   5  1653104   3  48242304  92 list</span>
<span class="go">     8   3735   1  1173680   2  49415984  94 _sre.SRE_Pattern</span>
<span class="go">     9   1209   0   456936   1  49872920  95 scrapy.http.headers.Headers</span>
<span class="go">&lt;1676 more rows. Type e.g. &#39;_.more&#39; to view.&gt;</span>
</pre></div>
</div>
<p>You can see that most space is used by dicts. Then, if you want to see from
which attribute those dicts are referenced, you could do:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span><span class="o">.</span><span class="n">bytype</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">byvia</span>
<span class="go">Partition of a set of 22307 objects. Total size = 16423880 bytes.</span>
<span class="go"> Index  Count   %     Size   % Cumulative  % Referred Via:</span>
<span class="go">     0  10982  49  9416336  57   9416336  57 &#39;.__dict__&#39;</span>
<span class="go">     1   1820   8  2681504  16  12097840  74 &#39;.__dict__&#39;, &#39;.func_globals&#39;</span>
<span class="go">     2   3097  14  1122904   7  13220744  80</span>
<span class="go">     3    990   4   277200   2  13497944  82 &quot;[&#39;cookies&#39;]&quot;</span>
<span class="go">     4    987   4   276360   2  13774304  84 &quot;[&#39;cache&#39;]&quot;</span>
<span class="go">     5    985   4   275800   2  14050104  86 &quot;[&#39;meta&#39;]&quot;</span>
<span class="go">     6    897   4   251160   2  14301264  87 &#39;[2]&#39;</span>
<span class="go">     7      1   0   196888   1  14498152  88 &quot;[&#39;moduleDict&#39;]&quot;, &quot;[&#39;modules&#39;]&quot;</span>
<span class="go">     8    672   3   188160   1  14686312  89 &quot;[&#39;cb_kwargs&#39;]&quot;</span>
<span class="go">     9     27   0   155016   1  14841328  90 &#39;[1]&#39;</span>
<span class="go">&lt;333 more rows. Type e.g. &#39;_.more&#39; to view.&gt;</span>
</pre></div>
</div>
<p>As you can see, the Guppy module is very powerful but also requires some deep
knowledge about Python internals. For more info about Guppy, refer to the
<a class="reference external" href="http://guppy-pe.sourceforge.net/">Guppy documentation</a>.</p>
</div>
<div class="section" id="debugging-memory-leaks-with-muppy">
<span id="topics-leaks-muppy"></span><h4>Debugging memory leaks with muppy<a class="headerlink" href="#debugging-memory-leaks-with-muppy" title="Permalink to this headline">¶</a></h4>
<p>If you’re using Python 3, you can use muppy from <a class="reference external" href="https://pypi.org/project/Pympler/">Pympler</a>.</p>
<p>If you use <code class="docutils literal notranslate"><span class="pre">pip</span></code>, you can install muppy with the following command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">Pympler</span>
</pre></div>
</div>
<p>Here’s an example to view all Python objects available in
the heap using muppy:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pympler</span> <span class="k">import</span> <span class="n">muppy</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">all_objects</span> <span class="o">=</span> <span class="n">muppy</span><span class="o">.</span><span class="n">get_objects</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">len</span><span class="p">(</span><span class="n">all_objects</span><span class="p">)</span>
<span class="go">28667</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">pympler</span> <span class="k">import</span> <span class="n">summary</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">suml</span> <span class="o">=</span> <span class="n">summary</span><span class="o">.</span><span class="n">summarize</span><span class="p">(</span><span class="n">all_objects</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">summary</span><span class="o">.</span><span class="n">print_</span><span class="p">(</span><span class="n">suml</span><span class="p">)</span>
<span class="go">                               types |   # objects |   total size</span>
<span class="go">==================================== | =========== | ============</span>
<span class="go">                         &lt;class &#39;str |        9822 |      1.10 MB</span>
<span class="go">                        &lt;class &#39;dict |        1658 |    856.62 KB</span>
<span class="go">                        &lt;class &#39;type |         436 |    443.60 KB</span>
<span class="go">                        &lt;class &#39;code |        2974 |    419.56 KB</span>
<span class="go">          &lt;class &#39;_io.BufferedWriter |           2 |    256.34 KB</span>
<span class="go">                         &lt;class &#39;set |         420 |    159.88 KB</span>
<span class="go">          &lt;class &#39;_io.BufferedReader |           1 |    128.17 KB</span>
<span class="go">          &lt;class &#39;wrapper_descriptor |        1130 |     88.28 KB</span>
<span class="go">                       &lt;class &#39;tuple |        1304 |     86.57 KB</span>
<span class="go">                     &lt;class &#39;weakref |        1013 |     79.14 KB</span>
<span class="go">  &lt;class &#39;builtin_function_or_method |         958 |     67.36 KB</span>
<span class="go">           &lt;class &#39;method_descriptor |         865 |     60.82 KB</span>
<span class="go">                 &lt;class &#39;abc.ABCMeta |          62 |     59.96 KB</span>
<span class="go">                        &lt;class &#39;list |         446 |     58.52 KB</span>
<span class="go">                         &lt;class &#39;int |        1425 |     43.20 KB</span>
</pre></div>
</div>
<p>For more info about muppy, refer to the <a class="reference external" href="https://pythonhosted.org/Pympler/muppy.html">muppy documentation</a>.</p>
</div>
<div class="section" id="leaks-without-leaks">
<span id="topics-leaks-without-leaks"></span><h4>Leaks without leaks<a class="headerlink" href="#leaks-without-leaks" title="Permalink to this headline">¶</a></h4>
<p>Sometimes, you may notice that the memory usage of your Scrapy process will
only increase, but never decrease. Unfortunately, this could happen even
though neither Scrapy nor your project are leaking memory. This is due to a
(not so well) known problem of Python, which may not return released memory to
the operating system in some cases. For more information on this issue see:</p>
<ul class="simple">
<li><a class="reference external" href="http://www.evanjones.ca/python-memory.html">Python Memory Management</a></li>
<li><a class="reference external" href="http://www.evanjones.ca/python-memory-part2.html">Python Memory Management Part 2</a></li>
<li><a class="reference external" href="http://www.evanjones.ca/python-memory-part3.html">Python Memory Management Part 3</a></li>
</ul>
<p>The improvements proposed by Evan Jones, which are detailed in <a class="reference external" href="http://www.evanjones.ca/memoryallocator/">this paper</a>,
got merged in Python 2.5, but this only reduces the problem, it doesn’t fix it
completely. To quote the paper:</p>
<blockquote>
<div><em>Unfortunately, this patch can only free an arena if there are no more
objects allocated in it anymore. This means that fragmentation is a large
issue. An application could have many megabytes of free memory, scattered
throughout all the arenas, but it will be unable to free any of it. This is
a problem experienced by all memory allocators. The only way to solve it is
to move to a compacting garbage collector, which is able to move objects in
memory. This would require significant changes to the Python interpreter.</em></div></blockquote>
<p>To keep memory consumption reasonable you can split the job into several
smaller jobs or enable <a class="reference internal" href="index.html#topics-jobs"><span class="std std-ref">persistent job queue</span></a>
and stop/start spider from time to time.</p>
</div>
</div>
<span id="document-topics/media-pipeline"></span><div class="section" id="downloading-and-processing-files-and-images">
<span id="topics-media-pipeline"></span><h3>Downloading and processing files and images<a class="headerlink" href="#downloading-and-processing-files-and-images" title="Permalink to this headline">¶</a></h3>
<p>Scrapy provides reusable <a class="reference internal" href="index.html#document-topics/item-pipeline"><span class="doc">item pipelines</span></a> for
downloading files attached to a particular item (for example, when you scrape
products and also want to download their images locally). These pipelines share
a bit of functionality and structure (we refer to them as media pipelines), but
typically you’ll either use the Files Pipeline or the Images Pipeline.</p>
<p>Both pipelines implement these features:</p>
<ul class="simple">
<li>Avoid re-downloading media that was downloaded recently</li>
<li>Specifying where to store the media (filesystem directory, Amazon S3 bucket,
Google Cloud Storage bucket)</li>
</ul>
<p>The Images Pipeline has a few extra functions for processing images:</p>
<ul class="simple">
<li>Convert all downloaded images to a common format (JPG) and mode (RGB)</li>
<li>Thumbnail generation</li>
<li>Check images width/height to make sure they meet a minimum constraint</li>
</ul>
<p>The pipelines also keep an internal queue of those media URLs which are currently
being scheduled for download, and connect those responses that arrive containing
the same media to that queue. This avoids downloading the same media more than
once when it’s shared by several items.</p>
<div class="section" id="using-the-files-pipeline">
<h4>Using the Files Pipeline<a class="headerlink" href="#using-the-files-pipeline" title="Permalink to this headline">¶</a></h4>
<p>The typical workflow, when using the <code class="xref py py-class docutils literal notranslate"><span class="pre">FilesPipeline</span></code> goes like
this:</p>
<ol class="arabic simple">
<li>In a Spider, you scrape an item and put the URLs of the desired into a
<code class="docutils literal notranslate"><span class="pre">file_urls</span></code> field.</li>
<li>The item is returned from the spider and goes to the item pipeline.</li>
<li>When the item reaches the <code class="xref py py-class docutils literal notranslate"><span class="pre">FilesPipeline</span></code>, the URLs in the
<code class="docutils literal notranslate"><span class="pre">file_urls</span></code> field are scheduled for download using the standard
Scrapy scheduler and downloader (which means the scheduler and downloader
middlewares are reused), but with a higher priority, processing them before other
pages are scraped. The item remains “locked” at that particular pipeline stage
until the files have finish downloading (or fail for some reason).</li>
<li>When the files are downloaded, another field (<code class="docutils literal notranslate"><span class="pre">files</span></code>) will be populated
with the results. This field will contain a list of dicts with information
about the downloaded files, such as the downloaded path, the original
scraped url (taken from the <code class="docutils literal notranslate"><span class="pre">file_urls</span></code> field) , and the file checksum.
The files in the list of the <code class="docutils literal notranslate"><span class="pre">files</span></code> field will retain the same order of
the original <code class="docutils literal notranslate"><span class="pre">file_urls</span></code> field. If some file failed downloading, an
error will be logged and the file won’t be present in the <code class="docutils literal notranslate"><span class="pre">files</span></code> field.</li>
</ol>
</div>
<div class="section" id="using-the-images-pipeline">
<h4>Using the Images Pipeline<a class="headerlink" href="#using-the-images-pipeline" title="Permalink to this headline">¶</a></h4>
<p>Using the <a class="reference internal" href="#scrapy.pipelines.images.ImagesPipeline" title="scrapy.pipelines.images.ImagesPipeline"><code class="xref py py-class docutils literal notranslate"><span class="pre">ImagesPipeline</span></code></a> is a lot like using the <code class="xref py py-class docutils literal notranslate"><span class="pre">FilesPipeline</span></code>,
except the default field names used are different: you use <code class="docutils literal notranslate"><span class="pre">image_urls</span></code> for
the image URLs of an item and it will populate an <code class="docutils literal notranslate"><span class="pre">images</span></code> field for the information
about the downloaded images.</p>
<p>The advantage of using the <a class="reference internal" href="#scrapy.pipelines.images.ImagesPipeline" title="scrapy.pipelines.images.ImagesPipeline"><code class="xref py py-class docutils literal notranslate"><span class="pre">ImagesPipeline</span></code></a> for image files is that you
can configure some extra functions like generating thumbnails and filtering
the images based on their size.</p>
<p>The Images Pipeline uses <a class="reference external" href="https://github.com/python-pillow/Pillow">Pillow</a> for thumbnailing and normalizing images to
JPEG/RGB format, so you need to install this library in order to use it.
<a class="reference external" href="http://www.pythonware.com/products/pil/">Python Imaging Library</a> (PIL) should also work in most cases, but it is known
to cause troubles in some setups, so we recommend to use <a class="reference external" href="https://github.com/python-pillow/Pillow">Pillow</a> instead of
PIL.</p>
</div>
<div class="section" id="enabling-your-media-pipeline">
<span id="topics-media-pipeline-enabling"></span><h4>Enabling your Media Pipeline<a class="headerlink" href="#enabling-your-media-pipeline" title="Permalink to this headline">¶</a></h4>
<span class="target" id="std:setting-IMAGES_STORE"></span><p id="std:setting-FILES_STORE">To enable your media pipeline you must first add it to your project
<a class="reference internal" href="index.html#std:setting-ITEM_PIPELINES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ITEM_PIPELINES</span></code></a> setting.</p>
<p>For Images Pipeline, use:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ITEM_PIPELINES</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;scrapy.pipelines.images.ImagesPipeline&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
</pre></div>
</div>
<p>For Files Pipeline, use:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ITEM_PIPELINES</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;scrapy.pipelines.files.FilesPipeline&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">You can also use both the Files and Images Pipeline at the same time.</p>
</div>
<p>Then, configure the target storage setting to a valid value that will be used
for storing the downloaded images. Otherwise the pipeline will remain disabled,
even if you include it in the <a class="reference internal" href="index.html#std:setting-ITEM_PIPELINES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ITEM_PIPELINES</span></code></a> setting.</p>
<p>For the Files Pipeline, set the <a class="reference internal" href="#std:setting-FILES_STORE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FILES_STORE</span></code></a> setting:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">FILES_STORE</span> <span class="o">=</span> <span class="s1">&#39;/path/to/valid/dir&#39;</span>
</pre></div>
</div>
<p>For the Images Pipeline, set the <a class="reference internal" href="#std:setting-IMAGES_STORE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_STORE</span></code></a> setting:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">IMAGES_STORE</span> <span class="o">=</span> <span class="s1">&#39;/path/to/valid/dir&#39;</span>
</pre></div>
</div>
</div>
<div class="section" id="supported-storage">
<h4>Supported Storage<a class="headerlink" href="#supported-storage" title="Permalink to this headline">¶</a></h4>
<p>File system is currently the only officially supported storage, but there are
also support for storing files in <a class="reference external" href="https://aws.amazon.com/s3/">Amazon S3</a> and <a class="reference external" href="https://cloud.google.com/storage/">Google Cloud Storage</a>.</p>
<div class="section" id="file-system-storage">
<h5>File system storage<a class="headerlink" href="#file-system-storage" title="Permalink to this headline">¶</a></h5>
<p>The files are stored using a <a class="reference external" href="https://en.wikipedia.org/wiki/SHA_hash_functions">SHA1 hash</a> of their URLs for the file names.</p>
<p>For example, the following image URL:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">www</span><span class="o">.</span><span class="n">example</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">image</span><span class="o">.</span><span class="n">jpg</span>
</pre></div>
</div>
<p>Whose <code class="docutils literal notranslate"><span class="pre">SHA1</span> <span class="pre">hash</span></code> is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">3</span><span class="n">afec3b4765f8f0a07b78f98c07b83f013567a0a</span>
</pre></div>
</div>
<p>Will be downloaded and stored in the following file:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">IMAGES_STORE</span><span class="o">&gt;/</span><span class="n">full</span><span class="o">/</span><span class="mi">3</span><span class="n">afec3b4765f8f0a07b78f98c07b83f013567a0a</span><span class="o">.</span><span class="n">jpg</span>
</pre></div>
</div>
<p>Where:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">&lt;IMAGES_STORE&gt;</span></code> is the directory defined in <a class="reference internal" href="#std:setting-IMAGES_STORE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_STORE</span></code></a> setting
for the Images Pipeline.</li>
<li><code class="docutils literal notranslate"><span class="pre">full</span></code> is a sub-directory to separate full images from thumbnails (if
used). For more info see <a class="reference internal" href="#topics-images-thumbnails"><span class="std std-ref">Thumbnail generation for images</span></a>.</li>
</ul>
</div>
<div class="section" id="amazon-s3-storage">
<h5>Amazon S3 storage<a class="headerlink" href="#amazon-s3-storage" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:setting-FILES_STORE_S3_ACL"></span><p id="std:setting-IMAGES_STORE_S3_ACL"><a class="reference internal" href="#std:setting-FILES_STORE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FILES_STORE</span></code></a> and <a class="reference internal" href="#std:setting-IMAGES_STORE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_STORE</span></code></a> can represent an Amazon S3
bucket. Scrapy will automatically upload the files to the bucket.</p>
<p>For example, this is a valid <a class="reference internal" href="#std:setting-IMAGES_STORE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_STORE</span></code></a> value:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">IMAGES_STORE</span> <span class="o">=</span> <span class="s1">&#39;s3://bucket/images&#39;</span>
</pre></div>
</div>
<p>You can modify the Access Control List (ACL) policy used for the stored files,
which is defined by the <a class="reference internal" href="#std:setting-FILES_STORE_S3_ACL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FILES_STORE_S3_ACL</span></code></a> and
<a class="reference internal" href="#std:setting-IMAGES_STORE_S3_ACL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_STORE_S3_ACL</span></code></a> settings. By default, the ACL is set to
<code class="docutils literal notranslate"><span class="pre">private</span></code>. To make the files publicly available use the <code class="docutils literal notranslate"><span class="pre">public-read</span></code>
policy:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">IMAGES_STORE_S3_ACL</span> <span class="o">=</span> <span class="s1">&#39;public-read&#39;</span>
</pre></div>
</div>
<p>For more information, see <a class="reference external" href="https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl">canned ACLs</a> in the Amazon S3 Developer Guide.</p>
<p>Because Scrapy uses <code class="docutils literal notranslate"><span class="pre">boto</span></code> / <code class="docutils literal notranslate"><span class="pre">botocore</span></code> internally you can also use other S3-like storages. Storages like
self-hosted <a class="reference external" href="https://github.com/minio/minio">Minio</a> or <a class="reference external" href="https://s3.scality.com/">s3.scality</a>. All you need to do is set endpoint option in you Scrapy settings:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">AWS_ENDPOINT_URL</span> <span class="o">=</span> <span class="s1">&#39;http://minio.example.com:9000&#39;</span>
</pre></div>
</div>
<p>For self-hosting you also might feel the need not to use SSL and not to verify SSL connection:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">AWS_USE_SSL</span> <span class="o">=</span> <span class="kc">False</span> <span class="c1"># or True (None by default)</span>
<span class="n">AWS_VERIFY</span> <span class="o">=</span> <span class="kc">False</span> <span class="c1"># or True (None by default)</span>
</pre></div>
</div>
</div>
<div class="section" id="id1">
<h5>Google Cloud Storage<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:setting-GCS_PROJECT_ID"></span><span class="target" id="std:setting-FILES_STORE_GCS_ACL"></span><p id="std:setting-IMAGES_STORE_GCS_ACL"><a class="reference internal" href="#std:setting-FILES_STORE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FILES_STORE</span></code></a> and <a class="reference internal" href="#std:setting-IMAGES_STORE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_STORE</span></code></a> can represent a Google Cloud Storage
bucket. Scrapy will automatically upload the files to the bucket. (requires <a class="reference external" href="https://cloud.google.com/storage/docs/reference/libraries#client-libraries-install-python">google-cloud-storage</a> )</p>
<p>For example, these are valid <a class="reference internal" href="#std:setting-IMAGES_STORE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_STORE</span></code></a> and <a class="reference internal" href="#std:setting-GCS_PROJECT_ID"><code class="xref std std-setting docutils literal notranslate"><span class="pre">GCS_PROJECT_ID</span></code></a> settings:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">IMAGES_STORE</span> <span class="o">=</span> <span class="s1">&#39;gs://bucket/images/&#39;</span>
<span class="n">GCS_PROJECT_ID</span> <span class="o">=</span> <span class="s1">&#39;project_id&#39;</span>
</pre></div>
</div>
<p>For information about authentication, see this <a class="reference external" href="https://cloud.google.com/docs/authentication/production">documentation</a>.</p>
<p>You can modify the Access Control List (ACL) policy used for the stored files,
which is defined by the <a class="reference internal" href="#std:setting-FILES_STORE_GCS_ACL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FILES_STORE_GCS_ACL</span></code></a> and
<a class="reference internal" href="#std:setting-IMAGES_STORE_GCS_ACL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_STORE_GCS_ACL</span></code></a> settings. By default, the ACL is set to
<code class="docutils literal notranslate"><span class="pre">''</span></code> (empty string) which means that Cloud Storage applies the bucket’s default object ACL to the object.
To make the files publicly available use the <code class="docutils literal notranslate"><span class="pre">publicRead</span></code>
policy:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">IMAGES_STORE_GCS_ACL</span> <span class="o">=</span> <span class="s1">&#39;publicRead&#39;</span>
</pre></div>
</div>
<p>For more information, see <a class="reference external" href="https://cloud.google.com/storage/docs/access-control/lists#predefined-acl">Predefined ACLs</a> in the Google Cloud Platform Developer Guide.</p>
</div>
</div>
<div class="section" id="usage-example">
<h4>Usage example<a class="headerlink" href="#usage-example" title="Permalink to this headline">¶</a></h4>
<span class="target" id="std:setting-FILES_URLS_FIELD"></span><span class="target" id="std:setting-FILES_RESULT_FIELD"></span><span class="target" id="std:setting-IMAGES_URLS_FIELD"></span><p id="std:setting-IMAGES_RESULT_FIELD">In order to use a media pipeline first, <a class="reference internal" href="#topics-media-pipeline-enabling"><span class="std std-ref">enable it</span></a>.</p>
<p>Then, if a spider returns a dict with the URLs key (<code class="docutils literal notranslate"><span class="pre">file_urls</span></code> or
<code class="docutils literal notranslate"><span class="pre">image_urls</span></code>, for the Files or Images Pipeline respectively), the pipeline will
put the results under respective key (<code class="docutils literal notranslate"><span class="pre">files</span></code> or <code class="docutils literal notranslate"><span class="pre">images</span></code>).</p>
<p>If you prefer to use <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a>, then define a custom item with the
necessary fields, like in this example for Images Pipeline:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">class</span> <span class="nc">MyItem</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>

    <span class="c1"># ... other item fields ...</span>
    <span class="n">image_urls</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">images</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
</pre></div>
</div>
<p>If you want to use another field name for the URLs key or for the results key,
it is also possible to override it.</p>
<p>For the Files Pipeline, set <a class="reference internal" href="#std:setting-FILES_URLS_FIELD"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FILES_URLS_FIELD</span></code></a> and/or
<a class="reference internal" href="#std:setting-FILES_RESULT_FIELD"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FILES_RESULT_FIELD</span></code></a> settings:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">FILES_URLS_FIELD</span> <span class="o">=</span> <span class="s1">&#39;field_name_for_your_files_urls&#39;</span>
<span class="n">FILES_RESULT_FIELD</span> <span class="o">=</span> <span class="s1">&#39;field_name_for_your_processed_files&#39;</span>
</pre></div>
</div>
<p>For the Images Pipeline, set <a class="reference internal" href="#std:setting-IMAGES_URLS_FIELD"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_URLS_FIELD</span></code></a> and/or
<a class="reference internal" href="#std:setting-IMAGES_RESULT_FIELD"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_RESULT_FIELD</span></code></a> settings:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">IMAGES_URLS_FIELD</span> <span class="o">=</span> <span class="s1">&#39;field_name_for_your_images_urls&#39;</span>
<span class="n">IMAGES_RESULT_FIELD</span> <span class="o">=</span> <span class="s1">&#39;field_name_for_your_processed_images&#39;</span>
</pre></div>
</div>
<p>If you need something more complex and want to override the custom pipeline
behaviour, see <a class="reference internal" href="#topics-media-pipeline-override"><span class="std std-ref">Extending the Media Pipelines</span></a>.</p>
<p>If you have multiple image pipelines inheriting from ImagePipeline and you want
to have different settings in different pipelines you can set setting keys
preceded with uppercase name of your pipeline class. E.g. if your pipeline is
called MyPipeline and you want to have custom IMAGES_URLS_FIELD you define
setting MYPIPELINE_IMAGES_URLS_FIELD and your custom settings will be used.</p>
</div>
<div class="section" id="additional-features">
<h4>Additional features<a class="headerlink" href="#additional-features" title="Permalink to this headline">¶</a></h4>
<div class="section" id="file-expiration">
<h5>File expiration<a class="headerlink" href="#file-expiration" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:setting-IMAGES_EXPIRES"></span><p id="std:setting-FILES_EXPIRES">The Image Pipeline avoids downloading files that were downloaded recently. To
adjust this retention delay use the <a class="reference internal" href="#std:setting-FILES_EXPIRES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FILES_EXPIRES</span></code></a> setting (or
<a class="reference internal" href="#std:setting-IMAGES_EXPIRES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_EXPIRES</span></code></a>, in case of Images Pipeline), which
specifies the delay in number of days:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># 120 days of delay for files expiration</span>
<span class="n">FILES_EXPIRES</span> <span class="o">=</span> <span class="mi">120</span>

<span class="c1"># 30 days of delay for images expiration</span>
<span class="n">IMAGES_EXPIRES</span> <span class="o">=</span> <span class="mi">30</span>
</pre></div>
</div>
<p>The default value for both settings is 90 days.</p>
<p>If you have pipeline that subclasses FilesPipeline and you’d like to have
different setting for it you can set setting keys preceded by uppercase
class name. E.g. given pipeline class called MyPipeline you can set setting key:</p>
<blockquote>
<div>MYPIPELINE_FILES_EXPIRES = 180</div></blockquote>
<p>and pipeline class MyPipeline will have expiration time set to 180.</p>
</div>
<div class="section" id="thumbnail-generation-for-images">
<span id="topics-images-thumbnails"></span><h5>Thumbnail generation for images<a class="headerlink" href="#thumbnail-generation-for-images" title="Permalink to this headline">¶</a></h5>
<p>The Images Pipeline can automatically create thumbnails of the downloaded
images.</p>
<p id="std:setting-IMAGES_THUMBS">In order to use this feature, you must set <a class="reference internal" href="#std:setting-IMAGES_THUMBS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_THUMBS</span></code></a> to a dictionary
where the keys are the thumbnail names and the values are their dimensions.</p>
<p>For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">IMAGES_THUMBS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;small&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span>
    <span class="s1">&#39;big&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">270</span><span class="p">,</span> <span class="mi">270</span><span class="p">),</span>
<span class="p">}</span>
</pre></div>
</div>
<p>When you use this feature, the Images Pipeline will create thumbnails of the
each specified size with this format:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">IMAGES_STORE</span><span class="o">&gt;/</span><span class="n">thumbs</span><span class="o">/&lt;</span><span class="n">size_name</span><span class="o">&gt;/&lt;</span><span class="n">image_id</span><span class="o">&gt;.</span><span class="n">jpg</span>
</pre></div>
</div>
<p>Where:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">&lt;size_name&gt;</span></code> is the one specified in the <a class="reference internal" href="#std:setting-IMAGES_THUMBS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_THUMBS</span></code></a>
dictionary keys (<code class="docutils literal notranslate"><span class="pre">small</span></code>, <code class="docutils literal notranslate"><span class="pre">big</span></code>, etc)</li>
<li><code class="docutils literal notranslate"><span class="pre">&lt;image_id&gt;</span></code> is the <a class="reference external" href="https://en.wikipedia.org/wiki/SHA_hash_functions">SHA1 hash</a> of the image url</li>
</ul>
<p>Example of image files stored using <code class="docutils literal notranslate"><span class="pre">small</span></code> and <code class="docutils literal notranslate"><span class="pre">big</span></code> thumbnail names:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">IMAGES_STORE</span><span class="o">&gt;/</span><span class="n">full</span><span class="o">/</span><span class="mi">63</span><span class="n">bbfea82b8880ed33cdb762aa11fab722a90a24</span><span class="o">.</span><span class="n">jpg</span>
<span class="o">&lt;</span><span class="n">IMAGES_STORE</span><span class="o">&gt;/</span><span class="n">thumbs</span><span class="o">/</span><span class="n">small</span><span class="o">/</span><span class="mi">63</span><span class="n">bbfea82b8880ed33cdb762aa11fab722a90a24</span><span class="o">.</span><span class="n">jpg</span>
<span class="o">&lt;</span><span class="n">IMAGES_STORE</span><span class="o">&gt;/</span><span class="n">thumbs</span><span class="o">/</span><span class="n">big</span><span class="o">/</span><span class="mi">63</span><span class="n">bbfea82b8880ed33cdb762aa11fab722a90a24</span><span class="o">.</span><span class="n">jpg</span>
</pre></div>
</div>
<p>The first one is the full image, as downloaded from the site.</p>
</div>
<div class="section" id="filtering-out-small-images">
<h5>Filtering out small images<a class="headerlink" href="#filtering-out-small-images" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:setting-IMAGES_MIN_HEIGHT"></span><p id="std:setting-IMAGES_MIN_WIDTH">When using the Images Pipeline, you can drop images which are too small, by
specifying the minimum allowed size in the <a class="reference internal" href="#std:setting-IMAGES_MIN_HEIGHT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_MIN_HEIGHT</span></code></a> and
<a class="reference internal" href="#std:setting-IMAGES_MIN_WIDTH"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_MIN_WIDTH</span></code></a> settings.</p>
<p>For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">IMAGES_MIN_HEIGHT</span> <span class="o">=</span> <span class="mi">110</span>
<span class="n">IMAGES_MIN_WIDTH</span> <span class="o">=</span> <span class="mi">110</span>
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The size constraints don’t affect thumbnail generation at all.</p>
</div>
<p>It is possible to set just one size constraint or both. When setting both of
them, only images that satisfy both minimum sizes will be saved. For the
above example, images of sizes (105 x 105) or (105 x 200) or (200 x 105) will
all be dropped because at least one dimension is shorter than the constraint.</p>
<p>By default, there are no size constraints, so all images are processed.</p>
</div>
<div class="section" id="allowing-redirections">
<h5>Allowing redirections<a class="headerlink" href="#allowing-redirections" title="Permalink to this headline">¶</a></h5>
<p id="std:setting-MEDIA_ALLOW_REDIRECTS">By default media pipelines ignore redirects, i.e. an HTTP redirection
to a media file URL request will mean the media download is considered failed.</p>
<p>To handle media redirections, set this setting to <code class="docutils literal notranslate"><span class="pre">True</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">MEDIA_ALLOW_REDIRECTS</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="module-scrapy.pipelines.files">
<span id="extending-the-media-pipelines"></span><span id="topics-media-pipeline-override"></span><h4>Extending the Media Pipelines<a class="headerlink" href="#module-scrapy.pipelines.files" title="Permalink to this headline">¶</a></h4>
<p>See here the methods that you can override in your custom Files Pipeline:</p>
<dl class="class">
<dt id="scrapy.pipelines.files.FilesPipeline">
<em class="property">class </em><code class="descclassname">scrapy.pipelines.files.</code><code class="descname">FilesPipeline</code><a class="headerlink" href="#scrapy.pipelines.files.FilesPipeline" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="scrapy.pipelines.files.FilesPipeline.file_path">
<code class="descname">file_path</code><span class="sig-paren">(</span><em>request</em>, <em>response</em>, <em>info</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.pipelines.files.FilesPipeline.file_path" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called once per downloaded item. It returns the
download path of the file originating from the specified
<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">response</span></code></a>.</p>
<p>In addition to <code class="docutils literal notranslate"><span class="pre">response</span></code>, this method receives the original
<code class="xref py py-class docutils literal notranslate"><span class="pre">request</span></code> and
<code class="xref py py-class docutils literal notranslate"><span class="pre">info</span></code>.</p>
<p>You can override this method to customize the download path of each file.</p>
<p>For example, if file URLs end like regular paths (e.g.
<code class="docutils literal notranslate"><span class="pre">https://example.com/a/b/c/foo.png</span></code>), you can use the following
approach to download all files into the <code class="docutils literal notranslate"><span class="pre">files</span></code> folder with their
original filenames (e.g. <code class="docutils literal notranslate"><span class="pre">files/foo.png</span></code>):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">urllib.parse</span> <span class="k">import</span> <span class="n">urlparse</span>

<span class="kn">from</span> <span class="nn">scrapy.pipelines.files</span> <span class="k">import</span> <span class="n">FilesPipeline</span>

<span class="k">class</span> <span class="nc">MyFilesPipeline</span><span class="p">(</span><span class="n">FilesPipeline</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">file_path</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">info</span><span class="p">):</span>
        <span class="k">return</span> <span class="s1">&#39;files/&#39;</span> <span class="o">+</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">urlparse</span><span class="p">(</span><span class="n">request</span><span class="o">.</span><span class="n">url</span><span class="p">)</span><span class="o">.</span><span class="n">path</span><span class="p">)</span>
</pre></div>
</div>
<p>By default the <a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.file_path" title="scrapy.pipelines.files.FilesPipeline.file_path"><code class="xref py py-meth docutils literal notranslate"><span class="pre">file_path()</span></code></a> method returns
<code class="docutils literal notranslate"><span class="pre">full/&lt;request</span> <span class="pre">URL</span> <span class="pre">hash&gt;.&lt;extension&gt;</span></code>.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.pipelines.files.FilesPipeline.get_media_requests">
<code class="descname">get_media_requests</code><span class="sig-paren">(</span><em>item</em>, <em>info</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.pipelines.files.FilesPipeline.get_media_requests" title="Permalink to this definition">¶</a></dt>
<dd><p>As seen on the workflow, the pipeline will get the URLs of the images to
download from the item. In order to do this, you can override the
<a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.get_media_requests" title="scrapy.pipelines.files.FilesPipeline.get_media_requests"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_media_requests()</span></code></a> method and return a Request for each
file URL:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_media_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">info</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">file_url</span> <span class="ow">in</span> <span class="n">item</span><span class="p">[</span><span class="s1">&#39;file_urls&#39;</span><span class="p">]:</span>
        <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">file_url</span><span class="p">)</span>
</pre></div>
</div>
<p>Those requests will be processed by the pipeline and, when they have finished
downloading, the results will be sent to the
<a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.item_completed" title="scrapy.pipelines.files.FilesPipeline.item_completed"><code class="xref py py-meth docutils literal notranslate"><span class="pre">item_completed()</span></code></a> method, as a list of 2-element tuples.
Each tuple will contain <code class="docutils literal notranslate"><span class="pre">(success,</span> <span class="pre">file_info_or_error)</span></code> where:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">success</span></code> is a boolean which is <code class="docutils literal notranslate"><span class="pre">True</span></code> if the image was downloaded
successfully or <code class="docutils literal notranslate"><span class="pre">False</span></code> if it failed for some reason</li>
<li><code class="docutils literal notranslate"><span class="pre">file_info_or_error</span></code> is a dict containing the following keys (if success
is <code class="docutils literal notranslate"><span class="pre">True</span></code>) or a <a class="reference external" href="https://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html">Twisted Failure</a> if there was a problem.<ul>
<li><code class="docutils literal notranslate"><span class="pre">url</span></code> - the url where the file was downloaded from. This is the url of
the request returned from the <a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.get_media_requests" title="scrapy.pipelines.files.FilesPipeline.get_media_requests"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_media_requests()</span></code></a>
method.</li>
<li><code class="docutils literal notranslate"><span class="pre">path</span></code> - the path (relative to <a class="reference internal" href="#std:setting-FILES_STORE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FILES_STORE</span></code></a>) where the file
was stored</li>
<li><code class="docutils literal notranslate"><span class="pre">checksum</span></code> - a <a class="reference external" href="https://en.wikipedia.org/wiki/MD5">MD5 hash</a> of the image contents</li>
</ul>
</li>
</ul>
<p>The list of tuples received by <a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.item_completed" title="scrapy.pipelines.files.FilesPipeline.item_completed"><code class="xref py py-meth docutils literal notranslate"><span class="pre">item_completed()</span></code></a> is
guaranteed to retain the same order of the requests returned from the
<a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.get_media_requests" title="scrapy.pipelines.files.FilesPipeline.get_media_requests"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_media_requests()</span></code></a> method.</p>
<p>Here’s a typical value of the <code class="docutils literal notranslate"><span class="pre">results</span></code> argument:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[(</span><span class="kc">True</span><span class="p">,</span>
  <span class="p">{</span><span class="s1">&#39;checksum&#39;</span><span class="p">:</span> <span class="s1">&#39;2b00042f7481c7b056c4b410d28f33cf&#39;</span><span class="p">,</span>
   <span class="s1">&#39;path&#39;</span><span class="p">:</span> <span class="s1">&#39;full/0a79c461a4062ac383dc4fade7bc09f1384a3910.jpg&#39;</span><span class="p">,</span>
   <span class="s1">&#39;url&#39;</span><span class="p">:</span> <span class="s1">&#39;http://www.example.com/files/product1.pdf&#39;</span><span class="p">}),</span>
 <span class="p">(</span><span class="kc">False</span><span class="p">,</span>
  <span class="n">Failure</span><span class="p">(</span><span class="o">...</span><span class="p">))]</span>
</pre></div>
</div>
<p>By default the <a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.get_media_requests" title="scrapy.pipelines.files.FilesPipeline.get_media_requests"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_media_requests()</span></code></a> method returns <code class="docutils literal notranslate"><span class="pre">None</span></code> which
means there are no files to download for the item.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.pipelines.files.FilesPipeline.item_completed">
<code class="descname">item_completed</code><span class="sig-paren">(</span><em>results</em>, <em>item</em>, <em>info</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.pipelines.files.FilesPipeline.item_completed" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.item_completed" title="scrapy.pipelines.files.FilesPipeline.item_completed"><code class="xref py py-meth docutils literal notranslate"><span class="pre">FilesPipeline.item_completed()</span></code></a> method called when all file
requests for a single item have completed (either finished downloading, or
failed for some reason).</p>
<p>The <a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.item_completed" title="scrapy.pipelines.files.FilesPipeline.item_completed"><code class="xref py py-meth docutils literal notranslate"><span class="pre">item_completed()</span></code></a> method must return the
output that will be sent to subsequent item pipeline stages, so you must
return (or drop) the item, as you would in any pipeline.</p>
<p>Here is an example of the <a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.item_completed" title="scrapy.pipelines.files.FilesPipeline.item_completed"><code class="xref py py-meth docutils literal notranslate"><span class="pre">item_completed()</span></code></a> method where we
store the downloaded file paths (passed in results) in the <code class="docutils literal notranslate"><span class="pre">file_paths</span></code>
item field, and we drop the item if it doesn’t contain any files:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.exceptions</span> <span class="k">import</span> <span class="n">DropItem</span>

<span class="k">def</span> <span class="nf">item_completed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">info</span><span class="p">):</span>
    <span class="n">file_paths</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;path&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">ok</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">results</span> <span class="k">if</span> <span class="n">ok</span><span class="p">]</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">file_paths</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">DropItem</span><span class="p">(</span><span class="s2">&quot;Item contains no files&quot;</span><span class="p">)</span>
    <span class="n">item</span><span class="p">[</span><span class="s1">&#39;file_paths&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">file_paths</span>
    <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
<p>By default, the <a class="reference internal" href="#scrapy.pipelines.files.FilesPipeline.item_completed" title="scrapy.pipelines.files.FilesPipeline.item_completed"><code class="xref py py-meth docutils literal notranslate"><span class="pre">item_completed()</span></code></a> method returns the item.</p>
</dd></dl>

</dd></dl>

<span class="target" id="module-scrapy.pipelines.images"></span><p>See here the methods that you can override in your custom Images Pipeline:</p>
<dl class="class">
<dt id="scrapy.pipelines.images.ImagesPipeline">
<em class="property">class </em><code class="descclassname">scrapy.pipelines.images.</code><code class="descname">ImagesPipeline</code><a class="headerlink" href="#scrapy.pipelines.images.ImagesPipeline" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div>The <a class="reference internal" href="#scrapy.pipelines.images.ImagesPipeline" title="scrapy.pipelines.images.ImagesPipeline"><code class="xref py py-class docutils literal notranslate"><span class="pre">ImagesPipeline</span></code></a> is an extension of the <code class="xref py py-class docutils literal notranslate"><span class="pre">FilesPipeline</span></code>,
customizing the field names and adding custom behavior for images.</div></blockquote>
<dl class="method">
<dt id="scrapy.pipelines.images.ImagesPipeline.file_path">
<code class="descname">file_path</code><span class="sig-paren">(</span><em>request</em>, <em>response</em>, <em>info</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.pipelines.images.ImagesPipeline.file_path" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called once per downloaded item. It returns the
download path of the file originating from the specified
<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">response</span></code></a>.</p>
<p>In addition to <code class="docutils literal notranslate"><span class="pre">response</span></code>, this method receives the original
<code class="xref py py-class docutils literal notranslate"><span class="pre">request</span></code> and
<code class="xref py py-class docutils literal notranslate"><span class="pre">info</span></code>.</p>
<p>You can override this method to customize the download path of each file.</p>
<p>For example, if file URLs end like regular paths (e.g.
<code class="docutils literal notranslate"><span class="pre">https://example.com/a/b/c/foo.png</span></code>), you can use the following
approach to download all files into the <code class="docutils literal notranslate"><span class="pre">files</span></code> folder with their
original filenames (e.g. <code class="docutils literal notranslate"><span class="pre">files/foo.png</span></code>):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">urllib.parse</span> <span class="k">import</span> <span class="n">urlparse</span>

<span class="kn">from</span> <span class="nn">scrapy.pipelines.images</span> <span class="k">import</span> <span class="n">ImagesPipeline</span>

<span class="k">class</span> <span class="nc">MyImagesPipeline</span><span class="p">(</span><span class="n">ImagesPipeline</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">file_path</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">info</span><span class="p">):</span>
        <span class="k">return</span> <span class="s1">&#39;files/&#39;</span> <span class="o">+</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">urlparse</span><span class="p">(</span><span class="n">request</span><span class="o">.</span><span class="n">url</span><span class="p">)</span><span class="o">.</span><span class="n">path</span><span class="p">)</span>
</pre></div>
</div>
<p>By default the <a class="reference internal" href="#scrapy.pipelines.images.ImagesPipeline.file_path" title="scrapy.pipelines.images.ImagesPipeline.file_path"><code class="xref py py-meth docutils literal notranslate"><span class="pre">file_path()</span></code></a> method returns
<code class="docutils literal notranslate"><span class="pre">full/&lt;request</span> <span class="pre">URL</span> <span class="pre">hash&gt;.&lt;extension&gt;</span></code>.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.pipelines.images.ImagesPipeline.get_media_requests">
<code class="descname">get_media_requests</code><span class="sig-paren">(</span><em>item</em>, <em>info</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.pipelines.images.ImagesPipeline.get_media_requests" title="Permalink to this definition">¶</a></dt>
<dd><p>Works the same way as <code class="xref py py-meth docutils literal notranslate"><span class="pre">FilesPipeline.get_media_requests()</span></code> method,
but using a different field name for image urls.</p>
<p>Must return a Request for each image URL.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.pipelines.images.ImagesPipeline.item_completed">
<code class="descname">item_completed</code><span class="sig-paren">(</span><em>results</em>, <em>item</em>, <em>info</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.pipelines.images.ImagesPipeline.item_completed" title="Permalink to this definition">¶</a></dt>
<dd><p>The <a class="reference internal" href="#scrapy.pipelines.images.ImagesPipeline.item_completed" title="scrapy.pipelines.images.ImagesPipeline.item_completed"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ImagesPipeline.item_completed()</span></code></a> method is called when all image
requests for a single item have completed (either finished downloading, or
failed for some reason).</p>
<p>Works the same way as <code class="xref py py-meth docutils literal notranslate"><span class="pre">FilesPipeline.item_completed()</span></code> method,
but using a different field names for storing image downloading results.</p>
<p>By default, the <a class="reference internal" href="#scrapy.pipelines.images.ImagesPipeline.item_completed" title="scrapy.pipelines.images.ImagesPipeline.item_completed"><code class="xref py py-meth docutils literal notranslate"><span class="pre">item_completed()</span></code></a> method returns the item.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="custom-images-pipeline-example">
<h4>Custom Images pipeline example<a class="headerlink" href="#custom-images-pipeline-example" title="Permalink to this headline">¶</a></h4>
<p>Here is a full example of the Images Pipeline whose methods are examplified
above:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>
<span class="kn">from</span> <span class="nn">scrapy.pipelines.images</span> <span class="k">import</span> <span class="n">ImagesPipeline</span>
<span class="kn">from</span> <span class="nn">scrapy.exceptions</span> <span class="k">import</span> <span class="n">DropItem</span>

<span class="k">class</span> <span class="nc">MyImagesPipeline</span><span class="p">(</span><span class="n">ImagesPipeline</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">get_media_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">info</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">image_url</span> <span class="ow">in</span> <span class="n">item</span><span class="p">[</span><span class="s1">&#39;image_urls&#39;</span><span class="p">]:</span>
            <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">image_url</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">item_completed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">results</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">info</span><span class="p">):</span>
        <span class="n">image_paths</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;path&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">ok</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">results</span> <span class="k">if</span> <span class="n">ok</span><span class="p">]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">image_paths</span><span class="p">:</span>
            <span class="k">raise</span> <span class="n">DropItem</span><span class="p">(</span><span class="s2">&quot;Item contains no images&quot;</span><span class="p">)</span>
        <span class="n">item</span><span class="p">[</span><span class="s1">&#39;image_paths&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">image_paths</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
</div>
</div>
<span id="document-topics/deploy"></span><div class="section" id="deploying-spiders">
<span id="topics-deploy"></span><h3>Deploying Spiders<a class="headerlink" href="#deploying-spiders" title="Permalink to this headline">¶</a></h3>
<p>This section describes the different options you have for deploying your Scrapy
spiders to run them on a regular basis. Running Scrapy spiders in your local
machine is very convenient for the (early) development stage, but not so much
when you need to execute long-running spiders or move spiders to run in
production continuously. This is where the solutions for deploying Scrapy
spiders come in.</p>
<p>Popular choices for deploying Scrapy spiders are:</p>
<ul class="simple">
<li><a class="reference internal" href="#deploy-scrapyd"><span class="std std-ref">Scrapyd</span></a> (open source)</li>
<li><a class="reference internal" href="#deploy-scrapy-cloud"><span class="std std-ref">Scrapy Cloud</span></a> (cloud-based)</li>
</ul>
<div class="section" id="deploying-to-a-scrapyd-server">
<span id="deploy-scrapyd"></span><h4>Deploying to a Scrapyd Server<a class="headerlink" href="#deploying-to-a-scrapyd-server" title="Permalink to this headline">¶</a></h4>
<p><a class="reference external" href="https://github.com/scrapy/scrapyd">Scrapyd</a> is an open source application to run Scrapy spiders. It provides
a server with HTTP API, capable of running and monitoring Scrapy spiders.</p>
<p>To deploy spiders to Scrapyd, you can use the scrapyd-deploy tool provided by
the <a class="reference external" href="https://github.com/scrapy/scrapyd-client">scrapyd-client</a> package. Please refer to the <a class="reference external" href="https://scrapyd.readthedocs.io/en/latest/deploy.html">scrapyd-deploy
documentation</a> for more information.</p>
<p>Scrapyd is maintained by some of the Scrapy developers.</p>
</div>
<div class="section" id="deploying-to-scrapy-cloud">
<span id="deploy-scrapy-cloud"></span><h4>Deploying to Scrapy Cloud<a class="headerlink" href="#deploying-to-scrapy-cloud" title="Permalink to this headline">¶</a></h4>
<p><a class="reference external" href="https://scrapinghub.com/scrapy-cloud">Scrapy Cloud</a> is a hosted, cloud-based service by <a class="reference external" href="https://scrapinghub.com/">Scrapinghub</a>,
the company behind Scrapy.</p>
<p>Scrapy Cloud removes the need to setup and monitor servers
and provides a nice UI to manage spiders and review scraped items,
logs and stats.</p>
<p>To deploy spiders to Scrapy Cloud you can use the <a class="reference external" href="https://doc.scrapinghub.com/shub.html">shub</a> command line tool.
Please refer to the <a class="reference external" href="https://doc.scrapinghub.com/scrapy-cloud.html">Scrapy Cloud documentation</a> for more information.</p>
<p>Scrapy Cloud is compatible with Scrapyd and one can switch between
them as needed - the configuration is read from the <code class="docutils literal notranslate"><span class="pre">scrapy.cfg</span></code> file
just like <code class="docutils literal notranslate"><span class="pre">scrapyd-deploy</span></code>.</p>
</div>
</div>
<span id="document-topics/autothrottle"></span><div class="section" id="autothrottle-extension">
<span id="topics-autothrottle"></span><h3>AutoThrottle extension<a class="headerlink" href="#autothrottle-extension" title="Permalink to this headline">¶</a></h3>
<p>This is an extension for automatically throttling crawling speed based on load
of both the Scrapy server and the website you are crawling.</p>
<div class="section" id="design-goals">
<h4>Design goals<a class="headerlink" href="#design-goals" title="Permalink to this headline">¶</a></h4>
<ol class="arabic simple">
<li>be nicer to sites instead of using default download delay of zero</li>
<li>automatically adjust scrapy to the optimum crawling speed, so the user
doesn’t have to tune the download delays to find the optimum one.
The user only needs to specify the maximum concurrent requests
it allows, and the extension does the rest.</li>
</ol>
</div>
<div class="section" id="how-it-works">
<span id="autothrottle-algorithm"></span><h4>How it works<a class="headerlink" href="#how-it-works" title="Permalink to this headline">¶</a></h4>
<p>AutoThrottle extension adjusts download delays dynamically to make spider send
<a class="reference internal" href="#std:setting-AUTOTHROTTLE_TARGET_CONCURRENCY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code></a> concurrent requests on average
to each remote website.</p>
<p>It uses download latency to compute the delays. The main idea is the
following: if a server needs <code class="docutils literal notranslate"><span class="pre">latency</span></code> seconds to respond, a client
should send a request each <code class="docutils literal notranslate"><span class="pre">latency/N</span></code> seconds to have <code class="docutils literal notranslate"><span class="pre">N</span></code> requests
processed in parallel.</p>
<p>Instead of adjusting the delays one can just set a small fixed
download delay and impose hard limits on concurrency using
<a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a> or
<a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a> options. It will provide a similar
effect, but there are some important differences:</p>
<ul class="simple">
<li>because the download delay is small there will be occasional bursts
of requests;</li>
<li>often non-200 (error) responses can be returned faster than regular
responses, so with a small download delay and a hard concurrency limit
crawler will be sending requests to server faster when server starts to
return errors. But this is an opposite of what crawler should do - in case
of errors it makes more sense to slow down: these errors may be caused by
the high request rate.</li>
</ul>
<p>AutoThrottle doesn’t have these issues.</p>
</div>
<div class="section" id="throttling-algorithm">
<h4>Throttling algorithm<a class="headerlink" href="#throttling-algorithm" title="Permalink to this headline">¶</a></h4>
<p>AutoThrottle algorithm adjusts download delays based on the following rules:</p>
<ol class="arabic simple">
<li>spiders always start with a download delay of
<a class="reference internal" href="#std:setting-AUTOTHROTTLE_START_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AUTOTHROTTLE_START_DELAY</span></code></a>;</li>
<li>when a response is received, the target download delay is calculated as
<code class="docutils literal notranslate"><span class="pre">latency</span> <span class="pre">/</span> <span class="pre">N</span></code> where <code class="docutils literal notranslate"><span class="pre">latency</span></code> is a latency of the response,
and <code class="docutils literal notranslate"><span class="pre">N</span></code> is <a class="reference internal" href="#std:setting-AUTOTHROTTLE_TARGET_CONCURRENCY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code></a>.</li>
<li>download delay for next requests is set to the average of previous
download delay and the target download delay;</li>
<li>latencies of non-200 responses are not allowed to decrease the delay;</li>
<li>download delay can’t become less than <a class="reference internal" href="index.html#std:setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_DELAY</span></code></a> or greater
than <a class="reference internal" href="#std:setting-AUTOTHROTTLE_MAX_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AUTOTHROTTLE_MAX_DELAY</span></code></a></li>
</ol>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The AutoThrottle extension honours the standard Scrapy settings for
concurrency and delay. This means that it will respect
<a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a> and
<a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a> options and
never set a download delay lower than <a class="reference internal" href="index.html#std:setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_DELAY</span></code></a>.</p>
</div>
<p id="download-latency">In Scrapy, the download latency is measured as the time elapsed between
establishing the TCP connection and receiving the HTTP headers.</p>
<p>Note that these latencies are very hard to measure accurately in a cooperative
multitasking environment because Scrapy may be busy processing a spider
callback, for example, and unable to attend downloads. However, these latencies
should still give a reasonable estimate of how busy Scrapy (and ultimately, the
server) is, and this extension builds on that premise.</p>
</div>
<div class="section" id="settings">
<h4>Settings<a class="headerlink" href="#settings" title="Permalink to this headline">¶</a></h4>
<p>The settings used to control the AutoThrottle extension are:</p>
<ul class="simple">
<li><a class="reference internal" href="#std:setting-AUTOTHROTTLE_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AUTOTHROTTLE_ENABLED</span></code></a></li>
<li><a class="reference internal" href="#std:setting-AUTOTHROTTLE_START_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AUTOTHROTTLE_START_DELAY</span></code></a></li>
<li><a class="reference internal" href="#std:setting-AUTOTHROTTLE_MAX_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AUTOTHROTTLE_MAX_DELAY</span></code></a></li>
<li><a class="reference internal" href="#std:setting-AUTOTHROTTLE_TARGET_CONCURRENCY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code></a></li>
<li><a class="reference internal" href="#std:setting-AUTOTHROTTLE_DEBUG"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AUTOTHROTTLE_DEBUG</span></code></a></li>
<li><a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a></li>
<li><a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a></li>
<li><a class="reference internal" href="index.html#std:setting-DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_DELAY</span></code></a></li>
</ul>
<p>For more information see <a class="reference internal" href="#autothrottle-algorithm"><span class="std std-ref">How it works</span></a>.</p>
<div class="section" id="autothrottle-enabled">
<span id="std:setting-AUTOTHROTTLE_ENABLED"></span><h5>AUTOTHROTTLE_ENABLED<a class="headerlink" href="#autothrottle-enabled" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>Enables the AutoThrottle extension.</p>
</div>
<div class="section" id="autothrottle-start-delay">
<span id="std:setting-AUTOTHROTTLE_START_DELAY"></span><h5>AUTOTHROTTLE_START_DELAY<a class="headerlink" href="#autothrottle-start-delay" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">5.0</span></code></p>
<p>The initial download delay (in seconds).</p>
</div>
<div class="section" id="autothrottle-max-delay">
<span id="std:setting-AUTOTHROTTLE_MAX_DELAY"></span><h5>AUTOTHROTTLE_MAX_DELAY<a class="headerlink" href="#autothrottle-max-delay" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">60.0</span></code></p>
<p>The maximum download delay (in seconds) to be set in case of high latencies.</p>
</div>
<div class="section" id="autothrottle-target-concurrency">
<span id="std:setting-AUTOTHROTTLE_TARGET_CONCURRENCY"></span><h5>AUTOTHROTTLE_TARGET_CONCURRENCY<a class="headerlink" href="#autothrottle-target-concurrency" title="Permalink to this headline">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.1.</span></p>
</div>
<p>Default: <code class="docutils literal notranslate"><span class="pre">1.0</span></code></p>
<p>Average number of requests Scrapy should be sending in parallel to remote
websites.</p>
<p>By default, AutoThrottle adjusts the delay to send a single
concurrent request to each of the remote websites. Set this option to
a higher value (e.g. <code class="docutils literal notranslate"><span class="pre">2.0</span></code>) to increase the throughput and the load on remote
servers. A lower <code class="docutils literal notranslate"><span class="pre">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code> value
(e.g. <code class="docutils literal notranslate"><span class="pre">0.5</span></code>) makes the crawler more conservative and polite.</p>
<p>Note that <a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a>
and <a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a> options are still respected
when AutoThrottle extension is enabled. This means that if
<code class="docutils literal notranslate"><span class="pre">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code> is set to a value higher than
<a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a> or
<a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a>, the crawler won’t reach this number
of concurrent requests.</p>
<p>At every given time point Scrapy can be sending more or less concurrent
requests than <code class="docutils literal notranslate"><span class="pre">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code>; it is a suggested
value the crawler tries to approach, not a hard limit.</p>
</div>
<div class="section" id="autothrottle-debug">
<span id="std:setting-AUTOTHROTTLE_DEBUG"></span><h5>AUTOTHROTTLE_DEBUG<a class="headerlink" href="#autothrottle-debug" title="Permalink to this headline">¶</a></h5>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>Enable AutoThrottle debug mode which will display stats on every response
received, so you can see how the throttling parameters are being adjusted in
real time.</p>
</div>
</div>
</div>
<span id="document-topics/benchmarking"></span><div class="section" id="benchmarking">
<span id="id1"></span><h3>Benchmarking<a class="headerlink" href="#benchmarking" title="Permalink to this headline">¶</a></h3>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17.</span></p>
</div>
<p>Scrapy comes with a simple benchmarking suite that spawns a local HTTP server
and crawls it at the maximum possible speed. The goal of this benchmarking is
to get an idea of how Scrapy performs in your hardware, in order to have a
common baseline for comparisons. It uses a simple spider that does nothing and
just follows links.</p>
<p>To run it use:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">bench</span>
</pre></div>
</div>
<p>You should see an output like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">48</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">log</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Scrapy</span> <span class="mf">1.2</span><span class="o">.</span><span class="mi">2</span> <span class="n">started</span> <span class="p">(</span><span class="n">bot</span><span class="p">:</span> <span class="n">quotesbot</span><span class="p">)</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">48</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">log</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Overridden</span> <span class="n">settings</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;CLOSESPIDER_TIMEOUT&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span> <span class="s1">&#39;ROBOTSTXT_OBEY&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span> <span class="s1">&#39;SPIDER_MODULES&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;quotesbot.spiders&#39;</span><span class="p">],</span> <span class="s1">&#39;LOGSTATS_INTERVAL&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;BOT_NAME&#39;</span><span class="p">:</span> <span class="s1">&#39;quotesbot&#39;</span><span class="p">,</span> <span class="s1">&#39;LOG_LEVEL&#39;</span><span class="p">:</span> <span class="s1">&#39;INFO&#39;</span><span class="p">,</span> <span class="s1">&#39;NEWSPIDER_MODULE&#39;</span><span class="p">:</span> <span class="s1">&#39;quotesbot.spiders&#39;</span><span class="p">}</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">49</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">middleware</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Enabled</span> <span class="n">extensions</span><span class="p">:</span>
<span class="p">[</span><span class="s1">&#39;scrapy.extensions.closespider.CloseSpider&#39;</span><span class="p">,</span>
 <span class="s1">&#39;scrapy.extensions.logstats.LogStats&#39;</span><span class="p">,</span>
 <span class="s1">&#39;scrapy.extensions.telnet.TelnetConsole&#39;</span><span class="p">,</span>
 <span class="s1">&#39;scrapy.extensions.corestats.CoreStats&#39;</span><span class="p">]</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">49</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">middleware</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Enabled</span> <span class="n">downloader</span> <span class="n">middlewares</span><span class="p">:</span>
<span class="p">[</span><span class="s1">&#39;scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware&#39;</span><span class="p">,</span>
 <span class="s1">&#39;scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware&#39;</span><span class="p">,</span>
 <span class="s1">&#39;scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware&#39;</span><span class="p">,</span>
 <span class="s1">&#39;scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware&#39;</span><span class="p">,</span>
 <span class="s1">&#39;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&#39;</span><span class="p">,</span>
 <span class="s1">&#39;scrapy.downloadermiddlewares.retry.RetryMiddleware&#39;</span><span class="p">,</span>
 <span class="s1">&#39;scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware&#39;</span><span class="p">,</span>
 <span class="s1">&#39;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&#39;</span><span class="p">,</span>
 <span class="s1">&#39;scrapy.downloadermiddlewares.redirect.RedirectMiddleware&#39;</span><span class="p">,</span>
 <span class="s1">&#39;scrapy.downloadermiddlewares.cookies.CookiesMiddleware&#39;</span><span class="p">,</span>
 <span class="s1">&#39;scrapy.downloadermiddlewares.stats.DownloaderStats&#39;</span><span class="p">]</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">49</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">middleware</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Enabled</span> <span class="n">spider</span> <span class="n">middlewares</span><span class="p">:</span>
<span class="p">[</span><span class="s1">&#39;scrapy.spidermiddlewares.httperror.HttpErrorMiddleware&#39;</span><span class="p">,</span>
 <span class="s1">&#39;scrapy.spidermiddlewares.offsite.OffsiteMiddleware&#39;</span><span class="p">,</span>
 <span class="s1">&#39;scrapy.spidermiddlewares.referer.RefererMiddleware&#39;</span><span class="p">,</span>
 <span class="s1">&#39;scrapy.spidermiddlewares.urllength.UrlLengthMiddleware&#39;</span><span class="p">,</span>
 <span class="s1">&#39;scrapy.spidermiddlewares.depth.DepthMiddleware&#39;</span><span class="p">]</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">49</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">middleware</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Enabled</span> <span class="n">item</span> <span class="n">pipelines</span><span class="p">:</span>
<span class="p">[]</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">49</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">engine</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Spider</span> <span class="n">opened</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">49</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">extensions</span><span class="o">.</span><span class="n">logstats</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Crawled</span> <span class="mi">0</span> <span class="n">pages</span> <span class="p">(</span><span class="n">at</span> <span class="mi">0</span> <span class="n">pages</span><span class="o">/</span><span class="nb">min</span><span class="p">),</span> <span class="n">scraped</span> <span class="mi">0</span> <span class="n">items</span> <span class="p">(</span><span class="n">at</span> <span class="mi">0</span> <span class="n">items</span><span class="o">/</span><span class="nb">min</span><span class="p">)</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">50</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">extensions</span><span class="o">.</span><span class="n">logstats</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Crawled</span> <span class="mi">70</span> <span class="n">pages</span> <span class="p">(</span><span class="n">at</span> <span class="mi">4200</span> <span class="n">pages</span><span class="o">/</span><span class="nb">min</span><span class="p">),</span> <span class="n">scraped</span> <span class="mi">0</span> <span class="n">items</span> <span class="p">(</span><span class="n">at</span> <span class="mi">0</span> <span class="n">items</span><span class="o">/</span><span class="nb">min</span><span class="p">)</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">51</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">extensions</span><span class="o">.</span><span class="n">logstats</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Crawled</span> <span class="mi">134</span> <span class="n">pages</span> <span class="p">(</span><span class="n">at</span> <span class="mi">3840</span> <span class="n">pages</span><span class="o">/</span><span class="nb">min</span><span class="p">),</span> <span class="n">scraped</span> <span class="mi">0</span> <span class="n">items</span> <span class="p">(</span><span class="n">at</span> <span class="mi">0</span> <span class="n">items</span><span class="o">/</span><span class="nb">min</span><span class="p">)</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">52</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">extensions</span><span class="o">.</span><span class="n">logstats</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Crawled</span> <span class="mi">198</span> <span class="n">pages</span> <span class="p">(</span><span class="n">at</span> <span class="mi">3840</span> <span class="n">pages</span><span class="o">/</span><span class="nb">min</span><span class="p">),</span> <span class="n">scraped</span> <span class="mi">0</span> <span class="n">items</span> <span class="p">(</span><span class="n">at</span> <span class="mi">0</span> <span class="n">items</span><span class="o">/</span><span class="nb">min</span><span class="p">)</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">53</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">extensions</span><span class="o">.</span><span class="n">logstats</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Crawled</span> <span class="mi">254</span> <span class="n">pages</span> <span class="p">(</span><span class="n">at</span> <span class="mi">3360</span> <span class="n">pages</span><span class="o">/</span><span class="nb">min</span><span class="p">),</span> <span class="n">scraped</span> <span class="mi">0</span> <span class="n">items</span> <span class="p">(</span><span class="n">at</span> <span class="mi">0</span> <span class="n">items</span><span class="o">/</span><span class="nb">min</span><span class="p">)</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">54</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">extensions</span><span class="o">.</span><span class="n">logstats</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Crawled</span> <span class="mi">302</span> <span class="n">pages</span> <span class="p">(</span><span class="n">at</span> <span class="mi">2880</span> <span class="n">pages</span><span class="o">/</span><span class="nb">min</span><span class="p">),</span> <span class="n">scraped</span> <span class="mi">0</span> <span class="n">items</span> <span class="p">(</span><span class="n">at</span> <span class="mi">0</span> <span class="n">items</span><span class="o">/</span><span class="nb">min</span><span class="p">)</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">55</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">extensions</span><span class="o">.</span><span class="n">logstats</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Crawled</span> <span class="mi">358</span> <span class="n">pages</span> <span class="p">(</span><span class="n">at</span> <span class="mi">3360</span> <span class="n">pages</span><span class="o">/</span><span class="nb">min</span><span class="p">),</span> <span class="n">scraped</span> <span class="mi">0</span> <span class="n">items</span> <span class="p">(</span><span class="n">at</span> <span class="mi">0</span> <span class="n">items</span><span class="o">/</span><span class="nb">min</span><span class="p">)</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">56</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">extensions</span><span class="o">.</span><span class="n">logstats</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Crawled</span> <span class="mi">406</span> <span class="n">pages</span> <span class="p">(</span><span class="n">at</span> <span class="mi">2880</span> <span class="n">pages</span><span class="o">/</span><span class="nb">min</span><span class="p">),</span> <span class="n">scraped</span> <span class="mi">0</span> <span class="n">items</span> <span class="p">(</span><span class="n">at</span> <span class="mi">0</span> <span class="n">items</span><span class="o">/</span><span class="nb">min</span><span class="p">)</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">57</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">extensions</span><span class="o">.</span><span class="n">logstats</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Crawled</span> <span class="mi">438</span> <span class="n">pages</span> <span class="p">(</span><span class="n">at</span> <span class="mi">1920</span> <span class="n">pages</span><span class="o">/</span><span class="nb">min</span><span class="p">),</span> <span class="n">scraped</span> <span class="mi">0</span> <span class="n">items</span> <span class="p">(</span><span class="n">at</span> <span class="mi">0</span> <span class="n">items</span><span class="o">/</span><span class="nb">min</span><span class="p">)</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">58</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">extensions</span><span class="o">.</span><span class="n">logstats</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Crawled</span> <span class="mi">470</span> <span class="n">pages</span> <span class="p">(</span><span class="n">at</span> <span class="mi">1920</span> <span class="n">pages</span><span class="o">/</span><span class="nb">min</span><span class="p">),</span> <span class="n">scraped</span> <span class="mi">0</span> <span class="n">items</span> <span class="p">(</span><span class="n">at</span> <span class="mi">0</span> <span class="n">items</span><span class="o">/</span><span class="nb">min</span><span class="p">)</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">59</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">engine</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Closing</span> <span class="n">spider</span> <span class="p">(</span><span class="n">closespider_timeout</span><span class="p">)</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">18</span><span class="p">:</span><span class="mi">59</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">extensions</span><span class="o">.</span><span class="n">logstats</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Crawled</span> <span class="mi">518</span> <span class="n">pages</span> <span class="p">(</span><span class="n">at</span> <span class="mi">2880</span> <span class="n">pages</span><span class="o">/</span><span class="nb">min</span><span class="p">),</span> <span class="n">scraped</span> <span class="mi">0</span> <span class="n">items</span> <span class="p">(</span><span class="n">at</span> <span class="mi">0</span> <span class="n">items</span><span class="o">/</span><span class="nb">min</span><span class="p">)</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">19</span><span class="p">:</span><span class="mi">00</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">statscollectors</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Dumping</span> <span class="n">Scrapy</span> <span class="n">stats</span><span class="p">:</span>
<span class="p">{</span><span class="s1">&#39;downloader/request_bytes&#39;</span><span class="p">:</span> <span class="mi">229995</span><span class="p">,</span>
 <span class="s1">&#39;downloader/request_count&#39;</span><span class="p">:</span> <span class="mi">534</span><span class="p">,</span>
 <span class="s1">&#39;downloader/request_method_count/GET&#39;</span><span class="p">:</span> <span class="mi">534</span><span class="p">,</span>
 <span class="s1">&#39;downloader/response_bytes&#39;</span><span class="p">:</span> <span class="mi">1565504</span><span class="p">,</span>
 <span class="s1">&#39;downloader/response_count&#39;</span><span class="p">:</span> <span class="mi">534</span><span class="p">,</span>
 <span class="s1">&#39;downloader/response_status_count/200&#39;</span><span class="p">:</span> <span class="mi">534</span><span class="p">,</span>
 <span class="s1">&#39;finish_reason&#39;</span><span class="p">:</span> <span class="s1">&#39;closespider_timeout&#39;</span><span class="p">,</span>
 <span class="s1">&#39;finish_time&#39;</span><span class="p">:</span> <span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2016</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">19</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">647725</span><span class="p">),</span>
 <span class="s1">&#39;log_count/INFO&#39;</span><span class="p">:</span> <span class="mi">17</span><span class="p">,</span>
 <span class="s1">&#39;request_depth_max&#39;</span><span class="p">:</span> <span class="mi">19</span><span class="p">,</span>
 <span class="s1">&#39;response_received_count&#39;</span><span class="p">:</span> <span class="mi">534</span><span class="p">,</span>
 <span class="s1">&#39;scheduler/dequeued&#39;</span><span class="p">:</span> <span class="mi">533</span><span class="p">,</span>
 <span class="s1">&#39;scheduler/dequeued/memory&#39;</span><span class="p">:</span> <span class="mi">533</span><span class="p">,</span>
 <span class="s1">&#39;scheduler/enqueued&#39;</span><span class="p">:</span> <span class="mi">10661</span><span class="p">,</span>
 <span class="s1">&#39;scheduler/enqueued/memory&#39;</span><span class="p">:</span> <span class="mi">10661</span><span class="p">,</span>
 <span class="s1">&#39;start_time&#39;</span><span class="p">:</span> <span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="p">(</span><span class="mi">2016</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">49</span><span class="p">,</span> <span class="mi">799869</span><span class="p">)}</span>
<span class="mi">2016</span><span class="o">-</span><span class="mi">12</span><span class="o">-</span><span class="mi">16</span> <span class="mi">21</span><span class="p">:</span><span class="mi">19</span><span class="p">:</span><span class="mi">00</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">engine</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Spider</span> <span class="n">closed</span> <span class="p">(</span><span class="n">closespider_timeout</span><span class="p">)</span>
</pre></div>
</div>
<p>That tells you that Scrapy is able to crawl about 3000 pages per minute in the
hardware where you run it. Note that this is a very simple spider intended to
follow links, any custom spider you write will probably do more stuff which
results in slower crawl rates. How slower depends on how much your spider does
and how well it’s written.</p>
<p>In the future, more cases will be added to the benchmarking suite to cover
other common scenarios.</p>
</div>
<span id="document-topics/jobs"></span><div class="section" id="jobs-pausing-and-resuming-crawls">
<span id="topics-jobs"></span><h3>Jobs: pausing and resuming crawls<a class="headerlink" href="#jobs-pausing-and-resuming-crawls" title="Permalink to this headline">¶</a></h3>
<p>Sometimes, for big sites, it’s desirable to pause crawls and be able to resume
them later.</p>
<p>Scrapy supports this functionality out of the box by providing the following
facilities:</p>
<ul class="simple">
<li>a scheduler that persists scheduled requests on disk</li>
<li>a duplicates filter that persists visited requests on disk</li>
<li>an extension that keeps some spider state (key/value pairs) persistent
between batches</li>
</ul>
<div class="section" id="job-directory">
<h4>Job directory<a class="headerlink" href="#job-directory" title="Permalink to this headline">¶</a></h4>
<p>To enable persistence support you just need to define a <em>job directory</em> through
the <code class="docutils literal notranslate"><span class="pre">JOBDIR</span></code> setting. This directory will be for storing all required data to
keep the state of a single job (ie. a spider run).  It’s important to note that
this directory must not be shared by different spiders, or even different
jobs/runs of the same spider, as it’s meant to be used for storing the state of
a <em>single</em> job.</p>
</div>
<div class="section" id="how-to-use-it">
<h4>How to use it<a class="headerlink" href="#how-to-use-it" title="Permalink to this headline">¶</a></h4>
<p>To start a spider with persistence support enabled, run it like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">crawl</span> <span class="n">somespider</span> <span class="o">-</span><span class="n">s</span> <span class="n">JOBDIR</span><span class="o">=</span><span class="n">crawls</span><span class="o">/</span><span class="n">somespider</span><span class="o">-</span><span class="mi">1</span>
</pre></div>
</div>
<p>Then, you can stop the spider safely at any time (by pressing Ctrl-C or sending
a signal), and resume it later by issuing the same command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span> <span class="n">crawl</span> <span class="n">somespider</span> <span class="o">-</span><span class="n">s</span> <span class="n">JOBDIR</span><span class="o">=</span><span class="n">crawls</span><span class="o">/</span><span class="n">somespider</span><span class="o">-</span><span class="mi">1</span>
</pre></div>
</div>
</div>
<div class="section" id="keeping-persistent-state-between-batches">
<h4>Keeping persistent state between batches<a class="headerlink" href="#keeping-persistent-state-between-batches" title="Permalink to this headline">¶</a></h4>
<p>Sometimes you’ll want to keep some persistent spider state between pause/resume
batches. You can use the <code class="docutils literal notranslate"><span class="pre">spider.state</span></code> attribute for that, which should be a
dict. There’s a built-in extension that takes care of serializing, storing and
loading that attribute from the job directory, when the spider starts and
stops.</p>
<p>Here’s an example of a callback that uses the spider state (other spider code
is omitted for brevity):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">parse_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="c1"># parse item here</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="s1">&#39;items_count&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;items_count&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
<div class="section" id="persistence-gotchas">
<h4>Persistence gotchas<a class="headerlink" href="#persistence-gotchas" title="Permalink to this headline">¶</a></h4>
<p>There are a few things to keep in mind if you want to be able to use the Scrapy
persistence support:</p>
<div class="section" id="cookies-expiration">
<h5>Cookies expiration<a class="headerlink" href="#cookies-expiration" title="Permalink to this headline">¶</a></h5>
<p>Cookies may expire. So, if you don’t resume your spider quickly the requests
scheduled may no longer work. This won’t be an issue if you spider doesn’t rely
on cookies.</p>
</div>
<div class="section" id="request-serialization">
<h5>Request serialization<a class="headerlink" href="#request-serialization" title="Permalink to this headline">¶</a></h5>
<p>Requests must be serializable by the <code class="docutils literal notranslate"><span class="pre">pickle</span></code> module, in order for persistence
to work, so you should make sure that your requests are serializable.</p>
<p>The most common issue here is to use <code class="docutils literal notranslate"><span class="pre">lambda</span></code> functions on request callbacks that
can’t be persisted.</p>
<p>So, for example, this won’t work:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">some_callback</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="n">somearg</span> <span class="o">=</span> <span class="s1">&#39;test&#39;</span>
    <span class="k">return</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="s1">&#39;http://www.example.com&#39;</span><span class="p">,</span>
                          <span class="n">callback</span><span class="o">=</span><span class="k">lambda</span> <span class="n">r</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">other_callback</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">somearg</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">other_callback</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">somearg</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;the argument passed is: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">somearg</span><span class="p">)</span>
</pre></div>
</div>
<p>But this will:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">some_callback</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="n">somearg</span> <span class="o">=</span> <span class="s1">&#39;test&#39;</span>
    <span class="k">return</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="s1">&#39;http://www.example.com&#39;</span><span class="p">,</span>
                          <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">other_callback</span><span class="p">,</span> <span class="n">cb_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;somearg&#39;</span><span class="p">:</span> <span class="n">somearg</span><span class="p">})</span>

<span class="k">def</span> <span class="nf">other_callback</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">,</span> <span class="n">somearg</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;the argument passed is: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">somearg</span><span class="p">)</span>
</pre></div>
</div>
<p>If you wish to log the requests that couldn’t be serialized, you can set the
<a class="reference internal" href="index.html#std:setting-SCHEDULER_DEBUG"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SCHEDULER_DEBUG</span></code></a> setting to <code class="docutils literal notranslate"><span class="pre">True</span></code> in the project’s settings page.
It is <code class="docutils literal notranslate"><span class="pre">False</span></code> by default.</p>
</div>
</div>
</div>
</div>
<dl class="docutils">
<dt><a class="reference internal" href="index.html#document-faq"><span class="doc">Frequently Asked Questions</span></a></dt>
<dd>Get answers to most frequently asked questions.</dd>
<dt><a class="reference internal" href="index.html#document-topics/debug"><span class="doc">Debugging Spiders</span></a></dt>
<dd>Learn how to debug common problems of your scrapy spider.</dd>
<dt><a class="reference internal" href="index.html#document-topics/contracts"><span class="doc">Spiders Contracts</span></a></dt>
<dd>Learn how to use contracts for testing your spiders.</dd>
<dt><a class="reference internal" href="index.html#document-topics/practices"><span class="doc">Common Practices</span></a></dt>
<dd>Get familiar with some Scrapy common practices.</dd>
<dt><a class="reference internal" href="index.html#document-topics/broad-crawls"><span class="doc">Broad Crawls</span></a></dt>
<dd>Tune Scrapy for crawling a lot domains in parallel.</dd>
<dt><a class="reference internal" href="index.html#document-topics/developer-tools"><span class="doc">Using your browser’s Developer Tools for scraping</span></a></dt>
<dd>Learn how to scrape with your browser’s developer tools.</dd>
<dt><a class="reference internal" href="index.html#document-topics/dynamic-content"><span class="doc">Selecting dynamically-loaded content</span></a></dt>
<dd>Read webpage data that is loaded dynamically.</dd>
<dt><a class="reference internal" href="index.html#document-topics/leaks"><span class="doc">Debugging memory leaks</span></a></dt>
<dd>Learn how to find and get rid of memory leaks in your crawler.</dd>
<dt><a class="reference internal" href="index.html#document-topics/media-pipeline"><span class="doc">Downloading and processing files and images</span></a></dt>
<dd>Download files and/or images associated with your scraped items.</dd>
<dt><a class="reference internal" href="index.html#document-topics/deploy"><span class="doc">Deploying Spiders</span></a></dt>
<dd>Deploying your Scrapy spiders and run them in a remote server.</dd>
<dt><a class="reference internal" href="index.html#document-topics/autothrottle"><span class="doc">AutoThrottle extension</span></a></dt>
<dd>Adjust crawl rate dynamically based on load.</dd>
<dt><a class="reference internal" href="index.html#document-topics/benchmarking"><span class="doc">Benchmarking</span></a></dt>
<dd>Check how Scrapy performs on your hardware.</dd>
<dt><a class="reference internal" href="index.html#document-topics/jobs"><span class="doc">Jobs: pausing and resuming crawls</span></a></dt>
<dd>Learn how to pause and resume crawls for large spiders.</dd>
</dl>
</div>
<div class="section" id="extending-scrapy">
<span id="id1"></span><h2>Extending Scrapy<a class="headerlink" href="#extending-scrapy" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-topics/architecture"></span><div class="section" id="architecture-overview">
<span id="topics-architecture"></span><h3>Architecture overview<a class="headerlink" href="#architecture-overview" title="Permalink to this headline">¶</a></h3>
<p>This document describes the architecture of Scrapy and how its components
interact.</p>
<div class="section" id="overview">
<h4>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h4>
<p>The following diagram shows an overview of the Scrapy architecture with its
components and an outline of the data flow that takes place inside the system
(shown by the red arrows). A brief description of the components is included
below with links for more detailed information about them. The data flow is
also described below.</p>
</div>
<div class="section" id="data-flow">
<span id="id1"></span><h4>Data flow<a class="headerlink" href="#data-flow" title="Permalink to this headline">¶</a></h4>
<a class="reference internal image-reference" href="_images/scrapy_architecture_02.png"><img alt="Scrapy architecture" src="_images/scrapy_architecture_02.png" style="width: 700px; height: 470px;" /></a>
<p>The data flow in Scrapy is controlled by the execution engine, and goes like
this:</p>
<ol class="arabic simple">
<li>The <a class="reference internal" href="#component-engine"><span class="std std-ref">Engine</span></a> gets the initial Requests to crawl from the
<a class="reference internal" href="#component-spiders"><span class="std std-ref">Spider</span></a>.</li>
<li>The <a class="reference internal" href="#component-engine"><span class="std std-ref">Engine</span></a> schedules the Requests in the
<a class="reference internal" href="#component-scheduler"><span class="std std-ref">Scheduler</span></a> and asks for the
next Requests to crawl.</li>
<li>The <a class="reference internal" href="#component-scheduler"><span class="std std-ref">Scheduler</span></a> returns the next Requests
to the <a class="reference internal" href="#component-engine"><span class="std std-ref">Engine</span></a>.</li>
<li>The <a class="reference internal" href="#component-engine"><span class="std std-ref">Engine</span></a> sends the Requests to the
<a class="reference internal" href="#component-downloader"><span class="std std-ref">Downloader</span></a>, passing through the
<a class="reference internal" href="#component-downloader-middleware"><span class="std std-ref">Downloader Middlewares</span></a> (see
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_request()</span></code></a>).</li>
<li>Once the page finishes downloading the
<a class="reference internal" href="#component-downloader"><span class="std std-ref">Downloader</span></a> generates a Response (with
that page) and sends it to the Engine, passing through the
<a class="reference internal" href="#component-downloader-middleware"><span class="std std-ref">Downloader Middlewares</span></a> (see
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_response()</span></code></a>).</li>
<li>The <a class="reference internal" href="#component-engine"><span class="std std-ref">Engine</span></a> receives the Response from the
<a class="reference internal" href="#component-downloader"><span class="std std-ref">Downloader</span></a> and sends it to the
<a class="reference internal" href="#component-spiders"><span class="std std-ref">Spider</span></a> for processing, passing
through the <a class="reference internal" href="#component-spider-middleware"><span class="std std-ref">Spider Middleware</span></a> (see
<a class="reference internal" href="index.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_input()</span></code></a>).</li>
<li>The <a class="reference internal" href="#component-spiders"><span class="std std-ref">Spider</span></a> processes the Response and returns
scraped items and new Requests (to follow) to the
<a class="reference internal" href="#component-engine"><span class="std std-ref">Engine</span></a>, passing through the
<a class="reference internal" href="#component-spider-middleware"><span class="std std-ref">Spider Middleware</span></a> (see
<a class="reference internal" href="index.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_output()</span></code></a>).</li>
<li>The <a class="reference internal" href="#component-engine"><span class="std std-ref">Engine</span></a> sends processed items to
<a class="reference internal" href="#component-pipelines"><span class="std std-ref">Item Pipelines</span></a>, then send processed Requests to
the <a class="reference internal" href="#component-scheduler"><span class="std std-ref">Scheduler</span></a> and asks for possible next Requests
to crawl.</li>
<li>The process repeats (from step 1) until there are no more requests from the
<a class="reference internal" href="#component-scheduler"><span class="std std-ref">Scheduler</span></a>.</li>
</ol>
</div>
<div class="section" id="components">
<h4>Components<a class="headerlink" href="#components" title="Permalink to this headline">¶</a></h4>
<div class="section" id="scrapy-engine">
<span id="component-engine"></span><h5>Scrapy Engine<a class="headerlink" href="#scrapy-engine" title="Permalink to this headline">¶</a></h5>
<p>The engine is responsible for controlling the data flow between all components
of the system, and triggering events when certain actions occur. See the
<a class="reference internal" href="#data-flow"><span class="std std-ref">Data Flow</span></a> section above for more details.</p>
</div>
<div class="section" id="scheduler">
<span id="component-scheduler"></span><h5>Scheduler<a class="headerlink" href="#scheduler" title="Permalink to this headline">¶</a></h5>
<p>The Scheduler receives requests from the engine and enqueues them for feeding
them later (also to the engine) when the engine requests them.</p>
</div>
<div class="section" id="downloader">
<span id="component-downloader"></span><h5>Downloader<a class="headerlink" href="#downloader" title="Permalink to this headline">¶</a></h5>
<p>The Downloader is responsible for fetching web pages and feeding them to the
engine which, in turn, feeds them to the spiders.</p>
</div>
<div class="section" id="spiders">
<span id="component-spiders"></span><h5>Spiders<a class="headerlink" href="#spiders" title="Permalink to this headline">¶</a></h5>
<p>Spiders are custom classes written by Scrapy users to parse responses and
extract items (aka scraped items) from them or additional requests to
follow. For more information see <a class="reference internal" href="index.html#topics-spiders"><span class="std std-ref">Spiders</span></a>.</p>
</div>
<div class="section" id="item-pipeline">
<span id="component-pipelines"></span><h5>Item Pipeline<a class="headerlink" href="#item-pipeline" title="Permalink to this headline">¶</a></h5>
<p>The Item Pipeline is responsible for processing the items once they have been
extracted (or scraped) by the spiders. Typical tasks include cleansing,
validation and persistence (like storing the item in a database). For more
information see <a class="reference internal" href="index.html#topics-item-pipeline"><span class="std std-ref">Item Pipeline</span></a>.</p>
</div>
<div class="section" id="downloader-middlewares">
<span id="component-downloader-middleware"></span><h5>Downloader middlewares<a class="headerlink" href="#downloader-middlewares" title="Permalink to this headline">¶</a></h5>
<p>Downloader middlewares are specific hooks that sit between the Engine and the
Downloader and process requests when they pass from the Engine to the
Downloader, and responses that pass from Downloader to the Engine.</p>
<p>Use a Downloader middleware if you need to do one of the following:</p>
<ul class="simple">
<li>process a request just before it is sent to the Downloader
(i.e. right before Scrapy sends the request to the website);</li>
<li>change received response before passing it to a spider;</li>
<li>send a new Request instead of passing received response to a spider;</li>
<li>pass response to a spider without fetching a web page;</li>
<li>silently drop some requests.</li>
</ul>
<p>For more information see <a class="reference internal" href="index.html#topics-downloader-middleware"><span class="std std-ref">Downloader Middleware</span></a>.</p>
</div>
<div class="section" id="spider-middlewares">
<span id="component-spider-middleware"></span><h5>Spider middlewares<a class="headerlink" href="#spider-middlewares" title="Permalink to this headline">¶</a></h5>
<p>Spider middlewares are specific hooks that sit between the Engine and the
Spiders and are able to process spider input (responses) and output (items and
requests).</p>
<p>Use a Spider middleware if you need to</p>
<ul class="simple">
<li>post-process output of spider callbacks - change/add/remove requests or items;</li>
<li>post-process start_requests;</li>
<li>handle spider exceptions;</li>
<li>call errback instead of callback for some of the requests based on response
content.</li>
</ul>
<p>For more information see <a class="reference internal" href="index.html#topics-spider-middleware"><span class="std std-ref">Spider Middleware</span></a>.</p>
</div>
</div>
<div class="section" id="event-driven-networking">
<h4>Event-driven networking<a class="headerlink" href="#event-driven-networking" title="Permalink to this headline">¶</a></h4>
<p>Scrapy is written with <a class="reference external" href="https://twistedmatrix.com/trac/">Twisted</a>, a popular event-driven networking framework
for Python. Thus, it’s implemented using a non-blocking (aka asynchronous) code
for concurrency.</p>
<p>For more information about asynchronous programming and Twisted see these
links:</p>
<ul class="simple">
<li><a class="reference external" href="https://twistedmatrix.com/documents/current/core/howto/defer-intro.html">Introduction to Deferreds in Twisted</a></li>
<li><a class="reference external" href="http://jessenoller.com/blog/2009/02/11/twisted-hello-asynchronous-programming/">Twisted - hello, asynchronous programming</a></li>
<li><a class="reference external" href="http://krondo.com/an-introduction-to-asynchronous-programming-and-twisted/">Twisted Introduction - Krondo</a></li>
</ul>
</div>
</div>
<span id="document-topics/downloader-middleware"></span><div class="section" id="downloader-middleware">
<span id="topics-downloader-middleware"></span><h3>Downloader Middleware<a class="headerlink" href="#downloader-middleware" title="Permalink to this headline">¶</a></h3>
<p>The downloader middleware is a framework of hooks into Scrapy’s
request/response processing.  It’s a light, low-level system for globally
altering Scrapy’s requests and responses.</p>
<div class="section" id="activating-a-downloader-middleware">
<span id="topics-downloader-middleware-setting"></span><h4>Activating a downloader middleware<a class="headerlink" href="#activating-a-downloader-middleware" title="Permalink to this headline">¶</a></h4>
<p>To activate a downloader middleware component, add it to the
<a class="reference internal" href="index.html#std:setting-DOWNLOADER_MIDDLEWARES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_MIDDLEWARES</span></code></a> setting, which is a dict whose keys are the
middleware class paths and their values are the middleware orders.</p>
<p>Here’s an example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">DOWNLOADER_MIDDLEWARES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;myproject.middlewares.CustomDownloaderMiddleware&#39;</span><span class="p">:</span> <span class="mi">543</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The <a class="reference internal" href="index.html#std:setting-DOWNLOADER_MIDDLEWARES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_MIDDLEWARES</span></code></a> setting is merged with the
<a class="reference internal" href="index.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_MIDDLEWARES_BASE</span></code></a> setting defined in Scrapy (and not meant
to be overridden) and then sorted by order to get the final sorted list of
enabled middlewares: the first middleware is the one closer to the engine and
the last is the one closer to the downloader. In other words,
the <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_request()</span></code></a>
method of each middleware will be invoked in increasing
middleware order (100, 200, 300, …) and the <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_response()</span></code></a> method
of each middleware will be invoked in decreasing order.</p>
<p>To decide which order to assign to your middleware see the
<a class="reference internal" href="index.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_MIDDLEWARES_BASE</span></code></a> setting and pick a value according to
where you want to insert the middleware. The order does matter because each
middleware performs a different action and your middleware could depend on some
previous (or subsequent) middleware being applied.</p>
<p>If you want to disable a built-in middleware (the ones defined in
<a class="reference internal" href="index.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_MIDDLEWARES_BASE</span></code></a> and enabled by default) you must define it
in your project’s <a class="reference internal" href="index.html#std:setting-DOWNLOADER_MIDDLEWARES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_MIDDLEWARES</span></code></a> setting and assign <code class="docutils literal notranslate"><span class="pre">None</span></code>
as its value.  For example, if you want to disable the user-agent middleware:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">DOWNLOADER_MIDDLEWARES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;myproject.middlewares.CustomDownloaderMiddleware&#39;</span><span class="p">:</span> <span class="mi">543</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Finally, keep in mind that some middlewares may need to be enabled through a
particular setting. See each middleware documentation for more info.</p>
</div>
<div class="section" id="writing-your-own-downloader-middleware">
<span id="topics-downloader-middleware-custom"></span><h4>Writing your own downloader middleware<a class="headerlink" href="#writing-your-own-downloader-middleware" title="Permalink to this headline">¶</a></h4>
<p>Each downloader middleware is a Python class that defines one or more of the
methods defined below.</p>
<p>The main entry point is the <code class="docutils literal notranslate"><span class="pre">from_crawler</span></code> class method, which receives a
<a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> instance. The <a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a>
object gives you access, for example, to the <a class="reference internal" href="index.html#topics-settings"><span class="std std-ref">settings</span></a>.</p>
<span class="target" id="module-scrapy.downloadermiddlewares"></span><dl class="class">
<dt id="scrapy.downloadermiddlewares.DownloaderMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.downloadermiddlewares.</code><code class="descname">DownloaderMiddleware</code><a class="headerlink" href="#scrapy.downloadermiddlewares.DownloaderMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Any of the downloader middleware methods may also return a deferred.</p>
</div>
<dl class="method">
<dt id="scrapy.downloadermiddlewares.DownloaderMiddleware.process_request">
<code class="descname">process_request</code><span class="sig-paren">(</span><em>request</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called for each request that goes through the download
middleware.</p>
<p><a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_request()</span></code></a> should either: return <code class="docutils literal notranslate"><span class="pre">None</span></code>, return a
<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object, return a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a>
object, or raise <a class="reference internal" href="index.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><code class="xref py py-exc docutils literal notranslate"><span class="pre">IgnoreRequest</span></code></a>.</p>
<p>If it returns <code class="docutils literal notranslate"><span class="pre">None</span></code>, Scrapy will continue processing this request, executing all
other middlewares until, finally, the appropriate downloader handler is called
the request performed (and its response downloaded).</p>
<p>If it returns a <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object, Scrapy won’t bother
calling <em>any</em> other <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_request()</span></code></a> or <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_exception()</span></code></a> methods,
or the appropriate download function; it’ll return that response. The <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_response()</span></code></a>
methods of installed middleware is always called on every response.</p>
<p>If it returns a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object, Scrapy will stop calling
process_request methods and reschedule the returned request. Once the newly returned
request is performed, the appropriate middleware chain will be called on
the downloaded response.</p>
<p>If it raises an <a class="reference internal" href="index.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><code class="xref py py-exc docutils literal notranslate"><span class="pre">IgnoreRequest</span></code></a> exception, the
<a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_exception()</span></code></a> methods of installed downloader middleware will be called.
If none of them handle the exception, the errback function of the request
(<code class="docutils literal notranslate"><span class="pre">Request.errback</span></code>) is called. If no code handles the raised exception, it is
ignored and not logged (unlike other exceptions).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>request</strong> (<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object) – the request being processed</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider for which this request is intended</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.downloadermiddlewares.DownloaderMiddleware.process_response">
<code class="descname">process_response</code><span class="sig-paren">(</span><em>request</em>, <em>response</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_response()</span></code></a> should either: return a <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a>
object, return a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object or
raise a <a class="reference internal" href="index.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><code class="xref py py-exc docutils literal notranslate"><span class="pre">IgnoreRequest</span></code></a> exception.</p>
<p>If it returns a <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> (it could be the same given
response, or a brand-new one), that response will continue to be processed
with the <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_response()</span></code></a> of the next middleware in the chain.</p>
<p>If it returns a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object, the middleware chain is
halted and the returned request is rescheduled to be downloaded in the future.
This is the same behavior as if a request is returned from <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_request()</span></code></a>.</p>
<p>If it raises an <a class="reference internal" href="index.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><code class="xref py py-exc docutils literal notranslate"><span class="pre">IgnoreRequest</span></code></a> exception, the errback
function of the request (<code class="docutils literal notranslate"><span class="pre">Request.errback</span></code>) is called. If no code handles the raised
exception, it is ignored and not logged (unlike other exceptions).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>request</strong> (is a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object) – the request that originated the response</li>
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object) – the response being processed</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider for which this response is intended</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception">
<code class="descname">process_exception</code><span class="sig-paren">(</span><em>request</em>, <em>exception</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="Permalink to this definition">¶</a></dt>
<dd><p>Scrapy calls <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_exception()</span></code></a> when a download handler
or a <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_request()</span></code></a> (from a downloader middleware) raises an
exception (including an <a class="reference internal" href="index.html#scrapy.exceptions.IgnoreRequest" title="scrapy.exceptions.IgnoreRequest"><code class="xref py py-exc docutils literal notranslate"><span class="pre">IgnoreRequest</span></code></a> exception)</p>
<p><a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_exception()</span></code></a> should return: either <code class="docutils literal notranslate"><span class="pre">None</span></code>,
a <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object, or a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object.</p>
<p>If it returns <code class="docutils literal notranslate"><span class="pre">None</span></code>, Scrapy will continue processing this exception,
executing any other <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_exception()</span></code></a> methods of installed middleware,
until no middleware is left and the default exception handling kicks in.</p>
<p>If it returns a <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object, the <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_response()</span></code></a>
method chain of installed middleware is started, and Scrapy won’t bother calling
any other <a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_exception()</span></code></a> methods of middleware.</p>
<p>If it returns a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object, the returned request is
rescheduled to be downloaded in the future. This stops the execution of
<a class="reference internal" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_exception()</span></code></a> methods of the middleware the same as returning a
response would.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>request</strong> (is a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object) – the request that generated the exception</li>
<li><strong>exception</strong> (an <code class="docutils literal notranslate"><span class="pre">Exception</span></code> object) – the raised exception</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider for which this request is intended</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.downloadermiddlewares.DownloaderMiddleware.from_crawler">
<code class="descname">from_crawler</code><span class="sig-paren">(</span><em>cls</em>, <em>crawler</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.downloadermiddlewares.DownloaderMiddleware.from_crawler" title="Permalink to this definition">¶</a></dt>
<dd><p>If present, this classmethod is called to create a middleware instance
from a <a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a>. It must return a new instance
of the middleware. Crawler object provides access to all Scrapy core
components like settings and signals; it is a way for middleware to
access them and hook its functionality into Scrapy.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>crawler</strong> (<a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> object) – crawler that uses this middleware</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="built-in-downloader-middleware-reference">
<span id="topics-downloader-middleware-ref"></span><h4>Built-in downloader middleware reference<a class="headerlink" href="#built-in-downloader-middleware-reference" title="Permalink to this headline">¶</a></h4>
<p>This page describes all downloader middleware components that come with
Scrapy. For information on how to use them and how to write your own downloader
middleware, see the <a class="reference internal" href="#topics-downloader-middleware"><span class="std std-ref">downloader middleware usage guide</span></a>.</p>
<p>For a list of the components enabled by default (and their orders) see the
<a class="reference internal" href="index.html#std:setting-DOWNLOADER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_MIDDLEWARES_BASE</span></code></a> setting.</p>
<div class="section" id="module-scrapy.downloadermiddlewares.cookies">
<span id="cookiesmiddleware"></span><span id="cookies-mw"></span><h5>CookiesMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.cookies" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.downloadermiddlewares.cookies.CookiesMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.downloadermiddlewares.cookies.</code><code class="descname">CookiesMiddleware</code><a class="headerlink" href="#scrapy.downloadermiddlewares.cookies.CookiesMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware enables working with sites that require cookies, such as
those that use sessions. It keeps track of cookies sent by web servers, and
send them back on subsequent requests (from that spider), just like web
browsers do.</p>
</dd></dl>

<p>The following settings can be used to configure the cookie middleware:</p>
<ul class="simple">
<li><a class="reference internal" href="#std:setting-COOKIES_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">COOKIES_ENABLED</span></code></a></li>
<li><a class="reference internal" href="#std:setting-COOKIES_DEBUG"><code class="xref std std-setting docutils literal notranslate"><span class="pre">COOKIES_DEBUG</span></code></a></li>
</ul>
<div class="section" id="multiple-cookie-sessions-per-spider">
<span id="std:reqmeta-cookiejar"></span><h6>Multiple cookie sessions per spider<a class="headerlink" href="#multiple-cookie-sessions-per-spider" title="Permalink to this headline">¶</a></h6>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.15.</span></p>
</div>
<p>There is support for keeping multiple cookie sessions per spider by using the
<a class="reference internal" href="#std:reqmeta-cookiejar"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">cookiejar</span></code></a> Request meta key. By default it uses a single cookie jar
(session), but you can pass an identifier to use different ones.</p>
<p>For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">url</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">urls</span><span class="p">):</span>
    <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">meta</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;cookiejar&#39;</span><span class="p">:</span> <span class="n">i</span><span class="p">},</span>
        <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse_page</span><span class="p">)</span>
</pre></div>
</div>
<p>Keep in mind that the <a class="reference internal" href="#std:reqmeta-cookiejar"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">cookiejar</span></code></a> meta key is not “sticky”. You need to keep
passing it along on subsequent requests. For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">parse_page</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="c1"># do some processing</span>
    <span class="k">return</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="s2">&quot;http://www.example.com/otherpage&quot;</span><span class="p">,</span>
        <span class="n">meta</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;cookiejar&#39;</span><span class="p">:</span> <span class="n">response</span><span class="o">.</span><span class="n">meta</span><span class="p">[</span><span class="s1">&#39;cookiejar&#39;</span><span class="p">]},</span>
        <span class="n">callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parse_other_page</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="cookies-enabled">
<span id="std:setting-COOKIES_ENABLED"></span><h6>COOKIES_ENABLED<a class="headerlink" href="#cookies-enabled" title="Permalink to this headline">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>Whether to enable the cookies middleware. If disabled, no cookies will be sent
to web servers.</p>
<p>Notice that despite the value of <a class="reference internal" href="#std:setting-COOKIES_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">COOKIES_ENABLED</span></code></a> setting if
<code class="docutils literal notranslate"><span class="pre">Request.</span></code><a class="reference internal" href="index.html#std:reqmeta-dont_merge_cookies"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">meta['dont_merge_cookies']</span></code></a>
evaluates to <code class="docutils literal notranslate"><span class="pre">True</span></code> the request cookies will <strong>not</strong> be sent to the
web server and received cookies in <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> will
<strong>not</strong> be merged with the existing cookies.</p>
<p>For more detailed information see the <code class="docutils literal notranslate"><span class="pre">cookies</span></code> parameter in
<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a>.</p>
</div>
<div class="section" id="cookies-debug">
<span id="std:setting-COOKIES_DEBUG"></span><h6>COOKIES_DEBUG<a class="headerlink" href="#cookies-debug" title="Permalink to this headline">¶</a></h6>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>If enabled, Scrapy will log all cookies sent in requests (ie. <code class="docutils literal notranslate"><span class="pre">Cookie</span></code>
header) and all cookies received in responses (ie. <code class="docutils literal notranslate"><span class="pre">Set-Cookie</span></code> header).</p>
<p>Here’s an example of a log with <a class="reference internal" href="#std:setting-COOKIES_DEBUG"><code class="xref std std-setting docutils literal notranslate"><span class="pre">COOKIES_DEBUG</span></code></a> enabled:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">2011</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">06</span> <span class="mi">14</span><span class="p">:</span><span class="mi">35</span><span class="p">:</span><span class="mi">10</span><span class="o">-</span><span class="mi">0300</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">engine</span><span class="p">]</span> <span class="n">INFO</span><span class="p">:</span> <span class="n">Spider</span> <span class="n">opened</span>
<span class="mi">2011</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">06</span> <span class="mi">14</span><span class="p">:</span><span class="mi">35</span><span class="p">:</span><span class="mi">10</span><span class="o">-</span><span class="mi">0300</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">downloadermiddlewares</span><span class="o">.</span><span class="n">cookies</span><span class="p">]</span> <span class="n">DEBUG</span><span class="p">:</span> <span class="n">Sending</span> <span class="n">cookies</span> <span class="n">to</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">GET</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">www</span><span class="o">.</span><span class="n">diningcity</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">netherlands</span><span class="o">/</span><span class="n">index</span><span class="o">.</span><span class="n">html</span><span class="o">&gt;</span>
        <span class="n">Cookie</span><span class="p">:</span> <span class="n">clientlanguage_nl</span><span class="o">=</span><span class="n">en_EN</span>
<span class="mi">2011</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">06</span> <span class="mi">14</span><span class="p">:</span><span class="mi">35</span><span class="p">:</span><span class="mi">14</span><span class="o">-</span><span class="mi">0300</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">downloadermiddlewares</span><span class="o">.</span><span class="n">cookies</span><span class="p">]</span> <span class="n">DEBUG</span><span class="p">:</span> <span class="n">Received</span> <span class="n">cookies</span> <span class="n">from</span><span class="p">:</span> <span class="o">&lt;</span><span class="mi">200</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">www</span><span class="o">.</span><span class="n">diningcity</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">netherlands</span><span class="o">/</span><span class="n">index</span><span class="o">.</span><span class="n">html</span><span class="o">&gt;</span>
        <span class="n">Set</span><span class="o">-</span><span class="n">Cookie</span><span class="p">:</span> <span class="n">JSESSIONID</span><span class="o">=</span><span class="n">B</span><span class="o">~</span><span class="n">FA4DC0C496C8762AE4F1A620EAB34F38</span><span class="p">;</span> <span class="n">Path</span><span class="o">=/</span>
        <span class="n">Set</span><span class="o">-</span><span class="n">Cookie</span><span class="p">:</span> <span class="n">ip_isocode</span><span class="o">=</span><span class="n">US</span>
        <span class="n">Set</span><span class="o">-</span><span class="n">Cookie</span><span class="p">:</span> <span class="n">clientlanguage_nl</span><span class="o">=</span><span class="n">en_EN</span><span class="p">;</span> <span class="n">Expires</span><span class="o">=</span><span class="n">Thu</span><span class="p">,</span> <span class="mi">07</span><span class="o">-</span><span class="n">Apr</span><span class="o">-</span><span class="mi">2011</span> <span class="mi">21</span><span class="p">:</span><span class="mi">21</span><span class="p">:</span><span class="mi">34</span> <span class="n">GMT</span><span class="p">;</span> <span class="n">Path</span><span class="o">=/</span>
<span class="mi">2011</span><span class="o">-</span><span class="mi">04</span><span class="o">-</span><span class="mi">06</span> <span class="mi">14</span><span class="p">:</span><span class="mi">49</span><span class="p">:</span><span class="mi">50</span><span class="o">-</span><span class="mi">0300</span> <span class="p">[</span><span class="n">scrapy</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">engine</span><span class="p">]</span> <span class="n">DEBUG</span><span class="p">:</span> <span class="n">Crawled</span> <span class="p">(</span><span class="mi">200</span><span class="p">)</span> <span class="o">&lt;</span><span class="n">GET</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">www</span><span class="o">.</span><span class="n">diningcity</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">netherlands</span><span class="o">/</span><span class="n">index</span><span class="o">.</span><span class="n">html</span><span class="o">&gt;</span> <span class="p">(</span><span class="n">referer</span><span class="p">:</span> <span class="kc">None</span><span class="p">)</span>
<span class="p">[</span><span class="o">...</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="module-scrapy.downloadermiddlewares.defaultheaders">
<span id="defaultheadersmiddleware"></span><h5>DefaultHeadersMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.defaultheaders" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.downloadermiddlewares.defaultheaders.</code><code class="descname">DefaultHeadersMiddleware</code><a class="headerlink" href="#scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware sets all default requests headers specified in the
<a class="reference internal" href="index.html#std:setting-DEFAULT_REQUEST_HEADERS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DEFAULT_REQUEST_HEADERS</span></code></a> setting.</p>
</dd></dl>

</div>
<div class="section" id="module-scrapy.downloadermiddlewares.downloadtimeout">
<span id="downloadtimeoutmiddleware"></span><h5>DownloadTimeoutMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.downloadtimeout" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.downloadermiddlewares.downloadtimeout.</code><code class="descname">DownloadTimeoutMiddleware</code><a class="headerlink" href="#scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware sets the download timeout for requests specified in the
<a class="reference internal" href="index.html#std:setting-DOWNLOAD_TIMEOUT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_TIMEOUT</span></code></a> setting or <code class="xref py py-attr docutils literal notranslate"><span class="pre">download_timeout</span></code>
spider attribute.</p>
</dd></dl>

<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">You can also set download timeout per-request using
<a class="reference internal" href="index.html#std:reqmeta-download_timeout"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">download_timeout</span></code></a> Request.meta key; this is supported
even when DownloadTimeoutMiddleware is disabled.</p>
</div>
</div>
<div class="section" id="module-scrapy.downloadermiddlewares.httpauth">
<span id="httpauthmiddleware"></span><h5>HttpAuthMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.httpauth" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.downloadermiddlewares.httpauth.</code><code class="descname">HttpAuthMiddleware</code><a class="headerlink" href="#scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware authenticates all requests generated from certain spiders
using <a class="reference external" href="https://en.wikipedia.org/wiki/Basic_access_authentication">Basic access authentication</a> (aka. HTTP auth).</p>
<p>To enable HTTP authentication from certain spiders, set the <code class="docutils literal notranslate"><span class="pre">http_user</span></code>
and <code class="docutils literal notranslate"><span class="pre">http_pass</span></code> attributes of those spiders.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.spiders</span> <span class="k">import</span> <span class="n">CrawlSpider</span>

<span class="k">class</span> <span class="nc">SomeIntranetSiteSpider</span><span class="p">(</span><span class="n">CrawlSpider</span><span class="p">):</span>

    <span class="n">http_user</span> <span class="o">=</span> <span class="s1">&#39;someuser&#39;</span>
    <span class="n">http_pass</span> <span class="o">=</span> <span class="s1">&#39;somepass&#39;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;intranet.example.com&#39;</span>

    <span class="c1"># .. rest of the spider code omitted ...</span>
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="module-scrapy.downloadermiddlewares.httpcache">
<span id="httpcachemiddleware"></span><h5>HttpCacheMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.httpcache" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.downloadermiddlewares.httpcache.</code><code class="descname">HttpCacheMiddleware</code><a class="headerlink" href="#scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware provides low-level cache to all HTTP requests and responses.
It has to be combined with a cache storage backend as well as a cache policy.</p>
<p>Scrapy ships with three HTTP cache storage backends:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="#httpcache-storage-fs"><span class="std std-ref">Filesystem storage backend (default)</span></a></li>
<li><a class="reference internal" href="#httpcache-storage-dbm"><span class="std std-ref">DBM storage backend</span></a></li>
<li><a class="reference internal" href="#httpcache-storage-leveldb"><span class="std std-ref">LevelDB storage backend</span></a></li>
</ul>
</div></blockquote>
<p>You can change the HTTP cache storage backend with the <a class="reference internal" href="#std:setting-HTTPCACHE_STORAGE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">HTTPCACHE_STORAGE</span></code></a>
setting. Or you can also <a class="reference internal" href="#httpcache-storage-custom"><span class="std std-ref">implement your own storage backend.</span></a></p>
<p>Scrapy ships with two HTTP cache policies:</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="#httpcache-policy-rfc2616"><span class="std std-ref">RFC2616 policy</span></a></li>
<li><a class="reference internal" href="#httpcache-policy-dummy"><span class="std std-ref">Dummy policy (default)</span></a></li>
</ul>
</div></blockquote>
<p>You can change the HTTP cache policy with the <a class="reference internal" href="#std:setting-HTTPCACHE_POLICY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">HTTPCACHE_POLICY</span></code></a>
setting. Or you can also implement your own policy.</p>
<p id="std:reqmeta-dont_cache">You can also avoid caching a response on every policy using <a class="reference internal" href="#std:reqmeta-dont_cache"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">dont_cache</span></code></a> meta key equals <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</dd></dl>

<div class="section" id="dummy-policy-default">
<span id="httpcache-policy-dummy"></span><h6>Dummy policy (default)<a class="headerlink" href="#dummy-policy-default" title="Permalink to this headline">¶</a></h6>
<p>This policy has no awareness of any HTTP Cache-Control directives.
Every request and its corresponding response are cached.  When the same
request is seen again, the response is returned without transferring
anything from the Internet.</p>
<p>The Dummy policy is useful for testing spiders faster (without having
to wait for downloads every time) and for trying your spider offline,
when an Internet connection is not available. The goal is to be able to
“replay” a spider run <em>exactly as it ran before</em>.</p>
<p>In order to use this policy, set:</p>
<ul class="simple">
<li><a class="reference internal" href="#std:setting-HTTPCACHE_POLICY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">HTTPCACHE_POLICY</span></code></a> to <code class="docutils literal notranslate"><span class="pre">scrapy.extensions.httpcache.DummyPolicy</span></code></li>
</ul>
</div>
<div class="section" id="rfc2616-policy">
<span id="httpcache-policy-rfc2616"></span><h6>RFC2616 policy<a class="headerlink" href="#rfc2616-policy" title="Permalink to this headline">¶</a></h6>
<p>This policy provides a RFC2616 compliant HTTP cache, i.e. with HTTP
Cache-Control awareness, aimed at production and used in continuous
runs to avoid downloading unmodified data (to save bandwidth and speed up crawls).</p>
<p>what is implemented:</p>
<ul>
<li><p class="first">Do not attempt to store responses/requests with <code class="docutils literal notranslate"><span class="pre">no-store</span></code> cache-control directive set</p>
</li>
<li><p class="first">Do not serve responses from cache if <code class="docutils literal notranslate"><span class="pre">no-cache</span></code> cache-control directive is set even for fresh responses</p>
</li>
<li><p class="first">Compute freshness lifetime from <code class="docutils literal notranslate"><span class="pre">max-age</span></code> cache-control directive</p>
</li>
<li><p class="first">Compute freshness lifetime from <code class="docutils literal notranslate"><span class="pre">Expires</span></code> response header</p>
</li>
<li><p class="first">Compute freshness lifetime from <code class="docutils literal notranslate"><span class="pre">Last-Modified</span></code> response header (heuristic used by Firefox)</p>
</li>
<li><p class="first">Compute current age from <code class="docutils literal notranslate"><span class="pre">Age</span></code> response header</p>
</li>
<li><p class="first">Compute current age from <code class="docutils literal notranslate"><span class="pre">Date</span></code> header</p>
</li>
<li><p class="first">Revalidate stale responses based on <code class="docutils literal notranslate"><span class="pre">Last-Modified</span></code> response header</p>
</li>
<li><p class="first">Revalidate stale responses based on <code class="docutils literal notranslate"><span class="pre">ETag</span></code> response header</p>
</li>
<li><p class="first">Set <code class="docutils literal notranslate"><span class="pre">Date</span></code> header for any received response missing it</p>
</li>
<li><p class="first">Support <code class="docutils literal notranslate"><span class="pre">max-stale</span></code> cache-control directive in requests</p>
<p>This allows spiders to be configured with the full RFC2616 cache policy,
but avoid revalidation on a request-by-request basis, while remaining
conformant with the HTTP spec.</p>
<p>Example:</p>
<p>Add <code class="docutils literal notranslate"><span class="pre">Cache-Control:</span> <span class="pre">max-stale=600</span></code> to Request headers to accept responses that
have exceeded their expiration time by no more than 600 seconds.</p>
<p>See also: RFC2616, 14.9.3</p>
</li>
</ul>
<p>what is missing:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">Pragma:</span> <span class="pre">no-cache</span></code> support <a class="reference external" href="https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.1">https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.1</a></li>
<li><code class="docutils literal notranslate"><span class="pre">Vary</span></code> header support <a class="reference external" href="https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.6">https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.6</a></li>
<li>Invalidation after updates or deletes <a class="reference external" href="https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.10">https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.10</a></li>
<li>… probably others ..</li>
</ul>
<p>In order to use this policy, set:</p>
<ul class="simple">
<li><a class="reference internal" href="#std:setting-HTTPCACHE_POLICY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">HTTPCACHE_POLICY</span></code></a> to <code class="docutils literal notranslate"><span class="pre">scrapy.extensions.httpcache.RFC2616Policy</span></code></li>
</ul>
</div>
<div class="section" id="filesystem-storage-backend-default">
<span id="httpcache-storage-fs"></span><h6>Filesystem storage backend (default)<a class="headerlink" href="#filesystem-storage-backend-default" title="Permalink to this headline">¶</a></h6>
<p>File system storage backend is available for the HTTP cache middleware.</p>
<p>In order to use this storage backend, set:</p>
<ul class="simple">
<li><a class="reference internal" href="#std:setting-HTTPCACHE_STORAGE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">HTTPCACHE_STORAGE</span></code></a> to <code class="docutils literal notranslate"><span class="pre">scrapy.extensions.httpcache.FilesystemCacheStorage</span></code></li>
</ul>
<p>Each request/response pair is stored in a different directory containing
the following files:</p>
<blockquote>
<div><ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">request_body</span></code> - the plain request body</li>
<li><code class="docutils literal notranslate"><span class="pre">request_headers</span></code> - the request headers (in raw HTTP format)</li>
<li><code class="docutils literal notranslate"><span class="pre">response_body</span></code> - the plain response body</li>
<li><code class="docutils literal notranslate"><span class="pre">response_headers</span></code> - the request headers (in raw HTTP format)</li>
<li><code class="docutils literal notranslate"><span class="pre">meta</span></code> - some metadata of this cache resource in Python <code class="docutils literal notranslate"><span class="pre">repr()</span></code> format
(grep-friendly format)</li>
<li><code class="docutils literal notranslate"><span class="pre">pickled_meta</span></code> - the same metadata in <code class="docutils literal notranslate"><span class="pre">meta</span></code> but pickled for more
efficient deserialization</li>
</ul>
</div></blockquote>
<p>The directory name is made from the request fingerprint (see
<code class="docutils literal notranslate"><span class="pre">scrapy.utils.request.fingerprint</span></code>), and one level of subdirectories is
used to avoid creating too many files into the same directory (which is
inefficient in many file systems). An example directory could be:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">/</span><span class="n">path</span><span class="o">/</span><span class="n">to</span><span class="o">/</span><span class="n">cache</span><span class="o">/</span><span class="nb">dir</span><span class="o">/</span><span class="n">example</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="mi">72</span><span class="o">/</span><span class="mi">72811</span><span class="n">f648e718090f041317756c03adb0ada46c7</span>
</pre></div>
</div>
</div>
<div class="section" id="dbm-storage-backend">
<span id="httpcache-storage-dbm"></span><h6>DBM storage backend<a class="headerlink" href="#dbm-storage-backend" title="Permalink to this headline">¶</a></h6>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.13.</span></p>
</div>
<p>A <a class="reference external" href="https://en.wikipedia.org/wiki/Dbm">DBM</a> storage backend is also available for the HTTP cache middleware.</p>
<p>By default, it uses the <a class="reference external" href="https://docs.python.org/2/library/anydbm.html">anydbm</a> module, but you can change it with the
<a class="reference internal" href="#std:setting-HTTPCACHE_DBM_MODULE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">HTTPCACHE_DBM_MODULE</span></code></a> setting.</p>
<p>In order to use this storage backend, set:</p>
<ul class="simple">
<li><a class="reference internal" href="#std:setting-HTTPCACHE_STORAGE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">HTTPCACHE_STORAGE</span></code></a> to <code class="docutils literal notranslate"><span class="pre">scrapy.extensions.httpcache.DbmCacheStorage</span></code></li>
</ul>
</div>
<div class="section" id="leveldb-storage-backend">
<span id="httpcache-storage-leveldb"></span><h6>LevelDB storage backend<a class="headerlink" href="#leveldb-storage-backend" title="Permalink to this headline">¶</a></h6>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.23.</span></p>
</div>
<p>A <a class="reference external" href="https://github.com/google/leveldb">LevelDB</a> storage backend is also available for the HTTP cache middleware.</p>
<p>This backend is not recommended for development because only one process can
access LevelDB databases at the same time, so you can’t run a crawl and open
the scrapy shell in parallel for the same spider.</p>
<p>In order to use this storage backend:</p>
<ul class="simple">
<li>set <a class="reference internal" href="#std:setting-HTTPCACHE_STORAGE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">HTTPCACHE_STORAGE</span></code></a> to <code class="docutils literal notranslate"><span class="pre">scrapy.extensions.httpcache.LeveldbCacheStorage</span></code></li>
<li>install <a class="reference external" href="https://pypi.python.org/pypi/leveldb">LevelDB python bindings</a> like <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">leveldb</span></code></li>
</ul>
</div>
<div class="section" id="writing-your-own-storage-backend">
<span id="httpcache-storage-custom"></span><h6>Writing your own storage backend<a class="headerlink" href="#writing-your-own-storage-backend" title="Permalink to this headline">¶</a></h6>
<p>You can implement a cache storage backend by creating a Python class that
defines the methods described below.</p>
<span class="target" id="module-scrapy.extensions.httpcache"></span><dl class="class">
<dt id="scrapy.extensions.httpcache.CacheStorage">
<em class="property">class </em><code class="descclassname">scrapy.extensions.httpcache.</code><code class="descname">CacheStorage</code><a class="headerlink" href="#scrapy.extensions.httpcache.CacheStorage" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="scrapy.extensions.httpcache.CacheStorage.open_spider">
<code class="descname">open_spider</code><span class="sig-paren">(</span><em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.extensions.httpcache.CacheStorage.open_spider" title="Permalink to this definition">¶</a></dt>
<dd><p>This method gets called after a spider has been opened for crawling. It handles
the <a class="reference internal" href="index.html#std:signal-spider_opened"><code class="xref std std-signal docutils literal notranslate"><span class="pre">open_spider</span></code></a> signal.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider which has been opened</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.extensions.httpcache.CacheStorage.close_spider">
<code class="descname">close_spider</code><span class="sig-paren">(</span><em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.extensions.httpcache.CacheStorage.close_spider" title="Permalink to this definition">¶</a></dt>
<dd><p>This method gets called after a spider has been closed. It handles
the <a class="reference internal" href="index.html#std:signal-spider_closed"><code class="xref std std-signal docutils literal notranslate"><span class="pre">close_spider</span></code></a> signal.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider which has been closed</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.extensions.httpcache.CacheStorage.retrieve_response">
<code class="descname">retrieve_response</code><span class="sig-paren">(</span><em>spider</em>, <em>request</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.extensions.httpcache.CacheStorage.retrieve_response" title="Permalink to this definition">¶</a></dt>
<dd><p>Return response if present in cache, or <code class="docutils literal notranslate"><span class="pre">None</span></code> otherwise.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider which generated the request</li>
<li><strong>request</strong> (<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object) – the request to find cached reponse for</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.extensions.httpcache.CacheStorage.store_response">
<code class="descname">store_response</code><span class="sig-paren">(</span><em>spider</em>, <em>request</em>, <em>response</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.extensions.httpcache.CacheStorage.store_response" title="Permalink to this definition">¶</a></dt>
<dd><p>Store the given response in the cache.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider for which the response is intended</li>
<li><strong>request</strong> (<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object) – the corresponding request the spider generated</li>
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object) – the response to store in the cache</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<p>In order to use your storage backend, set:</p>
<ul class="simple">
<li><a class="reference internal" href="#std:setting-HTTPCACHE_STORAGE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">HTTPCACHE_STORAGE</span></code></a> to the Python import path of your custom storage class.</li>
</ul>
</div>
<div class="section" id="httpcache-middleware-settings">
<h6>HTTPCache middleware settings<a class="headerlink" href="#httpcache-middleware-settings" title="Permalink to this headline">¶</a></h6>
<p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">HttpCacheMiddleware</span></code> can be configured through the following
settings:</p>
<div class="section" id="httpcache-enabled">
<span id="std:setting-HTTPCACHE_ENABLED"></span><h7>HTTPCACHE_ENABLED<a class="headerlink" href="#httpcache-enabled" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.11.</span></p>
</div>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>Whether the HTTP cache will be enabled.</p>
<div class="versionchanged">
<p><span class="versionmodified">Changed in version 0.11: </span>Before 0.11, <a class="reference internal" href="#std:setting-HTTPCACHE_DIR"><code class="xref std std-setting docutils literal notranslate"><span class="pre">HTTPCACHE_DIR</span></code></a> was used to enable cache.</p>
</div>
</div>
<div class="section" id="httpcache-expiration-secs">
<span id="std:setting-HTTPCACHE_EXPIRATION_SECS"></span><h7>HTTPCACHE_EXPIRATION_SECS<a class="headerlink" href="#httpcache-expiration-secs" title="Permalink to this headline">¶</a></h7>
<p>Default: <code class="docutils literal notranslate"><span class="pre">0</span></code></p>
<p>Expiration time for cached requests, in seconds.</p>
<p>Cached requests older than this time will be re-downloaded. If zero, cached
requests will never expire.</p>
<div class="versionchanged">
<p><span class="versionmodified">Changed in version 0.11: </span>Before 0.11, zero meant cached requests always expire.</p>
</div>
</div>
<div class="section" id="httpcache-dir">
<span id="std:setting-HTTPCACHE_DIR"></span><h7>HTTPCACHE_DIR<a class="headerlink" href="#httpcache-dir" title="Permalink to this headline">¶</a></h7>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'httpcache'</span></code></p>
<p>The directory to use for storing the (low-level) HTTP cache. If empty, the HTTP
cache will be disabled. If a relative path is given, is taken relative to the
project data dir. For more info see: <a class="reference internal" href="index.html#topics-project-structure"><span class="std std-ref">Default structure of Scrapy projects</span></a>.</p>
</div>
<div class="section" id="httpcache-ignore-http-codes">
<span id="std:setting-HTTPCACHE_IGNORE_HTTP_CODES"></span><h7>HTTPCACHE_IGNORE_HTTP_CODES<a class="headerlink" href="#httpcache-ignore-http-codes" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.10.</span></p>
</div>
<p>Default: <code class="docutils literal notranslate"><span class="pre">[]</span></code></p>
<p>Don’t cache response with these HTTP codes.</p>
</div>
<div class="section" id="httpcache-ignore-missing">
<span id="std:setting-HTTPCACHE_IGNORE_MISSING"></span><h7>HTTPCACHE_IGNORE_MISSING<a class="headerlink" href="#httpcache-ignore-missing" title="Permalink to this headline">¶</a></h7>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>If enabled, requests not found in the cache will be ignored instead of downloaded.</p>
</div>
<div class="section" id="httpcache-ignore-schemes">
<span id="std:setting-HTTPCACHE_IGNORE_SCHEMES"></span><h7>HTTPCACHE_IGNORE_SCHEMES<a class="headerlink" href="#httpcache-ignore-schemes" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.10.</span></p>
</div>
<p>Default: <code class="docutils literal notranslate"><span class="pre">['file']</span></code></p>
<p>Don’t cache responses with these URI schemes.</p>
</div>
<div class="section" id="httpcache-storage">
<span id="std:setting-HTTPCACHE_STORAGE"></span><h7>HTTPCACHE_STORAGE<a class="headerlink" href="#httpcache-storage" title="Permalink to this headline">¶</a></h7>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapy.extensions.httpcache.FilesystemCacheStorage'</span></code></p>
<p>The class which implements the cache storage backend.</p>
</div>
<div class="section" id="httpcache-dbm-module">
<span id="std:setting-HTTPCACHE_DBM_MODULE"></span><h7>HTTPCACHE_DBM_MODULE<a class="headerlink" href="#httpcache-dbm-module" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.13.</span></p>
</div>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'anydbm'</span></code></p>
<p>The database module to use in the <a class="reference internal" href="#httpcache-storage-dbm"><span class="std std-ref">DBM storage backend</span></a>. This setting is specific to the DBM backend.</p>
</div>
<div class="section" id="httpcache-policy">
<span id="std:setting-HTTPCACHE_POLICY"></span><h7>HTTPCACHE_POLICY<a class="headerlink" href="#httpcache-policy" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.18.</span></p>
</div>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapy.extensions.httpcache.DummyPolicy'</span></code></p>
<p>The class which implements the cache policy.</p>
</div>
<div class="section" id="httpcache-gzip">
<span id="std:setting-HTTPCACHE_GZIP"></span><h7>HTTPCACHE_GZIP<a class="headerlink" href="#httpcache-gzip" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.0.</span></p>
</div>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>If enabled, will compress all cached data with gzip.
This setting is specific to the Filesystem backend.</p>
</div>
<div class="section" id="httpcache-always-store">
<span id="std:setting-HTTPCACHE_ALWAYS_STORE"></span><h7>HTTPCACHE_ALWAYS_STORE<a class="headerlink" href="#httpcache-always-store" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.1.</span></p>
</div>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>If enabled, will cache pages unconditionally.</p>
<p>A spider may wish to have all responses available in the cache, for
future use with <code class="docutils literal notranslate"><span class="pre">Cache-Control:</span> <span class="pre">max-stale</span></code>, for instance. The
DummyPolicy caches all responses but never revalidates them, and
sometimes a more nuanced policy is desirable.</p>
<p>This setting still respects <code class="docutils literal notranslate"><span class="pre">Cache-Control:</span> <span class="pre">no-store</span></code> directives in responses.
If you don’t want that, filter <code class="docutils literal notranslate"><span class="pre">no-store</span></code> out of the Cache-Control headers in
responses you feedto the cache middleware.</p>
</div>
<div class="section" id="httpcache-ignore-response-cache-controls">
<span id="std:setting-HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS"></span><h7>HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS<a class="headerlink" href="#httpcache-ignore-response-cache-controls" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.1.</span></p>
</div>
<p>Default: <code class="docutils literal notranslate"><span class="pre">[]</span></code></p>
<p>List of Cache-Control directives in responses to be ignored.</p>
<p>Sites often set “no-store”, “no-cache”, “must-revalidate”, etc., but get
upset at the traffic a spider can generate if it respects those
directives. This allows to selectively ignore Cache-Control directives
that are known to be unimportant for the sites being crawled.</p>
<p>We assume that the spider will not issue Cache-Control directives
in requests unless it actually needs them, so directives in requests are
not filtered.</p>
</div>
</div>
</div>
<div class="section" id="module-scrapy.downloadermiddlewares.httpcompression">
<span id="httpcompressionmiddleware"></span><h5>HttpCompressionMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.httpcompression" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.downloadermiddlewares.httpcompression.</code><code class="descname">HttpCompressionMiddleware</code><a class="headerlink" href="#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware allows compressed (gzip, deflate) traffic to be
sent/received from web sites.</p>
<p>This middleware also supports decoding <a class="reference external" href="https://www.ietf.org/rfc/rfc7932.txt">brotli-compressed</a> responses,
provided <a class="reference external" href="https://pypi.python.org/pypi/brotlipy">brotlipy</a> is installed.</p>
</dd></dl>

<div class="section" id="httpcompressionmiddleware-settings">
<h6>HttpCompressionMiddleware Settings<a class="headerlink" href="#httpcompressionmiddleware-settings" title="Permalink to this headline">¶</a></h6>
<div class="section" id="compression-enabled">
<span id="std:setting-COMPRESSION_ENABLED"></span><h7>COMPRESSION_ENABLED<a class="headerlink" href="#compression-enabled" title="Permalink to this headline">¶</a></h7>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>Whether the Compression middleware will be enabled.</p>
</div>
</div>
</div>
<div class="section" id="module-scrapy.downloadermiddlewares.httpproxy">
<span id="httpproxymiddleware"></span><h5>HttpProxyMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.httpproxy" title="Permalink to this headline">¶</a></h5>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.8.</span></p>
</div>
<span class="target" id="std:reqmeta-proxy"></span><dl class="class">
<dt id="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.downloadermiddlewares.httpproxy.</code><code class="descname">HttpProxyMiddleware</code><a class="headerlink" href="#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware sets the HTTP proxy to use for requests, by setting the
<code class="docutils literal notranslate"><span class="pre">proxy</span></code> meta value for <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> objects.</p>
<p>Like the Python standard library modules <a class="reference external" href="https://docs.python.org/2/library/urllib.html">urllib</a> and <a class="reference external" href="https://docs.python.org/2/library/urllib2.html">urllib2</a>, it obeys
the following environment variables:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">http_proxy</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">https_proxy</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">no_proxy</span></code></li>
</ul>
<p>You can also set the meta key <code class="docutils literal notranslate"><span class="pre">proxy</span></code> per-request, to a value like
<code class="docutils literal notranslate"><span class="pre">http://some_proxy_server:port</span></code> or <code class="docutils literal notranslate"><span class="pre">http://username:password&#64;some_proxy_server:port</span></code>.
Keep in mind this value will take precedence over <code class="docutils literal notranslate"><span class="pre">http_proxy</span></code>/<code class="docutils literal notranslate"><span class="pre">https_proxy</span></code>
environment variables, and it will also ignore <code class="docutils literal notranslate"><span class="pre">no_proxy</span></code> environment variable.</p>
</dd></dl>

</div>
<div class="section" id="module-scrapy.downloadermiddlewares.redirect">
<span id="redirectmiddleware"></span><h5>RedirectMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.redirect" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.downloadermiddlewares.redirect.RedirectMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.downloadermiddlewares.redirect.</code><code class="descname">RedirectMiddleware</code><a class="headerlink" href="#scrapy.downloadermiddlewares.redirect.RedirectMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware handles redirection of requests based on response status.</p>
</dd></dl>

<p id="std:reqmeta-redirect_urls">The urls which the request goes through (while being redirected) can be found
in the <code class="docutils literal notranslate"><span class="pre">redirect_urls</span></code> <a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code></a> key.</p>
<p id="std:reqmeta-redirect_reasons">The reason behind each redirect in <a class="reference internal" href="#std:reqmeta-redirect_urls"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">redirect_urls</span></code></a> can be found in the
<code class="docutils literal notranslate"><span class="pre">redirect_reasons</span></code> <a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code></a> key. For
example: <code class="docutils literal notranslate"><span class="pre">[301,</span> <span class="pre">302,</span> <span class="pre">307,</span> <span class="pre">'meta</span> <span class="pre">refresh']</span></code>.</p>
<p>The format of a reason depends on the middleware that handled the corresponding
redirect. For example, <a class="reference internal" href="#scrapy.downloadermiddlewares.redirect.RedirectMiddleware" title="scrapy.downloadermiddlewares.redirect.RedirectMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">RedirectMiddleware</span></code></a> indicates the triggering
response status code as an integer, while <a class="reference internal" href="#scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware" title="scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">MetaRefreshMiddleware</span></code></a>
always uses the <code class="docutils literal notranslate"><span class="pre">'meta</span> <span class="pre">refresh'</span></code> string as reason.</p>
<p>The <a class="reference internal" href="#scrapy.downloadermiddlewares.redirect.RedirectMiddleware" title="scrapy.downloadermiddlewares.redirect.RedirectMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">RedirectMiddleware</span></code></a> can be configured through the following
settings (see the settings documentation for more info):</p>
<ul class="simple">
<li><a class="reference internal" href="#std:setting-REDIRECT_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">REDIRECT_ENABLED</span></code></a></li>
<li><a class="reference internal" href="index.html#std:setting-REDIRECT_MAX_TIMES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">REDIRECT_MAX_TIMES</span></code></a></li>
</ul>
<p id="std:reqmeta-dont_redirect">If <a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code></a> has <code class="docutils literal notranslate"><span class="pre">dont_redirect</span></code>
key set to True, the request will be ignored by this middleware.</p>
<p>If you want to handle some redirect status codes in your spider, you can
specify these in the <code class="docutils literal notranslate"><span class="pre">handle_httpstatus_list</span></code> spider attribute.</p>
<p>For example, if you want the redirect middleware to ignore 301 and 302
responses (and pass them through to your spider) you can do this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">CrawlSpider</span><span class="p">):</span>
    <span class="n">handle_httpstatus_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">301</span><span class="p">,</span> <span class="mi">302</span><span class="p">]</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">handle_httpstatus_list</span></code> key of <a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code></a> can also be used to specify which response codes to
allow on a per-request basis. You can also set the meta key
<code class="docutils literal notranslate"><span class="pre">handle_httpstatus_all</span></code> to <code class="docutils literal notranslate"><span class="pre">True</span></code> if you want to allow any response code
for a request.</p>
<div class="section" id="redirectmiddleware-settings">
<h6>RedirectMiddleware settings<a class="headerlink" href="#redirectmiddleware-settings" title="Permalink to this headline">¶</a></h6>
<div class="section" id="redirect-enabled">
<span id="std:setting-REDIRECT_ENABLED"></span><h7>REDIRECT_ENABLED<a class="headerlink" href="#redirect-enabled" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.13.</span></p>
</div>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>Whether the Redirect middleware will be enabled.</p>
</div>
<div class="section" id="redirect-max-times">
<span id="std:setting-REDIRECT_MAX_TIMES"></span><h7>REDIRECT_MAX_TIMES<a class="headerlink" href="#redirect-max-times" title="Permalink to this headline">¶</a></h7>
<p>Default: <code class="docutils literal notranslate"><span class="pre">20</span></code></p>
<p>The maximum number of redirections that will be followed for a single request.</p>
</div>
</div>
</div>
<div class="section" id="metarefreshmiddleware">
<h5>MetaRefreshMiddleware<a class="headerlink" href="#metarefreshmiddleware" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.downloadermiddlewares.redirect.</code><code class="descname">MetaRefreshMiddleware</code><a class="headerlink" href="#scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware handles redirection of requests based on meta-refresh html tag.</p>
</dd></dl>

<p>The <a class="reference internal" href="#scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware" title="scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">MetaRefreshMiddleware</span></code></a> can be configured through the following
settings (see the settings documentation for more info):</p>
<ul class="simple">
<li><a class="reference internal" href="#std:setting-METAREFRESH_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">METAREFRESH_ENABLED</span></code></a></li>
<li><a class="reference internal" href="#std:setting-METAREFRESH_IGNORE_TAGS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">METAREFRESH_IGNORE_TAGS</span></code></a></li>
<li><a class="reference internal" href="#std:setting-METAREFRESH_MAXDELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">METAREFRESH_MAXDELAY</span></code></a></li>
</ul>
<p>This middleware obey <a class="reference internal" href="index.html#std:setting-REDIRECT_MAX_TIMES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">REDIRECT_MAX_TIMES</span></code></a> setting, <a class="reference internal" href="#std:reqmeta-dont_redirect"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">dont_redirect</span></code></a>,
<a class="reference internal" href="#std:reqmeta-redirect_urls"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">redirect_urls</span></code></a> and <a class="reference internal" href="#std:reqmeta-redirect_reasons"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">redirect_reasons</span></code></a> request meta keys as described
for <a class="reference internal" href="#scrapy.downloadermiddlewares.redirect.RedirectMiddleware" title="scrapy.downloadermiddlewares.redirect.RedirectMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">RedirectMiddleware</span></code></a></p>
<div class="section" id="metarefreshmiddleware-settings">
<h6>MetaRefreshMiddleware settings<a class="headerlink" href="#metarefreshmiddleware-settings" title="Permalink to this headline">¶</a></h6>
<div class="section" id="metarefresh-enabled">
<span id="std:setting-METAREFRESH_ENABLED"></span><h7>METAREFRESH_ENABLED<a class="headerlink" href="#metarefresh-enabled" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.17.</span></p>
</div>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>Whether the Meta Refresh middleware will be enabled.</p>
</div>
<div class="section" id="metarefresh-ignore-tags">
<span id="std:setting-METAREFRESH_IGNORE_TAGS"></span><h7>METAREFRESH_IGNORE_TAGS<a class="headerlink" href="#metarefresh-ignore-tags" title="Permalink to this headline">¶</a></h7>
<p>Default: <code class="docutils literal notranslate"><span class="pre">['script',</span> <span class="pre">'noscript']</span></code></p>
<p>Meta tags within these tags are ignored.</p>
</div>
<div class="section" id="metarefresh-maxdelay">
<span id="std:setting-METAREFRESH_MAXDELAY"></span><h7>METAREFRESH_MAXDELAY<a class="headerlink" href="#metarefresh-maxdelay" title="Permalink to this headline">¶</a></h7>
<p>Default: <code class="docutils literal notranslate"><span class="pre">100</span></code></p>
<p>The maximum meta-refresh delay (in seconds) to follow the redirection.
Some sites use meta-refresh for redirecting to a session expired page, so we
restrict automatic redirection to the maximum delay.</p>
</div>
</div>
</div>
<div class="section" id="module-scrapy.downloadermiddlewares.retry">
<span id="retrymiddleware"></span><h5>RetryMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.retry" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.downloadermiddlewares.retry.RetryMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.downloadermiddlewares.retry.</code><code class="descname">RetryMiddleware</code><a class="headerlink" href="#scrapy.downloadermiddlewares.retry.RetryMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>A middleware to retry failed requests that are potentially caused by
temporary problems such as a connection timeout or HTTP 500 error.</p>
</dd></dl>

<p>Failed pages are collected on the scraping process and rescheduled at the
end, once the spider has finished crawling all regular (non failed) pages.</p>
<p>The <a class="reference internal" href="#scrapy.downloadermiddlewares.retry.RetryMiddleware" title="scrapy.downloadermiddlewares.retry.RetryMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">RetryMiddleware</span></code></a> can be configured through the following
settings (see the settings documentation for more info):</p>
<ul class="simple">
<li><a class="reference internal" href="#std:setting-RETRY_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_ENABLED</span></code></a></li>
<li><a class="reference internal" href="#std:setting-RETRY_TIMES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_TIMES</span></code></a></li>
<li><a class="reference internal" href="#std:setting-RETRY_HTTP_CODES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_HTTP_CODES</span></code></a></li>
</ul>
<p id="std:reqmeta-dont_retry">If <a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code></a> has <code class="docutils literal notranslate"><span class="pre">dont_retry</span></code> key
set to True, the request will be ignored by this middleware.</p>
<div class="section" id="retrymiddleware-settings">
<h6>RetryMiddleware Settings<a class="headerlink" href="#retrymiddleware-settings" title="Permalink to this headline">¶</a></h6>
<div class="section" id="retry-enabled">
<span id="std:setting-RETRY_ENABLED"></span><h7>RETRY_ENABLED<a class="headerlink" href="#retry-enabled" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.13.</span></p>
</div>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>Whether the Retry middleware will be enabled.</p>
</div>
<div class="section" id="retry-times">
<span id="std:setting-RETRY_TIMES"></span><h7>RETRY_TIMES<a class="headerlink" href="#retry-times" title="Permalink to this headline">¶</a></h7>
<p>Default: <code class="docutils literal notranslate"><span class="pre">2</span></code></p>
<p>Maximum number of times to retry, in addition to the first download.</p>
<p>Maximum number of retries can also be specified per-request using
<a class="reference internal" href="index.html#std:reqmeta-max_retry_times"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">max_retry_times</span></code></a> attribute of <a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code></a>.
When initialized, the <a class="reference internal" href="index.html#std:reqmeta-max_retry_times"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">max_retry_times</span></code></a> meta key takes higher
precedence over the <a class="reference internal" href="#std:setting-RETRY_TIMES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_TIMES</span></code></a> setting.</p>
</div>
<div class="section" id="retry-http-codes">
<span id="std:setting-RETRY_HTTP_CODES"></span><h7>RETRY_HTTP_CODES<a class="headerlink" href="#retry-http-codes" title="Permalink to this headline">¶</a></h7>
<p>Default: <code class="docutils literal notranslate"><span class="pre">[500,</span> <span class="pre">502,</span> <span class="pre">503,</span> <span class="pre">504,</span> <span class="pre">522,</span> <span class="pre">524,</span> <span class="pre">408]</span></code></p>
<p>Which HTTP response codes to retry. Other errors (DNS lookup issues,
connections lost, etc) are always retried.</p>
<p>In some cases you may want to add 400 to <a class="reference internal" href="#std:setting-RETRY_HTTP_CODES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_HTTP_CODES</span></code></a> because
it is a common code used to indicate server overload. It is not included by
default because HTTP specs say so.</p>
</div>
</div>
</div>
<div class="section" id="module-scrapy.downloadermiddlewares.robotstxt">
<span id="robotstxtmiddleware"></span><span id="topics-dlmw-robots"></span><h5>RobotsTxtMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.robotstxt" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.downloadermiddlewares.robotstxt.</code><code class="descname">RobotsTxtMiddleware</code><a class="headerlink" href="#scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>This middleware filters out requests forbidden by the robots.txt exclusion
standard.</p>
<p>To make sure Scrapy respects robots.txt make sure the middleware is enabled
and the <a class="reference internal" href="index.html#std:setting-ROBOTSTXT_OBEY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ROBOTSTXT_OBEY</span></code></a> setting is enabled.</p>
</dd></dl>

<p id="std:reqmeta-dont_obey_robotstxt">If <a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code></a> has
<code class="docutils literal notranslate"><span class="pre">dont_obey_robotstxt</span></code> key set to True
the request will be ignored by this middleware even if
<a class="reference internal" href="index.html#std:setting-ROBOTSTXT_OBEY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ROBOTSTXT_OBEY</span></code></a> is enabled.</p>
</div>
<div class="section" id="module-scrapy.downloadermiddlewares.stats">
<span id="downloaderstats"></span><h5>DownloaderStats<a class="headerlink" href="#module-scrapy.downloadermiddlewares.stats" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.downloadermiddlewares.stats.DownloaderStats">
<em class="property">class </em><code class="descclassname">scrapy.downloadermiddlewares.stats.</code><code class="descname">DownloaderStats</code><a class="headerlink" href="#scrapy.downloadermiddlewares.stats.DownloaderStats" title="Permalink to this definition">¶</a></dt>
<dd><p>Middleware that stores stats of all requests, responses and exceptions that
pass through it.</p>
<p>To use this middleware you must enable the <a class="reference internal" href="index.html#std:setting-DOWNLOADER_STATS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_STATS</span></code></a>
setting.</p>
</dd></dl>

</div>
<div class="section" id="module-scrapy.downloadermiddlewares.useragent">
<span id="useragentmiddleware"></span><h5>UserAgentMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.useragent" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.downloadermiddlewares.useragent.UserAgentMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.downloadermiddlewares.useragent.</code><code class="descname">UserAgentMiddleware</code><a class="headerlink" href="#scrapy.downloadermiddlewares.useragent.UserAgentMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>Middleware that allows spiders to override the default user agent.</p>
<p>In order for a spider to override the default user agent, its <code class="docutils literal notranslate"><span class="pre">user_agent</span></code>
attribute must be set.</p>
</dd></dl>

</div>
<div class="section" id="module-scrapy.downloadermiddlewares.ajaxcrawl">
<span id="ajaxcrawlmiddleware"></span><span id="ajaxcrawl-middleware"></span><h5>AjaxCrawlMiddleware<a class="headerlink" href="#module-scrapy.downloadermiddlewares.ajaxcrawl" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.downloadermiddlewares.ajaxcrawl.</code><code class="descname">AjaxCrawlMiddleware</code><a class="headerlink" href="#scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>Middleware that finds ‘AJAX crawlable’ page variants based
on meta-fragment html tag. See
<a class="reference external" href="https://developers.google.com/webmasters/ajax-crawling/docs/getting-started">https://developers.google.com/webmasters/ajax-crawling/docs/getting-started</a>
for more info.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Scrapy finds ‘AJAX crawlable’ pages for URLs like
<code class="docutils literal notranslate"><span class="pre">'http://example.com/!#foo=bar'</span></code> even without this middleware.
AjaxCrawlMiddleware is necessary when URL doesn’t contain <code class="docutils literal notranslate"><span class="pre">'!#'</span></code>.
This is often a case for ‘index’ or ‘main’ website pages.</p>
</div>
</dd></dl>

<div class="section" id="ajaxcrawlmiddleware-settings">
<h6>AjaxCrawlMiddleware Settings<a class="headerlink" href="#ajaxcrawlmiddleware-settings" title="Permalink to this headline">¶</a></h6>
<div class="section" id="ajaxcrawl-enabled">
<span id="std:setting-AJAXCRAWL_ENABLED"></span><h7>AJAXCRAWL_ENABLED<a class="headerlink" href="#ajaxcrawl-enabled" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.21.</span></p>
</div>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>Whether the AjaxCrawlMiddleware will be enabled. You may want to
enable it for <a class="reference internal" href="index.html#topics-broad-crawls"><span class="std std-ref">broad crawls</span></a>.</p>
</div>
</div>
<div class="section" id="httpproxymiddleware-settings">
<h6>HttpProxyMiddleware settings<a class="headerlink" href="#httpproxymiddleware-settings" title="Permalink to this headline">¶</a></h6>
<span class="target" id="std:setting-HTTPPROXY_ENABLED"></span><div class="section" id="httpproxy-enabled">
<span id="std:setting-HTTPPROXY_AUTH_ENCODING"></span><h7>HTTPPROXY_ENABLED<a class="headerlink" href="#httpproxy-enabled" title="Permalink to this headline">¶</a></h7>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>Whether or not to enable the <code class="xref py py-class docutils literal notranslate"><span class="pre">HttpProxyMiddleware</span></code>.</p>
</div>
<div class="section" id="httpproxy-auth-encoding">
<h7>HTTPPROXY_AUTH_ENCODING<a class="headerlink" href="#httpproxy-auth-encoding" title="Permalink to this headline">¶</a></h7>
<p>Default: <code class="docutils literal notranslate"><span class="pre">&quot;latin-1&quot;</span></code></p>
<p>The default encoding for proxy authentication on <code class="xref py py-class docutils literal notranslate"><span class="pre">HttpProxyMiddleware</span></code>.</p>
</div>
</div>
</div>
</div>
</div>
<span id="document-topics/spider-middleware"></span><div class="section" id="spider-middleware">
<span id="topics-spider-middleware"></span><h3>Spider Middleware<a class="headerlink" href="#spider-middleware" title="Permalink to this headline">¶</a></h3>
<p>The spider middleware is a framework of hooks into Scrapy’s spider processing
mechanism where you can plug custom functionality to process the responses that
are sent to <a class="reference internal" href="index.html#topics-spiders"><span class="std std-ref">Spiders</span></a> for processing and to process the requests
and items that are generated from spiders.</p>
<div class="section" id="activating-a-spider-middleware">
<span id="topics-spider-middleware-setting"></span><h4>Activating a spider middleware<a class="headerlink" href="#activating-a-spider-middleware" title="Permalink to this headline">¶</a></h4>
<p>To activate a spider middleware component, add it to the
<a class="reference internal" href="index.html#std:setting-SPIDER_MIDDLEWARES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_MIDDLEWARES</span></code></a> setting, which is a dict whose keys are the
middleware class path and their values are the middleware orders.</p>
<p>Here’s an example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">SPIDER_MIDDLEWARES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;myproject.middlewares.CustomSpiderMiddleware&#39;</span><span class="p">:</span> <span class="mi">543</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The <a class="reference internal" href="index.html#std:setting-SPIDER_MIDDLEWARES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_MIDDLEWARES</span></code></a> setting is merged with the
<a class="reference internal" href="index.html#std:setting-SPIDER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_MIDDLEWARES_BASE</span></code></a> setting defined in Scrapy (and not meant to
be overridden) and then sorted by order to get the final sorted list of enabled
middlewares: the first middleware is the one closer to the engine and the last
is the one closer to the spider. In other words,
the <a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_input()</span></code></a>
method of each middleware will be invoked in increasing
middleware order (100, 200, 300, …), and the
<a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_output()</span></code></a> method
of each middleware will be invoked in decreasing order.</p>
<p>To decide which order to assign to your middleware see the
<a class="reference internal" href="index.html#std:setting-SPIDER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_MIDDLEWARES_BASE</span></code></a> setting and pick a value according to where
you want to insert the middleware. The order does matter because each
middleware performs a different action and your middleware could depend on some
previous (or subsequent) middleware being applied.</p>
<p>If you want to disable a builtin middleware (the ones defined in
<a class="reference internal" href="index.html#std:setting-SPIDER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_MIDDLEWARES_BASE</span></code></a>, and enabled by default) you must define it
in your project <a class="reference internal" href="index.html#std:setting-SPIDER_MIDDLEWARES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_MIDDLEWARES</span></code></a> setting and assign <code class="docutils literal notranslate"><span class="pre">None</span></code> as its
value.  For example, if you want to disable the off-site middleware:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">SPIDER_MIDDLEWARES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;myproject.middlewares.CustomSpiderMiddleware&#39;</span><span class="p">:</span> <span class="mi">543</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.spidermiddlewares.offsite.OffsiteMiddleware&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Finally, keep in mind that some middlewares may need to be enabled through a
particular setting. See each middleware documentation for more info.</p>
</div>
<div class="section" id="writing-your-own-spider-middleware">
<span id="custom-spider-middleware"></span><h4>Writing your own spider middleware<a class="headerlink" href="#writing-your-own-spider-middleware" title="Permalink to this headline">¶</a></h4>
<p>Each spider middleware is a Python class that defines one or more of the
methods defined below.</p>
<p>The main entry point is the <code class="docutils literal notranslate"><span class="pre">from_crawler</span></code> class method, which receives a
<a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> instance. The <a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a>
object gives you access, for example, to the <a class="reference internal" href="index.html#topics-settings"><span class="std std-ref">settings</span></a>.</p>
<span class="target" id="module-scrapy.spidermiddlewares"></span><dl class="class">
<dt id="scrapy.spidermiddlewares.SpiderMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.spidermiddlewares.</code><code class="descname">SpiderMiddleware</code><a class="headerlink" href="#scrapy.spidermiddlewares.SpiderMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input">
<code class="descname">process_spider_input</code><span class="sig-paren">(</span><em>response</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called for each response that goes through the spider
middleware and into the spider, for processing.</p>
<p><a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_input()</span></code></a> should return <code class="docutils literal notranslate"><span class="pre">None</span></code> or raise an
exception.</p>
<p>If it returns <code class="docutils literal notranslate"><span class="pre">None</span></code>, Scrapy will continue processing this response,
executing all other middlewares until, finally, the response is handed
to the spider for processing.</p>
<p>If it raises an exception, Scrapy won’t bother calling any other spider
middleware <a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_input()</span></code></a> and will call the request
errback if there is one, otherwise it will start the <a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_exception()</span></code></a>
chain. The output of the errback is chained back in the other
direction for <a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_output()</span></code></a> to process it, or
<a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_exception()</span></code></a> if it raised an exception.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object) – the response being processed</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider for which this response is intended</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output">
<code class="descname">process_spider_output</code><span class="sig-paren">(</span><em>response</em>, <em>result</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called with the results returned from the Spider, after
it has processed the response.</p>
<p><a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_output()</span></code></a> must return an iterable of
<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a>, dict or <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a>
objects.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object) – the response which generated this output from the
spider</li>
<li><strong>result</strong> (an iterable of <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a>, dict
or <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a> objects) – the result returned by the spider</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider whose result is being processed</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception">
<code class="descname">process_spider_exception</code><span class="sig-paren">(</span><em>response</em>, <em>exception</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception" title="Permalink to this definition">¶</a></dt>
<dd><p>This method is called when a spider or <a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_output()</span></code></a>
method (from a previous spider middleware) raises an exception.</p>
<p><a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_exception()</span></code></a> should return either <code class="docutils literal notranslate"><span class="pre">None</span></code> or an
iterable of <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a>, dict or
<a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a> objects.</p>
<p>If it returns <code class="docutils literal notranslate"><span class="pre">None</span></code>, Scrapy will continue processing this exception,
executing any other <a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_exception()</span></code></a> in the following
middleware components, until no middleware components are left and the
exception reaches the engine (where it’s logged and discarded).</p>
<p>If it returns an iterable the <a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_output()</span></code></a> pipeline
kicks in, starting from the next spider middleware, and no other
<a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_exception()</span></code></a> will be called.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object) – the response being processed when the exception was
raised</li>
<li><strong>exception</strong> (<a class="reference external" href="https://docs.python.org/2/library/exceptions.html#exceptions.Exception">Exception</a> object) – the exception raised</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider which raised the exception</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.spidermiddlewares.SpiderMiddleware.process_start_requests">
<code class="descname">process_start_requests</code><span class="sig-paren">(</span><em>start_requests</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_start_requests" title="Permalink to this definition">¶</a></dt>
<dd><div class="versionadded">
<p><span class="versionmodified">New in version 0.15.</span></p>
</div>
<p>This method is called with the start requests of the spider, and works
similarly to the <a class="reference internal" href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_output()</span></code></a> method, except that it
doesn’t have a response associated and must return only requests (not
items).</p>
<p>It receives an iterable (in the <code class="docutils literal notranslate"><span class="pre">start_requests</span></code> parameter) and must
return another iterable of <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> objects.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">When implementing this method in your spider middleware, you
should always return an iterable (that follows the input one) and
not consume all <code class="docutils literal notranslate"><span class="pre">start_requests</span></code> iterator because it can be very
large (or even unbounded) and cause a memory overflow. The Scrapy
engine is designed to pull start requests while it has capacity to
process them, so the start requests iterator can be effectively
endless where there is some other condition for stopping the spider
(like a time limit or item/page count).</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>start_requests</strong> (an iterable of <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a>) – the start requests</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider to whom the start requests belong</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.spidermiddlewares.SpiderMiddleware.from_crawler">
<code class="descname">from_crawler</code><span class="sig-paren">(</span><em>cls</em>, <em>crawler</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spidermiddlewares.SpiderMiddleware.from_crawler" title="Permalink to this definition">¶</a></dt>
<dd><p>If present, this classmethod is called to create a middleware instance
from a <a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a>. It must return a new instance
of the middleware. Crawler object provides access to all Scrapy core
components like settings and signals; it is a way for middleware to
access them and hook its functionality into Scrapy.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>crawler</strong> (<a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> object) – crawler that uses this middleware</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="built-in-spider-middleware-reference">
<span id="topics-spider-middleware-ref"></span><h4>Built-in spider middleware reference<a class="headerlink" href="#built-in-spider-middleware-reference" title="Permalink to this headline">¶</a></h4>
<p>This page describes all spider middleware components that come with Scrapy. For
information on how to use them and how to write your own spider middleware, see
the <a class="reference internal" href="#topics-spider-middleware"><span class="std std-ref">spider middleware usage guide</span></a>.</p>
<p>For a list of the components enabled by default (and their orders) see the
<a class="reference internal" href="index.html#std:setting-SPIDER_MIDDLEWARES_BASE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_MIDDLEWARES_BASE</span></code></a> setting.</p>
<div class="section" id="module-scrapy.spidermiddlewares.depth">
<span id="depthmiddleware"></span><h5>DepthMiddleware<a class="headerlink" href="#module-scrapy.spidermiddlewares.depth" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.spidermiddlewares.depth.DepthMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.spidermiddlewares.depth.</code><code class="descname">DepthMiddleware</code><a class="headerlink" href="#scrapy.spidermiddlewares.depth.DepthMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>DepthMiddleware is used for tracking the depth of each Request inside the
site being scraped. It works by setting <code class="docutils literal notranslate"><span class="pre">request.meta['depth']</span> <span class="pre">=</span> <span class="pre">0</span></code> whenever
there is no value previously set (usually just the first Request) and
incrementing it by 1 otherwise.</p>
<p>It can be used to limit the maximum depth to scrape, control Request
priority based on their depth, and things like that.</p>
<p>The <a class="reference internal" href="#scrapy.spidermiddlewares.depth.DepthMiddleware" title="scrapy.spidermiddlewares.depth.DepthMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">DepthMiddleware</span></code></a> can be configured through the following
settings (see the settings documentation for more info):</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-DEPTH_LIMIT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DEPTH_LIMIT</span></code></a> - The maximum depth that will be allowed to
crawl for any site. If zero, no limit will be imposed.</li>
<li><a class="reference internal" href="index.html#std:setting-DEPTH_STATS_VERBOSE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DEPTH_STATS_VERBOSE</span></code></a> - Whether to collect the number of
requests for each depth.</li>
<li><a class="reference internal" href="index.html#std:setting-DEPTH_PRIORITY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DEPTH_PRIORITY</span></code></a> - Whether to prioritize the requests based on
their depth.</li>
</ul>
</div></blockquote>
</dd></dl>

</div>
<div class="section" id="module-scrapy.spidermiddlewares.httperror">
<span id="httperrormiddleware"></span><h5>HttpErrorMiddleware<a class="headerlink" href="#module-scrapy.spidermiddlewares.httperror" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.spidermiddlewares.httperror.HttpErrorMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.spidermiddlewares.httperror.</code><code class="descname">HttpErrorMiddleware</code><a class="headerlink" href="#scrapy.spidermiddlewares.httperror.HttpErrorMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>Filter out unsuccessful (erroneous) HTTP responses so that spiders don’t
have to deal with them, which (most of the time) imposes an overhead,
consumes more resources, and makes the spider logic more complex.</p>
</dd></dl>

<p>According to the <a class="reference external" href="https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html">HTTP standard</a>, successful responses are those whose
status codes are in the 200-300 range.</p>
<p>If you still want to process response codes outside that range, you can
specify which response codes the spider is able to handle using the
<code class="docutils literal notranslate"><span class="pre">handle_httpstatus_list</span></code> spider attribute or
<a class="reference internal" href="#std:setting-HTTPERROR_ALLOWED_CODES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">HTTPERROR_ALLOWED_CODES</span></code></a> setting.</p>
<p>For example, if you want your spider to handle 404 responses you can do
this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">CrawlSpider</span><span class="p">):</span>
    <span class="n">handle_httpstatus_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">404</span><span class="p">]</span>
</pre></div>
</div>
<span class="target" id="std:reqmeta-handle_httpstatus_list"></span><p id="std:reqmeta-handle_httpstatus_all">The <code class="docutils literal notranslate"><span class="pre">handle_httpstatus_list</span></code> key of <a class="reference internal" href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.meta</span></code></a> can also be used to specify which response codes to
allow on a per-request basis. You can also set the meta key <code class="docutils literal notranslate"><span class="pre">handle_httpstatus_all</span></code>
to <code class="docutils literal notranslate"><span class="pre">True</span></code> if you want to allow any response code for a request.</p>
<p>Keep in mind, however, that it’s usually a bad idea to handle non-200
responses, unless you really know what you’re doing.</p>
<p>For more information see: <a class="reference external" href="https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html">HTTP Status Code Definitions</a>.</p>
<div class="section" id="httperrormiddleware-settings">
<h6>HttpErrorMiddleware settings<a class="headerlink" href="#httperrormiddleware-settings" title="Permalink to this headline">¶</a></h6>
<div class="section" id="httperror-allowed-codes">
<span id="std:setting-HTTPERROR_ALLOWED_CODES"></span><h7>HTTPERROR_ALLOWED_CODES<a class="headerlink" href="#httperror-allowed-codes" title="Permalink to this headline">¶</a></h7>
<p>Default: <code class="docutils literal notranslate"><span class="pre">[]</span></code></p>
<p>Pass all responses with non-200 status codes contained in this list.</p>
</div>
<div class="section" id="httperror-allow-all">
<span id="std:setting-HTTPERROR_ALLOW_ALL"></span><h7>HTTPERROR_ALLOW_ALL<a class="headerlink" href="#httperror-allow-all" title="Permalink to this headline">¶</a></h7>
<p>Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>Pass all responses, regardless of its status code.</p>
</div>
</div>
</div>
<div class="section" id="module-scrapy.spidermiddlewares.offsite">
<span id="offsitemiddleware"></span><h5>OffsiteMiddleware<a class="headerlink" href="#module-scrapy.spidermiddlewares.offsite" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.spidermiddlewares.offsite.OffsiteMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.spidermiddlewares.offsite.</code><code class="descname">OffsiteMiddleware</code><a class="headerlink" href="#scrapy.spidermiddlewares.offsite.OffsiteMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>Filters out Requests for URLs outside the domains covered by the spider.</p>
<p>This middleware filters out every request whose host names aren’t in the
spider’s <a class="reference internal" href="index.html#scrapy.spiders.Spider.allowed_domains" title="scrapy.spiders.Spider.allowed_domains"><code class="xref py py-attr docutils literal notranslate"><span class="pre">allowed_domains</span></code></a> attribute.
All subdomains of any domain in the list are also allowed.
E.g. the rule <code class="docutils literal notranslate"><span class="pre">www.example.org</span></code> will also allow <code class="docutils literal notranslate"><span class="pre">bob.www.example.org</span></code>
but not <code class="docutils literal notranslate"><span class="pre">www2.example.com</span></code> nor <code class="docutils literal notranslate"><span class="pre">example.com</span></code>.</p>
<p>When your spider returns a request for a domain not belonging to those
covered by the spider, this middleware will log a debug message similar to
this one:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">DEBUG</span><span class="p">:</span> <span class="n">Filtered</span> <span class="n">offsite</span> <span class="n">request</span> <span class="n">to</span> <span class="s1">&#39;www.othersite.com&#39;</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">GET</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">www</span><span class="o">.</span><span class="n">othersite</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">some</span><span class="o">/</span><span class="n">page</span><span class="o">.</span><span class="n">html</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>To avoid filling the log with too much noise, it will only print one of
these messages for each new domain filtered. So, for example, if another
request for <code class="docutils literal notranslate"><span class="pre">www.othersite.com</span></code> is filtered, no log message will be
printed. But if a request for <code class="docutils literal notranslate"><span class="pre">someothersite.com</span></code> is filtered, a message
will be printed (but only for the first request filtered).</p>
<p>If the spider doesn’t define an
<a class="reference internal" href="index.html#scrapy.spiders.Spider.allowed_domains" title="scrapy.spiders.Spider.allowed_domains"><code class="xref py py-attr docutils literal notranslate"><span class="pre">allowed_domains</span></code></a> attribute, or the
attribute is empty, the offsite middleware will allow all requests.</p>
<p>If the request has the <code class="xref py py-attr docutils literal notranslate"><span class="pre">dont_filter</span></code> attribute
set, the offsite middleware will allow the request even if its domain is not
listed in allowed domains.</p>
</dd></dl>

</div>
<div class="section" id="module-scrapy.spidermiddlewares.referer">
<span id="referermiddleware"></span><h5>RefererMiddleware<a class="headerlink" href="#module-scrapy.spidermiddlewares.referer" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.spidermiddlewares.referer.RefererMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.spidermiddlewares.referer.</code><code class="descname">RefererMiddleware</code><a class="headerlink" href="#scrapy.spidermiddlewares.referer.RefererMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>Populates Request <code class="docutils literal notranslate"><span class="pre">Referer</span></code> header, based on the URL of the Response which
generated it.</p>
</dd></dl>

<div class="section" id="referermiddleware-settings">
<h6>RefererMiddleware settings<a class="headerlink" href="#referermiddleware-settings" title="Permalink to this headline">¶</a></h6>
<div class="section" id="referer-enabled">
<span id="std:setting-REFERER_ENABLED"></span><h7>REFERER_ENABLED<a class="headerlink" href="#referer-enabled" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.15.</span></p>
</div>
<p>Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p>
<p>Whether to enable referer middleware.</p>
</div>
<div class="section" id="referrer-policy">
<span id="std:setting-REFERRER_POLICY"></span><h7>REFERRER_POLICY<a class="headerlink" href="#referrer-policy" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 1.4.</span></p>
</div>
<p>Default: <code class="docutils literal notranslate"><span class="pre">'scrapy.spidermiddlewares.referer.DefaultReferrerPolicy'</span></code></p>
<p id="std:reqmeta-referrer_policy"><a class="reference external" href="https://www.w3.org/TR/referrer-policy">Referrer Policy</a> to apply when populating Request “Referer” header.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">You can also set the Referrer Policy per request,
using the special <code class="docutils literal notranslate"><span class="pre">&quot;referrer_policy&quot;</span></code> <a class="reference internal" href="index.html#topics-request-meta"><span class="std std-ref">Request.meta</span></a> key,
with the same acceptable values as for the <code class="docutils literal notranslate"><span class="pre">REFERRER_POLICY</span></code> setting.</p>
</div>
<div class="section" id="acceptable-values-for-referrer-policy">
<h8>Acceptable values for REFERRER_POLICY<a class="headerlink" href="#acceptable-values-for-referrer-policy" title="Permalink to this headline">¶</a></h8>
<ul class="simple">
<li>either a path to a <code class="docutils literal notranslate"><span class="pre">scrapy.spidermiddlewares.referer.ReferrerPolicy</span></code>
subclass — a custom policy or one of the built-in ones (see classes below),</li>
<li>or one of the standard W3C-defined string values,</li>
<li>or the special <code class="docutils literal notranslate"><span class="pre">&quot;scrapy-default&quot;</span></code>.</li>
</ul>
<table border="1" class="docutils">
<colgroup>
<col width="34%" />
<col width="66%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">String value</th>
<th class="head">Class name (as a string)</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><code class="docutils literal notranslate"><span class="pre">&quot;scrapy-default&quot;</span></code> (default)</td>
<td><a class="reference internal" href="#scrapy.spidermiddlewares.referer.DefaultReferrerPolicy" title="scrapy.spidermiddlewares.referer.DefaultReferrerPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.spidermiddlewares.referer.DefaultReferrerPolicy</span></code></a></td>
</tr>
<tr class="row-odd"><td><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer">“no-referrer”</a></td>
<td><a class="reference internal" href="#scrapy.spidermiddlewares.referer.NoReferrerPolicy" title="scrapy.spidermiddlewares.referer.NoReferrerPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.spidermiddlewares.referer.NoReferrerPolicy</span></code></a></td>
</tr>
<tr class="row-even"><td><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade">“no-referrer-when-downgrade”</a></td>
<td><a class="reference internal" href="#scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy" title="scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy</span></code></a></td>
</tr>
<tr class="row-odd"><td><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-same-origin">“same-origin”</a></td>
<td><a class="reference internal" href="#scrapy.spidermiddlewares.referer.SameOriginPolicy" title="scrapy.spidermiddlewares.referer.SameOriginPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.spidermiddlewares.referer.SameOriginPolicy</span></code></a></td>
</tr>
<tr class="row-even"><td><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-origin">“origin”</a></td>
<td><a class="reference internal" href="#scrapy.spidermiddlewares.referer.OriginPolicy" title="scrapy.spidermiddlewares.referer.OriginPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.spidermiddlewares.referer.OriginPolicy</span></code></a></td>
</tr>
<tr class="row-odd"><td><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin">“strict-origin”</a></td>
<td><a class="reference internal" href="#scrapy.spidermiddlewares.referer.StrictOriginPolicy" title="scrapy.spidermiddlewares.referer.StrictOriginPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.spidermiddlewares.referer.StrictOriginPolicy</span></code></a></td>
</tr>
<tr class="row-even"><td><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-origin-when-cross-origin">“origin-when-cross-origin”</a></td>
<td><a class="reference internal" href="#scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy" title="scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy</span></code></a></td>
</tr>
<tr class="row-odd"><td><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin-when-cross-origin">“strict-origin-when-cross-origin”</a></td>
<td><a class="reference internal" href="#scrapy.spidermiddlewares.referer.StrictOriginWhenCrossOriginPolicy" title="scrapy.spidermiddlewares.referer.StrictOriginWhenCrossOriginPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.spidermiddlewares.referer.StrictOriginWhenCrossOriginPolicy</span></code></a></td>
</tr>
<tr class="row-even"><td><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-unsafe-url">“unsafe-url”</a></td>
<td><a class="reference internal" href="#scrapy.spidermiddlewares.referer.UnsafeUrlPolicy" title="scrapy.spidermiddlewares.referer.UnsafeUrlPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.spidermiddlewares.referer.UnsafeUrlPolicy</span></code></a></td>
</tr>
</tbody>
</table>
<dl class="class">
<dt id="scrapy.spidermiddlewares.referer.DefaultReferrerPolicy">
<em class="property">class </em><code class="descclassname">scrapy.spidermiddlewares.referer.</code><code class="descname">DefaultReferrerPolicy</code><a class="headerlink" href="#scrapy.spidermiddlewares.referer.DefaultReferrerPolicy" title="Permalink to this definition">¶</a></dt>
<dd><p>A variant of “no-referrer-when-downgrade”,
with the addition that “Referer” is not sent if the parent request was
using <code class="docutils literal notranslate"><span class="pre">file://</span></code> or <code class="docutils literal notranslate"><span class="pre">s3://</span></code> scheme.</p>
</dd></dl>

<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p>Scrapy’s default referrer policy — just like <a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade">“no-referrer-when-downgrade”</a>,
the W3C-recommended value for browsers — will send a non-empty
“Referer” header from any <code class="docutils literal notranslate"><span class="pre">http(s)://</span></code> to any <code class="docutils literal notranslate"><span class="pre">https://</span></code> URL,
even if the domain is different.</p>
<p class="last"><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-same-origin">“same-origin”</a> may be a better choice if you want to remove referrer
information for cross-domain requests.</p>
</div>
<dl class="class">
<dt id="scrapy.spidermiddlewares.referer.NoReferrerPolicy">
<em class="property">class </em><code class="descclassname">scrapy.spidermiddlewares.referer.</code><code class="descname">NoReferrerPolicy</code><a class="headerlink" href="#scrapy.spidermiddlewares.referer.NoReferrerPolicy" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer">https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer</a></p>
<p>The simplest policy is “no-referrer”, which specifies that no referrer information
is to be sent along with requests made from a particular request client to any origin.
The header will be omitted entirely.</p>
</dd></dl>

<dl class="class">
<dt id="scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy">
<em class="property">class </em><code class="descclassname">scrapy.spidermiddlewares.referer.</code><code class="descname">NoReferrerWhenDowngradePolicy</code><a class="headerlink" href="#scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade">https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade</a></p>
<p>The “no-referrer-when-downgrade” policy sends a full URL along with requests
from a TLS-protected environment settings object to a potentially trustworthy URL,
and requests from clients which are not TLS-protected to any origin.</p>
<p>Requests from TLS-protected clients to non-potentially trustworthy URLs,
on the other hand, will contain no referrer information.
A Referer HTTP header will not be sent.</p>
<p>This is a user agent’s default behavior, if no policy is otherwise specified.</p>
</dd></dl>

<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>“no-referrer-when-downgrade” policy is the W3C-recommended default,
and is used by major web browsers.</p>
<p class="last">However, it is NOT Scrapy’s default referrer policy (see <a class="reference internal" href="#scrapy.spidermiddlewares.referer.DefaultReferrerPolicy" title="scrapy.spidermiddlewares.referer.DefaultReferrerPolicy"><code class="xref py py-class docutils literal notranslate"><span class="pre">DefaultReferrerPolicy</span></code></a>).</p>
</div>
<dl class="class">
<dt id="scrapy.spidermiddlewares.referer.SameOriginPolicy">
<em class="property">class </em><code class="descclassname">scrapy.spidermiddlewares.referer.</code><code class="descname">SameOriginPolicy</code><a class="headerlink" href="#scrapy.spidermiddlewares.referer.SameOriginPolicy" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-same-origin">https://www.w3.org/TR/referrer-policy/#referrer-policy-same-origin</a></p>
<p>The “same-origin” policy specifies that a full URL, stripped for use as a referrer,
is sent as referrer information when making same-origin requests from a particular request client.</p>
<p>Cross-origin requests, on the other hand, will contain no referrer information.
A Referer HTTP header will not be sent.</p>
</dd></dl>

<dl class="class">
<dt id="scrapy.spidermiddlewares.referer.OriginPolicy">
<em class="property">class </em><code class="descclassname">scrapy.spidermiddlewares.referer.</code><code class="descname">OriginPolicy</code><a class="headerlink" href="#scrapy.spidermiddlewares.referer.OriginPolicy" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-origin">https://www.w3.org/TR/referrer-policy/#referrer-policy-origin</a></p>
<p>The “origin” policy specifies that only the ASCII serialization
of the origin of the request client is sent as referrer information
when making both same-origin requests and cross-origin requests
from a particular request client.</p>
</dd></dl>

<dl class="class">
<dt id="scrapy.spidermiddlewares.referer.StrictOriginPolicy">
<em class="property">class </em><code class="descclassname">scrapy.spidermiddlewares.referer.</code><code class="descname">StrictOriginPolicy</code><a class="headerlink" href="#scrapy.spidermiddlewares.referer.StrictOriginPolicy" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin">https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin</a></p>
<p>The “strict-origin” policy sends the ASCII serialization
of the origin of the request client when making requests:
- from a TLS-protected environment settings object to a potentially trustworthy URL, and
- from non-TLS-protected environment settings objects to any origin.</p>
<p>Requests from TLS-protected request clients to non- potentially trustworthy URLs,
on the other hand, will contain no referrer information.
A Referer HTTP header will not be sent.</p>
</dd></dl>

<dl class="class">
<dt id="scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy">
<em class="property">class </em><code class="descclassname">scrapy.spidermiddlewares.referer.</code><code class="descname">OriginWhenCrossOriginPolicy</code><a class="headerlink" href="#scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-origin-when-cross-origin">https://www.w3.org/TR/referrer-policy/#referrer-policy-origin-when-cross-origin</a></p>
<p>The “origin-when-cross-origin” policy specifies that a full URL,
stripped for use as a referrer, is sent as referrer information
when making same-origin requests from a particular request client,
and only the ASCII serialization of the origin of the request client
is sent as referrer information when making cross-origin requests
from a particular request client.</p>
</dd></dl>

<dl class="class">
<dt id="scrapy.spidermiddlewares.referer.StrictOriginWhenCrossOriginPolicy">
<em class="property">class </em><code class="descclassname">scrapy.spidermiddlewares.referer.</code><code class="descname">StrictOriginWhenCrossOriginPolicy</code><a class="headerlink" href="#scrapy.spidermiddlewares.referer.StrictOriginWhenCrossOriginPolicy" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin-when-cross-origin">https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin-when-cross-origin</a></p>
<p>The “strict-origin-when-cross-origin” policy specifies that a full URL,
stripped for use as a referrer, is sent as referrer information
when making same-origin requests from a particular request client,
and only the ASCII serialization of the origin of the request client
when making cross-origin requests:</p>
<ul class="simple">
<li>from a TLS-protected environment settings object to a potentially trustworthy URL, and</li>
<li>from non-TLS-protected environment settings objects to any origin.</li>
</ul>
<p>Requests from TLS-protected clients to non- potentially trustworthy URLs,
on the other hand, will contain no referrer information.
A Referer HTTP header will not be sent.</p>
</dd></dl>

<dl class="class">
<dt id="scrapy.spidermiddlewares.referer.UnsafeUrlPolicy">
<em class="property">class </em><code class="descclassname">scrapy.spidermiddlewares.referer.</code><code class="descname">UnsafeUrlPolicy</code><a class="headerlink" href="#scrapy.spidermiddlewares.referer.UnsafeUrlPolicy" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://www.w3.org/TR/referrer-policy/#referrer-policy-unsafe-url">https://www.w3.org/TR/referrer-policy/#referrer-policy-unsafe-url</a></p>
<p>The “unsafe-url” policy specifies that a full URL, stripped for use as a referrer,
is sent along with both cross-origin requests
and same-origin requests made from a particular request client.</p>
<p>Note: The policy’s name doesn’t lie; it is unsafe.
This policy will leak origins and paths from TLS-protected resources
to insecure origins.
Carefully consider the impact of setting such a policy for potentially sensitive documents.</p>
</dd></dl>

<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">“unsafe-url” policy is NOT recommended.</p>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="module-scrapy.spidermiddlewares.urllength">
<span id="urllengthmiddleware"></span><h5>UrlLengthMiddleware<a class="headerlink" href="#module-scrapy.spidermiddlewares.urllength" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.spidermiddlewares.urllength.UrlLengthMiddleware">
<em class="property">class </em><code class="descclassname">scrapy.spidermiddlewares.urllength.</code><code class="descname">UrlLengthMiddleware</code><a class="headerlink" href="#scrapy.spidermiddlewares.urllength.UrlLengthMiddleware" title="Permalink to this definition">¶</a></dt>
<dd><p>Filters out requests with URLs longer than URLLENGTH_LIMIT</p>
<p>The <a class="reference internal" href="#scrapy.spidermiddlewares.urllength.UrlLengthMiddleware" title="scrapy.spidermiddlewares.urllength.UrlLengthMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">UrlLengthMiddleware</span></code></a> can be configured through the following
settings (see the settings documentation for more info):</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-URLLENGTH_LIMIT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">URLLENGTH_LIMIT</span></code></a> - The maximum URL length to allow for crawled URLs.</li>
</ul>
</div></blockquote>
</dd></dl>

</div>
</div>
</div>
<span id="document-topics/extensions"></span><div class="section" id="extensions">
<span id="topics-extensions"></span><h3>Extensions<a class="headerlink" href="#extensions" title="Permalink to this headline">¶</a></h3>
<p>The extensions framework provides a mechanism for inserting your own
custom functionality into Scrapy.</p>
<p>Extensions are just regular classes that are instantiated at Scrapy startup,
when extensions are initialized.</p>
<div class="section" id="extension-settings">
<h4>Extension settings<a class="headerlink" href="#extension-settings" title="Permalink to this headline">¶</a></h4>
<p>Extensions use the <a class="reference internal" href="index.html#topics-settings"><span class="std std-ref">Scrapy settings</span></a> to manage their
settings, just like any other Scrapy code.</p>
<p>It is customary for extensions to prefix their settings with their own name, to
avoid collision with existing (and future) extensions. For example, a
hypothetic extension to handle <a class="reference external" href="https://en.wikipedia.org/wiki/Sitemaps">Google Sitemaps</a> would use settings like
<code class="docutils literal notranslate"><span class="pre">GOOGLESITEMAP_ENABLED</span></code>, <code class="docutils literal notranslate"><span class="pre">GOOGLESITEMAP_DEPTH</span></code>, and so on.</p>
</div>
<div class="section" id="loading-activating-extensions">
<h4>Loading &amp; activating extensions<a class="headerlink" href="#loading-activating-extensions" title="Permalink to this headline">¶</a></h4>
<p>Extensions are loaded and activated at startup by instantiating a single
instance of the extension class. Therefore, all the extension initialization
code must be performed in the class constructor (<code class="docutils literal notranslate"><span class="pre">__init__</span></code> method).</p>
<p>To make an extension available, add it to the <a class="reference internal" href="index.html#std:setting-EXTENSIONS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">EXTENSIONS</span></code></a> setting in
your Scrapy settings. In <a class="reference internal" href="index.html#std:setting-EXTENSIONS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">EXTENSIONS</span></code></a>, each extension is represented
by a string: the full Python path to the extension’s class name. For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">EXTENSIONS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;scrapy.extensions.corestats.CoreStats&#39;</span><span class="p">:</span> <span class="mi">500</span><span class="p">,</span>
    <span class="s1">&#39;scrapy.extensions.telnet.TelnetConsole&#39;</span><span class="p">:</span> <span class="mi">500</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>As you can see, the <a class="reference internal" href="index.html#std:setting-EXTENSIONS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">EXTENSIONS</span></code></a> setting is a dict where the keys are
the extension paths, and their values are the orders, which define the
extension <em>loading</em> order. The <a class="reference internal" href="index.html#std:setting-EXTENSIONS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">EXTENSIONS</span></code></a> setting is merged with the
<a class="reference internal" href="index.html#std:setting-EXTENSIONS_BASE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">EXTENSIONS_BASE</span></code></a> setting defined in Scrapy (and not meant to be
overridden) and then sorted by order to get the final sorted list of enabled
extensions.</p>
<p>As extensions typically do not depend on each other, their loading order is
irrelevant in most cases. This is why the <a class="reference internal" href="index.html#std:setting-EXTENSIONS_BASE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">EXTENSIONS_BASE</span></code></a> setting
defines all extensions with the same order (<code class="docutils literal notranslate"><span class="pre">0</span></code>). However, this feature can
be exploited if you need to add an extension which depends on other extensions
already loaded.</p>
</div>
<div class="section" id="available-enabled-and-disabled-extensions">
<h4>Available, enabled and disabled extensions<a class="headerlink" href="#available-enabled-and-disabled-extensions" title="Permalink to this headline">¶</a></h4>
<p>Not all available extensions will be enabled. Some of them usually depend on a
particular setting. For example, the HTTP Cache extension is available by default
but disabled unless the <a class="reference internal" href="index.html#std:setting-HTTPCACHE_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">HTTPCACHE_ENABLED</span></code></a> setting is set.</p>
</div>
<div class="section" id="disabling-an-extension">
<h4>Disabling an extension<a class="headerlink" href="#disabling-an-extension" title="Permalink to this headline">¶</a></h4>
<p>In order to disable an extension that comes enabled by default (ie. those
included in the <a class="reference internal" href="index.html#std:setting-EXTENSIONS_BASE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">EXTENSIONS_BASE</span></code></a> setting) you must set its order to
<code class="docutils literal notranslate"><span class="pre">None</span></code>. For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">EXTENSIONS</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;scrapy.extensions.corestats.CoreStats&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="writing-your-own-extension">
<h4>Writing your own extension<a class="headerlink" href="#writing-your-own-extension" title="Permalink to this headline">¶</a></h4>
<p>Each extension is a Python class. The main entry point for a Scrapy extension
(this also includes middlewares and pipelines) is the <code class="docutils literal notranslate"><span class="pre">from_crawler</span></code>
class method which receives a <code class="docutils literal notranslate"><span class="pre">Crawler</span></code> instance. Through the Crawler object
you can access settings, signals, stats, and also control the crawling behaviour.</p>
<p>Typically, extensions connect to <a class="reference internal" href="index.html#topics-signals"><span class="std std-ref">signals</span></a> and perform
tasks triggered by them.</p>
<p>Finally, if the <code class="docutils literal notranslate"><span class="pre">from_crawler</span></code> method raises the
<a class="reference internal" href="index.html#scrapy.exceptions.NotConfigured" title="scrapy.exceptions.NotConfigured"><code class="xref py py-exc docutils literal notranslate"><span class="pre">NotConfigured</span></code></a> exception, the extension will be
disabled. Otherwise, the extension will be enabled.</p>
<div class="section" id="sample-extension">
<h5>Sample extension<a class="headerlink" href="#sample-extension" title="Permalink to this headline">¶</a></h5>
<p>Here we will implement a simple extension to illustrate the concepts described
in the previous section. This extension will log a message every time:</p>
<ul class="simple">
<li>a spider is opened</li>
<li>a spider is closed</li>
<li>a specific number of items are scraped</li>
</ul>
<p>The extension will be enabled through the <code class="docutils literal notranslate"><span class="pre">MYEXT_ENABLED</span></code> setting and the
number of items will be specified through the <code class="docutils literal notranslate"><span class="pre">MYEXT_ITEMCOUNT</span></code> setting.</p>
<p>Here is the code of such extension:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">from</span> <span class="nn">scrapy</span> <span class="k">import</span> <span class="n">signals</span>
<span class="kn">from</span> <span class="nn">scrapy.exceptions</span> <span class="k">import</span> <span class="n">NotConfigured</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">SpiderOpenCloseLogging</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item_count</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">item_count</span> <span class="o">=</span> <span class="n">item_count</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">items_scraped</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_crawler</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">crawler</span><span class="p">):</span>
        <span class="c1"># first check if the extension should be enabled and raise</span>
        <span class="c1"># NotConfigured otherwise</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">crawler</span><span class="o">.</span><span class="n">settings</span><span class="o">.</span><span class="n">getbool</span><span class="p">(</span><span class="s1">&#39;MYEXT_ENABLED&#39;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="n">NotConfigured</span>

        <span class="c1"># get the number of items from settings</span>
        <span class="n">item_count</span> <span class="o">=</span> <span class="n">crawler</span><span class="o">.</span><span class="n">settings</span><span class="o">.</span><span class="n">getint</span><span class="p">(</span><span class="s1">&#39;MYEXT_ITEMCOUNT&#39;</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>

        <span class="c1"># instantiate the extension object</span>
        <span class="n">ext</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">(</span><span class="n">item_count</span><span class="p">)</span>

        <span class="c1"># connect the extension object to signals</span>
        <span class="n">crawler</span><span class="o">.</span><span class="n">signals</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="n">ext</span><span class="o">.</span><span class="n">spider_opened</span><span class="p">,</span> <span class="n">signal</span><span class="o">=</span><span class="n">signals</span><span class="o">.</span><span class="n">spider_opened</span><span class="p">)</span>
        <span class="n">crawler</span><span class="o">.</span><span class="n">signals</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="n">ext</span><span class="o">.</span><span class="n">spider_closed</span><span class="p">,</span> <span class="n">signal</span><span class="o">=</span><span class="n">signals</span><span class="o">.</span><span class="n">spider_closed</span><span class="p">)</span>
        <span class="n">crawler</span><span class="o">.</span><span class="n">signals</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="n">ext</span><span class="o">.</span><span class="n">item_scraped</span><span class="p">,</span> <span class="n">signal</span><span class="o">=</span><span class="n">signals</span><span class="o">.</span><span class="n">item_scraped</span><span class="p">)</span>

        <span class="c1"># return the extension object</span>
        <span class="k">return</span> <span class="n">ext</span>

    <span class="k">def</span> <span class="nf">spider_opened</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;opened spider </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">spider</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">spider_closed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;closed spider </span><span class="si">%s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">spider</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">item_scraped</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">items_scraped</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">items_scraped</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">item_count</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;scraped </span><span class="si">%d</span><span class="s2"> items&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">items_scraped</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="built-in-extensions-reference">
<span id="topics-extensions-ref"></span><h4>Built-in extensions reference<a class="headerlink" href="#built-in-extensions-reference" title="Permalink to this headline">¶</a></h4>
<div class="section" id="general-purpose-extensions">
<h5>General purpose extensions<a class="headerlink" href="#general-purpose-extensions" title="Permalink to this headline">¶</a></h5>
<div class="section" id="module-scrapy.extensions.logstats">
<span id="log-stats-extension"></span><h6>Log Stats extension<a class="headerlink" href="#module-scrapy.extensions.logstats" title="Permalink to this headline">¶</a></h6>
<dl class="class">
<dt id="scrapy.extensions.logstats.LogStats">
<em class="property">class </em><code class="descclassname">scrapy.extensions.logstats.</code><code class="descname">LogStats</code><a class="headerlink" href="#scrapy.extensions.logstats.LogStats" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Log basic stats like crawled pages and scraped items.</p>
</div>
<div class="section" id="module-scrapy.extensions.corestats">
<span id="core-stats-extension"></span><h6>Core Stats extension<a class="headerlink" href="#module-scrapy.extensions.corestats" title="Permalink to this headline">¶</a></h6>
<dl class="class">
<dt id="scrapy.extensions.corestats.CoreStats">
<em class="property">class </em><code class="descclassname">scrapy.extensions.corestats.</code><code class="descname">CoreStats</code><a class="headerlink" href="#scrapy.extensions.corestats.CoreStats" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Enable the collection of core statistics, provided the stats collection is
enabled (see <a class="reference internal" href="index.html#topics-stats"><span class="std std-ref">Stats Collection</span></a>).</p>
</div>
<div class="section" id="module-scrapy.extensions.telnet">
<span id="telnet-console-extension"></span><span id="topics-extensions-ref-telnetconsole"></span><h6>Telnet console extension<a class="headerlink" href="#module-scrapy.extensions.telnet" title="Permalink to this headline">¶</a></h6>
<dl class="class">
<dt id="scrapy.extensions.telnet.scrapy.extensions.telnet.TelnetConsole">
<em class="property">class </em><code class="descclassname">scrapy.extensions.telnet.</code><code class="descname">TelnetConsole</code><a class="headerlink" href="#scrapy.extensions.telnet.scrapy.extensions.telnet.TelnetConsole" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Provides a telnet console for getting into a Python interpreter inside the
currently running Scrapy process, which can be very useful for debugging.</p>
<p>The telnet console must be enabled by the <a class="reference internal" href="index.html#std:setting-TELNETCONSOLE_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">TELNETCONSOLE_ENABLED</span></code></a>
setting, and the server will listen in the port specified in
<a class="reference internal" href="index.html#std:setting-TELNETCONSOLE_PORT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">TELNETCONSOLE_PORT</span></code></a>.</p>
</div>
<div class="section" id="module-scrapy.extensions.memusage">
<span id="memory-usage-extension"></span><span id="topics-extensions-ref-memusage"></span><h6>Memory usage extension<a class="headerlink" href="#module-scrapy.extensions.memusage" title="Permalink to this headline">¶</a></h6>
<dl class="class">
<dt id="scrapy.extensions.memusage.scrapy.extensions.memusage.MemoryUsage">
<em class="property">class </em><code class="descclassname">scrapy.extensions.memusage.</code><code class="descname">MemoryUsage</code><a class="headerlink" href="#scrapy.extensions.memusage.scrapy.extensions.memusage.MemoryUsage" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This extension does not work in Windows.</p>
</div>
<p>Monitors the memory used by the Scrapy process that runs the spider and:</p>
<ol class="arabic simple">
<li>sends a notification e-mail when it exceeds a certain value</li>
<li>closes the spider when it exceeds a certain value</li>
</ol>
<p>The notification e-mails can be triggered when a certain warning value is
reached (<a class="reference internal" href="index.html#std:setting-MEMUSAGE_WARNING_MB"><code class="xref std std-setting docutils literal notranslate"><span class="pre">MEMUSAGE_WARNING_MB</span></code></a>) and when the maximum value is reached
(<a class="reference internal" href="index.html#std:setting-MEMUSAGE_LIMIT_MB"><code class="xref std std-setting docutils literal notranslate"><span class="pre">MEMUSAGE_LIMIT_MB</span></code></a>) which will also cause the spider to be closed
and the Scrapy process to be terminated.</p>
<p>This extension is enabled by the <a class="reference internal" href="index.html#std:setting-MEMUSAGE_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">MEMUSAGE_ENABLED</span></code></a> setting and
can be configured with the following settings:</p>
<ul class="simple">
<li><a class="reference internal" href="index.html#std:setting-MEMUSAGE_LIMIT_MB"><code class="xref std std-setting docutils literal notranslate"><span class="pre">MEMUSAGE_LIMIT_MB</span></code></a></li>
<li><a class="reference internal" href="index.html#std:setting-MEMUSAGE_WARNING_MB"><code class="xref std std-setting docutils literal notranslate"><span class="pre">MEMUSAGE_WARNING_MB</span></code></a></li>
<li><a class="reference internal" href="index.html#std:setting-MEMUSAGE_NOTIFY_MAIL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">MEMUSAGE_NOTIFY_MAIL</span></code></a></li>
<li><a class="reference internal" href="index.html#std:setting-MEMUSAGE_CHECK_INTERVAL_SECONDS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">MEMUSAGE_CHECK_INTERVAL_SECONDS</span></code></a></li>
</ul>
</div>
<div class="section" id="module-scrapy.extensions.memdebug">
<span id="memory-debugger-extension"></span><h6>Memory debugger extension<a class="headerlink" href="#module-scrapy.extensions.memdebug" title="Permalink to this headline">¶</a></h6>
<dl class="class">
<dt id="scrapy.extensions.memdebug.scrapy.extensions.memdebug.MemoryDebugger">
<em class="property">class </em><code class="descclassname">scrapy.extensions.memdebug.</code><code class="descname">MemoryDebugger</code><a class="headerlink" href="#scrapy.extensions.memdebug.scrapy.extensions.memdebug.MemoryDebugger" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>An extension for debugging memory usage. It collects information about:</p>
<ul class="simple">
<li>objects uncollected by the Python garbage collector</li>
<li>objects left alive that shouldn’t. For more info, see <a class="reference internal" href="index.html#topics-leaks-trackrefs"><span class="std std-ref">Debugging memory leaks with trackref</span></a></li>
</ul>
<p>To enable this extension, turn on the <a class="reference internal" href="index.html#std:setting-MEMDEBUG_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">MEMDEBUG_ENABLED</span></code></a> setting. The
info will be stored in the stats.</p>
</div>
<div class="section" id="module-scrapy.extensions.closespider">
<span id="close-spider-extension"></span><h6>Close spider extension<a class="headerlink" href="#module-scrapy.extensions.closespider" title="Permalink to this headline">¶</a></h6>
<dl class="class">
<dt id="scrapy.extensions.closespider.scrapy.extensions.closespider.CloseSpider">
<em class="property">class </em><code class="descclassname">scrapy.extensions.closespider.</code><code class="descname">CloseSpider</code><a class="headerlink" href="#scrapy.extensions.closespider.scrapy.extensions.closespider.CloseSpider" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Closes a spider automatically when some conditions are met, using a specific
closing reason for each condition.</p>
<p>The conditions for closing a spider can be configured through the following
settings:</p>
<ul class="simple">
<li><a class="reference internal" href="#std:setting-CLOSESPIDER_TIMEOUT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CLOSESPIDER_TIMEOUT</span></code></a></li>
<li><a class="reference internal" href="#std:setting-CLOSESPIDER_ITEMCOUNT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CLOSESPIDER_ITEMCOUNT</span></code></a></li>
<li><a class="reference internal" href="#std:setting-CLOSESPIDER_PAGECOUNT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CLOSESPIDER_PAGECOUNT</span></code></a></li>
<li><a class="reference internal" href="#std:setting-CLOSESPIDER_ERRORCOUNT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CLOSESPIDER_ERRORCOUNT</span></code></a></li>
</ul>
<div class="section" id="closespider-timeout">
<span id="std:setting-CLOSESPIDER_TIMEOUT"></span><h7>CLOSESPIDER_TIMEOUT<a class="headerlink" href="#closespider-timeout" title="Permalink to this headline">¶</a></h7>
<p>Default: <code class="docutils literal notranslate"><span class="pre">0</span></code></p>
<p>An integer which specifies a number of seconds. If the spider remains open for
more than that number of second, it will be automatically closed with the
reason <code class="docutils literal notranslate"><span class="pre">closespider_timeout</span></code>. If zero (or non set), spiders won’t be closed by
timeout.</p>
</div>
<div class="section" id="closespider-itemcount">
<span id="std:setting-CLOSESPIDER_ITEMCOUNT"></span><h7>CLOSESPIDER_ITEMCOUNT<a class="headerlink" href="#closespider-itemcount" title="Permalink to this headline">¶</a></h7>
<p>Default: <code class="docutils literal notranslate"><span class="pre">0</span></code></p>
<p>An integer which specifies a number of items. If the spider scrapes more than
that amount and those items are passed by the item pipeline, the
spider will be closed with the reason <code class="docutils literal notranslate"><span class="pre">closespider_itemcount</span></code>.
Requests which  are currently in the downloader queue (up to
<a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS</span></code></a> requests) are still processed.
If zero (or non set), spiders won’t be closed by number of passed items.</p>
</div>
<div class="section" id="closespider-pagecount">
<span id="std:setting-CLOSESPIDER_PAGECOUNT"></span><h7>CLOSESPIDER_PAGECOUNT<a class="headerlink" href="#closespider-pagecount" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.11.</span></p>
</div>
<p>Default: <code class="docutils literal notranslate"><span class="pre">0</span></code></p>
<p>An integer which specifies the maximum number of responses to crawl. If the spider
crawls more than that, the spider will be closed with the reason
<code class="docutils literal notranslate"><span class="pre">closespider_pagecount</span></code>. If zero (or non set), spiders won’t be closed by
number of crawled responses.</p>
</div>
<div class="section" id="closespider-errorcount">
<span id="std:setting-CLOSESPIDER_ERRORCOUNT"></span><h7>CLOSESPIDER_ERRORCOUNT<a class="headerlink" href="#closespider-errorcount" title="Permalink to this headline">¶</a></h7>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.11.</span></p>
</div>
<p>Default: <code class="docutils literal notranslate"><span class="pre">0</span></code></p>
<p>An integer which specifies the maximum number of errors to receive before
closing the spider. If the spider generates more than that number of errors,
it will be closed with the reason <code class="docutils literal notranslate"><span class="pre">closespider_errorcount</span></code>. If zero (or non
set), spiders won’t be closed by number of errors.</p>
</div>
</div>
<div class="section" id="module-scrapy.extensions.statsmailer">
<span id="statsmailer-extension"></span><h6>StatsMailer extension<a class="headerlink" href="#module-scrapy.extensions.statsmailer" title="Permalink to this headline">¶</a></h6>
<dl class="class">
<dt id="scrapy.extensions.statsmailer.scrapy.extensions.statsmailer.StatsMailer">
<em class="property">class </em><code class="descclassname">scrapy.extensions.statsmailer.</code><code class="descname">StatsMailer</code><a class="headerlink" href="#scrapy.extensions.statsmailer.scrapy.extensions.statsmailer.StatsMailer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>This simple extension can be used to send a notification e-mail every time a
domain has finished scraping, including the Scrapy stats collected. The email
will be sent to all recipients specified in the <a class="reference internal" href="index.html#std:setting-STATSMAILER_RCPTS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">STATSMAILER_RCPTS</span></code></a>
setting.</p>
<span class="target" id="module-scrapy.extensions.debug"></span></div>
</div>
<div class="section" id="debugging-extensions">
<h5>Debugging extensions<a class="headerlink" href="#debugging-extensions" title="Permalink to this headline">¶</a></h5>
<div class="section" id="stack-trace-dump-extension">
<h6>Stack trace dump extension<a class="headerlink" href="#stack-trace-dump-extension" title="Permalink to this headline">¶</a></h6>
<dl class="class">
<dt id="scrapy.extensions.debug.scrapy.extensions.debug.StackTraceDump">
<em class="property">class </em><code class="descclassname">scrapy.extensions.debug.</code><code class="descname">StackTraceDump</code><a class="headerlink" href="#scrapy.extensions.debug.scrapy.extensions.debug.StackTraceDump" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Dumps information about the running process when a <a class="reference external" href="https://en.wikipedia.org/wiki/SIGQUIT">SIGQUIT</a> or <a class="reference external" href="https://en.wikipedia.org/wiki/SIGUSR1_and_SIGUSR2">SIGUSR2</a>
signal is received. The information dumped is the following:</p>
<ol class="arabic simple">
<li>engine status (using <code class="docutils literal notranslate"><span class="pre">scrapy.utils.engine.get_engine_status()</span></code>)</li>
<li>live references (see <a class="reference internal" href="index.html#topics-leaks-trackrefs"><span class="std std-ref">Debugging memory leaks with trackref</span></a>)</li>
<li>stack trace of all threads</li>
</ol>
<p>After the stack trace and engine status is dumped, the Scrapy process continues
running normally.</p>
<p>This extension only works on POSIX-compliant platforms (ie. not Windows),
because the <a class="reference external" href="https://en.wikipedia.org/wiki/SIGQUIT">SIGQUIT</a> and <a class="reference external" href="https://en.wikipedia.org/wiki/SIGUSR1_and_SIGUSR2">SIGUSR2</a> signals are not available on Windows.</p>
<p>There are at least two ways to send Scrapy the <a class="reference external" href="https://en.wikipedia.org/wiki/SIGQUIT">SIGQUIT</a> signal:</p>
<ol class="arabic">
<li><p class="first">By pressing Ctrl-while a Scrapy process is running (Linux only?)</p>
</li>
<li><p class="first">By running this command (assuming <code class="docutils literal notranslate"><span class="pre">&lt;pid&gt;</span></code> is the process id of the Scrapy
process):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">kill</span> <span class="o">-</span><span class="n">QUIT</span> <span class="o">&lt;</span><span class="n">pid</span><span class="o">&gt;</span>
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="debugger-extension">
<h6>Debugger extension<a class="headerlink" href="#debugger-extension" title="Permalink to this headline">¶</a></h6>
<dl class="class">
<dt id="scrapy.extensions.debug.scrapy.extensions.debug.Debugger">
<em class="property">class </em><code class="descclassname">scrapy.extensions.debug.</code><code class="descname">Debugger</code><a class="headerlink" href="#scrapy.extensions.debug.scrapy.extensions.debug.Debugger" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<p>Invokes a <a class="reference external" href="https://docs.python.org/2/library/pdb.html">Python debugger</a> inside a running Scrapy process when a <a class="reference external" href="https://en.wikipedia.org/wiki/SIGUSR1_and_SIGUSR2">SIGUSR2</a>
signal is received. After the debugger is exited, the Scrapy process continues
running normally.</p>
<p>For more info see <a class="reference external" href="https://pythonconquerstheuniverse.wordpress.com/2009/09/10/debugging-in-python/">Debugging in Python</a>.</p>
<p>This extension only works on POSIX-compliant platforms (ie. not Windows).</p>
</div>
</div>
</div>
</div>
<span id="document-topics/api"></span><div class="section" id="core-api">
<span id="topics-api"></span><h3>Core API<a class="headerlink" href="#core-api" title="Permalink to this headline">¶</a></h3>
<div class="versionadded">
<p><span class="versionmodified">New in version 0.15.</span></p>
</div>
<p>This section documents the Scrapy core API, and it’s intended for developers of
extensions and middlewares.</p>
<div class="section" id="crawler-api">
<span id="topics-api-crawler"></span><h4>Crawler API<a class="headerlink" href="#crawler-api" title="Permalink to this headline">¶</a></h4>
<p>The main entry point to Scrapy API is the <a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a>
object, passed to extensions through the <code class="docutils literal notranslate"><span class="pre">from_crawler</span></code> class method. This
object provides access to all Scrapy core components, and it’s the only way for
extensions to access them and hook their functionality into Scrapy.</p>
<span class="target" id="module-scrapy.crawler"></span><p>The Extension Manager is responsible for loading and keeping track of installed
extensions and it’s configured through the <a class="reference internal" href="index.html#std:setting-EXTENSIONS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">EXTENSIONS</span></code></a> setting which
contains a dictionary of all available extensions and their order similar to
how you <a class="reference internal" href="index.html#topics-downloader-middleware-setting"><span class="std std-ref">configure the downloader middlewares</span></a>.</p>
<dl class="class">
<dt id="scrapy.crawler.Crawler">
<em class="property">class </em><code class="descclassname">scrapy.crawler.</code><code class="descname">Crawler</code><span class="sig-paren">(</span><em>spidercls</em>, <em>settings</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.Crawler" title="Permalink to this definition">¶</a></dt>
<dd><p>The Crawler object must be instantiated with a
<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.spiders.Spider</span></code></a> subclass and a
<a class="reference internal" href="#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.settings.Settings</span></code></a> object.</p>
<dl class="attribute">
<dt id="scrapy.crawler.Crawler.settings">
<code class="descname">settings</code><a class="headerlink" href="#scrapy.crawler.Crawler.settings" title="Permalink to this definition">¶</a></dt>
<dd><p>The settings manager of this crawler.</p>
<p>This is used by extensions &amp; middlewares to access the Scrapy settings
of this crawler.</p>
<p>For an introduction on Scrapy settings see <a class="reference internal" href="index.html#topics-settings"><span class="std std-ref">Settings</span></a>.</p>
<p>For the API see <a class="reference internal" href="#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">Settings</span></code></a> class.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.crawler.Crawler.signals">
<code class="descname">signals</code><a class="headerlink" href="#scrapy.crawler.Crawler.signals" title="Permalink to this definition">¶</a></dt>
<dd><p>The signals manager of this crawler.</p>
<p>This is used by extensions &amp; middlewares to hook themselves into Scrapy
functionality.</p>
<p>For an introduction on signals see <a class="reference internal" href="index.html#topics-signals"><span class="std std-ref">Signals</span></a>.</p>
<p>For the API see <a class="reference internal" href="#scrapy.signalmanager.SignalManager" title="scrapy.signalmanager.SignalManager"><code class="xref py py-class docutils literal notranslate"><span class="pre">SignalManager</span></code></a> class.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.crawler.Crawler.stats">
<code class="descname">stats</code><a class="headerlink" href="#scrapy.crawler.Crawler.stats" title="Permalink to this definition">¶</a></dt>
<dd><p>The stats collector of this crawler.</p>
<p>This is used from extensions &amp; middlewares to record stats of their
behaviour, or access stats collected by other extensions.</p>
<p>For an introduction on stats collection see <a class="reference internal" href="index.html#topics-stats"><span class="std std-ref">Stats Collection</span></a>.</p>
<p>For the API see <a class="reference internal" href="#scrapy.statscollectors.StatsCollector" title="scrapy.statscollectors.StatsCollector"><code class="xref py py-class docutils literal notranslate"><span class="pre">StatsCollector</span></code></a> class.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.crawler.Crawler.extensions">
<code class="descname">extensions</code><a class="headerlink" href="#scrapy.crawler.Crawler.extensions" title="Permalink to this definition">¶</a></dt>
<dd><p>The extension manager that keeps track of enabled extensions.</p>
<p>Most extensions won’t need to access this attribute.</p>
<p>For an introduction on extensions and a list of available extensions on
Scrapy see <a class="reference internal" href="index.html#topics-extensions"><span class="std std-ref">Extensions</span></a>.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.crawler.Crawler.engine">
<code class="descname">engine</code><a class="headerlink" href="#scrapy.crawler.Crawler.engine" title="Permalink to this definition">¶</a></dt>
<dd><p>The execution engine, which coordinates the core crawling logic
between the scheduler, downloader and spiders.</p>
<p>Some extension may want to access the Scrapy engine, to inspect  or
modify the downloader and scheduler behaviour, although this is an
advanced use and this API is not yet stable.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.crawler.Crawler.spider">
<code class="descname">spider</code><a class="headerlink" href="#scrapy.crawler.Crawler.spider" title="Permalink to this definition">¶</a></dt>
<dd><p>Spider currently being crawled. This is an instance of the spider class
provided while constructing the crawler, and it is created after the
arguments given in the <a class="reference internal" href="#scrapy.crawler.Crawler.crawl" title="scrapy.crawler.Crawler.crawl"><code class="xref py py-meth docutils literal notranslate"><span class="pre">crawl()</span></code></a> method.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.crawler.Crawler.crawl">
<code class="descname">crawl</code><span class="sig-paren">(</span><em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.Crawler.crawl" title="Permalink to this definition">¶</a></dt>
<dd><p>Starts the crawler by instantiating its spider class with the given
<code class="docutils literal notranslate"><span class="pre">args</span></code> and <code class="docutils literal notranslate"><span class="pre">kwargs</span></code> arguments, while setting the execution engine in
motion.</p>
<p>Returns a deferred that is fired when the crawl is finished.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.crawler.Crawler.stop">
<code class="descname">stop</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.Crawler.stop" title="Permalink to this definition">¶</a></dt>
<dd><p>Starts a graceful stop of the crawler and returns a deferred that is
fired when the crawler is stopped.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="scrapy.crawler.CrawlerRunner">
<em class="property">class </em><code class="descclassname">scrapy.crawler.</code><code class="descname">CrawlerRunner</code><span class="sig-paren">(</span><em>settings=None</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.CrawlerRunner" title="Permalink to this definition">¶</a></dt>
<dd><p>This is a convenient helper class that keeps track of, manages and runs
crawlers inside an already setup Twisted <a class="reference external" href="https://twistedmatrix.com/documents/current/core/howto/reactor-basics.html">reactor</a>.</p>
<p>The CrawlerRunner object must be instantiated with a
<a class="reference internal" href="#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">Settings</span></code></a> object.</p>
<p>This class shouldn’t be needed (since Scrapy is responsible of using it
accordingly) unless writing scripts that manually handle the crawling
process. See <a class="reference internal" href="index.html#run-from-script"><span class="std std-ref">Run Scrapy from a script</span></a> for an example.</p>
<dl class="method">
<dt id="scrapy.crawler.CrawlerRunner.crawl">
<code class="descname">crawl</code><span class="sig-paren">(</span><em>crawler_or_spidercls</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.CrawlerRunner.crawl" title="Permalink to this definition">¶</a></dt>
<dd><p>Run a crawler with the provided arguments.</p>
<p>It will call the given Crawler’s <a class="reference internal" href="#scrapy.crawler.Crawler.crawl" title="scrapy.crawler.Crawler.crawl"><code class="xref py py-meth docutils literal notranslate"><span class="pre">crawl()</span></code></a> method, while
keeping track of it so it can be stopped later.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">crawler_or_spidercls</span></code> isn’t a <a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a>
instance, this method will try to create one using this parameter as
the spider class given to it.</p>
<p>Returns a deferred that is fired when the crawling is finished.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>crawler_or_spidercls</strong> (<a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> instance,
<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> subclass or string) – already created crawler, or a spider class
or spider’s name inside the project to create it</li>
<li><strong>args</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a>) – arguments to initialize the spider</li>
<li><strong>kwargs</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.7)"><em>dict</em></a>) – keyword arguments to initialize the spider</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.crawler.CrawlerRunner.crawlers">
<code class="descname">crawlers</code><a class="headerlink" href="#scrapy.crawler.CrawlerRunner.crawlers" title="Permalink to this definition">¶</a></dt>
<dd><p>Set of <a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">crawlers</span></code></a> started by <a class="reference internal" href="#scrapy.crawler.CrawlerRunner.crawl" title="scrapy.crawler.CrawlerRunner.crawl"><code class="xref py py-meth docutils literal notranslate"><span class="pre">crawl()</span></code></a> and managed by this class.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.crawler.CrawlerRunner.create_crawler">
<code class="descname">create_crawler</code><span class="sig-paren">(</span><em>crawler_or_spidercls</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.CrawlerRunner.create_crawler" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a <a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> object.</p>
<ul class="simple">
<li>If <code class="docutils literal notranslate"><span class="pre">crawler_or_spidercls</span></code> is a Crawler, it is returned as-is.</li>
<li>If <code class="docutils literal notranslate"><span class="pre">crawler_or_spidercls</span></code> is a Spider subclass, a new Crawler
is constructed for it.</li>
<li>If <code class="docutils literal notranslate"><span class="pre">crawler_or_spidercls</span></code> is a string, this function finds
a spider with this name in a Scrapy project (using spider loader),
then creates a Crawler instance for it.</li>
</ul>
</dd></dl>

<dl class="method">
<dt id="scrapy.crawler.CrawlerRunner.join">
<code class="descname">join</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.CrawlerRunner.join" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a deferred that is fired when all managed <a class="reference internal" href="#scrapy.crawler.CrawlerRunner.crawlers" title="scrapy.crawler.CrawlerRunner.crawlers"><code class="xref py py-attr docutils literal notranslate"><span class="pre">crawlers</span></code></a> have
completed their executions.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.crawler.CrawlerRunner.stop">
<code class="descname">stop</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.CrawlerRunner.stop" title="Permalink to this definition">¶</a></dt>
<dd><p>Stops simultaneously all the crawling jobs taking place.</p>
<p>Returns a deferred that is fired when they all have ended.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="scrapy.crawler.CrawlerProcess">
<em class="property">class </em><code class="descclassname">scrapy.crawler.</code><code class="descname">CrawlerProcess</code><span class="sig-paren">(</span><em>settings=None</em>, <em>install_root_handler=True</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.CrawlerProcess" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.crawler.CrawlerRunner</span></code></a></p>
<p>A class to run multiple scrapy crawlers in a process simultaneously.</p>
<p>This class extends <a class="reference internal" href="#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerRunner</span></code></a> by adding support
for starting a Twisted <a class="reference external" href="https://twistedmatrix.com/documents/current/core/howto/reactor-basics.html">reactor</a> and handling shutdown signals, like the
keyboard interrupt command Ctrl-C. It also configures top-level logging.</p>
<p>This utility should be a better fit than
<a class="reference internal" href="#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerRunner</span></code></a> if you aren’t running another
Twisted <a class="reference external" href="https://twistedmatrix.com/documents/current/core/howto/reactor-basics.html">reactor</a> within your application.</p>
<p>The CrawlerProcess object must be instantiated with a
<a class="reference internal" href="#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">Settings</span></code></a> object.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>install_root_handler</strong> – whether to install root logging handler
(default: True)</td>
</tr>
</tbody>
</table>
<p>This class shouldn’t be needed (since Scrapy is responsible of using it
accordingly) unless writing scripts that manually handle the crawling
process. See <a class="reference internal" href="index.html#run-from-script"><span class="std std-ref">Run Scrapy from a script</span></a> for an example.</p>
<dl class="method">
<dt id="scrapy.crawler.CrawlerProcess.crawl">
<code class="descname">crawl</code><span class="sig-paren">(</span><em>crawler_or_spidercls</em>, <em>*args</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.CrawlerProcess.crawl" title="Permalink to this definition">¶</a></dt>
<dd><p>Run a crawler with the provided arguments.</p>
<p>It will call the given Crawler’s <a class="reference internal" href="#scrapy.crawler.Crawler.crawl" title="scrapy.crawler.Crawler.crawl"><code class="xref py py-meth docutils literal notranslate"><span class="pre">crawl()</span></code></a> method, while
keeping track of it so it can be stopped later.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">crawler_or_spidercls</span></code> isn’t a <a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a>
instance, this method will try to create one using this parameter as
the spider class given to it.</p>
<p>Returns a deferred that is fired when the crawling is finished.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>crawler_or_spidercls</strong> (<a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> instance,
<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> subclass or string) – already created crawler, or a spider class
or spider’s name inside the project to create it</li>
<li><strong>args</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.7)"><em>list</em></a>) – arguments to initialize the spider</li>
<li><strong>kwargs</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.7)"><em>dict</em></a>) – keyword arguments to initialize the spider</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.crawler.CrawlerProcess.crawlers">
<code class="descname">crawlers</code><a class="headerlink" href="#scrapy.crawler.CrawlerProcess.crawlers" title="Permalink to this definition">¶</a></dt>
<dd><p>Set of <a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">crawlers</span></code></a> started by <a class="reference internal" href="#scrapy.crawler.CrawlerProcess.crawl" title="scrapy.crawler.CrawlerProcess.crawl"><code class="xref py py-meth docutils literal notranslate"><span class="pre">crawl()</span></code></a> and managed by this class.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.crawler.CrawlerProcess.create_crawler">
<code class="descname">create_crawler</code><span class="sig-paren">(</span><em>crawler_or_spidercls</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.CrawlerProcess.create_crawler" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a <a class="reference internal" href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a> object.</p>
<ul class="simple">
<li>If <code class="docutils literal notranslate"><span class="pre">crawler_or_spidercls</span></code> is a Crawler, it is returned as-is.</li>
<li>If <code class="docutils literal notranslate"><span class="pre">crawler_or_spidercls</span></code> is a Spider subclass, a new Crawler
is constructed for it.</li>
<li>If <code class="docutils literal notranslate"><span class="pre">crawler_or_spidercls</span></code> is a string, this function finds
a spider with this name in a Scrapy project (using spider loader),
then creates a Crawler instance for it.</li>
</ul>
</dd></dl>

<dl class="method">
<dt id="scrapy.crawler.CrawlerProcess.join">
<code class="descname">join</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.CrawlerProcess.join" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a deferred that is fired when all managed <a class="reference internal" href="#scrapy.crawler.CrawlerProcess.crawlers" title="scrapy.crawler.CrawlerProcess.crawlers"><code class="xref py py-attr docutils literal notranslate"><span class="pre">crawlers</span></code></a> have
completed their executions.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.crawler.CrawlerProcess.start">
<code class="descname">start</code><span class="sig-paren">(</span><em>stop_after_crawl=True</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.CrawlerProcess.start" title="Permalink to this definition">¶</a></dt>
<dd><p>This method starts a Twisted <a class="reference external" href="https://twistedmatrix.com/documents/current/core/howto/reactor-basics.html">reactor</a>, adjusts its pool size to
<a class="reference internal" href="index.html#std:setting-REACTOR_THREADPOOL_MAXSIZE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">REACTOR_THREADPOOL_MAXSIZE</span></code></a>, and installs a DNS cache based
on <a class="reference internal" href="index.html#std:setting-DNSCACHE_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DNSCACHE_ENABLED</span></code></a> and <a class="reference internal" href="index.html#std:setting-DNSCACHE_SIZE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DNSCACHE_SIZE</span></code></a>.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">stop_after_crawl</span></code> is True, the reactor will be stopped after all
crawlers have finished, using <a class="reference internal" href="#scrapy.crawler.CrawlerProcess.join" title="scrapy.crawler.CrawlerProcess.join"><code class="xref py py-meth docutils literal notranslate"><span class="pre">join()</span></code></a>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>stop_after_crawl</strong> (<em>boolean</em>) – stop or not the reactor when all
crawlers have finished</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.crawler.CrawlerProcess.stop">
<code class="descname">stop</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.crawler.CrawlerProcess.stop" title="Permalink to this definition">¶</a></dt>
<dd><p>Stops simultaneously all the crawling jobs taking place.</p>
<p>Returns a deferred that is fired when they all have ended.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-scrapy.settings">
<span id="settings-api"></span><span id="topics-api-settings"></span><h4>Settings API<a class="headerlink" href="#module-scrapy.settings" title="Permalink to this headline">¶</a></h4>
<dl class="attribute">
<dt id="scrapy.settings.SETTINGS_PRIORITIES">
<code class="descclassname">scrapy.settings.</code><code class="descname">SETTINGS_PRIORITIES</code><a class="headerlink" href="#scrapy.settings.SETTINGS_PRIORITIES" title="Permalink to this definition">¶</a></dt>
<dd><p>Dictionary that sets the key name and priority level of the default
settings priorities used in Scrapy.</p>
<p>Each item defines a settings entry point, giving it a code name for
identification and an integer priority. Greater priorities take more
precedence over lesser ones when setting and retrieving values in the
<a class="reference internal" href="#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">Settings</span></code></a> class.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">SETTINGS_PRIORITIES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;default&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;command&#39;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
    <span class="s1">&#39;project&#39;</span><span class="p">:</span> <span class="mi">20</span><span class="p">,</span>
    <span class="s1">&#39;spider&#39;</span><span class="p">:</span> <span class="mi">30</span><span class="p">,</span>
    <span class="s1">&#39;cmdline&#39;</span><span class="p">:</span> <span class="mi">40</span><span class="p">,</span>
<span class="p">}</span>
</pre></div>
</div>
<p>For a detailed explanation on each settings sources, see:
<a class="reference internal" href="index.html#topics-settings"><span class="std std-ref">Settings</span></a>.</p>
</dd></dl>

<dl class="function">
<dt id="scrapy.settings.get_settings_priority">
<code class="descclassname">scrapy.settings.</code><code class="descname">get_settings_priority</code><span class="sig-paren">(</span><em>priority</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.get_settings_priority" title="Permalink to this definition">¶</a></dt>
<dd><p>Small helper function that looks up a given string priority in the
<a class="reference internal" href="#scrapy.settings.SETTINGS_PRIORITIES" title="scrapy.settings.SETTINGS_PRIORITIES"><code class="xref py py-attr docutils literal notranslate"><span class="pre">SETTINGS_PRIORITIES</span></code></a> dictionary and returns its
numerical value, or directly returns a given numerical priority.</p>
</dd></dl>

<dl class="class">
<dt id="scrapy.settings.Settings">
<em class="property">class </em><code class="descclassname">scrapy.settings.</code><code class="descname">Settings</code><span class="sig-paren">(</span><em>values=None</em>, <em>priority='project'</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.Settings" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#scrapy.settings.BaseSettings" title="scrapy.settings.BaseSettings"><code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.settings.BaseSettings</span></code></a></p>
<p>This object stores Scrapy settings for the configuration of internal
components, and can be used for any further customization.</p>
<p>It is a direct subclass and supports all methods of
<a class="reference internal" href="#scrapy.settings.BaseSettings" title="scrapy.settings.BaseSettings"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseSettings</span></code></a>. Additionally, after instantiation
of this class, the new object will have the global default settings
described on <a class="reference internal" href="index.html#topics-settings-ref"><span class="std std-ref">Built-in settings reference</span></a> already populated.</p>
</dd></dl>

<dl class="class">
<dt id="scrapy.settings.BaseSettings">
<em class="property">class </em><code class="descclassname">scrapy.settings.</code><code class="descname">BaseSettings</code><span class="sig-paren">(</span><em>values=None</em>, <em>priority='project'</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings" title="Permalink to this definition">¶</a></dt>
<dd><p>Instances of this class behave like dictionaries, but store priorities
along with their <code class="docutils literal notranslate"><span class="pre">(key,</span> <span class="pre">value)</span></code> pairs, and can be frozen (i.e. marked
immutable).</p>
<p>Key-value entries can be passed on initialization with the <code class="docutils literal notranslate"><span class="pre">values</span></code>
argument, and they would take the <code class="docutils literal notranslate"><span class="pre">priority</span></code> level (unless <code class="docutils literal notranslate"><span class="pre">values</span></code> is
already an instance of <a class="reference internal" href="#scrapy.settings.BaseSettings" title="scrapy.settings.BaseSettings"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseSettings</span></code></a>, in which
case the existing priority levels will be kept).  If the <code class="docutils literal notranslate"><span class="pre">priority</span></code>
argument is a string, the priority name will be looked up in
<a class="reference internal" href="#scrapy.settings.SETTINGS_PRIORITIES" title="scrapy.settings.SETTINGS_PRIORITIES"><code class="xref py py-attr docutils literal notranslate"><span class="pre">SETTINGS_PRIORITIES</span></code></a>. Otherwise, a specific integer
should be provided.</p>
<p>Once the object is created, new settings can be loaded or updated with the
<a class="reference internal" href="#scrapy.settings.BaseSettings.set" title="scrapy.settings.BaseSettings.set"><code class="xref py py-meth docutils literal notranslate"><span class="pre">set()</span></code></a> method, and can be accessed with
the square bracket notation of dictionaries, or with the
<a class="reference internal" href="#scrapy.settings.BaseSettings.get" title="scrapy.settings.BaseSettings.get"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get()</span></code></a> method of the instance and its
value conversion variants. When requesting a stored key, the value with the
highest priority will be retrieved.</p>
<dl class="method">
<dt id="scrapy.settings.BaseSettings.copy">
<code class="descname">copy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.copy" title="Permalink to this definition">¶</a></dt>
<dd><p>Make a deep copy of current settings.</p>
<p>This method returns a new instance of the <a class="reference internal" href="#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">Settings</span></code></a> class,
populated with the same values and their priorities.</p>
<p>Modifications to the new object won’t be reflected on the original
settings.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.copy_to_dict">
<code class="descname">copy_to_dict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.copy_to_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Make a copy of current settings and convert to a dict.</p>
<p>This method returns a new dict populated with the same values
and their priorities as the current settings.</p>
<p>Modifications to the returned dict won’t be reflected on the original
settings.</p>
<p>This method can be useful for example for printing settings
in Scrapy shell.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.freeze">
<code class="descname">freeze</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.freeze" title="Permalink to this definition">¶</a></dt>
<dd><p>Disable further changes to the current settings.</p>
<p>After calling this method, the present state of the settings will become
immutable. Trying to change values through the <a class="reference internal" href="#scrapy.settings.BaseSettings.set" title="scrapy.settings.BaseSettings.set"><code class="xref py py-meth docutils literal notranslate"><span class="pre">set()</span></code></a> method and
its variants won’t be possible and will be alerted.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.frozencopy">
<code class="descname">frozencopy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.frozencopy" title="Permalink to this definition">¶</a></dt>
<dd><p>Return an immutable copy of the current settings.</p>
<p>Alias for a <a class="reference internal" href="#scrapy.settings.BaseSettings.freeze" title="scrapy.settings.BaseSettings.freeze"><code class="xref py py-meth docutils literal notranslate"><span class="pre">freeze()</span></code></a> call in the object returned by <a class="reference internal" href="#scrapy.settings.BaseSettings.copy" title="scrapy.settings.BaseSettings.copy"><code class="xref py py-meth docutils literal notranslate"><span class="pre">copy()</span></code></a>.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.get">
<code class="descname">get</code><span class="sig-paren">(</span><em>name</em>, <em>default=None</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.get" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a setting value without affecting its original type.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> (<em>string</em>) – the setting name</li>
<li><strong>default</strong> (<em>any</em>) – the value to return if no setting is found</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.getbool">
<code class="descname">getbool</code><span class="sig-paren">(</span><em>name</em>, <em>default=False</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.getbool" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a setting value as a boolean.</p>
<p><code class="docutils literal notranslate"><span class="pre">1</span></code>, <code class="docutils literal notranslate"><span class="pre">'1'</span></code>, <cite>True`</cite> and <code class="docutils literal notranslate"><span class="pre">'True'</span></code> return <code class="docutils literal notranslate"><span class="pre">True</span></code>,
while <code class="docutils literal notranslate"><span class="pre">0</span></code>, <code class="docutils literal notranslate"><span class="pre">'0'</span></code>, <code class="docutils literal notranslate"><span class="pre">False</span></code>, <code class="docutils literal notranslate"><span class="pre">'False'</span></code> and <code class="docutils literal notranslate"><span class="pre">None</span></code> return <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
<p>For example, settings populated through environment variables set to
<code class="docutils literal notranslate"><span class="pre">'0'</span></code> will return <code class="docutils literal notranslate"><span class="pre">False</span></code> when using this method.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> (<em>string</em>) – the setting name</li>
<li><strong>default</strong> (<em>any</em>) – the value to return if no setting is found</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.getdict">
<code class="descname">getdict</code><span class="sig-paren">(</span><em>name</em>, <em>default=None</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.getdict" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a setting value as a dictionary. If the setting original type is a
dictionary, a copy of it will be returned. If it is a string it will be
evaluated as a JSON dictionary. In the case that it is a
<a class="reference internal" href="#scrapy.settings.BaseSettings" title="scrapy.settings.BaseSettings"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseSettings</span></code></a> instance itself, it will be
converted to a dictionary, containing all its current settings values
as they would be returned by <a class="reference internal" href="#scrapy.settings.BaseSettings.get" title="scrapy.settings.BaseSettings.get"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get()</span></code></a>,
and losing all information about priority and mutability.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> (<em>string</em>) – the setting name</li>
<li><strong>default</strong> (<em>any</em>) – the value to return if no setting is found</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.getfloat">
<code class="descname">getfloat</code><span class="sig-paren">(</span><em>name</em>, <em>default=0.0</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.getfloat" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a setting value as a float.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> (<em>string</em>) – the setting name</li>
<li><strong>default</strong> (<em>any</em>) – the value to return if no setting is found</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.getint">
<code class="descname">getint</code><span class="sig-paren">(</span><em>name</em>, <em>default=0</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.getint" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a setting value as an int.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> (<em>string</em>) – the setting name</li>
<li><strong>default</strong> (<em>any</em>) – the value to return if no setting is found</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.getlist">
<code class="descname">getlist</code><span class="sig-paren">(</span><em>name</em>, <em>default=None</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.getlist" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a setting value as a list. If the setting original type is a list, a
copy of it will be returned. If it’s a string it will be split by “,”.</p>
<p>For example, settings populated through environment variables set to
<code class="docutils literal notranslate"><span class="pre">'one,two'</span></code> will return a list [‘one’, ‘two’] when using this method.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> (<em>string</em>) – the setting name</li>
<li><strong>default</strong> (<em>any</em>) – the value to return if no setting is found</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.getpriority">
<code class="descname">getpriority</code><span class="sig-paren">(</span><em>name</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.getpriority" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the current numerical priority value of a setting, or <code class="docutils literal notranslate"><span class="pre">None</span></code> if
the given <code class="docutils literal notranslate"><span class="pre">name</span></code> does not exist.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>name</strong> (<em>string</em>) – the setting name</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.getwithbase">
<code class="descname">getwithbase</code><span class="sig-paren">(</span><em>name</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.getwithbase" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a composition of a dictionary-like setting and its <cite>_BASE</cite>
counterpart.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>name</strong> (<em>string</em>) – name of the dictionary-like setting</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.maxpriority">
<code class="descname">maxpriority</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.maxpriority" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the numerical value of the highest priority present throughout
all settings, or the numerical value for <code class="docutils literal notranslate"><span class="pre">default</span></code> from
<a class="reference internal" href="#scrapy.settings.SETTINGS_PRIORITIES" title="scrapy.settings.SETTINGS_PRIORITIES"><code class="xref py py-attr docutils literal notranslate"><span class="pre">SETTINGS_PRIORITIES</span></code></a> if there are no settings
stored.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.set">
<code class="descname">set</code><span class="sig-paren">(</span><em>name</em>, <em>value</em>, <em>priority='project'</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.set" title="Permalink to this definition">¶</a></dt>
<dd><p>Store a key/value attribute with a given priority.</p>
<p>Settings should be populated <em>before</em> configuring the Crawler object
(through the <code class="xref py py-meth docutils literal notranslate"><span class="pre">configure()</span></code> method),
otherwise they won’t have any effect.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>name</strong> (<em>string</em>) – the setting name</li>
<li><strong>value</strong> (<em>any</em>) – the value to associate with the setting</li>
<li><strong>priority</strong> (<em>string</em><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the priority of the setting. Should be a key of
<a class="reference internal" href="#scrapy.settings.SETTINGS_PRIORITIES" title="scrapy.settings.SETTINGS_PRIORITIES"><code class="xref py py-attr docutils literal notranslate"><span class="pre">SETTINGS_PRIORITIES</span></code></a> or an integer</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.setmodule">
<code class="descname">setmodule</code><span class="sig-paren">(</span><em>module</em>, <em>priority='project'</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.setmodule" title="Permalink to this definition">¶</a></dt>
<dd><p>Store settings from a module with a given priority.</p>
<p>This is a helper function that calls
<a class="reference internal" href="#scrapy.settings.BaseSettings.set" title="scrapy.settings.BaseSettings.set"><code class="xref py py-meth docutils literal notranslate"><span class="pre">set()</span></code></a> for every globally declared
uppercase variable of <code class="docutils literal notranslate"><span class="pre">module</span></code> with the provided <code class="docutils literal notranslate"><span class="pre">priority</span></code>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>module</strong> (<em>module object</em><em> or </em><em>string</em>) – the module or the path of the module</li>
<li><strong>priority</strong> (<em>string</em><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the priority of the settings. Should be a key of
<a class="reference internal" href="#scrapy.settings.SETTINGS_PRIORITIES" title="scrapy.settings.SETTINGS_PRIORITIES"><code class="xref py py-attr docutils literal notranslate"><span class="pre">SETTINGS_PRIORITIES</span></code></a> or an integer</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.settings.BaseSettings.update">
<code class="descname">update</code><span class="sig-paren">(</span><em>values</em>, <em>priority='project'</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.settings.BaseSettings.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Store key/value pairs with a given priority.</p>
<p>This is a helper function that calls
<a class="reference internal" href="#scrapy.settings.BaseSettings.set" title="scrapy.settings.BaseSettings.set"><code class="xref py py-meth docutils literal notranslate"><span class="pre">set()</span></code></a> for every item of <code class="docutils literal notranslate"><span class="pre">values</span></code>
with the provided <code class="docutils literal notranslate"><span class="pre">priority</span></code>.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">values</span></code> is a string, it is assumed to be JSON-encoded and parsed
into a dict with <code class="docutils literal notranslate"><span class="pre">json.loads()</span></code> first. If it is a
<a class="reference internal" href="#scrapy.settings.BaseSettings" title="scrapy.settings.BaseSettings"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseSettings</span></code></a> instance, the per-key priorities
will be used and the <code class="docutils literal notranslate"><span class="pre">priority</span></code> parameter ignored. This allows
inserting/updating settings with different priorities with a single
command.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>values</strong> (dict or string or <a class="reference internal" href="#scrapy.settings.BaseSettings" title="scrapy.settings.BaseSettings"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseSettings</span></code></a>) – the settings names and values</li>
<li><strong>priority</strong> (<em>string</em><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – the priority of the settings. Should be a key of
<a class="reference internal" href="#scrapy.settings.SETTINGS_PRIORITIES" title="scrapy.settings.SETTINGS_PRIORITIES"><code class="xref py py-attr docutils literal notranslate"><span class="pre">SETTINGS_PRIORITIES</span></code></a> or an integer</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-scrapy.spiderloader">
<span id="spiderloader-api"></span><span id="topics-api-spiderloader"></span><h4>SpiderLoader API<a class="headerlink" href="#module-scrapy.spiderloader" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="scrapy.spiderloader.SpiderLoader">
<em class="property">class </em><code class="descclassname">scrapy.spiderloader.</code><code class="descname">SpiderLoader</code><a class="headerlink" href="#scrapy.spiderloader.SpiderLoader" title="Permalink to this definition">¶</a></dt>
<dd><p>This class is in charge of retrieving and handling the spider classes
defined across the project.</p>
<p>Custom spider loaders can be employed by specifying their path in the
<a class="reference internal" href="index.html#std:setting-SPIDER_LOADER_CLASS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_LOADER_CLASS</span></code></a> project setting. They must fully implement
the <code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.interfaces.ISpiderLoader</span></code> interface to guarantee an
errorless execution.</p>
<dl class="method">
<dt id="scrapy.spiderloader.SpiderLoader.from_settings">
<code class="descname">from_settings</code><span class="sig-paren">(</span><em>settings</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiderloader.SpiderLoader.from_settings" title="Permalink to this definition">¶</a></dt>
<dd><p>This class method is used by Scrapy to create an instance of the class.
It’s called with the current project settings, and it loads the spiders
found recursively in the modules of the <a class="reference internal" href="index.html#std:setting-SPIDER_MODULES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_MODULES</span></code></a>
setting.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>settings</strong> (<a class="reference internal" href="#scrapy.settings.Settings" title="scrapy.settings.Settings"><code class="xref py py-class docutils literal notranslate"><span class="pre">Settings</span></code></a> instance) – project settings</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.spiderloader.SpiderLoader.load">
<code class="descname">load</code><span class="sig-paren">(</span><em>spider_name</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiderloader.SpiderLoader.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the Spider class with the given name. It’ll look into the previously
loaded spiders for a spider class with name <code class="docutils literal notranslate"><span class="pre">spider_name</span></code> and will raise
a KeyError if not found.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>spider_name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – spider class name</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.spiderloader.SpiderLoader.list">
<code class="descname">list</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiderloader.SpiderLoader.list" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the names of the available spiders in the project.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.spiderloader.SpiderLoader.find_by_request">
<code class="descname">find_by_request</code><span class="sig-paren">(</span><em>request</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.spiderloader.SpiderLoader.find_by_request" title="Permalink to this definition">¶</a></dt>
<dd><p>List the spiders’ names that can handle the given request. Will try to
match the request’s url against the domains of the spiders.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>request</strong> (<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> instance) – queried request</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-scrapy.signalmanager">
<span id="signals-api"></span><span id="topics-api-signals"></span><h4>Signals API<a class="headerlink" href="#module-scrapy.signalmanager" title="Permalink to this headline">¶</a></h4>
<dl class="class">
<dt id="scrapy.signalmanager.SignalManager">
<em class="property">class </em><code class="descclassname">scrapy.signalmanager.</code><code class="descname">SignalManager</code><span class="sig-paren">(</span><em>sender=_Anonymous</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signalmanager.SignalManager" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="scrapy.signalmanager.SignalManager.connect">
<code class="descname">connect</code><span class="sig-paren">(</span><em>receiver</em>, <em>signal</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signalmanager.SignalManager.connect" title="Permalink to this definition">¶</a></dt>
<dd><p>Connect a receiver function to a signal.</p>
<p>The signal can be any object, although Scrapy comes with some
predefined signals that are documented in the <a class="reference internal" href="index.html#topics-signals"><span class="std std-ref">Signals</span></a>
section.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>receiver</strong> (<em>callable</em>) – the function to be connected</li>
<li><strong>signal</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.7)"><em>object</em></a>) – the signal to connect to</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.signalmanager.SignalManager.disconnect">
<code class="descname">disconnect</code><span class="sig-paren">(</span><em>receiver</em>, <em>signal</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signalmanager.SignalManager.disconnect" title="Permalink to this definition">¶</a></dt>
<dd><p>Disconnect a receiver function from a signal. This has the
opposite effect of the <a class="reference internal" href="#scrapy.signalmanager.SignalManager.connect" title="scrapy.signalmanager.SignalManager.connect"><code class="xref py py-meth docutils literal notranslate"><span class="pre">connect()</span></code></a> method, and the arguments
are the same.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.signalmanager.SignalManager.disconnect_all">
<code class="descname">disconnect_all</code><span class="sig-paren">(</span><em>signal</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signalmanager.SignalManager.disconnect_all" title="Permalink to this definition">¶</a></dt>
<dd><p>Disconnect all receivers from the given signal.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>signal</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.7)"><em>object</em></a>) – the signal to disconnect from</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.signalmanager.SignalManager.send_catch_log">
<code class="descname">send_catch_log</code><span class="sig-paren">(</span><em>signal</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signalmanager.SignalManager.send_catch_log" title="Permalink to this definition">¶</a></dt>
<dd><p>Send a signal, catch exceptions and log them.</p>
<p>The keyword arguments are passed to the signal handlers (connected
through the <a class="reference internal" href="#scrapy.signalmanager.SignalManager.connect" title="scrapy.signalmanager.SignalManager.connect"><code class="xref py py-meth docutils literal notranslate"><span class="pre">connect()</span></code></a> method).</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.signalmanager.SignalManager.send_catch_log_deferred">
<code class="descname">send_catch_log_deferred</code><span class="sig-paren">(</span><em>signal</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signalmanager.SignalManager.send_catch_log_deferred" title="Permalink to this definition">¶</a></dt>
<dd><p>Like <a class="reference internal" href="#scrapy.signalmanager.SignalManager.send_catch_log" title="scrapy.signalmanager.SignalManager.send_catch_log"><code class="xref py py-meth docutils literal notranslate"><span class="pre">send_catch_log()</span></code></a> but supports returning <a class="reference external" href="https://twistedmatrix.com/documents/current/core/howto/defer.html">deferreds</a> from
signal handlers.</p>
<p>Returns a Deferred that gets fired once all signal handlers
deferreds were fired. Send a signal, catch exceptions and log them.</p>
<p>The keyword arguments are passed to the signal handlers (connected
through the <a class="reference internal" href="#scrapy.signalmanager.SignalManager.connect" title="scrapy.signalmanager.SignalManager.connect"><code class="xref py py-meth docutils literal notranslate"><span class="pre">connect()</span></code></a> method).</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="stats-collector-api">
<span id="topics-api-stats"></span><h4>Stats Collector API<a class="headerlink" href="#stats-collector-api" title="Permalink to this headline">¶</a></h4>
<p>There are several Stats Collectors available under the
<a class="reference internal" href="#module-scrapy.statscollectors" title="scrapy.statscollectors: Stats Collectors"><code class="xref py py-mod docutils literal notranslate"><span class="pre">scrapy.statscollectors</span></code></a> module and they all implement the Stats
Collector API defined by the <a class="reference internal" href="#scrapy.statscollectors.StatsCollector" title="scrapy.statscollectors.StatsCollector"><code class="xref py py-class docutils literal notranslate"><span class="pre">StatsCollector</span></code></a>
class (which they all inherit from).</p>
<span class="target" id="module-scrapy.statscollectors"></span><dl class="class">
<dt id="scrapy.statscollectors.StatsCollector">
<em class="property">class </em><code class="descclassname">scrapy.statscollectors.</code><code class="descname">StatsCollector</code><a class="headerlink" href="#scrapy.statscollectors.StatsCollector" title="Permalink to this definition">¶</a></dt>
<dd><dl class="method">
<dt id="scrapy.statscollectors.StatsCollector.get_value">
<code class="descname">get_value</code><span class="sig-paren">(</span><em>key</em>, <em>default=None</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.get_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the value for the given stats key or default if it doesn’t exist.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscollectors.StatsCollector.get_stats">
<code class="descname">get_stats</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.get_stats" title="Permalink to this definition">¶</a></dt>
<dd><p>Get all stats from the currently running spider as a dict.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscollectors.StatsCollector.set_value">
<code class="descname">set_value</code><span class="sig-paren">(</span><em>key</em>, <em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.set_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the given value for the given stats key.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscollectors.StatsCollector.set_stats">
<code class="descname">set_stats</code><span class="sig-paren">(</span><em>stats</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.set_stats" title="Permalink to this definition">¶</a></dt>
<dd><p>Override the current stats with the dict passed in <code class="docutils literal notranslate"><span class="pre">stats</span></code> argument.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscollectors.StatsCollector.inc_value">
<code class="descname">inc_value</code><span class="sig-paren">(</span><em>key</em>, <em>count=1</em>, <em>start=0</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.inc_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Increment the value of the given stats key, by the given count,
assuming the start value given (when it’s not set).</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscollectors.StatsCollector.max_value">
<code class="descname">max_value</code><span class="sig-paren">(</span><em>key</em>, <em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.max_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the given value for the given key only if current value for the
same key is lower than value. If there is no current value for the
given key, the value is always set.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscollectors.StatsCollector.min_value">
<code class="descname">min_value</code><span class="sig-paren">(</span><em>key</em>, <em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.min_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the given value for the given key only if current value for the
same key is greater than value. If there is no current value for the
given key, the value is always set.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscollectors.StatsCollector.clear_stats">
<code class="descname">clear_stats</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.clear_stats" title="Permalink to this definition">¶</a></dt>
<dd><p>Clear all stats.</p>
</dd></dl>

<p>The following methods are not part of the stats collection api but instead
used when implementing custom stats collectors:</p>
<dl class="method">
<dt id="scrapy.statscollectors.StatsCollector.open_spider">
<code class="descname">open_spider</code><span class="sig-paren">(</span><em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.open_spider" title="Permalink to this definition">¶</a></dt>
<dd><p>Open the given spider for stats collection.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.statscollectors.StatsCollector.close_spider">
<code class="descname">close_spider</code><span class="sig-paren">(</span><em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.statscollectors.StatsCollector.close_spider" title="Permalink to this definition">¶</a></dt>
<dd><p>Close the given spider. After this is called, no more specific stats
can be accessed or collected.</p>
</dd></dl>

</dd></dl>

</div>
</div>
<span id="document-topics/signals"></span><div class="section" id="signals">
<span id="topics-signals"></span><h3>Signals<a class="headerlink" href="#signals" title="Permalink to this headline">¶</a></h3>
<p>Scrapy uses signals extensively to notify when certain events occur. You can
catch some of those signals in your Scrapy project (using an <a class="reference internal" href="index.html#topics-extensions"><span class="std std-ref">extension</span></a>, for example) to perform additional tasks or extend Scrapy
to add functionality not provided out of the box.</p>
<p>Even though signals provide several arguments, the handlers that catch them
don’t need to accept all of them - the signal dispatching mechanism will only
deliver the arguments that the handler receives.</p>
<p>You can connect to signals (or send your own) through the
<a class="reference internal" href="index.html#topics-api-signals"><span class="std std-ref">Signals API</span></a>.</p>
<p>Here is a simple example showing how you can catch signals and perform some action:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy</span> <span class="k">import</span> <span class="n">signals</span>
<span class="kn">from</span> <span class="nn">scrapy</span> <span class="k">import</span> <span class="n">Spider</span>


<span class="k">class</span> <span class="nc">DmozSpider</span><span class="p">(</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;dmoz&quot;</span>
    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;dmoz.org&quot;</span><span class="p">]</span>
    <span class="n">start_urls</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&quot;</span><span class="p">,</span>
        <span class="s2">&quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&quot;</span><span class="p">,</span>
    <span class="p">]</span>


    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_crawler</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">crawler</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">spider</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">DmozSpider</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">from_crawler</span><span class="p">(</span><span class="n">crawler</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">crawler</span><span class="o">.</span><span class="n">signals</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="n">spider</span><span class="o">.</span><span class="n">spider_closed</span><span class="p">,</span> <span class="n">signal</span><span class="o">=</span><span class="n">signals</span><span class="o">.</span><span class="n">spider_closed</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">spider</span>


    <span class="k">def</span> <span class="nf">spider_closed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="n">spider</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Spider closed: </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">spider</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">pass</span>
</pre></div>
</div>
<div class="section" id="deferred-signal-handlers">
<h4>Deferred signal handlers<a class="headerlink" href="#deferred-signal-handlers" title="Permalink to this headline">¶</a></h4>
<p>Some signals support returning <a class="reference external" href="https://twistedmatrix.com/documents/current/core/howto/defer.html">Twisted deferreds</a> from their handlers, see
the <a class="reference internal" href="#topics-signals-ref"><span class="std std-ref">Built-in signals reference</span></a> below to know which ones.</p>
</div>
<div class="section" id="module-scrapy.signals">
<span id="built-in-signals-reference"></span><span id="topics-signals-ref"></span><h4>Built-in signals reference<a class="headerlink" href="#module-scrapy.signals" title="Permalink to this headline">¶</a></h4>
<p>Here’s the list of Scrapy built-in signals and their meaning.</p>
<div class="section" id="engine-started">
<h5>engine_started<a class="headerlink" href="#engine-started" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:signal-engine_started"></span><dl class="function">
<dt id="scrapy.signals.engine_started">
<code class="descclassname">scrapy.signals.</code><code class="descname">engine_started</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.engine_started" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent when the Scrapy engine has started crawling.</p>
<p>This signal supports returning deferreds from their handlers.</p>
</dd></dl>

<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This signal may be fired <em>after</em> the <a class="reference internal" href="#std:signal-spider_opened"><code class="xref std std-signal docutils literal notranslate"><span class="pre">spider_opened</span></code></a> signal,
depending on how the spider was started. So <strong>don’t</strong> rely on this signal
getting fired before <a class="reference internal" href="#std:signal-spider_opened"><code class="xref std std-signal docutils literal notranslate"><span class="pre">spider_opened</span></code></a>.</p>
</div>
</div>
<div class="section" id="engine-stopped">
<h5>engine_stopped<a class="headerlink" href="#engine-stopped" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:signal-engine_stopped"></span><dl class="function">
<dt id="scrapy.signals.engine_stopped">
<code class="descclassname">scrapy.signals.</code><code class="descname">engine_stopped</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.engine_stopped" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent when the Scrapy engine is stopped (for example, when a crawling
process has finished).</p>
<p>This signal supports returning deferreds from their handlers.</p>
</dd></dl>

</div>
<div class="section" id="item-scraped">
<h5>item_scraped<a class="headerlink" href="#item-scraped" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:signal-item_scraped"></span><dl class="function">
<dt id="scrapy.signals.item_scraped">
<code class="descclassname">scrapy.signals.</code><code class="descname">item_scraped</code><span class="sig-paren">(</span><em>item</em>, <em>response</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.item_scraped" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent when an item has been scraped, after it has passed all the
<a class="reference internal" href="index.html#topics-item-pipeline"><span class="std std-ref">Item Pipeline</span></a> stages (without being dropped).</p>
<p>This signal supports returning deferreds from their handlers.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>item</strong> (dict or <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a> object) – the item scraped</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider which scraped the item</li>
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object) – the response from where the item was scraped</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="item-dropped">
<h5>item_dropped<a class="headerlink" href="#item-dropped" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:signal-item_dropped"></span><dl class="function">
<dt id="scrapy.signals.item_dropped">
<code class="descclassname">scrapy.signals.</code><code class="descname">item_dropped</code><span class="sig-paren">(</span><em>item</em>, <em>response</em>, <em>exception</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.item_dropped" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent after an item has been dropped from the <a class="reference internal" href="index.html#topics-item-pipeline"><span class="std std-ref">Item Pipeline</span></a>
when some stage raised a <a class="reference internal" href="index.html#scrapy.exceptions.DropItem" title="scrapy.exceptions.DropItem"><code class="xref py py-exc docutils literal notranslate"><span class="pre">DropItem</span></code></a> exception.</p>
<p>This signal supports returning deferreds from their handlers.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>item</strong> (dict or <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a> object) – the item dropped from the <a class="reference internal" href="index.html#topics-item-pipeline"><span class="std std-ref">Item Pipeline</span></a></li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider which scraped the item</li>
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object) – the response from where the item was dropped</li>
<li><strong>exception</strong> (<a class="reference internal" href="index.html#scrapy.exceptions.DropItem" title="scrapy.exceptions.DropItem"><code class="xref py py-exc docutils literal notranslate"><span class="pre">DropItem</span></code></a> exception) – the exception (which must be a
<a class="reference internal" href="index.html#scrapy.exceptions.DropItem" title="scrapy.exceptions.DropItem"><code class="xref py py-exc docutils literal notranslate"><span class="pre">DropItem</span></code></a> subclass) which caused the item
to be dropped</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="item-error">
<h5>item_error<a class="headerlink" href="#item-error" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:signal-item_error"></span><dl class="function">
<dt id="scrapy.signals.item_error">
<code class="descclassname">scrapy.signals.</code><code class="descname">item_error</code><span class="sig-paren">(</span><em>item</em>, <em>response</em>, <em>spider</em>, <em>failure</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.item_error" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent when a <a class="reference internal" href="index.html#topics-item-pipeline"><span class="std std-ref">Item Pipeline</span></a> generates an error (ie. raises
an exception), except <a class="reference internal" href="index.html#scrapy.exceptions.DropItem" title="scrapy.exceptions.DropItem"><code class="xref py py-exc docutils literal notranslate"><span class="pre">DropItem</span></code></a> exception.</p>
<p>This signal supports returning deferreds from their handlers.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>item</strong> (dict or <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a> object) – the item dropped from the <a class="reference internal" href="index.html#topics-item-pipeline"><span class="std std-ref">Item Pipeline</span></a></li>
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object) – the response being processed when the exception was raised</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider which raised the exception</li>
<li><strong>failure</strong> (<a class="reference external" href="https://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html">Failure</a> object) – the exception raised as a Twisted <a class="reference external" href="https://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html">Failure</a> object</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="spider-closed">
<h5>spider_closed<a class="headerlink" href="#spider-closed" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:signal-spider_closed"></span><dl class="function">
<dt id="scrapy.signals.spider_closed">
<code class="descclassname">scrapy.signals.</code><code class="descname">spider_closed</code><span class="sig-paren">(</span><em>spider</em>, <em>reason</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.spider_closed" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent after a spider has been closed. This can be used to release per-spider
resources reserved on <a class="reference internal" href="#std:signal-spider_opened"><code class="xref std std-signal docutils literal notranslate"><span class="pre">spider_opened</span></code></a>.</p>
<p>This signal supports returning deferreds from their handlers.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider which has been closed</li>
<li><strong>reason</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – a string which describes the reason why the spider was closed. If
it was closed because the spider has completed scraping, the reason
is <code class="docutils literal notranslate"><span class="pre">'finished'</span></code>. Otherwise, if the spider was manually closed by
calling the <code class="docutils literal notranslate"><span class="pre">close_spider</span></code> engine method, then the reason is the one
passed in the <code class="docutils literal notranslate"><span class="pre">reason</span></code> argument of that method (which defaults to
<code class="docutils literal notranslate"><span class="pre">'cancelled'</span></code>). If the engine was shutdown (for example, by hitting
Ctrl-C to stop it) the reason will be <code class="docutils literal notranslate"><span class="pre">'shutdown'</span></code>.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="spider-opened">
<h5>spider_opened<a class="headerlink" href="#spider-opened" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:signal-spider_opened"></span><dl class="function">
<dt id="scrapy.signals.spider_opened">
<code class="descclassname">scrapy.signals.</code><code class="descname">spider_opened</code><span class="sig-paren">(</span><em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.spider_opened" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent after a spider has been opened for crawling. This is typically used to
reserve per-spider resources, but can be used for any task that needs to be
performed when a spider is opened.</p>
<p>This signal supports returning deferreds from their handlers.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider which has been opened</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="spider-idle">
<h5>spider_idle<a class="headerlink" href="#spider-idle" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:signal-spider_idle"></span><dl class="function">
<dt id="scrapy.signals.spider_idle">
<code class="descclassname">scrapy.signals.</code><code class="descname">spider_idle</code><span class="sig-paren">(</span><em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.spider_idle" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent when a spider has gone idle, which means the spider has no further:</p>
<blockquote>
<div><ul class="simple">
<li>requests waiting to be downloaded</li>
<li>requests scheduled</li>
<li>items being processed in the item pipeline</li>
</ul>
</div></blockquote>
<p>If the idle state persists after all handlers of this signal have finished,
the engine starts closing the spider. After the spider has finished
closing, the <a class="reference internal" href="#std:signal-spider_closed"><code class="xref std std-signal docutils literal notranslate"><span class="pre">spider_closed</span></code></a> signal is sent.</p>
<p>You may raise a <a class="reference internal" href="index.html#scrapy.exceptions.DontCloseSpider" title="scrapy.exceptions.DontCloseSpider"><code class="xref py py-exc docutils literal notranslate"><span class="pre">DontCloseSpider</span></code></a> exception to
prevent the spider from being closed.</p>
<p>This signal does not support returning deferreds from their handlers.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider which has gone idle</td>
</tr>
</tbody>
</table>
</dd></dl>

<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Scheduling some requests in your <a class="reference internal" href="#std:signal-spider_idle"><code class="xref std std-signal docutils literal notranslate"><span class="pre">spider_idle</span></code></a> handler does
<strong>not</strong> guarantee that it can prevent the spider from being closed,
although it sometimes can. That’s because the spider may still remain idle
if all the scheduled requests are rejected by the scheduler (e.g. filtered
due to duplication).</p>
</div>
</div>
<div class="section" id="spider-error">
<h5>spider_error<a class="headerlink" href="#spider-error" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:signal-spider_error"></span><dl class="function">
<dt id="scrapy.signals.spider_error">
<code class="descclassname">scrapy.signals.</code><code class="descname">spider_error</code><span class="sig-paren">(</span><em>failure</em>, <em>response</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.spider_error" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent when a spider callback generates an error (ie. raises an exception).</p>
<p>This signal does not support returning deferreds from their handlers.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>failure</strong> (<a class="reference external" href="https://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html">Failure</a> object) – the exception raised as a Twisted <a class="reference external" href="https://twistedmatrix.com/documents/current/api/twisted.python.failure.Failure.html">Failure</a> object</li>
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object) – the response being processed when the exception was raised</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider which raised the exception</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="request-scheduled">
<h5>request_scheduled<a class="headerlink" href="#request-scheduled" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:signal-request_scheduled"></span><dl class="function">
<dt id="scrapy.signals.request_scheduled">
<code class="descclassname">scrapy.signals.</code><code class="descname">request_scheduled</code><span class="sig-paren">(</span><em>request</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.request_scheduled" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent when the engine schedules a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a>, to be
downloaded later.</p>
<p>The signal does not support returning deferreds from their handlers.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>request</strong> (<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object) – the request that reached the scheduler</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider that yielded the request</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="request-dropped">
<h5>request_dropped<a class="headerlink" href="#request-dropped" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:signal-request_dropped"></span><dl class="function">
<dt id="scrapy.signals.request_dropped">
<code class="descclassname">scrapy.signals.</code><code class="descname">request_dropped</code><span class="sig-paren">(</span><em>request</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.request_dropped" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent when a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a>, scheduled by the engine to be
downloaded later, is rejected by the scheduler.</p>
<p>The signal does not support returning deferreds from their handlers.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>request</strong> (<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object) – the request that reached the scheduler</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider that yielded the request</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="request-reached-downloader">
<h5>request_reached_downloader<a class="headerlink" href="#request-reached-downloader" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:signal-request_reached_downloader"></span><dl class="function">
<dt id="scrapy.signals.request_reached_downloader">
<code class="descclassname">scrapy.signals.</code><code class="descname">request_reached_downloader</code><span class="sig-paren">(</span><em>request</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.request_reached_downloader" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent when a <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> reached downloader.</p>
<p>The signal does not support returning deferreds from their handlers.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>request</strong> (<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object) – the request that reached downloader</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider that yielded the request</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="response-received">
<h5>response_received<a class="headerlink" href="#response-received" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:signal-response_received"></span><dl class="function">
<dt id="scrapy.signals.response_received">
<code class="descclassname">scrapy.signals.</code><code class="descname">response_received</code><span class="sig-paren">(</span><em>response</em>, <em>request</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.response_received" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent when the engine receives a new <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> from the
downloader.</p>
<p>This signal does not support returning deferreds from their handlers.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object) – the response received</li>
<li><strong>request</strong> (<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object) – the request that generated the response</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider for which the response is intended</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="response-downloaded">
<h5>response_downloaded<a class="headerlink" href="#response-downloaded" title="Permalink to this headline">¶</a></h5>
<span class="target" id="std:signal-response_downloaded"></span><dl class="function">
<dt id="scrapy.signals.response_downloaded">
<code class="descclassname">scrapy.signals.</code><code class="descname">response_downloaded</code><span class="sig-paren">(</span><em>response</em>, <em>request</em>, <em>spider</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.signals.response_downloaded" title="Permalink to this definition">¶</a></dt>
<dd><p>Sent by the downloader right after a <code class="docutils literal notranslate"><span class="pre">HTTPResponse</span></code> is downloaded.</p>
<p>This signal does not support returning deferreds from their handlers.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>response</strong> (<a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object) – the response downloaded</li>
<li><strong>request</strong> (<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> object) – the request that generated the response</li>
<li><strong>spider</strong> (<a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> object) – the spider for which the response is intended</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
</div>
<span id="document-topics/exporters"></span><div class="section" id="module-scrapy.exporters">
<span id="item-exporters"></span><span id="topics-exporters"></span><h3>Item Exporters<a class="headerlink" href="#module-scrapy.exporters" title="Permalink to this headline">¶</a></h3>
<p>Once you have scraped your items, you often want to persist or export those
items, to use the data in some other application. That is, after all, the whole
purpose of the scraping process.</p>
<p>For this purpose Scrapy provides a collection of Item Exporters for different
output formats, such as XML, CSV or JSON.</p>
<div class="section" id="using-item-exporters">
<h4>Using Item Exporters<a class="headerlink" href="#using-item-exporters" title="Permalink to this headline">¶</a></h4>
<p>If you are in a hurry, and just want to use an Item Exporter to output scraped
data see the <a class="reference internal" href="index.html#topics-feed-exports"><span class="std std-ref">Feed exports</span></a>. Otherwise, if you want to know how
Item Exporters work or need more custom functionality (not covered by the
default exports), continue reading below.</p>
<p>In order to use an Item Exporter, you  must instantiate it with its required
args. Each Item Exporter requires different arguments, so check each exporter
documentation to be sure, in <a class="reference internal" href="#topics-exporters-reference"><span class="std std-ref">Built-in Item Exporters reference</span></a>. After you have
instantiated your exporter, you have to:</p>
<p>1. call the method <a class="reference internal" href="#scrapy.exporters.BaseItemExporter.start_exporting" title="scrapy.exporters.BaseItemExporter.start_exporting"><code class="xref py py-meth docutils literal notranslate"><span class="pre">start_exporting()</span></code></a> in order to
signal the beginning of the exporting process</p>
<p>2. call the <a class="reference internal" href="#scrapy.exporters.BaseItemExporter.export_item" title="scrapy.exporters.BaseItemExporter.export_item"><code class="xref py py-meth docutils literal notranslate"><span class="pre">export_item()</span></code></a> method for each item you want
to export</p>
<p>3. and finally call the <a class="reference internal" href="#scrapy.exporters.BaseItemExporter.finish_exporting" title="scrapy.exporters.BaseItemExporter.finish_exporting"><code class="xref py py-meth docutils literal notranslate"><span class="pre">finish_exporting()</span></code></a> to signal
the end of the exporting process</p>
<p>Here you can see an <a class="reference internal" href="index.html#document-topics/item-pipeline"><span class="doc">Item Pipeline</span></a> which uses multiple
Item Exporters to group scraped items to different files according to the
value of one of their fields:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.exporters</span> <span class="k">import</span> <span class="n">XmlItemExporter</span>

<span class="k">class</span> <span class="nc">PerYearXmlExportPipeline</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Distribute items across multiple XML files according to their &#39;year&#39; field&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">open_spider</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">year_to_exporter</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">close_spider</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">exporter</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">year_to_exporter</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="n">exporter</span><span class="o">.</span><span class="n">finish_exporting</span><span class="p">()</span>
            <span class="n">exporter</span><span class="o">.</span><span class="n">file</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_exporter_for_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">):</span>
        <span class="n">year</span> <span class="o">=</span> <span class="n">item</span><span class="p">[</span><span class="s1">&#39;year&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">year</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">year_to_exporter</span><span class="p">:</span>
            <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1">.xml&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">year</span><span class="p">),</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span>
            <span class="n">exporter</span> <span class="o">=</span> <span class="n">XmlItemExporter</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
            <span class="n">exporter</span><span class="o">.</span><span class="n">start_exporting</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">year_to_exporter</span><span class="p">[</span><span class="n">year</span><span class="p">]</span> <span class="o">=</span> <span class="n">exporter</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">year_to_exporter</span><span class="p">[</span><span class="n">year</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">item</span><span class="p">,</span> <span class="n">spider</span><span class="p">):</span>
        <span class="n">exporter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_exporter_for_item</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
        <span class="n">exporter</span><span class="o">.</span><span class="n">export_item</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">item</span>
</pre></div>
</div>
</div>
<div class="section" id="serialization-of-item-fields">
<span id="topics-exporters-field-serialization"></span><h4>Serialization of item fields<a class="headerlink" href="#serialization-of-item-fields" title="Permalink to this headline">¶</a></h4>
<p>By default, the field values are passed unmodified to the underlying
serialization library, and the decision of how to serialize them is delegated
to each particular serialization library.</p>
<p>However, you can customize how each field value is serialized <em>before it is
passed to the serialization library</em>.</p>
<p>There are two ways to customize how a field will be serialized, which are
described next.</p>
<div class="section" id="declaring-a-serializer-in-the-field">
<span id="topics-exporters-serializers"></span><h5>1. Declaring a serializer in the field<a class="headerlink" href="#declaring-a-serializer-in-the-field" title="Permalink to this headline">¶</a></h5>
<p>If you use <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a> you can declare a serializer in the
<a class="reference internal" href="index.html#topics-items-fields"><span class="std std-ref">field metadata</span></a>. The serializer must be
a callable which receives a value and returns its serialized form.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scrapy</span>

<span class="k">def</span> <span class="nf">serialize_price</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
    <span class="k">return</span> <span class="s1">&#39;$ </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Product</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>
    <span class="n">price</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">(</span><span class="n">serializer</span><span class="o">=</span><span class="n">serialize_price</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="overriding-the-serialize-field-method">
<h5>2. Overriding the serialize_field() method<a class="headerlink" href="#overriding-the-serialize-field-method" title="Permalink to this headline">¶</a></h5>
<p>You can also override the <a class="reference internal" href="#scrapy.exporters.BaseItemExporter.serialize_field" title="scrapy.exporters.BaseItemExporter.serialize_field"><code class="xref py py-meth docutils literal notranslate"><span class="pre">serialize_field()</span></code></a> method to
customize how your field value will be exported.</p>
<p>Make sure you call the base class <a class="reference internal" href="#scrapy.exporters.BaseItemExporter.serialize_field" title="scrapy.exporters.BaseItemExporter.serialize_field"><code class="xref py py-meth docutils literal notranslate"><span class="pre">serialize_field()</span></code></a> method
after your custom code.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.exporter</span> <span class="k">import</span> <span class="n">XmlItemExporter</span>

<span class="k">class</span> <span class="nc">ProductXmlExporter</span><span class="p">(</span><span class="n">XmlItemExporter</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">serialize_field</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">field</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">field</span> <span class="o">==</span> <span class="s1">&#39;price&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="s1">&#39;$ </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">Product</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">serialize_field</span><span class="p">(</span><span class="n">field</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="built-in-item-exporters-reference">
<span id="topics-exporters-reference"></span><h4>Built-in Item Exporters reference<a class="headerlink" href="#built-in-item-exporters-reference" title="Permalink to this headline">¶</a></h4>
<p>Here is a list of the Item Exporters bundled with Scrapy. Some of them contain
output examples, which assume you’re exporting these two items:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Item</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;Color TV&#39;</span><span class="p">,</span> <span class="n">price</span><span class="o">=</span><span class="s1">&#39;1200&#39;</span><span class="p">)</span>
<span class="n">Item</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;DVD player&#39;</span><span class="p">,</span> <span class="n">price</span><span class="o">=</span><span class="s1">&#39;200&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="baseitemexporter">
<h5>BaseItemExporter<a class="headerlink" href="#baseitemexporter" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.exporters.BaseItemExporter">
<em class="property">class </em><code class="descclassname">scrapy.exporters.</code><code class="descname">BaseItemExporter</code><span class="sig-paren">(</span><em>fields_to_export=None</em>, <em>export_empty_fields=False</em>, <em>encoding='utf-8'</em>, <em>indent=0</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exporters.BaseItemExporter" title="Permalink to this definition">¶</a></dt>
<dd><p>This is the (abstract) base class for all Item Exporters. It provides
support for common features used by all (concrete) Item Exporters, such as
defining what fields to export, whether to export empty fields, or which
encoding to use.</p>
<p>These features can be configured through the constructor arguments which
populate their respective instance attributes: <a class="reference internal" href="#scrapy.exporters.BaseItemExporter.fields_to_export" title="scrapy.exporters.BaseItemExporter.fields_to_export"><code class="xref py py-attr docutils literal notranslate"><span class="pre">fields_to_export</span></code></a>,
<a class="reference internal" href="#scrapy.exporters.BaseItemExporter.export_empty_fields" title="scrapy.exporters.BaseItemExporter.export_empty_fields"><code class="xref py py-attr docutils literal notranslate"><span class="pre">export_empty_fields</span></code></a>, <a class="reference internal" href="#scrapy.exporters.BaseItemExporter.encoding" title="scrapy.exporters.BaseItemExporter.encoding"><code class="xref py py-attr docutils literal notranslate"><span class="pre">encoding</span></code></a>, <a class="reference internal" href="#scrapy.exporters.BaseItemExporter.indent" title="scrapy.exporters.BaseItemExporter.indent"><code class="xref py py-attr docutils literal notranslate"><span class="pre">indent</span></code></a>.</p>
<dl class="method">
<dt id="scrapy.exporters.BaseItemExporter.export_item">
<code class="descname">export_item</code><span class="sig-paren">(</span><em>item</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exporters.BaseItemExporter.export_item" title="Permalink to this definition">¶</a></dt>
<dd><p>Exports the given item. This method must be implemented in subclasses.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.exporters.BaseItemExporter.serialize_field">
<code class="descname">serialize_field</code><span class="sig-paren">(</span><em>field</em>, <em>name</em>, <em>value</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exporters.BaseItemExporter.serialize_field" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the serialized value for the given field. You can override this
method (in your custom Item Exporters) if you want to control how a
particular field or value will be serialized/exported.</p>
<p>By default, this method looks for a serializer <a class="reference internal" href="#topics-exporters-serializers"><span class="std std-ref">declared in the item
field</span></a> and returns the result of applying
that serializer to the value. If no serializer is found, it returns the
value unchanged except for <code class="docutils literal notranslate"><span class="pre">unicode</span></code> values which are encoded to
<code class="docutils literal notranslate"><span class="pre">str</span></code> using the encoding declared in the <a class="reference internal" href="#scrapy.exporters.BaseItemExporter.encoding" title="scrapy.exporters.BaseItemExporter.encoding"><code class="xref py py-attr docutils literal notranslate"><span class="pre">encoding</span></code></a> attribute.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>field</strong> (<a class="reference internal" href="index.html#scrapy.item.Field" title="scrapy.item.Field"><code class="xref py py-class docutils literal notranslate"><span class="pre">Field</span></code></a> object or an empty dict) – the field being serialized. If a raw dict is being
exported (not <a class="reference internal" href="index.html#scrapy.item.Item" title="scrapy.item.Item"><code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code></a>) <em>field</em> value is an empty dict.</li>
<li><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – the name of the field being serialized</li>
<li><strong>value</strong> – the value being serialized</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="scrapy.exporters.BaseItemExporter.start_exporting">
<code class="descname">start_exporting</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exporters.BaseItemExporter.start_exporting" title="Permalink to this definition">¶</a></dt>
<dd><p>Signal the beginning of the exporting process. Some exporters may use
this to generate some required header (for example, the
<a class="reference internal" href="#scrapy.exporters.XmlItemExporter" title="scrapy.exporters.XmlItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">XmlItemExporter</span></code></a>). You must call this method before exporting any
items.</p>
</dd></dl>

<dl class="method">
<dt id="scrapy.exporters.BaseItemExporter.finish_exporting">
<code class="descname">finish_exporting</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exporters.BaseItemExporter.finish_exporting" title="Permalink to this definition">¶</a></dt>
<dd><p>Signal the end of the exporting process. Some exporters may use this to
generate some required footer (for example, the
<a class="reference internal" href="#scrapy.exporters.XmlItemExporter" title="scrapy.exporters.XmlItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">XmlItemExporter</span></code></a>). You must always call this method after you
have no more items to export.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.exporters.BaseItemExporter.fields_to_export">
<code class="descname">fields_to_export</code><a class="headerlink" href="#scrapy.exporters.BaseItemExporter.fields_to_export" title="Permalink to this definition">¶</a></dt>
<dd><p>A list with the name of the fields that will be exported, or None if you
want to export all fields. Defaults to None.</p>
<p>Some exporters (like <a class="reference internal" href="#scrapy.exporters.CsvItemExporter" title="scrapy.exporters.CsvItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">CsvItemExporter</span></code></a>) respect the order of the
fields defined in this attribute.</p>
<p>Some exporters may require fields_to_export list in order to export the
data properly when spiders return dicts (not <code class="xref py py-class docutils literal notranslate"><span class="pre">Item</span></code> instances).</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.exporters.BaseItemExporter.export_empty_fields">
<code class="descname">export_empty_fields</code><a class="headerlink" href="#scrapy.exporters.BaseItemExporter.export_empty_fields" title="Permalink to this definition">¶</a></dt>
<dd><p>Whether to include empty/unpopulated item fields in the exported data.
Defaults to <code class="docutils literal notranslate"><span class="pre">False</span></code>. Some exporters (like <a class="reference internal" href="#scrapy.exporters.CsvItemExporter" title="scrapy.exporters.CsvItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">CsvItemExporter</span></code></a>)
ignore this attribute and always export all empty fields.</p>
<p>This option is ignored for dict items.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.exporters.BaseItemExporter.encoding">
<code class="descname">encoding</code><a class="headerlink" href="#scrapy.exporters.BaseItemExporter.encoding" title="Permalink to this definition">¶</a></dt>
<dd><p>The encoding that will be used to encode unicode values. This only
affects unicode values (which are always serialized to str using this
encoding). Other value types are passed unchanged to the specific
serialization library.</p>
</dd></dl>

<dl class="attribute">
<dt id="scrapy.exporters.BaseItemExporter.indent">
<code class="descname">indent</code><a class="headerlink" href="#scrapy.exporters.BaseItemExporter.indent" title="Permalink to this definition">¶</a></dt>
<dd><p>Amount of spaces used to indent the output on each level. Defaults to <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">indent=None</span></code> selects the most compact representation,
all items in the same line with no indentation</li>
<li><code class="docutils literal notranslate"><span class="pre">indent&lt;=0</span></code> each item on its own line, no indentation</li>
<li><code class="docutils literal notranslate"><span class="pre">indent&gt;0</span></code> each item on its own line, indented with the provided numeric value</li>
</ul>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="xmlitemexporter">
<h5>XmlItemExporter<a class="headerlink" href="#xmlitemexporter" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.exporters.XmlItemExporter">
<em class="property">class </em><code class="descclassname">scrapy.exporters.</code><code class="descname">XmlItemExporter</code><span class="sig-paren">(</span><em>file</em>, <em>item_element='item'</em>, <em>root_element='items'</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exporters.XmlItemExporter" title="Permalink to this definition">¶</a></dt>
<dd><p>Exports Items in XML format to the specified file object.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>file</strong> – the file-like object to use for exporting the data. Its <code class="docutils literal notranslate"><span class="pre">write</span></code> method should
accept <code class="docutils literal notranslate"><span class="pre">bytes</span></code> (a disk file opened in binary mode, a <code class="docutils literal notranslate"><span class="pre">io.BytesIO</span></code> object, etc)</li>
<li><strong>root_element</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – The name of root element in the exported XML.</li>
<li><strong>item_element</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – The name of each item element in the exported XML.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>The additional keyword arguments of this constructor are passed to the
<a class="reference internal" href="#scrapy.exporters.BaseItemExporter" title="scrapy.exporters.BaseItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseItemExporter</span></code></a> constructor.</p>
<p>A typical output of this exporter would be:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;
&lt;items&gt;
  &lt;item&gt;
    &lt;name&gt;Color TV&lt;/name&gt;
    &lt;price&gt;1200&lt;/price&gt;
 &lt;/item&gt;
  &lt;item&gt;
    &lt;name&gt;DVD player&lt;/name&gt;
    &lt;price&gt;200&lt;/price&gt;
 &lt;/item&gt;
&lt;/items&gt;
</pre></div>
</div>
<p>Unless overridden in the <code class="xref py py-meth docutils literal notranslate"><span class="pre">serialize_field()</span></code> method, multi-valued fields are
exported by serializing each value inside a <code class="docutils literal notranslate"><span class="pre">&lt;value&gt;</span></code> element. This is for
convenience, as multi-valued fields are very common.</p>
<p>For example, the item:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Item(name=[&#39;John&#39;, &#39;Doe&#39;], age=&#39;23&#39;)
</pre></div>
</div>
<p>Would be serialized as:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;
&lt;items&gt;
  &lt;item&gt;
    &lt;name&gt;
      &lt;value&gt;John&lt;/value&gt;
      &lt;value&gt;Doe&lt;/value&gt;
    &lt;/name&gt;
    &lt;age&gt;23&lt;/age&gt;
  &lt;/item&gt;
&lt;/items&gt;
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="csvitemexporter">
<h5>CsvItemExporter<a class="headerlink" href="#csvitemexporter" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.exporters.CsvItemExporter">
<em class="property">class </em><code class="descclassname">scrapy.exporters.</code><code class="descname">CsvItemExporter</code><span class="sig-paren">(</span><em>file</em>, <em>include_headers_line=True</em>, <em>join_multivalued='</em>, <em>'</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exporters.CsvItemExporter" title="Permalink to this definition">¶</a></dt>
<dd><p>Exports Items in CSV format to the given file-like object. If the
<code class="xref py py-attr docutils literal notranslate"><span class="pre">fields_to_export</span></code> attribute is set, it will be used to define the
CSV columns and their order. The <code class="xref py py-attr docutils literal notranslate"><span class="pre">export_empty_fields</span></code> attribute has
no effect on this exporter.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>file</strong> – the file-like object to use for exporting the data. Its <code class="docutils literal notranslate"><span class="pre">write</span></code> method should
accept <code class="docutils literal notranslate"><span class="pre">bytes</span></code> (a disk file opened in binary mode, a <code class="docutils literal notranslate"><span class="pre">io.BytesIO</span></code> object, etc)</li>
<li><strong>include_headers_line</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.7)"><em>str</em></a>) – If enabled, makes the exporter output a header
line with the field names taken from
<a class="reference internal" href="#scrapy.exporters.BaseItemExporter.fields_to_export" title="scrapy.exporters.BaseItemExporter.fields_to_export"><code class="xref py py-attr docutils literal notranslate"><span class="pre">BaseItemExporter.fields_to_export</span></code></a> or the first exported item fields.</li>
<li><strong>join_multivalued</strong> – The char (or chars) that will be used for joining
multi-valued fields, if found.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>The additional keyword arguments of this constructor are passed to the
<a class="reference internal" href="#scrapy.exporters.BaseItemExporter" title="scrapy.exporters.BaseItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseItemExporter</span></code></a> constructor, and the leftover arguments to the
<a class="reference external" href="https://docs.python.org/2/library/csv.html#csv.writer">csv.writer</a> constructor, so you can use any <code class="docutils literal notranslate"><span class="pre">csv.writer</span></code> constructor
argument to customize this exporter.</p>
<p>A typical output of this exporter would be:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>product,price
Color TV,1200
DVD player,200
</pre></div>
</div>
</dd></dl>

</div>
<div class="section" id="pickleitemexporter">
<h5>PickleItemExporter<a class="headerlink" href="#pickleitemexporter" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.exporters.PickleItemExporter">
<em class="property">class </em><code class="descclassname">scrapy.exporters.</code><code class="descname">PickleItemExporter</code><span class="sig-paren">(</span><em>file</em>, <em>protocol=0</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exporters.PickleItemExporter" title="Permalink to this definition">¶</a></dt>
<dd><p>Exports Items in pickle format to the given file-like object.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>file</strong> – the file-like object to use for exporting the data. Its <code class="docutils literal notranslate"><span class="pre">write</span></code> method should
accept <code class="docutils literal notranslate"><span class="pre">bytes</span></code> (a disk file opened in binary mode, a <code class="docutils literal notranslate"><span class="pre">io.BytesIO</span></code> object, etc)</li>
<li><strong>protocol</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.7)"><em>int</em></a>) – The pickle protocol to use.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>For more information, refer to the <a class="reference external" href="https://docs.python.org/2/library/pickle.html">pickle module documentation</a>.</p>
<p>The additional keyword arguments of this constructor are passed to the
<a class="reference internal" href="#scrapy.exporters.BaseItemExporter" title="scrapy.exporters.BaseItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseItemExporter</span></code></a> constructor.</p>
<p>Pickle isn’t a human readable format, so no output examples are provided.</p>
</dd></dl>

</div>
<div class="section" id="pprintitemexporter">
<h5>PprintItemExporter<a class="headerlink" href="#pprintitemexporter" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.exporters.PprintItemExporter">
<em class="property">class </em><code class="descclassname">scrapy.exporters.</code><code class="descname">PprintItemExporter</code><span class="sig-paren">(</span><em>file</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exporters.PprintItemExporter" title="Permalink to this definition">¶</a></dt>
<dd><p>Exports Items in pretty print format to the specified file object.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>file</strong> – the file-like object to use for exporting the data. Its <code class="docutils literal notranslate"><span class="pre">write</span></code> method should
accept <code class="docutils literal notranslate"><span class="pre">bytes</span></code> (a disk file opened in binary mode, a <code class="docutils literal notranslate"><span class="pre">io.BytesIO</span></code> object, etc)</td>
</tr>
</tbody>
</table>
<p>The additional keyword arguments of this constructor are passed to the
<a class="reference internal" href="#scrapy.exporters.BaseItemExporter" title="scrapy.exporters.BaseItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseItemExporter</span></code></a> constructor.</p>
<p>A typical output of this exporter would be:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>{&#39;name&#39;: &#39;Color TV&#39;, &#39;price&#39;: &#39;1200&#39;}
{&#39;name&#39;: &#39;DVD player&#39;, &#39;price&#39;: &#39;200&#39;}
</pre></div>
</div>
<p>Longer lines (when present) are pretty-formatted.</p>
</dd></dl>

</div>
<div class="section" id="jsonitemexporter">
<h5>JsonItemExporter<a class="headerlink" href="#jsonitemexporter" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.exporters.JsonItemExporter">
<em class="property">class </em><code class="descclassname">scrapy.exporters.</code><code class="descname">JsonItemExporter</code><span class="sig-paren">(</span><em>file</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exporters.JsonItemExporter" title="Permalink to this definition">¶</a></dt>
<dd><p>Exports Items in JSON format to the specified file-like object, writing all
objects as a list of objects. The additional constructor arguments are
passed to the <a class="reference internal" href="#scrapy.exporters.BaseItemExporter" title="scrapy.exporters.BaseItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseItemExporter</span></code></a> constructor, and the leftover
arguments to the <a class="reference external" href="https://docs.python.org/2/library/json.html#json.JSONEncoder">JSONEncoder</a> constructor, so you can use any
<a class="reference external" href="https://docs.python.org/2/library/json.html#json.JSONEncoder">JSONEncoder</a> constructor argument to customize this exporter.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>file</strong> – the file-like object to use for exporting the data. Its <code class="docutils literal notranslate"><span class="pre">write</span></code> method should
accept <code class="docutils literal notranslate"><span class="pre">bytes</span></code> (a disk file opened in binary mode, a <code class="docutils literal notranslate"><span class="pre">io.BytesIO</span></code> object, etc)</td>
</tr>
</tbody>
</table>
<p>A typical output of this exporter would be:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>[{&quot;name&quot;: &quot;Color TV&quot;, &quot;price&quot;: &quot;1200&quot;},
{&quot;name&quot;: &quot;DVD player&quot;, &quot;price&quot;: &quot;200&quot;}]
</pre></div>
</div>
<div class="admonition warning" id="json-with-large-data">
<p class="first admonition-title">Warning</p>
<p class="last">JSON is very simple and flexible serialization format, but it
doesn’t scale well for large amounts of data since incremental (aka.
stream-mode) parsing is not well supported (if at all) among JSON parsers
(on any language), and most of them just parse the entire object in
memory. If you want the power and simplicity of JSON with a more
stream-friendly format, consider using <a class="reference internal" href="#scrapy.exporters.JsonLinesItemExporter" title="scrapy.exporters.JsonLinesItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">JsonLinesItemExporter</span></code></a>
instead, or splitting the output in multiple chunks.</p>
</div>
</dd></dl>

</div>
<div class="section" id="jsonlinesitemexporter">
<h5>JsonLinesItemExporter<a class="headerlink" href="#jsonlinesitemexporter" title="Permalink to this headline">¶</a></h5>
<dl class="class">
<dt id="scrapy.exporters.JsonLinesItemExporter">
<em class="property">class </em><code class="descclassname">scrapy.exporters.</code><code class="descname">JsonLinesItemExporter</code><span class="sig-paren">(</span><em>file</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#scrapy.exporters.JsonLinesItemExporter" title="Permalink to this definition">¶</a></dt>
<dd><p>Exports Items in JSON format to the specified file-like object, writing one
JSON-encoded item per line. The additional constructor arguments are passed
to the <a class="reference internal" href="#scrapy.exporters.BaseItemExporter" title="scrapy.exporters.BaseItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseItemExporter</span></code></a> constructor, and the leftover arguments to
the <a class="reference external" href="https://docs.python.org/2/library/json.html#json.JSONEncoder">JSONEncoder</a> constructor, so you can use any <a class="reference external" href="https://docs.python.org/2/library/json.html#json.JSONEncoder">JSONEncoder</a>
constructor argument to customize this exporter.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>file</strong> – the file-like object to use for exporting the data. Its <code class="docutils literal notranslate"><span class="pre">write</span></code> method should
accept <code class="docutils literal notranslate"><span class="pre">bytes</span></code> (a disk file opened in binary mode, a <code class="docutils literal notranslate"><span class="pre">io.BytesIO</span></code> object, etc)</td>
</tr>
</tbody>
</table>
<p>A typical output of this exporter would be:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>{&quot;name&quot;: &quot;Color TV&quot;, &quot;price&quot;: &quot;1200&quot;}
{&quot;name&quot;: &quot;DVD player&quot;, &quot;price&quot;: &quot;200&quot;}
</pre></div>
</div>
<p>Unlike the one produced by <a class="reference internal" href="#scrapy.exporters.JsonItemExporter" title="scrapy.exporters.JsonItemExporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">JsonItemExporter</span></code></a>, the format produced by
this exporter is well suited for serializing large amounts of data.</p>
</dd></dl>

</div>
</div>
</div>
</div>
<dl class="docutils">
<dt><a class="reference internal" href="index.html#document-topics/architecture"><span class="doc">Architecture overview</span></a></dt>
<dd>Understand the Scrapy architecture.</dd>
<dt><a class="reference internal" href="index.html#document-topics/downloader-middleware"><span class="doc">Downloader Middleware</span></a></dt>
<dd>Customize how pages get requested and downloaded.</dd>
<dt><a class="reference internal" href="index.html#document-topics/spider-middleware"><span class="doc">Spider Middleware</span></a></dt>
<dd>Customize the input and output of your spiders.</dd>
<dt><a class="reference internal" href="index.html#document-topics/extensions"><span class="doc">Extensions</span></a></dt>
<dd>Extend Scrapy with your custom functionality</dd>
<dt><a class="reference internal" href="index.html#document-topics/api"><span class="doc">Core API</span></a></dt>
<dd>Use it on extensions and middlewares to extend Scrapy functionality</dd>
<dt><a class="reference internal" href="index.html#document-topics/signals"><span class="doc">Signals</span></a></dt>
<dd>See all available signals and how to work with them.</dd>
<dt><a class="reference internal" href="index.html#document-topics/exporters"><span class="doc">Item Exporters</span></a></dt>
<dd>Quickly export your scraped items to a file (XML, CSV, etc).</dd>
</dl>
</div>
<div class="section" id="all-the-rest">
<h2>All the rest<a class="headerlink" href="#all-the-rest" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<span id="document-news"></span><div class="section" id="release-notes">
<span id="news"></span><h3>Release notes<a class="headerlink" href="#release-notes" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Scrapy 1.x will be the last series supporting Python 2. Scrapy 2.0,
planned for Q4 2019 or Q1 2020, will support <strong>Python 3 only</strong>.</p>
</div>
<div class="section" id="scrapy-1-7-3-2019-08-01">
<h4>Scrapy 1.7.3 (2019-08-01)<a class="headerlink" href="#scrapy-1-7-3-2019-08-01" title="Permalink to this headline">¶</a></h4>
<p>Enforce lxml 4.3.5 or lower for Python 3.4 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3912">issue 3912</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3918">issue 3918</a>).</p>
</div>
<div class="section" id="scrapy-1-7-2-2019-07-23">
<h4>Scrapy 1.7.2 (2019-07-23)<a class="headerlink" href="#scrapy-1-7-2-2019-07-23" title="Permalink to this headline">¶</a></h4>
<p>Fix Python 2 support (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3889">issue 3889</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3893">issue 3893</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3896">issue 3896</a>).</p>
</div>
<div class="section" id="scrapy-1-7-1-2019-07-18">
<h4>Scrapy 1.7.1 (2019-07-18)<a class="headerlink" href="#scrapy-1-7-1-2019-07-18" title="Permalink to this headline">¶</a></h4>
<p>Re-packaging of Scrapy 1.7.0, which was missing some changes in PyPI.</p>
</div>
<div class="section" id="scrapy-1-7-0-2019-07-18">
<span id="release-1-7-0"></span><h4>Scrapy 1.7.0 (2019-07-18)<a class="headerlink" href="#scrapy-1-7-0-2019-07-18" title="Permalink to this headline">¶</a></h4>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Make sure you install Scrapy 1.7.1. The Scrapy 1.7.0 package in PyPI
is the result of an erroneous commit tagging and does not include all
the changes described below.</p>
</div>
<p>Highlights:</p>
<ul class="simple">
<li>Improvements for crawls targeting multiple domains</li>
<li>A cleaner way to pass arguments to callbacks</li>
<li>A new class for JSON requests</li>
<li>Improvements for rule-based spiders</li>
<li>New features for feed exports</li>
</ul>
<div class="section" id="backward-incompatible-changes">
<h5>Backward-incompatible changes<a class="headerlink" href="#backward-incompatible-changes" title="Permalink to this headline">¶</a></h5>
<ul>
<li><p class="first"><code class="docutils literal notranslate"><span class="pre">429</span></code> is now part of the <a class="reference internal" href="index.html#std:setting-RETRY_HTTP_CODES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_HTTP_CODES</span></code></a> setting by default</p>
<p>This change is <strong>backward incompatible</strong>. If you don’t want to retry
<code class="docutils literal notranslate"><span class="pre">429</span></code>, you must override <a class="reference internal" href="index.html#std:setting-RETRY_HTTP_CODES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_HTTP_CODES</span></code></a> accordingly.</p>
</li>
<li><p class="first"><a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a>,
<a class="reference internal" href="index.html#scrapy.crawler.CrawlerRunner.crawl" title="scrapy.crawler.CrawlerRunner.crawl"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerRunner.crawl</span></code></a> and
<a class="reference internal" href="index.html#scrapy.crawler.CrawlerRunner.create_crawler" title="scrapy.crawler.CrawlerRunner.create_crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerRunner.create_crawler</span></code></a>
no longer accept a <a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> subclass instance, they
only accept a <a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> subclass now.</p>
<p><a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> subclass instances were never meant to
work, and they were not working as one would expect: instead of using the
passed <a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> subclass instance, their
<a class="reference internal" href="index.html#scrapy.spiders.Spider.from_crawler" title="scrapy.spiders.Spider.from_crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">from_crawler</span></code></a> method was called to generate
a new instance.</p>
</li>
<li><p class="first">Non-default values for the <a class="reference internal" href="index.html#std:setting-SCHEDULER_PRIORITY_QUEUE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SCHEDULER_PRIORITY_QUEUE</span></code></a> setting
may stop working. Scheduler priority queue classes now need to handle
<a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a> objects instead of arbitrary Python data
structures.</p>
</li>
</ul>
<p>See also <a class="reference internal" href="#deprecation-removals"><span class="std std-ref">Deprecation removals</span></a> below.</p>
</div>
<div class="section" id="new-features">
<h5>New features<a class="headerlink" href="#new-features" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>A new scheduler priority queue,
<code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.pqueues.DownloaderAwarePriorityQueue</span></code>, may be
<a class="reference internal" href="index.html#broad-crawls-scheduler-priority-queue"><span class="std std-ref">enabled</span></a> for a significant
scheduling improvement on crawls targetting multiple web domains, at the
cost of no <a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a> support (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3520">issue 3520</a>)</li>
<li>A new <a class="reference internal" href="index.html#scrapy.http.Request.cb_kwargs" title="scrapy.http.Request.cb_kwargs"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Request.cb_kwargs</span></code></a> attribute
provides a cleaner way to pass keyword arguments to callback methods
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1138">issue 1138</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3563">issue 3563</a>)</li>
<li>A new <a class="reference internal" href="index.html#scrapy.http.JSONRequest" title="scrapy.http.JSONRequest"><code class="xref py py-class docutils literal notranslate"><span class="pre">JSONRequest</span></code></a> class offers a more convenient way
to build JSON requests (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3504">issue 3504</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3505">issue 3505</a>)</li>
<li>A <code class="docutils literal notranslate"><span class="pre">process_request</span></code> callback passed to the <a class="reference internal" href="index.html#scrapy.spiders.Rule" title="scrapy.spiders.Rule"><code class="xref py py-class docutils literal notranslate"><span class="pre">Rule</span></code></a>
constructor now receives the <a class="reference internal" href="index.html#scrapy.http.Response" title="scrapy.http.Response"><code class="xref py py-class docutils literal notranslate"><span class="pre">Response</span></code></a> object that
originated the request as its second argument (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3682">issue 3682</a>)</li>
<li>A new <code class="docutils literal notranslate"><span class="pre">restrict_text</span></code> parameter for the
<a class="reference internal" href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"><code class="xref py py-attr docutils literal notranslate"><span class="pre">LinkExtractor</span></code></a>
constructor allows filtering links by linking text (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3622">issue 3622</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3635">issue 3635</a>)</li>
<li>A new <a class="reference internal" href="index.html#std:setting-FEED_STORAGE_S3_ACL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_STORAGE_S3_ACL</span></code></a> setting allows defining a custom ACL
for feeds exported to Amazon S3 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3607">issue 3607</a>)</li>
<li>A new <a class="reference internal" href="index.html#std:setting-FEED_STORAGE_FTP_ACTIVE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_STORAGE_FTP_ACTIVE</span></code></a> setting allows using FTP’s active
connection mode for feeds exported to FTP servers (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3829">issue 3829</a>)</li>
<li>A new <a class="reference internal" href="index.html#std:setting-METAREFRESH_IGNORE_TAGS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">METAREFRESH_IGNORE_TAGS</span></code></a> setting allows overriding which
HTML tags are ignored when searching a response for HTML meta tags that
trigger a redirect (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1422">issue 1422</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3768">issue 3768</a>)</li>
<li>A new <a class="reference internal" href="index.html#std:reqmeta-redirect_reasons"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">redirect_reasons</span></code></a> request meta key exposes the reason
(status code, meta refresh) behind every followed redirect (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3581">issue 3581</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3687">issue 3687</a>)</li>
<li>The <code class="docutils literal notranslate"><span class="pre">SCRAPY_CHECK</span></code> variable is now set to the <code class="docutils literal notranslate"><span class="pre">true</span></code> string during runs
of the <a class="reference internal" href="index.html#std:command-check"><code class="xref std std-command docutils literal notranslate"><span class="pre">check</span></code></a> command, which allows <a class="reference internal" href="index.html#detecting-contract-check-runs"><span class="std std-ref">detecting contract
check runs from code</span></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3704">issue 3704</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3739">issue 3739</a>)</li>
<li>A new <code class="xref py py-meth docutils literal notranslate"><span class="pre">Item.deepcopy()</span></code> method makes it
easier to <a class="reference internal" href="index.html#copying-items"><span class="std std-ref">deep-copy items</span></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1493">issue 1493</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3671">issue 3671</a>)</li>
<li><a class="reference internal" href="index.html#scrapy.extensions.corestats.CoreStats" title="scrapy.extensions.corestats.CoreStats"><code class="xref py py-class docutils literal notranslate"><span class="pre">CoreStats</span></code></a> also logs
<code class="docutils literal notranslate"><span class="pre">elapsed_time_seconds</span></code> now (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3638">issue 3638</a>)</li>
<li>Exceptions from <a class="reference internal" href="index.html#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">ItemLoader</span></code></a> <a class="reference internal" href="index.html#topics-loaders-processors"><span class="std std-ref">input and output
processors</span></a> are now more verbose
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3836">issue 3836</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3840">issue 3840</a>)</li>
<li><a class="reference internal" href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">Crawler</span></code></a>,
<a class="reference internal" href="index.html#scrapy.crawler.CrawlerRunner.crawl" title="scrapy.crawler.CrawlerRunner.crawl"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerRunner.crawl</span></code></a> and
<a class="reference internal" href="index.html#scrapy.crawler.CrawlerRunner.create_crawler" title="scrapy.crawler.CrawlerRunner.create_crawler"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlerRunner.create_crawler</span></code></a>
now fail gracefully if they receive a <a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a>
subclass instance instead of the subclass itself (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2283">issue 2283</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3610">issue 3610</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3872">issue 3872</a>)</li>
</ul>
</div>
<div class="section" id="bug-fixes">
<h5>Bug fixes<a class="headerlink" href="#bug-fixes" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li><a class="reference internal" href="index.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception"><code class="xref py py-meth docutils literal notranslate"><span class="pre">process_spider_exception()</span></code></a>
is now also invoked for generators (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/220">issue 220</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2061">issue 2061</a>)</li>
<li>System exceptions like <a class="reference external" href="https://docs.python.org/library/exceptions.html#KeyboardInterrupt">KeyboardInterrupt</a> are no longer caught
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3726">issue 3726</a>)</li>
<li><a class="reference internal" href="index.html#scrapy.loader.ItemLoader.load_item" title="scrapy.loader.ItemLoader.load_item"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ItemLoader.load_item()</span></code></a> no
longer makes later calls to <a class="reference internal" href="index.html#scrapy.loader.ItemLoader.get_output_value" title="scrapy.loader.ItemLoader.get_output_value"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ItemLoader.get_output_value()</span></code></a> or
<a class="reference internal" href="index.html#scrapy.loader.ItemLoader.load_item" title="scrapy.loader.ItemLoader.load_item"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ItemLoader.load_item()</span></code></a> return
empty data (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3804">issue 3804</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3819">issue 3819</a>)</li>
<li>The images pipeline (<a class="reference internal" href="index.html#scrapy.pipelines.images.ImagesPipeline" title="scrapy.pipelines.images.ImagesPipeline"><code class="xref py py-class docutils literal notranslate"><span class="pre">ImagesPipeline</span></code></a>) no
longer ignores these Amazon S3 settings: <a class="reference internal" href="index.html#std:setting-AWS_ENDPOINT_URL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AWS_ENDPOINT_URL</span></code></a>,
<a class="reference internal" href="index.html#std:setting-AWS_REGION_NAME"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AWS_REGION_NAME</span></code></a>, <a class="reference internal" href="index.html#std:setting-AWS_USE_SSL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AWS_USE_SSL</span></code></a>, <a class="reference internal" href="index.html#std:setting-AWS_VERIFY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AWS_VERIFY</span></code></a>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3625">issue 3625</a>)</li>
<li>Fixed a memory leak in <code class="xref py py-class docutils literal notranslate"><span class="pre">MediaPipeline</span></code>
affecting, for example, non-200 responses and exceptions from custom
middlewares (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3813">issue 3813</a>)</li>
<li>Requests with private callbacks are now correctly unserialized from disk
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3790">issue 3790</a>)</li>
<li><a class="reference internal" href="index.html#scrapy.http.FormRequest.from_response" title="scrapy.http.FormRequest.from_response"><code class="xref py py-meth docutils literal notranslate"><span class="pre">FormRequest.from_response()</span></code></a>
now handles invalid methods like major web browsers (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3777">issue 3777</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3794">issue 3794</a>)</li>
</ul>
</div>
<div class="section" id="documentation">
<h5>Documentation<a class="headerlink" href="#documentation" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>A new topic, <a class="reference internal" href="index.html#topics-dynamic-content"><span class="std std-ref">Selecting dynamically-loaded content</span></a>, covers recommended approaches
to read dynamically-loaded data (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3703">issue 3703</a>)</li>
<li><a class="reference internal" href="index.html#topics-broad-crawls"><span class="std std-ref">Broad Crawls</span></a> now features information about memory usage
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1264">issue 1264</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3866">issue 3866</a>)</li>
<li>The documentation of <a class="reference internal" href="index.html#scrapy.spiders.Rule" title="scrapy.spiders.Rule"><code class="xref py py-class docutils literal notranslate"><span class="pre">Rule</span></code></a> now covers how to access
the text of a link when using <a class="reference internal" href="index.html#scrapy.spiders.CrawlSpider" title="scrapy.spiders.CrawlSpider"><code class="xref py py-class docutils literal notranslate"><span class="pre">CrawlSpider</span></code></a>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3711">issue 3711</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3712">issue 3712</a>)</li>
<li>A new section, <a class="reference internal" href="index.html#httpcache-storage-custom"><span class="std std-ref">Writing your own storage backend</span></a>, covers writing a custom
cache storage backend for
<a class="reference internal" href="index.html#scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware" title="scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">HttpCacheMiddleware</span></code></a>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3683">issue 3683</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3692">issue 3692</a>)</li>
<li>A new <a class="reference internal" href="index.html#faq"><span class="std std-ref">FAQ</span></a> entry, <a class="reference internal" href="index.html#faq-split-item"><span class="std std-ref">How to split an item into multiple items in an item pipeline?</span></a>, explains what to do
when you want to split an item into multiple items from an item pipeline
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2240">issue 2240</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3672">issue 3672</a>)</li>
<li>Updated the <a class="reference internal" href="index.html#faq-bfo-dfo"><span class="std std-ref">FAQ entry about crawl order</span></a> to explain why
the first few requests rarely follow the desired order (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1739">issue 1739</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3621">issue 3621</a>)</li>
<li>The <a class="reference internal" href="index.html#std:setting-LOGSTATS_INTERVAL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOGSTATS_INTERVAL</span></code></a> setting (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3730">issue 3730</a>), the
<a class="reference internal" href="index.html#scrapy.pipelines.files.FilesPipeline.file_path" title="scrapy.pipelines.files.FilesPipeline.file_path"><code class="xref py py-meth docutils literal notranslate"><span class="pre">FilesPipeline.file_path</span></code></a>
and
<a class="reference internal" href="index.html#scrapy.pipelines.images.ImagesPipeline.file_path" title="scrapy.pipelines.images.ImagesPipeline.file_path"><code class="xref py py-meth docutils literal notranslate"><span class="pre">ImagesPipeline.file_path</span></code></a>
methods (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2253">issue 2253</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3609">issue 3609</a>) and the
<a class="reference internal" href="index.html#scrapy.crawler.Crawler.stop" title="scrapy.crawler.Crawler.stop"><code class="xref py py-meth docutils literal notranslate"><span class="pre">Crawler.stop()</span></code></a> method (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3842">issue 3842</a>)
are now documented</li>
<li>Some parts of the documentation that were confusing or misleading are now
clearer (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1347">issue 1347</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1789">issue 1789</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2289">issue 2289</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3069">issue 3069</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3615">issue 3615</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3626">issue 3626</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3668">issue 3668</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3670">issue 3670</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3673">issue 3673</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3728">issue 3728</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3762">issue 3762</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3861">issue 3861</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3882">issue 3882</a>)</li>
<li>Minor documentation fixes (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3648">issue 3648</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3649">issue 3649</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3662">issue 3662</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3674">issue 3674</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3676">issue 3676</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3694">issue 3694</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3724">issue 3724</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3764">issue 3764</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3767">issue 3767</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3791">issue 3791</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3797">issue 3797</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3806">issue 3806</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3812">issue 3812</a>)</li>
</ul>
</div>
<div class="section" id="deprecation-removals">
<span id="id1"></span><h5>Deprecation removals<a class="headerlink" href="#deprecation-removals" title="Permalink to this headline">¶</a></h5>
<p>The following deprecated APIs have been removed (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3578">issue 3578</a>):</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">scrapy.conf</span></code> (use <a class="reference internal" href="index.html#scrapy.crawler.Crawler.settings" title="scrapy.crawler.Crawler.settings"><code class="xref py py-attr docutils literal notranslate"><span class="pre">Crawler.settings</span></code></a>)</li>
<li>From <code class="docutils literal notranslate"><span class="pre">scrapy.core.downloader.handlers</span></code>:<ul>
<li><code class="docutils literal notranslate"><span class="pre">http.HttpDownloadHandler</span></code> (use <code class="docutils literal notranslate"><span class="pre">http10.HTTP10DownloadHandler</span></code>)</li>
</ul>
</li>
<li><code class="docutils literal notranslate"><span class="pre">scrapy.loader.ItemLoader._get_values</span></code> (use <code class="docutils literal notranslate"><span class="pre">_get_xpathvalues</span></code>)</li>
<li><code class="docutils literal notranslate"><span class="pre">scrapy.loader.XPathItemLoader</span></code> (use <a class="reference internal" href="index.html#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">ItemLoader</span></code></a>)</li>
<li><code class="docutils literal notranslate"><span class="pre">scrapy.log</span></code> (see <a class="reference internal" href="index.html#topics-logging"><span class="std std-ref">Logging</span></a>)</li>
<li>From <code class="docutils literal notranslate"><span class="pre">scrapy.pipelines</span></code>:<ul>
<li><code class="docutils literal notranslate"><span class="pre">files.FilesPipeline.file_key</span></code> (use <code class="docutils literal notranslate"><span class="pre">file_path</span></code>)</li>
<li><code class="docutils literal notranslate"><span class="pre">images.ImagesPipeline.file_key</span></code> (use <code class="docutils literal notranslate"><span class="pre">file_path</span></code>)</li>
<li><code class="docutils literal notranslate"><span class="pre">images.ImagesPipeline.image_key</span></code> (use <code class="docutils literal notranslate"><span class="pre">file_path</span></code>)</li>
<li><code class="docutils literal notranslate"><span class="pre">images.ImagesPipeline.thumb_key</span></code> (use <code class="docutils literal notranslate"><span class="pre">thumb_path</span></code>)</li>
</ul>
</li>
<li>From both <code class="docutils literal notranslate"><span class="pre">scrapy.selector</span></code> and <code class="docutils literal notranslate"><span class="pre">scrapy.selector.lxmlsel</span></code>:<ul>
<li><code class="docutils literal notranslate"><span class="pre">HtmlXPathSelector</span></code> (use <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a>)</li>
<li><code class="docutils literal notranslate"><span class="pre">XmlXPathSelector</span></code> (use <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a>)</li>
<li><code class="docutils literal notranslate"><span class="pre">XPathSelector</span></code> (use <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a>)</li>
<li><code class="docutils literal notranslate"><span class="pre">XPathSelectorList</span></code> (use <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a>)</li>
</ul>
</li>
<li>From <code class="docutils literal notranslate"><span class="pre">scrapy.selector.csstranslator</span></code>:<ul>
<li><code class="docutils literal notranslate"><span class="pre">ScrapyGenericTranslator</span></code> (use <a class="reference external" href="https://parsel.readthedocs.io/en/latest/parsel.html#parsel.csstranslator.GenericTranslator">parsel.csstranslator.GenericTranslator</a>)</li>
<li><code class="docutils literal notranslate"><span class="pre">ScrapyHTMLTranslator</span></code> (use <a class="reference external" href="https://parsel.readthedocs.io/en/latest/parsel.html#parsel.csstranslator.HTMLTranslator">parsel.csstranslator.HTMLTranslator</a>)</li>
<li><code class="docutils literal notranslate"><span class="pre">ScrapyXPathExpr</span></code> (use <a class="reference external" href="https://parsel.readthedocs.io/en/latest/parsel.html#parsel.csstranslator.XPathExpr">parsel.csstranslator.XPathExpr</a>)</li>
</ul>
</li>
<li>From <a class="reference internal" href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector"><code class="xref py py-class docutils literal notranslate"><span class="pre">Selector</span></code></a>:<ul>
<li><code class="docutils literal notranslate"><span class="pre">_root</span></code> (both the constructor argument and the object property, use
<code class="docutils literal notranslate"><span class="pre">root</span></code>)</li>
<li><code class="docutils literal notranslate"><span class="pre">extract_unquoted</span></code> (use <code class="docutils literal notranslate"><span class="pre">getall</span></code>)</li>
<li><code class="docutils literal notranslate"><span class="pre">select</span></code> (use <code class="docutils literal notranslate"><span class="pre">xpath</span></code>)</li>
</ul>
</li>
<li>From <a class="reference internal" href="index.html#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList"><code class="xref py py-class docutils literal notranslate"><span class="pre">SelectorList</span></code></a>:<ul>
<li><code class="docutils literal notranslate"><span class="pre">extract_unquoted</span></code> (use <code class="docutils literal notranslate"><span class="pre">getall</span></code>)</li>
<li><code class="docutils literal notranslate"><span class="pre">select</span></code> (use <code class="docutils literal notranslate"><span class="pre">xpath</span></code>)</li>
<li><code class="docutils literal notranslate"><span class="pre">x</span></code> (use <code class="docutils literal notranslate"><span class="pre">xpath</span></code>)</li>
</ul>
</li>
<li><code class="docutils literal notranslate"><span class="pre">scrapy.spiders.BaseSpider</span></code> (use <a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a>)</li>
<li>From <a class="reference internal" href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider"><code class="xref py py-class docutils literal notranslate"><span class="pre">Spider</span></code></a> (and subclasses):<ul>
<li><code class="docutils literal notranslate"><span class="pre">DOWNLOAD_DELAY</span></code> (use <a class="reference internal" href="index.html#spider-download-delay-attribute"><span class="std std-ref">download_delay</span></a>)</li>
<li><code class="docutils literal notranslate"><span class="pre">set_crawler</span></code> (use <a class="reference internal" href="index.html#scrapy.spiders.Spider.from_crawler" title="scrapy.spiders.Spider.from_crawler"><code class="xref py py-meth docutils literal notranslate"><span class="pre">from_crawler()</span></code></a>)</li>
</ul>
</li>
<li><code class="docutils literal notranslate"><span class="pre">scrapy.spiders.spiders</span></code> (use <a class="reference internal" href="index.html#scrapy.spiderloader.SpiderLoader" title="scrapy.spiderloader.SpiderLoader"><code class="xref py py-class docutils literal notranslate"><span class="pre">SpiderLoader</span></code></a>)</li>
<li><code class="docutils literal notranslate"><span class="pre">scrapy.telnet</span></code> (use <a class="reference internal" href="index.html#module-scrapy.extensions.telnet" title="scrapy.extensions.telnet: Telnet console"><code class="xref py py-mod docutils literal notranslate"><span class="pre">scrapy.extensions.telnet</span></code></a>)</li>
<li>From <code class="docutils literal notranslate"><span class="pre">scrapy.utils.python</span></code>:<ul>
<li><code class="docutils literal notranslate"><span class="pre">str_to_unicode</span></code> (use <code class="docutils literal notranslate"><span class="pre">to_unicode</span></code>)</li>
<li><code class="docutils literal notranslate"><span class="pre">unicode_to_str</span></code> (use <code class="docutils literal notranslate"><span class="pre">to_bytes</span></code>)</li>
</ul>
</li>
<li><code class="docutils literal notranslate"><span class="pre">scrapy.utils.response.body_or_str</span></code></li>
</ul>
<p>The following deprecated settings have also been removed (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3578">issue 3578</a>):</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">SPIDER_MANAGER_CLASS</span></code> (use <a class="reference internal" href="index.html#std:setting-SPIDER_LOADER_CLASS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_LOADER_CLASS</span></code></a>)</li>
</ul>
</div>
<div class="section" id="deprecations">
<h5>Deprecations<a class="headerlink" href="#deprecations" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>The <code class="docutils literal notranslate"><span class="pre">queuelib.PriorityQueue</span></code> value for the
<a class="reference internal" href="index.html#std:setting-SCHEDULER_PRIORITY_QUEUE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SCHEDULER_PRIORITY_QUEUE</span></code></a> setting is deprecated. Use
<code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.pqueues.ScrapyPriorityQueue</span></code> instead.</li>
<li><code class="docutils literal notranslate"><span class="pre">process_request</span></code> callbacks passed to <a class="reference internal" href="index.html#scrapy.spiders.Rule" title="scrapy.spiders.Rule"><code class="xref py py-class docutils literal notranslate"><span class="pre">Rule</span></code></a> that
do not accept two arguments are deprecated.</li>
<li>The following modules are deprecated:<ul>
<li><code class="docutils literal notranslate"><span class="pre">scrapy.utils.http</span></code> (use <a class="reference external" href="https://w3lib.readthedocs.io/en/latest/w3lib.html#module-w3lib.http">w3lib.http</a>)</li>
<li><code class="docutils literal notranslate"><span class="pre">scrapy.utils.markup</span></code> (use <a class="reference external" href="https://w3lib.readthedocs.io/en/latest/w3lib.html#module-w3lib.html">w3lib.html</a>)</li>
<li><code class="docutils literal notranslate"><span class="pre">scrapy.utils.multipart</span></code> (use <a class="reference external" href="https://urllib3.readthedocs.io/en/latest/index.html">urllib3</a>)</li>
</ul>
</li>
<li>The <code class="docutils literal notranslate"><span class="pre">scrapy.utils.datatypes.MergeDict</span></code> class is deprecated for Python 3
code bases. Use <a class="reference external" href="https://docs.python.org/3/library/collections.html#collections.ChainMap" title="(in Python v3.7)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ChainMap</span></code></a> instead. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3878">issue 3878</a>)</li>
<li>The <code class="docutils literal notranslate"><span class="pre">scrapy.utils.gz.is_gzipped</span></code> function is deprecated. Use
<code class="docutils literal notranslate"><span class="pre">scrapy.utils.gz.gzip_magic_number</span></code> instead.</li>
</ul>
</div>
<div class="section" id="other-changes">
<h5>Other changes<a class="headerlink" href="#other-changes" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>It is now possible to run all tests from the same <a class="reference external" href="https://pypi.python.org/pypi/tox">tox</a> environment in
parallel; the documentation now covers <a class="reference internal" href="index.html#running-tests"><span class="std std-ref">this and other ways to run
tests</span></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3707">issue 3707</a>)</li>
<li>It is now possible to generate an API documentation coverage report
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3806">issue 3806</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3810">issue 3810</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3860">issue 3860</a>)</li>
<li>The <a class="reference internal" href="index.html#documentation-policies"><span class="std std-ref">documentation policies</span></a> now require
<a class="reference external" href="https://docs.python.org/glossary.html#term-docstring">docstrings</a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3701">issue 3701</a>) that follow <a class="reference external" href="https://www.python.org/dev/peps/pep-0257/">PEP 257</a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3748">issue 3748</a>)</li>
<li>Internal fixes and cleanup (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3629">issue 3629</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3643">issue 3643</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3684">issue 3684</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3698">issue 3698</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3734">issue 3734</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3735">issue 3735</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3736">issue 3736</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3737">issue 3737</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3809">issue 3809</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3821">issue 3821</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3825">issue 3825</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3827">issue 3827</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3833">issue 3833</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3857">issue 3857</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3877">issue 3877</a>)</li>
</ul>
</div>
</div>
<div class="section" id="scrapy-1-6-0-2019-01-30">
<span id="release-1-6-0"></span><h4>Scrapy 1.6.0 (2019-01-30)<a class="headerlink" href="#scrapy-1-6-0-2019-01-30" title="Permalink to this headline">¶</a></h4>
<p>Highlights:</p>
<ul class="simple">
<li>better Windows support;</li>
<li>Python 3.7 compatibility;</li>
<li>big documentation improvements, including a switch
from <code class="docutils literal notranslate"><span class="pre">.extract_first()</span></code> + <code class="docutils literal notranslate"><span class="pre">.extract()</span></code> API to <code class="docutils literal notranslate"><span class="pre">.get()</span></code> + <code class="docutils literal notranslate"><span class="pre">.getall()</span></code>
API;</li>
<li>feed exports, FilePipeline and MediaPipeline improvements;</li>
<li>better extensibility: <a class="reference internal" href="index.html#std:signal-item_error"><code class="xref std std-signal docutils literal notranslate"><span class="pre">item_error</span></code></a> and
<a class="reference internal" href="index.html#std:signal-request_reached_downloader"><code class="xref std std-signal docutils literal notranslate"><span class="pre">request_reached_downloader</span></code></a> signals; <code class="docutils literal notranslate"><span class="pre">from_crawler</span></code> support
for feed exporters, feed storages and dupefilters.</li>
<li><code class="docutils literal notranslate"><span class="pre">scrapy.contracts</span></code> fixes and new features;</li>
<li>telnet console security improvements, first released as a
backport in <a class="reference internal" href="#release-1-5-2"><span class="std std-ref">Scrapy 1.5.2 (2019-01-22)</span></a>;</li>
<li>clean-up of the deprecated code;</li>
<li>various bug fixes, small new features and usability improvements across
the codebase.</li>
</ul>
<div class="section" id="selector-api-changes">
<h5>Selector API changes<a class="headerlink" href="#selector-api-changes" title="Permalink to this headline">¶</a></h5>
<p>While these are not changes in Scrapy itself, but rather in the <a class="reference external" href="https://github.com/scrapy/parsel">parsel</a>
library which Scrapy uses for xpath/css selectors, these changes are
worth mentioning here. Scrapy now depends on parsel &gt;= 1.5, and
Scrapy documentation is updated to follow recent <code class="docutils literal notranslate"><span class="pre">parsel</span></code> API conventions.</p>
<p>Most visible change is that <code class="docutils literal notranslate"><span class="pre">.get()</span></code> and <code class="docutils literal notranslate"><span class="pre">.getall()</span></code> selector
methods are now preferred over <code class="docutils literal notranslate"><span class="pre">.extract_first()</span></code> and <code class="docutils literal notranslate"><span class="pre">.extract()</span></code>.
We feel that these new methods result in a more concise and readable code.
See <a class="reference internal" href="index.html#old-extraction-api"><span class="std std-ref">extract() and extract_first()</span></a> for more details.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">There are currently <strong>no plans</strong> to deprecate <code class="docutils literal notranslate"><span class="pre">.extract()</span></code>
and <code class="docutils literal notranslate"><span class="pre">.extract_first()</span></code> methods.</p>
</div>
<p>Another useful new feature is the introduction of <code class="docutils literal notranslate"><span class="pre">Selector.attrib</span></code> and
<code class="docutils literal notranslate"><span class="pre">SelectorList.attrib</span></code> properties, which make it easier to get
attributes of HTML elements. See <a class="reference internal" href="index.html#selecting-attributes"><span class="std std-ref">Selecting element attributes</span></a>.</p>
<p>CSS selectors are cached in parsel &gt;= 1.5, which makes them faster
when the same CSS path is used many times. This is very common in
case of Scrapy spiders: callbacks are usually called several times,
on different pages.</p>
<p>If you’re using custom <code class="docutils literal notranslate"><span class="pre">Selector</span></code> or <code class="docutils literal notranslate"><span class="pre">SelectorList</span></code> subclasses,
a <strong>backward incompatible</strong> change in parsel may affect your code.
See <a class="reference external" href="https://parsel.readthedocs.io/en/latest/history.html">parsel changelog</a> for a detailed description, as well as for the
full list of improvements.</p>
</div>
<div class="section" id="telnet-console">
<h5>Telnet console<a class="headerlink" href="#telnet-console" title="Permalink to this headline">¶</a></h5>
<p><strong>Backward incompatible</strong>: Scrapy’s telnet console now requires username
and password. See <a class="reference internal" href="index.html#topics-telnetconsole"><span class="std std-ref">Telnet Console</span></a> for more details. This change
fixes a <strong>security issue</strong>; see <a class="reference internal" href="#release-1-5-2"><span class="std std-ref">Scrapy 1.5.2 (2019-01-22)</span></a> release notes for details.</p>
</div>
<div class="section" id="new-extensibility-features">
<h5>New extensibility features<a class="headerlink" href="#new-extensibility-features" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">from_crawler</span></code> support is added to feed exporters and feed storages. This,
among other things, allows to access Scrapy settings from custom feed
storages and exporters (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1605">issue 1605</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3348">issue 3348</a>).</li>
<li><code class="docutils literal notranslate"><span class="pre">from_crawler</span></code> support is added to dupefilters (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2956">issue 2956</a>); this allows
to access e.g. settings or a spider from a dupefilter.</li>
<li><a class="reference internal" href="index.html#std:signal-item_error"><code class="xref std std-signal docutils literal notranslate"><span class="pre">item_error</span></code></a> is fired when an error happens in a pipeline
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3256">issue 3256</a>);</li>
<li><a class="reference internal" href="index.html#std:signal-request_reached_downloader"><code class="xref std std-signal docutils literal notranslate"><span class="pre">request_reached_downloader</span></code></a> is fired when Downloader gets
a new Request; this signal can be useful e.g. for custom Schedulers
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3393">issue 3393</a>).</li>
<li>new SitemapSpider <a class="reference internal" href="index.html#scrapy.spiders.SitemapSpider.sitemap_filter" title="scrapy.spiders.SitemapSpider.sitemap_filter"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sitemap_filter()</span></code></a> method which allows
to select sitemap entries based on their attributes in SitemapSpider
subclasses (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3512">issue 3512</a>).</li>
<li>Lazy loading of Downloader Handlers is now optional; this enables better
initialization error handling in custom Downloader Handlers (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3394">issue 3394</a>).</li>
</ul>
</div>
<div class="section" id="new-filepipeline-and-mediapipeline-features">
<h5>New FilePipeline and MediaPipeline features<a class="headerlink" href="#new-filepipeline-and-mediapipeline-features" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Expose more options for S3FilesStore: <a class="reference internal" href="index.html#std:setting-AWS_ENDPOINT_URL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AWS_ENDPOINT_URL</span></code></a>,
<a class="reference internal" href="index.html#std:setting-AWS_USE_SSL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AWS_USE_SSL</span></code></a>, <a class="reference internal" href="index.html#std:setting-AWS_VERIFY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AWS_VERIFY</span></code></a>, <a class="reference internal" href="index.html#std:setting-AWS_REGION_NAME"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AWS_REGION_NAME</span></code></a>.
For example, this allows to use alternative or self-hosted
AWS-compatible providers (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2609">issue 2609</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3548">issue 3548</a>).</li>
<li>ACL support for Google Cloud Storage: <a class="reference internal" href="index.html#std:setting-FILES_STORE_GCS_ACL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FILES_STORE_GCS_ACL</span></code></a> and
<a class="reference internal" href="index.html#std:setting-IMAGES_STORE_GCS_ACL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_STORE_GCS_ACL</span></code></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3199">issue 3199</a>).</li>
</ul>
</div>
<div class="section" id="scrapy-contracts-improvements">
<h5><code class="docutils literal notranslate"><span class="pre">scrapy.contracts</span></code> improvements<a class="headerlink" href="#scrapy-contracts-improvements" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Exceptions in contracts code are handled better (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3377">issue 3377</a>);</li>
<li><code class="docutils literal notranslate"><span class="pre">dont_filter=True</span></code> is used for contract requests, which allows to test
different callbacks with the same URL (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3381">issue 3381</a>);</li>
<li><code class="docutils literal notranslate"><span class="pre">request_cls</span></code> attribute in Contract subclasses allow to use different
Request classes in contracts, for example FormRequest (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3383">issue 3383</a>).</li>
<li>Fixed errback handling in contracts, e.g. for cases where a contract
is executed for URL which returns non-200 response (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3371">issue 3371</a>).</li>
</ul>
</div>
<div class="section" id="usability-improvements">
<h5>Usability improvements<a class="headerlink" href="#usability-improvements" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>more stats for RobotsTxtMiddleware (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3100">issue 3100</a>)</li>
<li>INFO log level is used to show telnet host/port (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3115">issue 3115</a>)</li>
<li>a message is added to IgnoreRequest in RobotsTxtMiddleware (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3113">issue 3113</a>)</li>
<li>better validation of <code class="docutils literal notranslate"><span class="pre">url</span></code> argument in <code class="docutils literal notranslate"><span class="pre">Response.follow</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3131">issue 3131</a>)</li>
<li>non-zero exit code is returned from Scrapy commands when error happens
on spider inititalization (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3226">issue 3226</a>)</li>
<li>Link extraction improvements: “ftp” is added to scheme list (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3152">issue 3152</a>);
“flv” is added to common video extensions (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3165">issue 3165</a>)</li>
<li>better error message when an exporter is disabled (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3358">issue 3358</a>);</li>
<li><code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">shell</span> <span class="pre">--help</span></code> mentions syntax required for local files
(<code class="docutils literal notranslate"><span class="pre">./file.html</span></code>) - <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3496">issue 3496</a>.</li>
<li>Referer header value is added to RFPDupeFilter log messages (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3588">issue 3588</a>)</li>
</ul>
</div>
<div class="section" id="id2">
<h5>Bug fixes<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>fixed issue with extra blank lines in .csv exports under Windows
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3039">issue 3039</a>);</li>
<li>proper handling of pickling errors in Python 3 when serializing objects
for disk queues (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3082">issue 3082</a>)</li>
<li>flags are now preserved when copying Requests (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3342">issue 3342</a>);</li>
<li>FormRequest.from_response clickdata shouldn’t ignore elements with
<code class="docutils literal notranslate"><span class="pre">input[type=image]</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3153">issue 3153</a>).</li>
<li>FormRequest.from_response should preserve duplicate keys (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3247">issue 3247</a>)</li>
</ul>
</div>
<div class="section" id="documentation-improvements">
<h5>Documentation improvements<a class="headerlink" href="#documentation-improvements" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Docs are re-written to suggest .get/.getall API instead of
.extract/.extract_first. Also, <a class="reference internal" href="index.html#topics-selectors"><span class="std std-ref">Selectors</span></a> docs are updated
and re-structured to match latest parsel docs; they now contain more topics,
such as <a class="reference internal" href="index.html#selecting-attributes"><span class="std std-ref">Selecting element attributes</span></a> or <a class="reference internal" href="index.html#topics-selectors-css-extensions"><span class="std std-ref">Extensions to CSS Selectors</span></a>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3390">issue 3390</a>).</li>
<li><a class="reference internal" href="index.html#topics-developer-tools"><span class="std std-ref">Using your browser’s Developer Tools for scraping</span></a> is a new tutorial which replaces
old Firefox and Firebug tutorials (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3400">issue 3400</a>).</li>
<li>SCRAPY_PROJECT environment variable is documented (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3518">issue 3518</a>);</li>
<li>troubleshooting section is added to install instructions (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3517">issue 3517</a>);</li>
<li>improved links to beginner resources in the tutorial
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3367">issue 3367</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3468">issue 3468</a>);</li>
<li>fixed <a class="reference internal" href="index.html#std:setting-RETRY_HTTP_CODES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_HTTP_CODES</span></code></a> default values in docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3335">issue 3335</a>);</li>
<li>remove unused <code class="docutils literal notranslate"><span class="pre">DEPTH_STATS</span></code> option from docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3245">issue 3245</a>);</li>
<li>other cleanups (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3347">issue 3347</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3350">issue 3350</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3445">issue 3445</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3544">issue 3544</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3605">issue 3605</a>).</li>
</ul>
</div>
<div class="section" id="id3">
<h5>Deprecation removals<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h5>
<p>Compatibility shims for pre-1.0 Scrapy module names are removed
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3318">issue 3318</a>):</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">scrapy.command</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">scrapy.contrib</span></code> (with all submodules)</li>
<li><code class="docutils literal notranslate"><span class="pre">scrapy.contrib_exp</span></code> (with all submodules)</li>
<li><code class="docutils literal notranslate"><span class="pre">scrapy.dupefilter</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">scrapy.linkextractor</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">scrapy.project</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">scrapy.spider</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">scrapy.spidermanager</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">scrapy.squeue</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">scrapy.stats</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">scrapy.statscol</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">scrapy.utils.decorator</span></code></li>
</ul>
<p>See <a class="reference internal" href="#module-relocations"><span class="std std-ref">Module Relocations</span></a> for more information, or use suggestions
from Scrapy 1.5.x deprecation warnings to update your code.</p>
<p>Other deprecation removals:</p>
<ul class="simple">
<li>Deprecated scrapy.interfaces.ISpiderManager is removed; please use
scrapy.interfaces.ISpiderLoader.</li>
<li>Deprecated <code class="docutils literal notranslate"><span class="pre">CrawlerSettings</span></code> class is removed (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3327">issue 3327</a>).</li>
<li>Deprecated <code class="docutils literal notranslate"><span class="pre">Settings.overrides</span></code> and <code class="docutils literal notranslate"><span class="pre">Settings.defaults</span></code> attributes
are removed (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3327">issue 3327</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3359">issue 3359</a>).</li>
</ul>
</div>
<div class="section" id="other-improvements-cleanups">
<h5>Other improvements, cleanups<a class="headerlink" href="#other-improvements-cleanups" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>All Scrapy tests now pass on Windows; Scrapy testing suite is executed
in a Windows environment on CI (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3315">issue 3315</a>).</li>
<li>Python 3.7 support (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3326">issue 3326</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3150">issue 3150</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3547">issue 3547</a>).</li>
<li>Testing and CI fixes (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3526">issue 3526</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3538">issue 3538</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3308">issue 3308</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3311">issue 3311</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3309">issue 3309</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3305">issue 3305</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3210">issue 3210</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3299">issue 3299</a>)</li>
<li><code class="docutils literal notranslate"><span class="pre">scrapy.http.cookies.CookieJar.clear</span></code> accepts “domain”, “path” and “name”
optional arguments (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3231">issue 3231</a>).</li>
<li>additional files are included to sdist (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3495">issue 3495</a>);</li>
<li>code style fixes (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3405">issue 3405</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3304">issue 3304</a>);</li>
<li>unneeded .strip() call is removed (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3519">issue 3519</a>);</li>
<li>collections.deque is used to store MiddlewareManager methods instead
of a list (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3476">issue 3476</a>)</li>
</ul>
</div>
</div>
<div class="section" id="scrapy-1-5-2-2019-01-22">
<span id="release-1-5-2"></span><h4>Scrapy 1.5.2 (2019-01-22)<a class="headerlink" href="#scrapy-1-5-2-2019-01-22" title="Permalink to this headline">¶</a></h4>
<ul>
<li><p class="first"><em>Security bugfix</em>: Telnet console extension can be easily exploited by rogue
websites POSTing content to <a class="reference external" href="http://localhost:6023">http://localhost:6023</a>, we haven’t found a way to
exploit it from Scrapy, but it is very easy to trick a browser to do so and
elevates the risk for local development environment.</p>
<p><em>The fix is backward incompatible</em>, it enables telnet user-password
authentication by default with a random generated password. If you can’t
upgrade right away, please consider setting <code class="xref std std-setting docutils literal notranslate"><span class="pre">TELNET_CONSOLE_PORT</span></code>
out of its default value.</p>
<p>See <a class="reference internal" href="index.html#topics-telnetconsole"><span class="std std-ref">telnet console</span></a> documentation for more info</p>
</li>
<li><p class="first">Backport CI build failure under GCE environemnt due to boto import error.</p>
</li>
</ul>
</div>
<div class="section" id="scrapy-1-5-1-2018-07-12">
<span id="release-1-5-1"></span><h4>Scrapy 1.5.1 (2018-07-12)<a class="headerlink" href="#scrapy-1-5-1-2018-07-12" title="Permalink to this headline">¶</a></h4>
<p>This is a maintenance release with important bug fixes, but no new features:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">O(N^2)</span></code> gzip decompression issue which affected Python 3 and PyPy
is fixed (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3281">issue 3281</a>);</li>
<li>skipping of TLS validation errors is improved (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3166">issue 3166</a>);</li>
<li>Ctrl-C handling is fixed in Python 3.5+ (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3096">issue 3096</a>);</li>
<li>testing fixes (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3092">issue 3092</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3263">issue 3263</a>);</li>
<li>documentation improvements (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3058">issue 3058</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3059">issue 3059</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3089">issue 3089</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3123">issue 3123</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3127">issue 3127</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3189">issue 3189</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3224">issue 3224</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3280">issue 3280</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3279">issue 3279</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3201">issue 3201</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3260">issue 3260</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3284">issue 3284</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3298">issue 3298</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3294">issue 3294</a>).</li>
</ul>
</div>
<div class="section" id="scrapy-1-5-0-2017-12-29">
<span id="release-1-5-0"></span><h4>Scrapy 1.5.0 (2017-12-29)<a class="headerlink" href="#scrapy-1-5-0-2017-12-29" title="Permalink to this headline">¶</a></h4>
<p>This release brings small new features and improvements across the codebase.
Some highlights:</p>
<ul class="simple">
<li>Google Cloud Storage is supported in FilesPipeline and ImagesPipeline.</li>
<li>Crawling with proxy servers becomes more efficient, as connections
to proxies can be reused now.</li>
<li>Warnings, exception and logging messages are improved to make debugging
easier.</li>
<li><code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">parse</span></code> command now allows to set custom request meta via
<code class="docutils literal notranslate"><span class="pre">--meta</span></code> argument.</li>
<li>Compatibility with Python 3.6, PyPy and PyPy3 is improved;
PyPy and PyPy3 are now supported officially, by running tests on CI.</li>
<li>Better default handling of HTTP 308, 522 and 524 status codes.</li>
<li>Documentation is improved, as usual.</li>
</ul>
<div class="section" id="id4">
<h5>Backward Incompatible Changes<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Scrapy 1.5 drops support for Python 3.3.</li>
<li>Default Scrapy User-Agent now uses https link to scrapy.org (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2983">issue 2983</a>).
<strong>This is technically backward-incompatible</strong>; override
<a class="reference internal" href="index.html#std:setting-USER_AGENT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">USER_AGENT</span></code></a> if you relied on old value.</li>
<li>Logging of settings overridden by <code class="docutils literal notranslate"><span class="pre">custom_settings</span></code> is fixed;
<strong>this is technically backward-incompatible</strong> because the logger
changes from <code class="docutils literal notranslate"><span class="pre">[scrapy.utils.log]</span></code> to <code class="docutils literal notranslate"><span class="pre">[scrapy.crawler]</span></code>. If you’re
parsing Scrapy logs, please update your log parsers (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1343">issue 1343</a>).</li>
<li>LinkExtractor now ignores <code class="docutils literal notranslate"><span class="pre">m4v</span></code> extension by default, this is change
in behavior.</li>
<li>522 and 524 status codes are added to <code class="docutils literal notranslate"><span class="pre">RETRY_HTTP_CODES</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2851">issue 2851</a>)</li>
</ul>
</div>
<div class="section" id="id5">
<h5>New features<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Support <code class="docutils literal notranslate"><span class="pre">&lt;link&gt;</span></code> tags in <code class="docutils literal notranslate"><span class="pre">Response.follow</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2785">issue 2785</a>)</li>
<li>Support for <code class="docutils literal notranslate"><span class="pre">ptpython</span></code> REPL (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2654">issue 2654</a>)</li>
<li>Google Cloud Storage support for FilesPipeline and ImagesPipeline
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2923">issue 2923</a>).</li>
<li>New <code class="docutils literal notranslate"><span class="pre">--meta</span></code> option of the “scrapy parse” command allows to pass additional
request.meta (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2883">issue 2883</a>)</li>
<li>Populate spider variable when using <code class="docutils literal notranslate"><span class="pre">shell.inspect_response</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2812">issue 2812</a>)</li>
<li>Handle HTTP 308 Permanent Redirect (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2844">issue 2844</a>)</li>
<li>Add 522 and 524 to <code class="docutils literal notranslate"><span class="pre">RETRY_HTTP_CODES</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2851">issue 2851</a>)</li>
<li>Log versions information at startup (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2857">issue 2857</a>)</li>
<li><code class="docutils literal notranslate"><span class="pre">scrapy.mail.MailSender</span></code> now works in Python 3 (it requires Twisted 17.9.0)</li>
<li>Connections to proxy servers are reused (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2743">issue 2743</a>)</li>
<li>Add template for a downloader middleware (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2755">issue 2755</a>)</li>
<li>Explicit message for NotImplementedError when parse callback not defined
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2831">issue 2831</a>)</li>
<li>CrawlerProcess got an option to disable installation of root log handler
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2921">issue 2921</a>)</li>
<li>LinkExtractor now ignores <code class="docutils literal notranslate"><span class="pre">m4v</span></code> extension by default</li>
<li>Better log messages for responses over <a class="reference internal" href="index.html#std:setting-DOWNLOAD_WARNSIZE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_WARNSIZE</span></code></a> and
<a class="reference internal" href="index.html#std:setting-DOWNLOAD_MAXSIZE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_MAXSIZE</span></code></a> limits (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2927">issue 2927</a>)</li>
<li>Show warning when a URL is put to <code class="docutils literal notranslate"><span class="pre">Spider.allowed_domains</span></code> instead of
a domain (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2250">issue 2250</a>).</li>
</ul>
</div>
<div class="section" id="id6">
<h5>Bug fixes<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Fix logging of settings overridden by <code class="docutils literal notranslate"><span class="pre">custom_settings</span></code>;
<strong>this is technically backward-incompatible</strong> because the logger
changes from <code class="docutils literal notranslate"><span class="pre">[scrapy.utils.log]</span></code> to <code class="docutils literal notranslate"><span class="pre">[scrapy.crawler]</span></code>, so please
update your log parsers if needed (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1343">issue 1343</a>)</li>
<li>Default Scrapy User-Agent now uses https link to scrapy.org (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2983">issue 2983</a>).
<strong>This is technically backward-incompatible</strong>; override
<a class="reference internal" href="index.html#std:setting-USER_AGENT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">USER_AGENT</span></code></a> if you relied on old value.</li>
<li>Fix PyPy and PyPy3 test failures, support them officially
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2793">issue 2793</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2935">issue 2935</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2990">issue 2990</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/3050">issue 3050</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2213">issue 2213</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3048">issue 3048</a>)</li>
<li>Fix DNS resolver when <code class="docutils literal notranslate"><span class="pre">DNSCACHE_ENABLED=False</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2811">issue 2811</a>)</li>
<li>Add <code class="docutils literal notranslate"><span class="pre">cryptography</span></code> for Debian Jessie tox test env (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2848">issue 2848</a>)</li>
<li>Add verification to check if Request callback is callable (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2766">issue 2766</a>)</li>
<li>Port <code class="docutils literal notranslate"><span class="pre">extras/qpsclient.py</span></code> to Python 3 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2849">issue 2849</a>)</li>
<li>Use getfullargspec under the scenes for Python 3 to stop DeprecationWarning
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2862">issue 2862</a>)</li>
<li>Update deprecated test aliases (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2876">issue 2876</a>)</li>
<li>Fix <code class="docutils literal notranslate"><span class="pre">SitemapSpider</span></code> support for alternate links (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2853">issue 2853</a>)</li>
</ul>
</div>
<div class="section" id="docs">
<h5>Docs<a class="headerlink" href="#docs" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Added missing bullet point for the <code class="docutils literal notranslate"><span class="pre">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code>
setting. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2756">issue 2756</a>)</li>
<li>Update Contributing docs, document new support channels
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2762">issue 2762</a>, issue:<cite>3038</cite>)</li>
<li>Include references to Scrapy subreddit in the docs</li>
<li>Fix broken links; use <a class="reference external" href="https://">https://</a> for external links
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2978">issue 2978</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2982">issue 2982</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2958">issue 2958</a>)</li>
<li>Document CloseSpider extension better (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2759">issue 2759</a>)</li>
<li>Use <code class="docutils literal notranslate"><span class="pre">pymongo.collection.Collection.insert_one()</span></code> in MongoDB example
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2781">issue 2781</a>)</li>
<li>Spelling mistake and typos
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2828">issue 2828</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2837">issue 2837</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2884">issue 2884</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2924">issue 2924</a>)</li>
<li>Clarify <code class="docutils literal notranslate"><span class="pre">CSVFeedSpider.headers</span></code> documentation (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2826">issue 2826</a>)</li>
<li>Document <code class="docutils literal notranslate"><span class="pre">DontCloseSpider</span></code> exception and clarify <code class="docutils literal notranslate"><span class="pre">spider_idle</span></code>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2791">issue 2791</a>)</li>
<li>Update “Releases” section in README (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2764">issue 2764</a>)</li>
<li>Fix rst syntax in <code class="docutils literal notranslate"><span class="pre">DOWNLOAD_FAIL_ON_DATALOSS</span></code> docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2763">issue 2763</a>)</li>
<li>Small fix in description of startproject arguments (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2866">issue 2866</a>)</li>
<li>Clarify data types in Response.body docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2922">issue 2922</a>)</li>
<li>Add a note about <code class="docutils literal notranslate"><span class="pre">request.meta['depth']</span></code> to DepthMiddleware docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2374">issue 2374</a>)</li>
<li>Add a note about <code class="docutils literal notranslate"><span class="pre">request.meta['dont_merge_cookies']</span></code> to CookiesMiddleware
docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2999">issue 2999</a>)</li>
<li>Up-to-date example of project structure (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2964">issue 2964</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2976">issue 2976</a>)</li>
<li>A better example of ItemExporters usage (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2989">issue 2989</a>)</li>
<li>Document <code class="docutils literal notranslate"><span class="pre">from_crawler</span></code> methods for spider and downloader middlewares
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/3019">issue 3019</a>)</li>
</ul>
</div>
</div>
<div class="section" id="scrapy-1-4-0-2017-05-18">
<span id="release-1-4-0"></span><h4>Scrapy 1.4.0 (2017-05-18)<a class="headerlink" href="#scrapy-1-4-0-2017-05-18" title="Permalink to this headline">¶</a></h4>
<p>Scrapy 1.4 does not bring that many breathtaking new features
but quite a few handy improvements nonetheless.</p>
<p>Scrapy now supports anonymous FTP sessions with customizable user and
password via the new <a class="reference internal" href="index.html#std:setting-FTP_USER"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FTP_USER</span></code></a> and <a class="reference internal" href="index.html#std:setting-FTP_PASSWORD"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FTP_PASSWORD</span></code></a> settings.
And if you’re using Twisted version 17.1.0 or above, FTP is now available
with Python 3.</p>
<p>There’s a new <a class="reference internal" href="index.html#scrapy.http.TextResponse.follow" title="scrapy.http.TextResponse.follow"><code class="xref py py-meth docutils literal notranslate"><span class="pre">response.follow</span></code></a> method
for creating requests; <strong>it is now a recommended way to create Requests
in Scrapy spiders</strong>. This method makes it easier to write correct
spiders; <code class="docutils literal notranslate"><span class="pre">response.follow</span></code> has several advantages over creating
<code class="docutils literal notranslate"><span class="pre">scrapy.Request</span></code> objects directly:</p>
<ul class="simple">
<li>it handles relative URLs;</li>
<li>it works properly with non-ascii URLs on non-UTF8 pages;</li>
<li>in addition to absolute and relative URLs it supports Selectors;
for <code class="docutils literal notranslate"><span class="pre">&lt;a&gt;</span></code> elements it can also extract their href values.</li>
</ul>
<p>For example, instead of this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">href</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;li.page a::attr(href)&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">():</span>
    <span class="n">url</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">urljoin</span><span class="p">(</span><span class="n">href</span><span class="p">)</span>
    <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="n">response</span><span class="o">.</span><span class="n">encoding</span><span class="p">)</span>
</pre></div>
</div>
<p>One can now write this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">response</span><span class="o">.</span><span class="n">css</span><span class="p">(</span><span class="s1">&#39;li.page a&#39;</span><span class="p">):</span>
    <span class="k">yield</span> <span class="n">response</span><span class="o">.</span><span class="n">follow</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">parse</span><span class="p">)</span>
</pre></div>
</div>
<p>Link extractors are also improved. They work similarly to what a regular
modern browser would do: leading and trailing whitespace are removed
from attributes (think <code class="docutils literal notranslate"><span class="pre">href=&quot;</span>&#160;&#160; <span class="pre">http://example.com&quot;</span></code>) when building
<code class="docutils literal notranslate"><span class="pre">Link</span></code> objects. This whitespace-stripping also happens for <code class="docutils literal notranslate"><span class="pre">action</span></code>
attributes with <code class="docutils literal notranslate"><span class="pre">FormRequest</span></code>.</p>
<p><strong>Please also note that link extractors do not canonicalize URLs by default
anymore.</strong> This was puzzling users every now and then, and it’s not what
browsers do in fact, so we removed that extra transformation on extracted
links.</p>
<p>For those of you wanting more control on the <code class="docutils literal notranslate"><span class="pre">Referer:</span></code> header that Scrapy
sends when following links, you can set your own <code class="docutils literal notranslate"><span class="pre">Referrer</span> <span class="pre">Policy</span></code>.
Prior to Scrapy 1.4, the default <code class="docutils literal notranslate"><span class="pre">RefererMiddleware</span></code> would simply and
blindly set it to the URL of the response that generated the HTTP request
(which could leak information on your URL seeds).
By default, Scrapy now behaves much like your regular browser does.
And this policy is fully customizable with W3C standard values
(or with something really custom of your own if you wish).
See <a class="reference internal" href="index.html#std:setting-REFERRER_POLICY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">REFERRER_POLICY</span></code></a> for details.</p>
<p>To make Scrapy spiders easier to debug, Scrapy logs more stats by default
in 1.4: memory usage stats, detailed retry stats, detailed HTTP error code
stats. A similar change is that HTTP cache path is also visible in logs now.</p>
<p>Last but not least, Scrapy now has the option to make JSON and XML items
more human-readable, with newlines between items and even custom indenting
offset, using the new <a class="reference internal" href="index.html#std:setting-FEED_EXPORT_INDENT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_EXPORT_INDENT</span></code></a> setting.</p>
<p>Enjoy! (Or read on for the rest of changes in this release.)</p>
<div class="section" id="deprecations-and-backward-incompatible-changes">
<h5>Deprecations and Backward Incompatible Changes<a class="headerlink" href="#deprecations-and-backward-incompatible-changes" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Default to <code class="docutils literal notranslate"><span class="pre">canonicalize=False</span></code> in <code class="xref py py-class docutils literal notranslate"><span class="pre">scrapy.linkextractors.LinkExtractor</span></code>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2537">issue 2537</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1941">issue 1941</a> and <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1982">issue 1982</a>):
<strong>warning, this is technically backward-incompatible</strong></li>
<li>Enable memusage extension by default (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2539">issue 2539</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2187">issue 2187</a>);
<strong>this is technically backward-incompatible</strong> so please check if you have
any non-default <code class="docutils literal notranslate"><span class="pre">MEMUSAGE_***</span></code> options set.</li>
<li><code class="docutils literal notranslate"><span class="pre">EDITOR</span></code> environment variable now takes precedence over <code class="docutils literal notranslate"><span class="pre">EDITOR</span></code>
option defined in settings.py (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1829">issue 1829</a>); Scrapy default settings
no longer depend on environment variables. <strong>This is technically a backward
incompatible change</strong>.</li>
<li><code class="docutils literal notranslate"><span class="pre">Spider.make_requests_from_url</span></code> is deprecated
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1728">issue 1728</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1495">issue 1495</a>).</li>
</ul>
</div>
<div class="section" id="id7">
<h5>New Features<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Accept proxy credentials in <a class="reference internal" href="index.html#std:reqmeta-proxy"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">proxy</span></code></a> request meta key (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2526">issue 2526</a>)</li>
<li>Support <a class="reference external" href="https://github.com/google/brotli">brotli</a>-compressed content; requires optional <a class="reference external" href="https://github.com/python-hyper/brotlipy/">brotlipy</a>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2535">issue 2535</a>)</li>
<li>New <a class="reference internal" href="index.html#response-follow-example"><span class="std std-ref">response.follow</span></a> shortcut
for creating requests (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1940">issue 1940</a>)</li>
<li>Added <code class="docutils literal notranslate"><span class="pre">flags</span></code> argument and attribute to <a class="reference internal" href="index.html#scrapy.http.Request" title="scrapy.http.Request"><code class="xref py py-class docutils literal notranslate"><span class="pre">Request</span></code></a>
objects (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2047">issue 2047</a>)</li>
<li>Support Anonymous FTP (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2342">issue 2342</a>)</li>
<li>Added <code class="docutils literal notranslate"><span class="pre">retry/count</span></code>, <code class="docutils literal notranslate"><span class="pre">retry/max_reached</span></code> and <code class="docutils literal notranslate"><span class="pre">retry/reason_count/&lt;reason&gt;</span></code>
stats to <a class="reference internal" href="index.html#scrapy.downloadermiddlewares.retry.RetryMiddleware" title="scrapy.downloadermiddlewares.retry.RetryMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">RetryMiddleware</span></code></a>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2543">issue 2543</a>)</li>
<li>Added <code class="docutils literal notranslate"><span class="pre">httperror/response_ignored_count</span></code> and <code class="docutils literal notranslate"><span class="pre">httperror/response_ignored_status_count/&lt;status&gt;</span></code>
stats to <a class="reference internal" href="index.html#scrapy.spidermiddlewares.httperror.HttpErrorMiddleware" title="scrapy.spidermiddlewares.httperror.HttpErrorMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">HttpErrorMiddleware</span></code></a>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2566">issue 2566</a>)</li>
<li>Customizable <a class="reference internal" href="index.html#std:setting-REFERRER_POLICY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">Referrer</span> <span class="pre">policy</span></code></a> in
<a class="reference internal" href="index.html#scrapy.spidermiddlewares.referer.RefererMiddleware" title="scrapy.spidermiddlewares.referer.RefererMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">RefererMiddleware</span></code></a>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2306">issue 2306</a>)</li>
<li>New <code class="docutils literal notranslate"><span class="pre">data:</span></code> URI download handler (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2334">issue 2334</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2156">issue 2156</a>)</li>
<li>Log cache directory when HTTP Cache is used (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2611">issue 2611</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2604">issue 2604</a>)</li>
<li>Warn users when project contains duplicate spider names (fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2181">issue 2181</a>)</li>
<li><code class="xref py py-class docutils literal notranslate"><span class="pre">CaselessDict</span></code> now accepts <code class="docutils literal notranslate"><span class="pre">Mapping</span></code> instances and not only dicts (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2646">issue 2646</a>)</li>
<li><a class="reference internal" href="index.html#topics-media-pipeline"><span class="std std-ref">Media downloads</span></a>, with <code class="xref py py-class docutils literal notranslate"><span class="pre">FilesPipelines</span></code>
or <code class="xref py py-class docutils literal notranslate"><span class="pre">ImagesPipelines</span></code>, can now optionally handle HTTP redirects
using the new <a class="reference internal" href="index.html#std:setting-MEDIA_ALLOW_REDIRECTS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">MEDIA_ALLOW_REDIRECTS</span></code></a> setting (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2616">issue 2616</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2004">issue 2004</a>)</li>
<li>Accept non-complete responses from websites using a new
<a class="reference internal" href="index.html#std:setting-DOWNLOAD_FAIL_ON_DATALOSS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_FAIL_ON_DATALOSS</span></code></a> setting (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2590">issue 2590</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2586">issue 2586</a>)</li>
<li>Optional pretty-printing of JSON and XML items via
<a class="reference internal" href="index.html#std:setting-FEED_EXPORT_INDENT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_EXPORT_INDENT</span></code></a> setting (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2456">issue 2456</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1327">issue 1327</a>)</li>
<li>Allow dropping fields in <code class="docutils literal notranslate"><span class="pre">FormRequest.from_response</span></code> formdata when
<code class="docutils literal notranslate"><span class="pre">None</span></code> value is passed (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/667">issue 667</a>)</li>
<li>Per-request retry times with the new <a class="reference internal" href="index.html#std:reqmeta-max_retry_times"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">max_retry_times</span></code></a> meta key
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2642">issue 2642</a>)</li>
<li><code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">-m</span> <span class="pre">scrapy</span></code> as a more explicit alternative to <code class="docutils literal notranslate"><span class="pre">scrapy</span></code> command
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2740">issue 2740</a>)</li>
</ul>
</div>
<div class="section" id="id8">
<h5>Bug fixes<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>LinkExtractor now strips leading and trailing whitespaces from attributes
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2547">issue 2547</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1614">issue 1614</a>)</li>
<li>Properly handle whitespaces in action attribute in <code class="xref py py-class docutils literal notranslate"><span class="pre">FormRequest</span></code>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2548">issue 2548</a>)</li>
<li>Buffer CONNECT response bytes from proxy until all HTTP headers are received
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2495">issue 2495</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2491">issue 2491</a>)</li>
<li>FTP downloader now works on Python 3, provided you use Twisted&gt;=17.1
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2599">issue 2599</a>)</li>
<li>Use body to choose response type after decompressing content (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2393">issue 2393</a>,
fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2145">issue 2145</a>)</li>
<li>Always decompress <code class="docutils literal notranslate"><span class="pre">Content-Encoding:</span> <span class="pre">gzip</span></code> at <a class="reference internal" href="index.html#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware" title="scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware"><code class="xref py py-class docutils literal notranslate"><span class="pre">HttpCompressionMiddleware</span></code></a> stage (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2391">issue 2391</a>)</li>
<li>Respect custom log level in <code class="docutils literal notranslate"><span class="pre">Spider.custom_settings</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2581">issue 2581</a>,
fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1612">issue 1612</a>)</li>
<li>‘make htmlview’ fix for macOS (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2661">issue 2661</a>)</li>
<li>Remove “commands” from the command list  (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2695">issue 2695</a>)</li>
<li>Fix duplicate Content-Length header for POST requests with empty body (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2677">issue 2677</a>)</li>
<li>Properly cancel large downloads, i.e. above <a class="reference internal" href="index.html#std:setting-DOWNLOAD_MAXSIZE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_MAXSIZE</span></code></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1616">issue 1616</a>)</li>
<li>ImagesPipeline: fixed processing of transparent PNG images with palette
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2675">issue 2675</a>)</li>
</ul>
</div>
<div class="section" id="cleanups-refactoring">
<h5>Cleanups &amp; Refactoring<a class="headerlink" href="#cleanups-refactoring" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Tests: remove temp files and folders (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2570">issue 2570</a>),
fixed ProjectUtilsTest on OS X (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2569">issue 2569</a>),
use portable pypy for Linux on Travis CI (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2710">issue 2710</a>)</li>
<li>Separate building request from <code class="docutils literal notranslate"><span class="pre">_requests_to_follow</span></code> in CrawlSpider (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2562">issue 2562</a>)</li>
<li>Remove “Python 3 progress” badge (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2567">issue 2567</a>)</li>
<li>Add a couple more lines to <code class="docutils literal notranslate"><span class="pre">.gitignore</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2557">issue 2557</a>)</li>
<li>Remove bumpversion prerelease configuration (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2159">issue 2159</a>)</li>
<li>Add codecov.yml file (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2750">issue 2750</a>)</li>
<li>Set context factory implementation based on Twisted version (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2577">issue 2577</a>,
fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2560">issue 2560</a>)</li>
<li>Add omitted <code class="docutils literal notranslate"><span class="pre">self</span></code> arguments in default project middleware template (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2595">issue 2595</a>)</li>
<li>Remove redundant <code class="docutils literal notranslate"><span class="pre">slot.add_request()</span></code> call in ExecutionEngine (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2617">issue 2617</a>)</li>
<li>Catch more specific <code class="docutils literal notranslate"><span class="pre">os.error</span></code> exception in <code class="xref py py-class docutils literal notranslate"><span class="pre">FSFilesStore</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2644">issue 2644</a>)</li>
<li>Change “localhost” test server certificate (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2720">issue 2720</a>)</li>
<li>Remove unused <code class="docutils literal notranslate"><span class="pre">MEMUSAGE_REPORT</span></code> setting (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2576">issue 2576</a>)</li>
</ul>
</div>
<div class="section" id="id9">
<h5>Documentation<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Binary mode is required for exporters (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2564">issue 2564</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2553">issue 2553</a>)</li>
<li>Mention issue with <a class="reference internal" href="index.html#scrapy.http.FormRequest.from_response" title="scrapy.http.FormRequest.from_response"><code class="xref py py-meth docutils literal notranslate"><span class="pre">FormRequest.from_response</span></code></a> due to bug in lxml (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2572">issue 2572</a>)</li>
<li>Use single quotes uniformly in templates (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2596">issue 2596</a>)</li>
<li>Document <code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">ftp_user</span></code> and <code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">ftp_password</span></code> meta keys (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2587">issue 2587</a>)</li>
<li>Removed section on deprecated <code class="docutils literal notranslate"><span class="pre">contrib/</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2636">issue 2636</a>)</li>
<li>Recommend Anaconda when installing Scrapy on Windows
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2477">issue 2477</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2475">issue 2475</a>)</li>
<li>FAQ: rewrite note on Python 3 support on Windows (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2690">issue 2690</a>)</li>
<li>Rearrange selector sections (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2705">issue 2705</a>)</li>
<li>Remove <code class="docutils literal notranslate"><span class="pre">__nonzero__</span></code> from <code class="xref py py-class docutils literal notranslate"><span class="pre">SelectorList</span></code> docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2683">issue 2683</a>)</li>
<li>Mention how to disable request filtering in documentation of
<a class="reference internal" href="index.html#std:setting-DUPEFILTER_CLASS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DUPEFILTER_CLASS</span></code></a> setting (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2714">issue 2714</a>)</li>
<li>Add sphinx_rtd_theme to docs setup readme (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2668">issue 2668</a>)</li>
<li>Open file in text mode in JSON item writer example (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2729">issue 2729</a>)</li>
<li>Clarify <code class="docutils literal notranslate"><span class="pre">allowed_domains</span></code> example (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2670">issue 2670</a>)</li>
</ul>
</div>
</div>
<div class="section" id="scrapy-1-3-3-2017-03-10">
<span id="release-1-3-3"></span><h4>Scrapy 1.3.3 (2017-03-10)<a class="headerlink" href="#scrapy-1-3-3-2017-03-10" title="Permalink to this headline">¶</a></h4>
<div class="section" id="id10">
<h5>Bug fixes<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Make <code class="docutils literal notranslate"><span class="pre">SpiderLoader</span></code> raise <code class="docutils literal notranslate"><span class="pre">ImportError</span></code> again by default for missing
dependencies and wrong <a class="reference internal" href="index.html#std:setting-SPIDER_MODULES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_MODULES</span></code></a>.
These exceptions were silenced as warnings since 1.3.0.
A new setting is introduced to toggle between warning or exception if needed ;
see <a class="reference internal" href="index.html#std:setting-SPIDER_LOADER_WARN_ONLY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SPIDER_LOADER_WARN_ONLY</span></code></a> for details.</li>
</ul>
</div>
</div>
<div class="section" id="scrapy-1-3-2-2017-02-13">
<span id="release-1-3-2"></span><h4>Scrapy 1.3.2 (2017-02-13)<a class="headerlink" href="#scrapy-1-3-2-2017-02-13" title="Permalink to this headline">¶</a></h4>
<div class="section" id="id11">
<h5>Bug fixes<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Preserve request class when converting to/from dicts (utils.reqser) (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2510">issue 2510</a>).</li>
<li>Use consistent selectors for author field in tutorial (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2551">issue 2551</a>).</li>
<li>Fix TLS compatibility in Twisted 17+ (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2558">issue 2558</a>)</li>
</ul>
</div>
</div>
<div class="section" id="scrapy-1-3-1-2017-02-08">
<span id="release-1-3-1"></span><h4>Scrapy 1.3.1 (2017-02-08)<a class="headerlink" href="#scrapy-1-3-1-2017-02-08" title="Permalink to this headline">¶</a></h4>
<div class="section" id="id12">
<h5>New features<a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Support <code class="docutils literal notranslate"><span class="pre">'True'</span></code> and <code class="docutils literal notranslate"><span class="pre">'False'</span></code> string values for boolean settings (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2519">issue 2519</a>);
you can now do something like <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">crawl</span> <span class="pre">myspider</span> <span class="pre">-s</span> <span class="pre">REDIRECT_ENABLED=False</span></code>.</li>
<li>Support kwargs with <code class="docutils literal notranslate"><span class="pre">response.xpath()</span></code> to use <a class="reference internal" href="index.html#topics-selectors-xpath-variables"><span class="std std-ref">XPath variables</span></a>
and ad-hoc namespaces declarations ;
this requires at least Parsel v1.1 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2457">issue 2457</a>).</li>
<li>Add support for Python 3.6 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2485">issue 2485</a>).</li>
<li>Run tests on PyPy (warning: some tests still fail, so PyPy is not supported yet).</li>
</ul>
</div>
<div class="section" id="id13">
<h5>Bug fixes<a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Enforce <code class="docutils literal notranslate"><span class="pre">DNS_TIMEOUT</span></code> setting (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2496">issue 2496</a>).</li>
<li>Fix <a class="reference internal" href="index.html#std:command-view"><code class="xref std std-command docutils literal notranslate"><span class="pre">view</span></code></a> command ; it was a regression in v1.3.0 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2503">issue 2503</a>).</li>
<li>Fix tests regarding <code class="docutils literal notranslate"><span class="pre">*_EXPIRES</span> <span class="pre">settings</span></code> with Files/Images pipelines (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2460">issue 2460</a>).</li>
<li>Fix name of generated pipeline class when using basic project template (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2466">issue 2466</a>).</li>
<li>Fix compatiblity with Twisted 17+ (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2496">issue 2496</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2528">issue 2528</a>).</li>
<li>Fix <code class="docutils literal notranslate"><span class="pre">scrapy.Item</span></code> inheritance on Python 3.6 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2511">issue 2511</a>).</li>
<li>Enforce numeric values for components order in <code class="docutils literal notranslate"><span class="pre">SPIDER_MIDDLEWARES</span></code>,
<code class="docutils literal notranslate"><span class="pre">DOWNLOADER_MIDDLEWARES</span></code>, <code class="docutils literal notranslate"><span class="pre">EXTENIONS</span></code> and <code class="docutils literal notranslate"><span class="pre">SPIDER_CONTRACTS</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2420">issue 2420</a>).</li>
</ul>
</div>
<div class="section" id="id14">
<h5>Documentation<a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Reword Code of Coduct section and upgrade to Contributor Covenant v1.4
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2469">issue 2469</a>).</li>
<li>Clarify that passing spider arguments converts them to spider attributes
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2483">issue 2483</a>).</li>
<li>Document <code class="docutils literal notranslate"><span class="pre">formid</span></code> argument on <code class="docutils literal notranslate"><span class="pre">FormRequest.from_response()</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2497">issue 2497</a>).</li>
<li>Add .rst extension to README files (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2507">issue 2507</a>).</li>
<li>Mention LevelDB cache storage backend (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2525">issue 2525</a>).</li>
<li>Use <code class="docutils literal notranslate"><span class="pre">yield</span></code> in sample callback code (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2533">issue 2533</a>).</li>
<li>Add note about HTML entities decoding with <code class="docutils literal notranslate"><span class="pre">.re()/.re_first()</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1704">issue 1704</a>).</li>
<li>Typos (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2512">issue 2512</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2534">issue 2534</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2531">issue 2531</a>).</li>
</ul>
</div>
<div class="section" id="cleanups">
<h5>Cleanups<a class="headerlink" href="#cleanups" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Remove reduntant check in <code class="docutils literal notranslate"><span class="pre">MetaRefreshMiddleware</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2542">issue 2542</a>).</li>
<li>Faster checks in <code class="docutils literal notranslate"><span class="pre">LinkExtractor</span></code> for allow/deny patterns (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2538">issue 2538</a>).</li>
<li>Remove dead code supporting old Twisted versions (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2544">issue 2544</a>).</li>
</ul>
</div>
</div>
<div class="section" id="scrapy-1-3-0-2016-12-21">
<span id="release-1-3-0"></span><h4>Scrapy 1.3.0 (2016-12-21)<a class="headerlink" href="#scrapy-1-3-0-2016-12-21" title="Permalink to this headline">¶</a></h4>
<p>This release comes rather soon after 1.2.2 for one main reason:
it was found out that releases since 0.18 up to 1.2.2 (included) use
some backported code from Twisted (<code class="docutils literal notranslate"><span class="pre">scrapy.xlib.tx.*</span></code>),
even if newer Twisted modules are available.
Scrapy now uses <code class="docutils literal notranslate"><span class="pre">twisted.web.client</span></code> and <code class="docutils literal notranslate"><span class="pre">twisted.internet.endpoints</span></code> directly.
(See also cleanups below.)</p>
<p>As it is a major change, we wanted to get the bug fix out quickly
while not breaking any projects using the 1.2 series.</p>
<div class="section" id="id15">
<h5>New Features<a class="headerlink" href="#id15" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">MailSender</span></code> now accepts single strings as values for <code class="docutils literal notranslate"><span class="pre">to</span></code> and <code class="docutils literal notranslate"><span class="pre">cc</span></code>
arguments (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2272">issue 2272</a>)</li>
<li><code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">fetch</span> <span class="pre">url</span></code>, <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">shell</span> <span class="pre">url</span></code> and <code class="docutils literal notranslate"><span class="pre">fetch(url)</span></code> inside
scrapy shell now follow HTTP redirections by default (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2290">issue 2290</a>);
See <a class="reference internal" href="index.html#std:command-fetch"><code class="xref std std-command docutils literal notranslate"><span class="pre">fetch</span></code></a> and <a class="reference internal" href="index.html#std:command-shell"><code class="xref std std-command docutils literal notranslate"><span class="pre">shell</span></code></a> for details.</li>
<li><code class="docutils literal notranslate"><span class="pre">HttpErrorMiddleware</span></code> now logs errors with <code class="docutils literal notranslate"><span class="pre">INFO</span></code> level instead of <code class="docutils literal notranslate"><span class="pre">DEBUG</span></code>;
this is technically <strong>backward incompatible</strong> so please check your log parsers.</li>
<li>By default, logger names now use a long-form path, e.g. <code class="docutils literal notranslate"><span class="pre">[scrapy.extensions.logstats]</span></code>,
instead of the shorter “top-level” variant of prior releases (e.g. <code class="docutils literal notranslate"><span class="pre">[scrapy]</span></code>);
this is <strong>backward incompatible</strong> if you have log parsers expecting the short
logger name part. You can switch back to short logger names using <a class="reference internal" href="index.html#std:setting-LOG_SHORT_NAMES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">LOG_SHORT_NAMES</span></code></a>
set to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</li>
</ul>
</div>
<div class="section" id="dependencies-cleanups">
<h5>Dependencies &amp; Cleanups<a class="headerlink" href="#dependencies-cleanups" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Scrapy now requires Twisted &gt;= 13.1 which is the case for many Linux
distributions already.</li>
<li>As a consequence, we got rid of <code class="docutils literal notranslate"><span class="pre">scrapy.xlib.tx.*</span></code> modules, which
copied some of Twisted code for users stuck with an “old” Twisted version</li>
<li><code class="docutils literal notranslate"><span class="pre">ChunkedTransferMiddleware</span></code> is deprecated and removed from the default
downloader middlewares.</li>
</ul>
</div>
</div>
<div class="section" id="scrapy-1-2-3-2017-03-03">
<span id="release-1-2-3"></span><h4>Scrapy 1.2.3 (2017-03-03)<a class="headerlink" href="#scrapy-1-2-3-2017-03-03" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Packaging fix: disallow unsupported Twisted versions in setup.py</li>
</ul>
</div>
<div class="section" id="scrapy-1-2-2-2016-12-06">
<span id="release-1-2-2"></span><h4>Scrapy 1.2.2 (2016-12-06)<a class="headerlink" href="#scrapy-1-2-2-2016-12-06" title="Permalink to this headline">¶</a></h4>
<div class="section" id="id16">
<h5>Bug fixes<a class="headerlink" href="#id16" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Fix a cryptic traceback when a pipeline fails on <code class="docutils literal notranslate"><span class="pre">open_spider()</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2011">issue 2011</a>)</li>
<li>Fix embedded IPython shell variables (fixing <a class="reference external" href="https://github.com/scrapy/scrapy/issues/396">issue 396</a> that re-appeared
in 1.2.0, fixed in <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2418">issue 2418</a>)</li>
<li>A couple of patches when dealing with robots.txt:<ul>
<li>handle (non-standard) relative sitemap URLs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2390">issue 2390</a>)</li>
<li>handle non-ASCII URLs and User-Agents in Python 2 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2373">issue 2373</a>)</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="id17">
<h5>Documentation<a class="headerlink" href="#id17" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Document <code class="docutils literal notranslate"><span class="pre">&quot;download_latency&quot;</span></code> key in <code class="docutils literal notranslate"><span class="pre">Request</span></code>’s <code class="docutils literal notranslate"><span class="pre">meta</span></code> dict (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2033">issue 2033</a>)</li>
<li>Remove page on (deprecated &amp; unsupported) Ubuntu packages from ToC (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2335">issue 2335</a>)</li>
<li>A few fixed typos (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2346">issue 2346</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2369">issue 2369</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2369">issue 2369</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2380">issue 2380</a>)
and clarifications (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2354">issue 2354</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2325">issue 2325</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2414">issue 2414</a>)</li>
</ul>
</div>
<div class="section" id="id18">
<h5>Other changes<a class="headerlink" href="#id18" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Advertize <a class="reference external" href="https://anaconda.org/conda-forge/scrapy">conda-forge</a> as Scrapy’s official conda channel (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2387">issue 2387</a>)</li>
<li>More helpful error messages when trying to use <code class="docutils literal notranslate"><span class="pre">.css()</span></code> or <code class="docutils literal notranslate"><span class="pre">.xpath()</span></code>
on non-Text Responses (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2264">issue 2264</a>)</li>
<li><code class="docutils literal notranslate"><span class="pre">startproject</span></code> command now generates a sample <code class="docutils literal notranslate"><span class="pre">middlewares.py</span></code> file (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2335">issue 2335</a>)</li>
<li>Add more dependencies’ version info in <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">version</span></code> verbose output (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2404">issue 2404</a>)</li>
<li>Remove all <code class="docutils literal notranslate"><span class="pre">*.pyc</span></code> files from source distribution (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2386">issue 2386</a>)</li>
</ul>
</div>
</div>
<div class="section" id="scrapy-1-2-1-2016-10-21">
<span id="release-1-2-1"></span><h4>Scrapy 1.2.1 (2016-10-21)<a class="headerlink" href="#scrapy-1-2-1-2016-10-21" title="Permalink to this headline">¶</a></h4>
<div class="section" id="id19">
<h5>Bug fixes<a class="headerlink" href="#id19" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Include OpenSSL’s more permissive default ciphers when establishing
TLS/SSL connections (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2314">issue 2314</a>).</li>
<li>Fix “Location” HTTP header decoding on non-ASCII URL redirects (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2321">issue 2321</a>).</li>
</ul>
</div>
<div class="section" id="id20">
<h5>Documentation<a class="headerlink" href="#id20" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Fix JsonWriterPipeline example (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2302">issue 2302</a>).</li>
<li>Various notes: <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2330">issue 2330</a> on spider names,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2329">issue 2329</a> on middleware methods processing order,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2327">issue 2327</a> on getting multi-valued HTTP headers as lists.</li>
</ul>
</div>
<div class="section" id="id21">
<h5>Other changes<a class="headerlink" href="#id21" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Removed <code class="docutils literal notranslate"><span class="pre">www.</span></code> from <code class="docutils literal notranslate"><span class="pre">start_urls</span></code> in built-in spider templates (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2299">issue 2299</a>).</li>
</ul>
</div>
</div>
<div class="section" id="scrapy-1-2-0-2016-10-03">
<span id="release-1-2-0"></span><h4>Scrapy 1.2.0 (2016-10-03)<a class="headerlink" href="#scrapy-1-2-0-2016-10-03" title="Permalink to this headline">¶</a></h4>
<div class="section" id="id22">
<h5>New Features<a class="headerlink" href="#id22" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>New <a class="reference internal" href="index.html#std:setting-FEED_EXPORT_ENCODING"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_EXPORT_ENCODING</span></code></a> setting to customize the encoding
used when writing items to a file.
This can be used to turn off <code class="docutils literal notranslate"><span class="pre">\uXXXX</span></code> escapes in JSON output.
This is also useful for those wanting something else than UTF-8
for XML or CSV output (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2034">issue 2034</a>).</li>
<li><code class="docutils literal notranslate"><span class="pre">startproject</span></code> command now supports an optional destination directory
to override the default one based on the project name (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2005">issue 2005</a>).</li>
<li>New <a class="reference internal" href="index.html#std:setting-SCHEDULER_DEBUG"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SCHEDULER_DEBUG</span></code></a> setting to log requests serialization
failures (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1610">issue 1610</a>).</li>
<li>JSON encoder now supports serialization of <code class="docutils literal notranslate"><span class="pre">set</span></code> instances (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2058">issue 2058</a>).</li>
<li>Interpret <code class="docutils literal notranslate"><span class="pre">application/json-amazonui-streaming</span></code> as <code class="docutils literal notranslate"><span class="pre">TextResponse</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1503">issue 1503</a>).</li>
<li><code class="docutils literal notranslate"><span class="pre">scrapy</span></code> is imported by default when using shell tools (<a class="reference internal" href="index.html#std:command-shell"><code class="xref std std-command docutils literal notranslate"><span class="pre">shell</span></code></a>,
<a class="reference internal" href="index.html#topics-shell-inspect-response"><span class="std std-ref">inspect_response</span></a>) (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2248">issue 2248</a>).</li>
</ul>
</div>
<div class="section" id="id23">
<h5>Bug fixes<a class="headerlink" href="#id23" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>DefaultRequestHeaders middleware now runs before UserAgent middleware
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2088">issue 2088</a>). <strong>Warning: this is technically backward incompatible</strong>,
though we consider this a bug fix.</li>
<li>HTTP cache extension and plugins that use the <code class="docutils literal notranslate"><span class="pre">.scrapy</span></code> data directory now
work outside projects (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1581">issue 1581</a>).  <strong>Warning: this is technically
backward incompatible</strong>, though we consider this a bug fix.</li>
<li><code class="docutils literal notranslate"><span class="pre">Selector</span></code> does not allow passing both <code class="docutils literal notranslate"><span class="pre">response</span></code> and <code class="docutils literal notranslate"><span class="pre">text</span></code> anymore
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2153">issue 2153</a>).</li>
<li>Fixed logging of wrong callback name with <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">parse</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2169">issue 2169</a>).</li>
<li>Fix for an odd gzip decompression bug (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1606">issue 1606</a>).</li>
<li>Fix for selected callbacks when using <code class="docutils literal notranslate"><span class="pre">CrawlSpider</span></code> with <a class="reference internal" href="index.html#std:command-parse"><code class="xref std std-command docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">parse</span></code></a>
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2225">issue 2225</a>).</li>
<li>Fix for invalid JSON and XML files when spider yields no items (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/872">issue 872</a>).</li>
<li>Implement <code class="docutils literal notranslate"><span class="pre">flush()</span></code> fpr <code class="docutils literal notranslate"><span class="pre">StreamLogger</span></code> avoiding a warning in logs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2125">issue 2125</a>).</li>
</ul>
</div>
<div class="section" id="refactoring">
<h5>Refactoring<a class="headerlink" href="#refactoring" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">canonicalize_url</span></code> has been moved to <a class="reference external" href="https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.url.canonicalize_url">w3lib.url</a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2168">issue 2168</a>).</li>
</ul>
</div>
<div class="section" id="tests-requirements">
<h5>Tests &amp; Requirements<a class="headerlink" href="#tests-requirements" title="Permalink to this headline">¶</a></h5>
<p>Scrapy’s new requirements baseline is Debian 8 “Jessie”. It was previously
Ubuntu 12.04 Precise.
What this means in practice is that we run continuous integration tests
with these (main) packages versions at a minimum:
Twisted 14.0, pyOpenSSL 0.14, lxml 3.4.</p>
<p>Scrapy may very well work with older versions of these packages
(the code base still has switches for older Twisted versions for example)
but it is not guaranteed (because it’s not tested anymore).</p>
</div>
<div class="section" id="id24">
<h5>Documentation<a class="headerlink" href="#id24" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Grammar fixes: <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2128">issue 2128</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1566">issue 1566</a>.</li>
<li>Download stats badge removed from README (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2160">issue 2160</a>).</li>
<li>New scrapy <a class="reference internal" href="index.html#topics-architecture"><span class="std std-ref">architecture diagram</span></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2165">issue 2165</a>).</li>
<li>Updated <code class="docutils literal notranslate"><span class="pre">Response</span></code> parameters documentation (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2197">issue 2197</a>).</li>
<li>Reworded misleading <a class="reference internal" href="index.html#std:setting-RANDOMIZE_DOWNLOAD_DELAY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RANDOMIZE_DOWNLOAD_DELAY</span></code></a> description (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2190">issue 2190</a>).</li>
<li>Add StackOverflow as a support channel (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2257">issue 2257</a>).</li>
</ul>
</div>
</div>
<div class="section" id="scrapy-1-1-4-2017-03-03">
<span id="release-1-1-4"></span><h4>Scrapy 1.1.4 (2017-03-03)<a class="headerlink" href="#scrapy-1-1-4-2017-03-03" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Packaging fix: disallow unsupported Twisted versions in setup.py</li>
</ul>
</div>
<div class="section" id="scrapy-1-1-3-2016-09-22">
<span id="release-1-1-3"></span><h4>Scrapy 1.1.3 (2016-09-22)<a class="headerlink" href="#scrapy-1-1-3-2016-09-22" title="Permalink to this headline">¶</a></h4>
<div class="section" id="id25">
<h5>Bug fixes<a class="headerlink" href="#id25" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Class attributes for subclasses of <code class="docutils literal notranslate"><span class="pre">ImagesPipeline</span></code> and <code class="docutils literal notranslate"><span class="pre">FilesPipeline</span></code>
work as they did before 1.1.1 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2243">issue 2243</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2198">issue 2198</a>)</li>
</ul>
</div>
<div class="section" id="id26">
<h5>Documentation<a class="headerlink" href="#id26" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li><a class="reference internal" href="index.html#intro-overview"><span class="std std-ref">Overview</span></a> and <a class="reference internal" href="index.html#intro-tutorial"><span class="std std-ref">tutorial</span></a>
rewritten to use <a class="reference external" href="http://toscrape.com">http://toscrape.com</a> websites
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2236">issue 2236</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2249">issue 2249</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2252">issue 2252</a>).</li>
</ul>
</div>
</div>
<div class="section" id="scrapy-1-1-2-2016-08-18">
<span id="release-1-1-2"></span><h4>Scrapy 1.1.2 (2016-08-18)<a class="headerlink" href="#scrapy-1-1-2-2016-08-18" title="Permalink to this headline">¶</a></h4>
<div class="section" id="id27">
<h5>Bug fixes<a class="headerlink" href="#id27" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Introduce a missing <a class="reference internal" href="index.html#std:setting-IMAGES_STORE_S3_ACL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_STORE_S3_ACL</span></code></a> setting to override
the default ACL policy in <code class="docutils literal notranslate"><span class="pre">ImagesPipeline</span></code> when uploading images to S3
(note that default ACL policy is “private” – instead of “public-read” –
since Scrapy 1.1.0)</li>
<li><a class="reference internal" href="index.html#std:setting-IMAGES_EXPIRES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">IMAGES_EXPIRES</span></code></a> default value set back to 90
(the regression was introduced in 1.1.1)</li>
</ul>
</div>
</div>
<div class="section" id="scrapy-1-1-1-2016-07-13">
<span id="release-1-1-1"></span><h4>Scrapy 1.1.1 (2016-07-13)<a class="headerlink" href="#scrapy-1-1-1-2016-07-13" title="Permalink to this headline">¶</a></h4>
<div class="section" id="id28">
<h5>Bug fixes<a class="headerlink" href="#id28" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Add “Host” header in CONNECT requests to HTTPS proxies (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2069">issue 2069</a>)</li>
<li>Use response <code class="docutils literal notranslate"><span class="pre">body</span></code> when choosing response class
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2001">issue 2001</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2000">issue 2000</a>)</li>
<li>Do not fail on canonicalizing URLs with wrong netlocs
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2038">issue 2038</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2010">issue 2010</a>)</li>
<li>a few fixes for <code class="docutils literal notranslate"><span class="pre">HttpCompressionMiddleware</span></code> (and <code class="docutils literal notranslate"><span class="pre">SitemapSpider</span></code>):<ul>
<li>Do not decode HEAD responses (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2008">issue 2008</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1899">issue 1899</a>)</li>
<li>Handle charset parameter in gzip Content-Type header
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2050">issue 2050</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2049">issue 2049</a>)</li>
<li>Do not decompress gzip octet-stream responses
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2065">issue 2065</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2063">issue 2063</a>)</li>
</ul>
</li>
<li>Catch (and ignore with a warning) exception when verifying certificate
against IP-address hosts (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2094">issue 2094</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2092">issue 2092</a>)</li>
<li>Make <code class="docutils literal notranslate"><span class="pre">FilesPipeline</span></code> and <code class="docutils literal notranslate"><span class="pre">ImagesPipeline</span></code> backward compatible again
regarding the use of legacy class attributes for customization
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1989">issue 1989</a>, fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1985">issue 1985</a>)</li>
</ul>
</div>
<div class="section" id="id29">
<h5>New features<a class="headerlink" href="#id29" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Enable genspider command outside project folder (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2052">issue 2052</a>)</li>
<li>Retry HTTPS CONNECT <code class="docutils literal notranslate"><span class="pre">TunnelError</span></code> by default (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1974">issue 1974</a>)</li>
</ul>
</div>
<div class="section" id="id30">
<h5>Documentation<a class="headerlink" href="#id30" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">FEED_TEMPDIR</span></code> setting at lexicographical position (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/9b3c72c">commit 9b3c72c</a>)</li>
<li>Use idiomatic <code class="docutils literal notranslate"><span class="pre">.extract_first()</span></code> in overview (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1994">issue 1994</a>)</li>
<li>Update years in copyright notice (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c2c8036">commit c2c8036</a>)</li>
<li>Add information and example on errbacks (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1995">issue 1995</a>)</li>
<li>Use “url” variable in downloader middleware example (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2015">issue 2015</a>)</li>
<li>Grammar fixes (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2054">issue 2054</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/2120">issue 2120</a>)</li>
<li>New FAQ entry on using BeautifulSoup in spider callbacks (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2048">issue 2048</a>)</li>
<li>Add notes about scrapy not working on Windows with Python 3 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2060">issue 2060</a>)</li>
<li>Encourage complete titles in pull requests (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2026">issue 2026</a>)</li>
</ul>
</div>
<div class="section" id="tests">
<h5>Tests<a class="headerlink" href="#tests" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Upgrade py.test requirement on Travis CI and Pin pytest-cov to 2.2.1 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/2095">issue 2095</a>)</li>
</ul>
</div>
</div>
<div class="section" id="scrapy-1-1-0-2016-05-11">
<span id="release-1-1-0"></span><h4>Scrapy 1.1.0 (2016-05-11)<a class="headerlink" href="#scrapy-1-1-0-2016-05-11" title="Permalink to this headline">¶</a></h4>
<p>This 1.1 release brings a lot of interesting features and bug fixes:</p>
<ul class="simple">
<li>Scrapy 1.1 has beta Python 3 support (requires Twisted &gt;= 15.5). See
<a class="reference internal" href="#news-betapy3"><span class="std std-ref">Beta Python 3 Support</span></a> for more details and some limitations.</li>
<li>Hot new features:<ul>
<li>Item loaders now support nested loaders (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1467">issue 1467</a>).</li>
<li><code class="docutils literal notranslate"><span class="pre">FormRequest.from_response</span></code> improvements (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1382">issue 1382</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1137">issue 1137</a>).</li>
<li>Added setting <a class="reference internal" href="index.html#std:setting-AUTOTHROTTLE_TARGET_CONCURRENCY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AUTOTHROTTLE_TARGET_CONCURRENCY</span></code></a> and improved
AutoThrottle docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1324">issue 1324</a>).</li>
<li>Added <code class="docutils literal notranslate"><span class="pre">response.text</span></code> to get body as unicode (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1730">issue 1730</a>).</li>
<li>Anonymous S3 connections (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1358">issue 1358</a>).</li>
<li>Deferreds in downloader middlewares (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1473">issue 1473</a>). This enables better
robots.txt handling (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1471">issue 1471</a>).</li>
<li>HTTP caching now follows RFC2616 more closely, added settings
<a class="reference internal" href="index.html#std:setting-HTTPCACHE_ALWAYS_STORE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">HTTPCACHE_ALWAYS_STORE</span></code></a> and
<a class="reference internal" href="index.html#std:setting-HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS</span></code></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1151">issue 1151</a>).</li>
<li>Selectors were extracted to the <a class="reference external" href="https://github.com/scrapy/parsel">parsel</a> library (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1409">issue 1409</a>). This means
you can use Scrapy Selectors without Scrapy and also upgrade the
selectors engine without needing to upgrade Scrapy.</li>
<li>HTTPS downloader now does TLS protocol negotiation by default,
instead of forcing TLS 1.0. You can also set the SSL/TLS method
using the new <a class="reference internal" href="index.html#std:setting-DOWNLOADER_CLIENT_TLS_METHOD"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOADER_CLIENT_TLS_METHOD</span></code></a>.</li>
</ul>
</li>
<li>These bug fixes may require your attention:<ul>
<li>Don’t retry bad requests (HTTP 400) by default (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1289">issue 1289</a>).
If you need the old behavior, add <code class="docutils literal notranslate"><span class="pre">400</span></code> to <a class="reference internal" href="index.html#std:setting-RETRY_HTTP_CODES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_HTTP_CODES</span></code></a>.</li>
<li>Fix shell files argument handling (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1710">issue 1710</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1550">issue 1550</a>).
If you try <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">shell</span> <span class="pre">index.html</span></code> it will try to load the URL <a class="reference external" href="http://index.html">http://index.html</a>,
use <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">shell</span> <span class="pre">./index.html</span></code> to load a local file.</li>
<li>Robots.txt compliance is now enabled by default for newly-created projects
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1724">issue 1724</a>). Scrapy will also wait for robots.txt to be downloaded
before proceeding with the crawl (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1735">issue 1735</a>). If you want to disable
this behavior, update <a class="reference internal" href="index.html#std:setting-ROBOTSTXT_OBEY"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ROBOTSTXT_OBEY</span></code></a> in <code class="docutils literal notranslate"><span class="pre">settings.py</span></code> file
after creating a new project.</li>
<li>Exporters now work on unicode, instead of bytes by default (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1080">issue 1080</a>).
If you use <code class="docutils literal notranslate"><span class="pre">PythonItemExporter</span></code>, you may want to update your code to
disable binary mode which is now deprecated.</li>
<li>Accept XML node names containing dots as valid (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1533">issue 1533</a>).</li>
<li>When uploading files or images to S3 (with <code class="docutils literal notranslate"><span class="pre">FilesPipeline</span></code> or
<code class="docutils literal notranslate"><span class="pre">ImagesPipeline</span></code>), the default ACL policy is now “private” instead
of “public” <strong>Warning: backward incompatible!</strong>.
You can use <a class="reference internal" href="index.html#std:setting-FILES_STORE_S3_ACL"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FILES_STORE_S3_ACL</span></code></a> to change it.</li>
<li>We’ve reimplemented <code class="docutils literal notranslate"><span class="pre">canonicalize_url()</span></code> for more correct output,
especially for URLs with non-ASCII characters (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1947">issue 1947</a>).
This could change link extractors output compared to previous scrapy versions.
This may also invalidate some cache entries you could still have from pre-1.1 runs.
<strong>Warning: backward incompatible!</strong>.</li>
</ul>
</li>
</ul>
<p>Keep reading for more details on other improvements and bug fixes.</p>
<div class="section" id="beta-python-3-support">
<span id="news-betapy3"></span><h5>Beta Python 3 Support<a class="headerlink" href="#beta-python-3-support" title="Permalink to this headline">¶</a></h5>
<p>We have been <a class="reference external" href="https://github.com/scrapy/scrapy/wiki/Python-3-Porting">hard at work to make Scrapy run on Python 3</a>. As a result, now
you can run spiders on Python 3.3, 3.4 and 3.5 (Twisted &gt;= 15.5 required). Some
features are still missing (and some may never be ported).</p>
<p>Almost all builtin extensions/middlewares are expected to work.
However, we are aware of some limitations in Python 3:</p>
<ul class="simple">
<li>Scrapy does not work on Windows with Python 3</li>
<li>Sending emails is not supported</li>
<li>FTP download handler is not supported</li>
<li>Telnet console is not supported</li>
</ul>
</div>
<div class="section" id="additional-new-features-and-enhancements">
<h5>Additional New Features and Enhancements<a class="headerlink" href="#additional-new-features-and-enhancements" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Scrapy now has a <a class="reference external" href="https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md">Code of Conduct</a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1681">issue 1681</a>).</li>
<li>Command line tool now has completion for zsh (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/934">issue 934</a>).</li>
<li>Improvements to <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">shell</span></code>:<ul>
<li>Support for bpython and configure preferred Python shell via
<code class="docutils literal notranslate"><span class="pre">SCRAPY_PYTHON_SHELL</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1100">issue 1100</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1444">issue 1444</a>).</li>
<li>Support URLs without scheme (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1498">issue 1498</a>)
<strong>Warning: backward incompatible!</strong></li>
<li>Bring back support for relative file path (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1710">issue 1710</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1550">issue 1550</a>).</li>
</ul>
</li>
<li>Added <a class="reference internal" href="index.html#std:setting-MEMUSAGE_CHECK_INTERVAL_SECONDS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">MEMUSAGE_CHECK_INTERVAL_SECONDS</span></code></a> setting to change default check
interval (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1282">issue 1282</a>).</li>
<li>Download handlers are now lazy-loaded on first request using their
scheme (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1390">issue 1390</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1421">issue 1421</a>).</li>
<li>HTTPS download handlers do not force TLS 1.0 anymore; instead,
OpenSSL’s <code class="docutils literal notranslate"><span class="pre">SSLv23_method()/TLS_method()</span></code> is used allowing to try
negotiating with the remote hosts the highest TLS protocol version
it can (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1794">issue 1794</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1629">issue 1629</a>).</li>
<li><code class="docutils literal notranslate"><span class="pre">RedirectMiddleware</span></code> now skips the status codes from
<code class="docutils literal notranslate"><span class="pre">handle_httpstatus_list</span></code> on spider attribute
or in <code class="docutils literal notranslate"><span class="pre">Request</span></code>’s <code class="docutils literal notranslate"><span class="pre">meta</span></code> key (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1334">issue 1334</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1364">issue 1364</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1447">issue 1447</a>).</li>
<li>Form submission:<ul>
<li>now works with <code class="docutils literal notranslate"><span class="pre">&lt;button&gt;</span></code> elements too (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1469">issue 1469</a>).</li>
<li>an empty string is now used for submit buttons without a value
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1472">issue 1472</a>)</li>
</ul>
</li>
<li>Dict-like settings now have per-key priorities
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1135">issue 1135</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1149">issue 1149</a> and <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1586">issue 1586</a>).</li>
<li>Sending non-ASCII emails (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1662">issue 1662</a>)</li>
<li><code class="docutils literal notranslate"><span class="pre">CloseSpider</span></code> and <code class="docutils literal notranslate"><span class="pre">SpiderState</span></code> extensions now get disabled if no relevant
setting is set (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1723">issue 1723</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1725">issue 1725</a>).</li>
<li>Added method <code class="docutils literal notranslate"><span class="pre">ExecutionEngine.close</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1423">issue 1423</a>).</li>
<li>Added method <code class="docutils literal notranslate"><span class="pre">CrawlerRunner.create_crawler</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1528">issue 1528</a>).</li>
<li>Scheduler priority queue can now be customized via
<a class="reference internal" href="index.html#std:setting-SCHEDULER_PRIORITY_QUEUE"><code class="xref std std-setting docutils literal notranslate"><span class="pre">SCHEDULER_PRIORITY_QUEUE</span></code></a> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1822">issue 1822</a>).</li>
<li><code class="docutils literal notranslate"><span class="pre">.pps</span></code> links are now ignored by default in link extractors (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1835">issue 1835</a>).</li>
<li>temporary data folder for FTP and S3 feed storages can be customized
using a new <a class="reference internal" href="index.html#std:setting-FEED_TEMPDIR"><code class="xref std std-setting docutils literal notranslate"><span class="pre">FEED_TEMPDIR</span></code></a> setting (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1847">issue 1847</a>).</li>
<li><code class="docutils literal notranslate"><span class="pre">FilesPipeline</span></code> and <code class="docutils literal notranslate"><span class="pre">ImagesPipeline</span></code> settings are now instance attributes
instead of class attributes, enabling spider-specific behaviors (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1891">issue 1891</a>).</li>
<li><code class="docutils literal notranslate"><span class="pre">JsonItemExporter</span></code> now formats opening and closing square brackets
on their own line (first and last lines of output file) (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1950">issue 1950</a>).</li>
<li>If available, <code class="docutils literal notranslate"><span class="pre">botocore</span></code> is used for <code class="docutils literal notranslate"><span class="pre">S3FeedStorage</span></code>, <code class="docutils literal notranslate"><span class="pre">S3DownloadHandler</span></code>
and <code class="docutils literal notranslate"><span class="pre">S3FilesStore</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1761">issue 1761</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1883">issue 1883</a>).</li>
<li>Tons of documentation updates and related fixes (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1291">issue 1291</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1302">issue 1302</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1335">issue 1335</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1683">issue 1683</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1660">issue 1660</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1642">issue 1642</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1721">issue 1721</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1727">issue 1727</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1879">issue 1879</a>).</li>
<li>Other refactoring, optimizations and cleanup (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1476">issue 1476</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1481">issue 1481</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1477">issue 1477</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1315">issue 1315</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1290">issue 1290</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1750">issue 1750</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1881">issue 1881</a>).</li>
</ul>
</div>
<div class="section" id="deprecations-and-removals">
<h5>Deprecations and Removals<a class="headerlink" href="#deprecations-and-removals" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Added <code class="docutils literal notranslate"><span class="pre">to_bytes</span></code> and <code class="docutils literal notranslate"><span class="pre">to_unicode</span></code>, deprecated <code class="docutils literal notranslate"><span class="pre">str_to_unicode</span></code> and
<code class="docutils literal notranslate"><span class="pre">unicode_to_str</span></code> functions (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/778">issue 778</a>).</li>
<li><code class="docutils literal notranslate"><span class="pre">binary_is_text</span></code> is introduced, to replace use of <code class="docutils literal notranslate"><span class="pre">isbinarytext</span></code>
(but with inverse return value) (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1851">issue 1851</a>)</li>
<li>The <code class="docutils literal notranslate"><span class="pre">optional_features</span></code> set has been removed (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1359">issue 1359</a>).</li>
<li>The <code class="docutils literal notranslate"><span class="pre">--lsprof</span></code> command line option has been removed (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1689">issue 1689</a>).
<strong>Warning: backward incompatible</strong>, but doesn’t break user code.</li>
<li>The following datatypes were deprecated (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1720">issue 1720</a>):<ul>
<li><code class="docutils literal notranslate"><span class="pre">scrapy.utils.datatypes.MultiValueDictKeyError</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">scrapy.utils.datatypes.MultiValueDict</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">scrapy.utils.datatypes.SiteNode</span></code></li>
</ul>
</li>
<li>The previously bundled <code class="docutils literal notranslate"><span class="pre">scrapy.xlib.pydispatch</span></code> library was deprecated and
replaced by <a class="reference external" href="https://pypi.python.org/pypi/PyDispatcher">pydispatcher</a>.</li>
</ul>
</div>
<div class="section" id="relocations">
<h5>Relocations<a class="headerlink" href="#relocations" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">telnetconsole</span></code> was relocated to <code class="docutils literal notranslate"><span class="pre">extensions/</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1524">issue 1524</a>).<ul>
<li>Note: telnet is not enabled on Python 3
(<a class="reference external" href="https://github.com/scrapy/scrapy/pull/1524#issuecomment-146985595">https://github.com/scrapy/scrapy/pull/1524#issuecomment-146985595</a>)</li>
</ul>
</li>
</ul>
</div>
<div class="section" id="bugfixes">
<h5>Bugfixes<a class="headerlink" href="#bugfixes" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Scrapy does not retry requests that got a <code class="docutils literal notranslate"><span class="pre">HTTP</span> <span class="pre">400</span> <span class="pre">Bad</span> <span class="pre">Request</span></code>
response anymore (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1289">issue 1289</a>). <strong>Warning: backward incompatible!</strong></li>
<li>Support empty password for http_proxy config (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1274">issue 1274</a>).</li>
<li>Interpret <code class="docutils literal notranslate"><span class="pre">application/x-json</span></code> as <code class="docutils literal notranslate"><span class="pre">TextResponse</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1333">issue 1333</a>).</li>
<li>Support link rel attribute with multiple values (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1201">issue 1201</a>).</li>
<li>Fixed <code class="docutils literal notranslate"><span class="pre">scrapy.http.FormRequest.from_response</span></code> when there is a <code class="docutils literal notranslate"><span class="pre">&lt;base&gt;</span></code>
tag (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1564">issue 1564</a>).</li>
<li>Fixed <a class="reference internal" href="index.html#std:setting-TEMPLATES_DIR"><code class="xref std std-setting docutils literal notranslate"><span class="pre">TEMPLATES_DIR</span></code></a> handling (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1575">issue 1575</a>).</li>
<li>Various <code class="docutils literal notranslate"><span class="pre">FormRequest</span></code> fixes (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1595">issue 1595</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1596">issue 1596</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1597">issue 1597</a>).</li>
<li>Makes <code class="docutils literal notranslate"><span class="pre">_monkeypatches</span></code> more robust (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1634">issue 1634</a>).</li>
<li>Fixed bug on <code class="docutils literal notranslate"><span class="pre">XMLItemExporter</span></code> with non-string fields in
items (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1738">issue 1738</a>).</li>
<li>Fixed startproject command in OS X (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1635">issue 1635</a>).</li>
<li>Fixed PythonItemExporter and CSVExporter for non-string item
types (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1737">issue 1737</a>).</li>
<li>Various logging related fixes (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1294">issue 1294</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1419">issue 1419</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1263">issue 1263</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1624">issue 1624</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1654">issue 1654</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1722">issue 1722</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1726">issue 1726</a> and <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1303">issue 1303</a>).</li>
<li>Fixed bug in <code class="docutils literal notranslate"><span class="pre">utils.template.render_templatefile()</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1212">issue 1212</a>).</li>
<li>sitemaps extraction from <code class="docutils literal notranslate"><span class="pre">robots.txt</span></code> is now case-insensitive (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1902">issue 1902</a>).</li>
<li>HTTPS+CONNECT tunnels could get mixed up when using multiple proxies
to same remote host (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1912">issue 1912</a>).</li>
</ul>
</div>
</div>
<div class="section" id="scrapy-1-0-7-2017-03-03">
<span id="release-1-0-7"></span><h4>Scrapy 1.0.7 (2017-03-03)<a class="headerlink" href="#scrapy-1-0-7-2017-03-03" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Packaging fix: disallow unsupported Twisted versions in setup.py</li>
</ul>
</div>
<div class="section" id="scrapy-1-0-6-2016-05-04">
<span id="release-1-0-6"></span><h4>Scrapy 1.0.6 (2016-05-04)<a class="headerlink" href="#scrapy-1-0-6-2016-05-04" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>FIX: RetryMiddleware is now robust to non-standard HTTP status codes (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1857">issue 1857</a>)</li>
<li>FIX: Filestorage HTTP cache was checking wrong modified time (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1875">issue 1875</a>)</li>
<li>DOC: Support for Sphinx 1.4+ (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1893">issue 1893</a>)</li>
<li>DOC: Consistency in selectors examples (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1869">issue 1869</a>)</li>
</ul>
</div>
<div class="section" id="scrapy-1-0-5-2016-02-04">
<span id="release-1-0-5"></span><h4>Scrapy 1.0.5 (2016-02-04)<a class="headerlink" href="#scrapy-1-0-5-2016-02-04" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>FIX: [Backport] Ignore bogus links in LinkExtractors (fixes <a class="reference external" href="https://github.com/scrapy/scrapy/issues/907">issue 907</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/commit/108195e">commit 108195e</a>)</li>
<li>TST: Changed buildbot makefile to use ‘pytest’ (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1f3d90a">commit 1f3d90a</a>)</li>
<li>DOC: Fixed typos in tutorial and media-pipeline (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/808a9ea">commit 808a9ea</a> and <a class="reference external" href="https://github.com/scrapy/scrapy/commit/803bd87">commit 803bd87</a>)</li>
<li>DOC: Add AjaxCrawlMiddleware to DOWNLOADER_MIDDLEWARES_BASE in settings docs (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/aa94121">commit aa94121</a>)</li>
</ul>
</div>
<div class="section" id="scrapy-1-0-4-2015-12-30">
<span id="release-1-0-4"></span><h4>Scrapy 1.0.4 (2015-12-30)<a class="headerlink" href="#scrapy-1-0-4-2015-12-30" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Ignoring xlib/tx folder, depending on Twisted version. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/7dfa979">commit 7dfa979</a>)</li>
<li>Run on new travis-ci infra (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/6e42f0b">commit 6e42f0b</a>)</li>
<li>Spelling fixes (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/823a1cc">commit 823a1cc</a>)</li>
<li>escape nodename in xmliter regex (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/da3c155">commit da3c155</a>)</li>
<li>test xml nodename with dots (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/4418fc3">commit 4418fc3</a>)</li>
<li>TST don’t use broken Pillow version in tests (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/a55078c">commit a55078c</a>)</li>
<li>disable log on version command. closes #1426 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/86fc330">commit 86fc330</a>)</li>
<li>disable log on startproject command (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/db4c9fe">commit db4c9fe</a>)</li>
<li>Add PyPI download stats badge (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/df2b944">commit df2b944</a>)</li>
<li>don’t run tests twice on Travis if a PR is made from a scrapy/scrapy branch (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/a83ab41">commit a83ab41</a>)</li>
<li>Add Python 3 porting status badge to the README (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/73ac80d">commit 73ac80d</a>)</li>
<li>fixed RFPDupeFilter persistence (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/97d080e">commit 97d080e</a>)</li>
<li>TST a test to show that dupefilter persistence is not working (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/97f2fb3">commit 97f2fb3</a>)</li>
<li>explicit close file on <a class="reference external" href="file://">file://</a> scheme handler (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/d9b4850">commit d9b4850</a>)</li>
<li>Disable dupefilter in shell (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c0d0734">commit c0d0734</a>)</li>
<li>DOC: Add captions to toctrees which appear in sidebar (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/aa239ad">commit aa239ad</a>)</li>
<li>DOC Removed pywin32 from install instructions as it’s already declared as dependency. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/10eb400">commit 10eb400</a>)</li>
<li>Added installation notes about using Conda for Windows and other OSes. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1c3600a">commit 1c3600a</a>)</li>
<li>Fixed minor grammar issues. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/7f4ddd5">commit 7f4ddd5</a>)</li>
<li>fixed a typo in the documentation. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b71f677">commit b71f677</a>)</li>
<li>Version 1 now exists (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5456c0e">commit 5456c0e</a>)</li>
<li>fix another invalid xpath error (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/0a1366e">commit 0a1366e</a>)</li>
<li>fix ValueError: Invalid XPath: //div/[id=”not-exists”]/text() on selectors.rst (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/ca8d60f">commit ca8d60f</a>)</li>
<li>Typos corrections (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/7067117">commit 7067117</a>)</li>
<li>fix typos in downloader-middleware.rst and exceptions.rst, middlware -&gt; middleware (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/32f115c">commit 32f115c</a>)</li>
<li>Add note to ubuntu install section about debian compatibility (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/23fda69">commit 23fda69</a>)</li>
<li>Replace alternative OSX install workaround with virtualenv (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/98b63ee">commit 98b63ee</a>)</li>
<li>Reference Homebrew’s homepage for installation instructions (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1925db1">commit 1925db1</a>)</li>
<li>Add oldest supported tox version to contributing docs (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5d10d6d">commit 5d10d6d</a>)</li>
<li>Note in install docs about pip being already included in python&gt;=2.7.9 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/85c980e">commit 85c980e</a>)</li>
<li>Add non-python dependencies to Ubuntu install section in the docs (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/fbd010d">commit fbd010d</a>)</li>
<li>Add OS X installation section to docs (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/d8f4cba">commit d8f4cba</a>)</li>
<li>DOC(ENH): specify path to rtd theme explicitly (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/de73b1a">commit de73b1a</a>)</li>
<li>minor: scrapy.Spider docs grammar (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1ddcc7b">commit 1ddcc7b</a>)</li>
<li>Make common practices sample code match the comments (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1b85bcf">commit 1b85bcf</a>)</li>
<li>nextcall repetitive calls (heartbeats). (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/55f7104">commit 55f7104</a>)</li>
<li>Backport fix compatibility with Twisted 15.4.0 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b262411">commit b262411</a>)</li>
<li>pin pytest to 2.7.3 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/a6535c2">commit a6535c2</a>)</li>
<li>Merge pull request #1512 from mgedmin/patch-1 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8876111">commit 8876111</a>)</li>
<li>Merge pull request #1513 from mgedmin/patch-2 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5d4daf8">commit 5d4daf8</a>)</li>
<li>Typo (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/f8d0682">commit f8d0682</a>)</li>
<li>Fix list formatting (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5f83a93">commit 5f83a93</a>)</li>
<li>fix scrapy squeue tests after recent changes to queuelib (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/3365c01">commit 3365c01</a>)</li>
<li>Merge pull request #1475 from rweindl/patch-1 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/2d688cd">commit 2d688cd</a>)</li>
<li>Update tutorial.rst (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/fbc1f25">commit fbc1f25</a>)</li>
<li>Merge pull request #1449 from rhoekman/patch-1 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/7d6538c">commit 7d6538c</a>)</li>
<li>Small grammatical change (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8752294">commit 8752294</a>)</li>
<li>Add openssl version to version command (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/13c45ac">commit 13c45ac</a>)</li>
</ul>
</div>
<div class="section" id="scrapy-1-0-3-2015-08-11">
<span id="release-1-0-3"></span><h4>Scrapy 1.0.3 (2015-08-11)<a class="headerlink" href="#scrapy-1-0-3-2015-08-11" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>add service_identity to scrapy install_requires (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/cbc2501">commit cbc2501</a>)</li>
<li>Workaround for travis#296 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/66af9cd">commit 66af9cd</a>)</li>
</ul>
</div>
<div class="section" id="scrapy-1-0-2-2015-08-06">
<span id="release-1-0-2"></span><h4>Scrapy 1.0.2 (2015-08-06)<a class="headerlink" href="#scrapy-1-0-2-2015-08-06" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Twisted 15.3.0 does not raises PicklingError serializing lambda functions (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b04dd7d">commit b04dd7d</a>)</li>
<li>Minor method name fix (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/6f85c7f">commit 6f85c7f</a>)</li>
<li>minor: scrapy.Spider grammar and clarity (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/9c9d2e0">commit 9c9d2e0</a>)</li>
<li>Put a blurb about support channels in CONTRIBUTING (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c63882b">commit c63882b</a>)</li>
<li>Fixed typos (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/a9ae7b0">commit a9ae7b0</a>)</li>
<li>Fix doc reference. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/7c8a4fe">commit 7c8a4fe</a>)</li>
</ul>
</div>
<div class="section" id="scrapy-1-0-1-2015-07-01">
<span id="release-1-0-1"></span><h4>Scrapy 1.0.1 (2015-07-01)<a class="headerlink" href="#scrapy-1-0-1-2015-07-01" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Unquote request path before passing to FTPClient, it already escape paths (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/cc00ad2">commit cc00ad2</a>)</li>
<li>include tests/ to source distribution in MANIFEST.in (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/eca227e">commit eca227e</a>)</li>
<li>DOC Fix SelectJmes documentation (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b8567bc">commit b8567bc</a>)</li>
<li>DOC Bring Ubuntu and Archlinux outside of Windows subsection (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/392233f">commit 392233f</a>)</li>
<li>DOC remove version suffix from ubuntu package (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5303c66">commit 5303c66</a>)</li>
<li>DOC Update release date for 1.0 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c89fa29">commit c89fa29</a>)</li>
</ul>
</div>
<div class="section" id="scrapy-1-0-0-2015-06-19">
<span id="release-1-0-0"></span><h4>Scrapy 1.0.0 (2015-06-19)<a class="headerlink" href="#scrapy-1-0-0-2015-06-19" title="Permalink to this headline">¶</a></h4>
<p>You will find a lot of new features and bugfixes in this major release.  Make
sure to check our updated <a class="reference internal" href="index.html#intro-overview"><span class="std std-ref">overview</span></a> to get a glance of
some of the changes, along with our brushed <a class="reference internal" href="index.html#intro-tutorial"><span class="std std-ref">tutorial</span></a>.</p>
<div class="section" id="support-for-returning-dictionaries-in-spiders">
<h5>Support for returning dictionaries in spiders<a class="headerlink" href="#support-for-returning-dictionaries-in-spiders" title="Permalink to this headline">¶</a></h5>
<p>Declaring and returning Scrapy Items is no longer necessary to collect the
scraped data from your spider, you can now return explicit dictionaries
instead.</p>
<p><em>Classic version</em></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MyItem</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
    <span class="n">url</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>

<span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">MyItem</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">)</span>
</pre></div>
</div>
<p><em>New version</em></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;url&#39;</span><span class="p">:</span> <span class="n">response</span><span class="o">.</span><span class="n">url</span><span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="per-spider-settings-gsoc-2014">
<h5>Per-spider settings (GSoC 2014)<a class="headerlink" href="#per-spider-settings-gsoc-2014" title="Permalink to this headline">¶</a></h5>
<p>Last Google Summer of Code project accomplished an important redesign of the
mechanism used for populating settings, introducing explicit priorities to
override any given setting. As an extension of that goal, we included a new
level of priority for settings that act exclusively for a single spider,
allowing them to redefine project settings.</p>
<p>Start using it by defining a <a class="reference internal" href="index.html#scrapy.spiders.Spider.custom_settings" title="scrapy.spiders.Spider.custom_settings"><code class="xref py py-attr docutils literal notranslate"><span class="pre">custom_settings</span></code></a>
class variable in your spider:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="n">custom_settings</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;DOWNLOAD_DELAY&quot;</span><span class="p">:</span> <span class="mf">5.0</span><span class="p">,</span>
        <span class="s2">&quot;RETRY_ENABLED&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">}</span>
</pre></div>
</div>
<p>Read more about settings population: <a class="reference internal" href="index.html#topics-settings"><span class="std std-ref">Settings</span></a></p>
</div>
<div class="section" id="python-logging">
<h5>Python Logging<a class="headerlink" href="#python-logging" title="Permalink to this headline">¶</a></h5>
<p>Scrapy 1.0 has moved away from Twisted logging to support Python built in’s
as default logging system. We’re maintaining backward compatibility for most
of the old custom interface to call logging functions, but you’ll get
warnings to switch to the Python logging API entirely.</p>
<p><em>Old version</em></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy</span> <span class="k">import</span> <span class="n">log</span>
<span class="n">log</span><span class="o">.</span><span class="n">msg</span><span class="p">(</span><span class="s1">&#39;MESSAGE&#39;</span><span class="p">,</span> <span class="n">log</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
</pre></div>
</div>
<p><em>New version</em></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">logging</span>
<span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;MESSAGE&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Logging with spiders remains the same, but on top of the
<a class="reference internal" href="index.html#scrapy.spiders.Spider.log" title="scrapy.spiders.Spider.log"><code class="xref py py-meth docutils literal notranslate"><span class="pre">log()</span></code></a> method you’ll have access to a custom
<a class="reference internal" href="index.html#scrapy.spiders.Spider.logger" title="scrapy.spiders.Spider.logger"><code class="xref py py-attr docutils literal notranslate"><span class="pre">logger</span></code></a> created for the spider to issue log
events:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MySpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;Response received&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Read more in the logging documentation: <a class="reference internal" href="index.html#topics-logging"><span class="std std-ref">Logging</span></a></p>
</div>
<div class="section" id="crawler-api-refactoring-gsoc-2014">
<h5>Crawler API refactoring (GSoC 2014)<a class="headerlink" href="#crawler-api-refactoring-gsoc-2014" title="Permalink to this headline">¶</a></h5>
<p>Another milestone for last Google Summer of Code was a refactoring of the
internal API, seeking a simpler and easier usage. Check new core interface
in: <a class="reference internal" href="index.html#topics-api"><span class="std std-ref">Core API</span></a></p>
<p>A common situation where you will face these changes is while running Scrapy
from scripts. Here’s a quick example of how to run a Spider manually with the
new API:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scrapy.crawler</span> <span class="k">import</span> <span class="n">CrawlerProcess</span>

<span class="n">process</span> <span class="o">=</span> <span class="n">CrawlerProcess</span><span class="p">({</span>
    <span class="s1">&#39;USER_AGENT&#39;</span><span class="p">:</span> <span class="s1">&#39;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)&#39;</span>
<span class="p">})</span>
<span class="n">process</span><span class="o">.</span><span class="n">crawl</span><span class="p">(</span><span class="n">MySpider</span><span class="p">)</span>
<span class="n">process</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
</pre></div>
</div>
<p>Bear in mind this feature is still under development and its API may change
until it reaches a stable status.</p>
<p>See more examples for scripts running Scrapy: <a class="reference internal" href="index.html#topics-practices"><span class="std std-ref">Common Practices</span></a></p>
</div>
<div class="section" id="module-relocations">
<span id="id31"></span><h5>Module Relocations<a class="headerlink" href="#module-relocations" title="Permalink to this headline">¶</a></h5>
<p>There’s been a large rearrangement of modules trying to improve the general
structure of Scrapy. Main changes were separating various subpackages into
new projects and dissolving both <code class="docutils literal notranslate"><span class="pre">scrapy.contrib</span></code> and <code class="docutils literal notranslate"><span class="pre">scrapy.contrib_exp</span></code>
into top level packages. Backward compatibility was kept among internal
relocations, while importing deprecated modules expect warnings indicating
their new place.</p>
<div class="section" id="full-list-of-relocations">
<h6>Full list of relocations<a class="headerlink" href="#full-list-of-relocations" title="Permalink to this headline">¶</a></h6>
<p>Outsourced packages</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">These extensions went through some minor changes, e.g. some setting names
were changed. Please check the documentation in each new repository to
get familiar with the new usage.</p>
</div>
<table border="1" class="docutils">
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Old location</th>
<th class="head">New location</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>scrapy.commands.deploy</td>
<td><a class="reference external" href="https://github.com/scrapy/scrapyd-client">scrapyd-client</a>
(See other alternatives here:
<a class="reference internal" href="index.html#topics-deploy"><span class="std std-ref">Deploying Spiders</span></a>)</td>
</tr>
<tr class="row-odd"><td>scrapy.contrib.djangoitem</td>
<td><a class="reference external" href="https://github.com/scrapy-plugins/scrapy-djangoitem">scrapy-djangoitem</a></td>
</tr>
<tr class="row-even"><td>scrapy.webservice</td>
<td><a class="reference external" href="https://github.com/scrapy-plugins/scrapy-jsonrpc">scrapy-jsonrpc</a></td>
</tr>
</tbody>
</table>
<p><code class="docutils literal notranslate"><span class="pre">scrapy.contrib_exp</span></code> and <code class="docutils literal notranslate"><span class="pre">scrapy.contrib</span></code> dissolutions</p>
<table border="1" class="docutils">
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Old location</th>
<th class="head">New location</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>scrapy.contrib_exp.downloadermiddleware.decompression</td>
<td>scrapy.downloadermiddlewares.decompression</td>
</tr>
<tr class="row-odd"><td>scrapy.contrib_exp.iterators</td>
<td>scrapy.utils.iterators</td>
</tr>
<tr class="row-even"><td>scrapy.contrib.downloadermiddleware</td>
<td>scrapy.downloadermiddlewares</td>
</tr>
<tr class="row-odd"><td>scrapy.contrib.exporter</td>
<td>scrapy.exporters</td>
</tr>
<tr class="row-even"><td>scrapy.contrib.linkextractors</td>
<td>scrapy.linkextractors</td>
</tr>
<tr class="row-odd"><td>scrapy.contrib.loader</td>
<td>scrapy.loader</td>
</tr>
<tr class="row-even"><td>scrapy.contrib.loader.processor</td>
<td>scrapy.loader.processors</td>
</tr>
<tr class="row-odd"><td>scrapy.contrib.pipeline</td>
<td>scrapy.pipelines</td>
</tr>
<tr class="row-even"><td>scrapy.contrib.spidermiddleware</td>
<td>scrapy.spidermiddlewares</td>
</tr>
<tr class="row-odd"><td>scrapy.contrib.spiders</td>
<td>scrapy.spiders</td>
</tr>
<tr class="row-even"><td><ul class="first last simple">
<li>scrapy.contrib.closespider</li>
<li>scrapy.contrib.corestats</li>
<li>scrapy.contrib.debug</li>
<li>scrapy.contrib.feedexport</li>
<li>scrapy.contrib.httpcache</li>
<li>scrapy.contrib.logstats</li>
<li>scrapy.contrib.memdebug</li>
<li>scrapy.contrib.memusage</li>
<li>scrapy.contrib.spiderstate</li>
<li>scrapy.contrib.statsmailer</li>
<li>scrapy.contrib.throttle</li>
</ul>
</td>
<td>scrapy.extensions.*</td>
</tr>
</tbody>
</table>
<p>Plural renames and Modules unification</p>
<table border="1" class="docutils">
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Old location</th>
<th class="head">New location</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>scrapy.command</td>
<td>scrapy.commands</td>
</tr>
<tr class="row-odd"><td>scrapy.dupefilter</td>
<td>scrapy.dupefilters</td>
</tr>
<tr class="row-even"><td>scrapy.linkextractor</td>
<td>scrapy.linkextractors</td>
</tr>
<tr class="row-odd"><td>scrapy.spider</td>
<td>scrapy.spiders</td>
</tr>
<tr class="row-even"><td>scrapy.squeue</td>
<td>scrapy.squeues</td>
</tr>
<tr class="row-odd"><td>scrapy.statscol</td>
<td>scrapy.statscollectors</td>
</tr>
<tr class="row-even"><td>scrapy.utils.decorator</td>
<td>scrapy.utils.decorators</td>
</tr>
</tbody>
</table>
<p>Class renames</p>
<table border="1" class="docutils">
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Old location</th>
<th class="head">New location</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>scrapy.spidermanager.SpiderManager</td>
<td>scrapy.spiderloader.SpiderLoader</td>
</tr>
</tbody>
</table>
<p>Settings renames</p>
<table border="1" class="docutils">
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Old location</th>
<th class="head">New location</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>SPIDER_MANAGER_CLASS</td>
<td>SPIDER_LOADER_CLASS</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="changelog">
<h5>Changelog<a class="headerlink" href="#changelog" title="Permalink to this headline">¶</a></h5>
<p>New Features and Enhancements</p>
<ul class="simple">
<li>Python logging (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1060">issue 1060</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1235">issue 1235</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1236">issue 1236</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1240">issue 1240</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1259">issue 1259</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1278">issue 1278</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1286">issue 1286</a>)</li>
<li>FEED_EXPORT_FIELDS option (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1159">issue 1159</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1224">issue 1224</a>)</li>
<li>Dns cache size and timeout options (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1132">issue 1132</a>)</li>
<li>support namespace prefix in xmliter_lxml (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/963">issue 963</a>)</li>
<li>Reactor threadpool max size setting (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1123">issue 1123</a>)</li>
<li>Allow spiders to return dicts. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1081">issue 1081</a>)</li>
<li>Add Response.urljoin() helper (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1086">issue 1086</a>)</li>
<li>look in ~/.config/scrapy.cfg for user config (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1098">issue 1098</a>)</li>
<li>handle TLS SNI (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1101">issue 1101</a>)</li>
<li>Selectorlist extract first (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/624">issue 624</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1145">issue 1145</a>)</li>
<li>Added JmesSelect (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1016">issue 1016</a>)</li>
<li>add gzip compression to filesystem http cache backend (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1020">issue 1020</a>)</li>
<li>CSS support in link extractors (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/983">issue 983</a>)</li>
<li>httpcache dont_cache meta #19 #689 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/821">issue 821</a>)</li>
<li>add signal to be sent when request is dropped by the scheduler
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/961">issue 961</a>)</li>
<li>avoid download large response (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/946">issue 946</a>)</li>
<li>Allow to specify the quotechar in CSVFeedSpider (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/882">issue 882</a>)</li>
<li>Add referer to “Spider error processing” log message (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/795">issue 795</a>)</li>
<li>process robots.txt once (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/896">issue 896</a>)</li>
<li>GSoC Per-spider settings (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/854">issue 854</a>)</li>
<li>Add project name validation (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/817">issue 817</a>)</li>
<li>GSoC API cleanup (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/816">issue 816</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1128">issue 1128</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1147">issue 1147</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1148">issue 1148</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1156">issue 1156</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1185">issue 1185</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1187">issue 1187</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1258">issue 1258</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1268">issue 1268</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1276">issue 1276</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1285">issue 1285</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1284">issue 1284</a>)</li>
<li>Be more responsive with IO operations (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1074">issue 1074</a> and <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1075">issue 1075</a>)</li>
<li>Do leveldb compaction for httpcache on closing (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1297">issue 1297</a>)</li>
</ul>
<p>Deprecations and Removals</p>
<ul class="simple">
<li>Deprecate htmlparser link extractor (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1205">issue 1205</a>)</li>
<li>remove deprecated code from FeedExporter (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1155">issue 1155</a>)</li>
<li>a leftover for.15 compatibility (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/925">issue 925</a>)</li>
<li>drop support for CONCURRENT_REQUESTS_PER_SPIDER (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/895">issue 895</a>)</li>
<li>Drop old engine code (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/911">issue 911</a>)</li>
<li>Deprecate SgmlLinkExtractor (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/777">issue 777</a>)</li>
</ul>
<p>Relocations</p>
<ul class="simple">
<li>Move exporters/__init__.py to exporters.py (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1242">issue 1242</a>)</li>
<li>Move base classes to their packages (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1218">issue 1218</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1233">issue 1233</a>)</li>
<li>Module relocation (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1181">issue 1181</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1210">issue 1210</a>)</li>
<li>rename SpiderManager to SpiderLoader (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1166">issue 1166</a>)</li>
<li>Remove djangoitem (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1177">issue 1177</a>)</li>
<li>remove scrapy deploy command (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1102">issue 1102</a>)</li>
<li>dissolve contrib_exp (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1134">issue 1134</a>)</li>
<li>Deleted bin folder from root, fixes #913 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/914">issue 914</a>)</li>
<li>Remove jsonrpc based webservice (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/859">issue 859</a>)</li>
<li>Move Test cases under project root dir (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/827">issue 827</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/841">issue 841</a>)</li>
<li>Fix backward incompatibility for relocated paths in settings
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1267">issue 1267</a>)</li>
</ul>
<p>Documentation</p>
<ul class="simple">
<li>CrawlerProcess documentation (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1190">issue 1190</a>)</li>
<li>Favoring web scraping over screen scraping in the descriptions
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1188">issue 1188</a>)</li>
<li>Some improvements for Scrapy tutorial (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1180">issue 1180</a>)</li>
<li>Documenting Files Pipeline together with Images Pipeline (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1150">issue 1150</a>)</li>
<li>deployment docs tweaks (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1164">issue 1164</a>)</li>
<li>Added deployment section covering scrapyd-deploy and shub (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1124">issue 1124</a>)</li>
<li>Adding more settings to project template (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1073">issue 1073</a>)</li>
<li>some improvements to overview page (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1106">issue 1106</a>)</li>
<li>Updated link in docs/topics/architecture.rst (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/647">issue 647</a>)</li>
<li>DOC reorder topics (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1022">issue 1022</a>)</li>
<li>updating list of Request.meta special keys (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1071">issue 1071</a>)</li>
<li>DOC document download_timeout (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/898">issue 898</a>)</li>
<li>DOC simplify extension docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/893">issue 893</a>)</li>
<li>Leaks docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/894">issue 894</a>)</li>
<li>DOC document from_crawler method for item pipelines (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/904">issue 904</a>)</li>
<li>Spider_error doesn’t support deferreds (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1292">issue 1292</a>)</li>
<li>Corrections &amp; Sphinx related fixes (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1220">issue 1220</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1219">issue 1219</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1196">issue 1196</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1172">issue 1172</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1171">issue 1171</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1169">issue 1169</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1160">issue 1160</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1154">issue 1154</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1127">issue 1127</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1112">issue 1112</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1105">issue 1105</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1041">issue 1041</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1082">issue 1082</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1033">issue 1033</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/944">issue 944</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/866">issue 866</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/864">issue 864</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/796">issue 796</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1260">issue 1260</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1271">issue 1271</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1293">issue 1293</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1298">issue 1298</a>)</li>
</ul>
<p>Bugfixes</p>
<ul class="simple">
<li>Item multi inheritance fix (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/353">issue 353</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/1228">issue 1228</a>)</li>
<li>ItemLoader.load_item: iterate over copy of fields (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/722">issue 722</a>)</li>
<li>Fix Unhandled error in Deferred (RobotsTxtMiddleware) (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1131">issue 1131</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1197">issue 1197</a>)</li>
<li>Force to read DOWNLOAD_TIMEOUT as int (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/954">issue 954</a>)</li>
<li>scrapy.utils.misc.load_object should print full traceback (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/902">issue 902</a>)</li>
<li>Fix bug for “.local” host name (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/878">issue 878</a>)</li>
<li>Fix for Enabled extensions, middlewares, pipelines info not printed
anymore (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/879">issue 879</a>)</li>
<li>fix dont_merge_cookies bad behaviour when set to false on meta
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/846">issue 846</a>)</li>
</ul>
<p>Python 3 In Progress Support</p>
<ul class="simple">
<li>disable scrapy.telnet if twisted.conch is not available (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1161">issue 1161</a>)</li>
<li>fix Python 3 syntax errors in ajaxcrawl.py (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1162">issue 1162</a>)</li>
<li>more python3 compatibility changes for urllib (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1121">issue 1121</a>)</li>
<li>assertItemsEqual was renamed to assertCountEqual in Python 3.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1070">issue 1070</a>)</li>
<li>Import unittest.mock if available. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1066">issue 1066</a>)</li>
<li>updated deprecated cgi.parse_qsl to use six’s parse_qsl (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/909">issue 909</a>)</li>
<li>Prevent Python 3 port regressions (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/830">issue 830</a>)</li>
<li>PY3: use MutableMapping for python 3 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/810">issue 810</a>)</li>
<li>PY3: use six.BytesIO and six.moves.cStringIO (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/803">issue 803</a>)</li>
<li>PY3: fix xmlrpclib and email imports (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/801">issue 801</a>)</li>
<li>PY3: use six for robotparser and urlparse (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/800">issue 800</a>)</li>
<li>PY3: use six.iterkeys, six.iteritems, and tempfile (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/799">issue 799</a>)</li>
<li>PY3: fix has_key and use six.moves.configparser (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/798">issue 798</a>)</li>
<li>PY3: use six.moves.cPickle (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/797">issue 797</a>)</li>
<li>PY3 make it possible to run some tests in Python3 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/776">issue 776</a>)</li>
</ul>
<p>Tests</p>
<ul class="simple">
<li>remove unnecessary lines from py3-ignores (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1243">issue 1243</a>)</li>
<li>Fix remaining warnings from pytest while collecting tests (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1206">issue 1206</a>)</li>
<li>Add docs build to travis (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1234">issue 1234</a>)</li>
<li>TST don’t collect tests from deprecated modules. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1165">issue 1165</a>)</li>
<li>install service_identity package in tests to prevent warnings
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1168">issue 1168</a>)</li>
<li>Fix deprecated settings API in tests (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1152">issue 1152</a>)</li>
<li>Add test for webclient with POST method and no body given (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1089">issue 1089</a>)</li>
<li>py3-ignores.txt supports comments (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1044">issue 1044</a>)</li>
<li>modernize some of the asserts (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/835">issue 835</a>)</li>
<li>selector.__repr__ test (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/779">issue 779</a>)</li>
</ul>
<p>Code refactoring</p>
<ul class="simple">
<li>CSVFeedSpider cleanup: use iterate_spider_output (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1079">issue 1079</a>)</li>
<li>remove unnecessary check from scrapy.utils.spider.iter_spider_output
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/1078">issue 1078</a>)</li>
<li>Pydispatch pep8 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/992">issue 992</a>)</li>
<li>Removed unused ‘load=False’ parameter from walk_modules() (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/871">issue 871</a>)</li>
<li>For consistency, use <code class="docutils literal notranslate"><span class="pre">job_dir</span></code> helper in <code class="docutils literal notranslate"><span class="pre">SpiderState</span></code> extension.
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/805">issue 805</a>)</li>
<li>rename “sflo” local variables to less cryptic “log_observer” (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/775">issue 775</a>)</li>
</ul>
</div>
</div>
<div class="section" id="scrapy-0-24-6-2015-04-20">
<h4>Scrapy 0.24.6 (2015-04-20)<a class="headerlink" href="#scrapy-0-24-6-2015-04-20" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>encode invalid xpath with unicode_escape under PY2 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/07cb3e5">commit 07cb3e5</a>)</li>
<li>fix IPython shell scope issue and load IPython user config (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/2c8e573">commit 2c8e573</a>)</li>
<li>Fix small typo in the docs (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/d694019">commit d694019</a>)</li>
<li>Fix small typo (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/f92fa83">commit f92fa83</a>)</li>
<li>Converted sel.xpath() calls to response.xpath() in Extracting the data (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c2c6d15">commit c2c6d15</a>)</li>
</ul>
</div>
<div class="section" id="scrapy-0-24-5-2015-02-25">
<h4>Scrapy 0.24.5 (2015-02-25)<a class="headerlink" href="#scrapy-0-24-5-2015-02-25" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Support new _getEndpoint Agent signatures on Twisted 15.0.0 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/540b9bc">commit 540b9bc</a>)</li>
<li>DOC a couple more references are fixed (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b4c454b">commit b4c454b</a>)</li>
<li>DOC fix a reference (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/e3c1260">commit e3c1260</a>)</li>
<li>t.i.b.ThreadedResolver is now a new-style class (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/9e13f42">commit 9e13f42</a>)</li>
<li>S3DownloadHandler: fix auth for requests with quoted paths/query params (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/cdb9a0b">commit cdb9a0b</a>)</li>
<li>fixed the variable types in mailsender documentation (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/bb3a848">commit bb3a848</a>)</li>
<li>Reset items_scraped instead of item_count (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/edb07a4">commit edb07a4</a>)</li>
<li>Tentative attention message about what document to read for contributions (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/7ee6f7a">commit 7ee6f7a</a>)</li>
<li>mitmproxy 0.10.1 needs netlib 0.10.1 too (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/874fcdd">commit 874fcdd</a>)</li>
<li>pin mitmproxy 0.10.1 as &gt;0.11 does not work with tests (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c6b21f0">commit c6b21f0</a>)</li>
<li>Test the parse command locally instead of against an external url (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c3a6628">commit c3a6628</a>)</li>
<li>Patches Twisted issue while closing the connection pool on HTTPDownloadHandler (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/d0bf957">commit d0bf957</a>)</li>
<li>Updates documentation on dynamic item classes. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/eeb589a">commit eeb589a</a>)</li>
<li>Merge pull request #943 from Lazar-T/patch-3 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5fdab02">commit 5fdab02</a>)</li>
<li>typo (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b0ae199">commit b0ae199</a>)</li>
<li>pywin32 is required by Twisted. closes #937 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5cb0cfb">commit 5cb0cfb</a>)</li>
<li>Update install.rst (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/781286b">commit 781286b</a>)</li>
<li>Merge pull request #928 from Lazar-T/patch-1 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b415d04">commit b415d04</a>)</li>
<li>comma instead of fullstop (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/627b9ba">commit 627b9ba</a>)</li>
<li>Merge pull request #885 from jsma/patch-1 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/de909ad">commit de909ad</a>)</li>
<li>Update request-response.rst (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/3f3263d">commit 3f3263d</a>)</li>
<li>SgmlLinkExtractor - fix for parsing &lt;area&gt; tag with Unicode present (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/49b40f0">commit 49b40f0</a>)</li>
</ul>
</div>
<div class="section" id="scrapy-0-24-4-2014-08-09">
<h4>Scrapy 0.24.4 (2014-08-09)<a class="headerlink" href="#scrapy-0-24-4-2014-08-09" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>pem file is used by mockserver and required by scrapy bench (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5eddc68">commit 5eddc68</a>)</li>
<li>scrapy bench needs scrapy.tests* (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/d6cb999">commit d6cb999</a>)</li>
</ul>
</div>
<div class="section" id="scrapy-0-24-3-2014-08-09">
<h4>Scrapy 0.24.3 (2014-08-09)<a class="headerlink" href="#scrapy-0-24-3-2014-08-09" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>no need to waste travis-ci time on py3 for 0.24 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8e080c1">commit 8e080c1</a>)</li>
<li>Update installation docs (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1d0c096">commit 1d0c096</a>)</li>
<li>There is a trove classifier for Scrapy framework! (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/4c701d7">commit 4c701d7</a>)</li>
<li>update other places where w3lib version is mentioned (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/d109c13">commit d109c13</a>)</li>
<li>Update w3lib requirement to 1.8.0 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/39d2ce5">commit 39d2ce5</a>)</li>
<li>Use w3lib.html.replace_entities() (remove_entities() is deprecated) (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/180d3ad">commit 180d3ad</a>)</li>
<li>set zip_safe=False (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/a51ee8b">commit a51ee8b</a>)</li>
<li>do not ship tests package (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/ee3b371">commit ee3b371</a>)</li>
<li>scrapy.bat is not needed anymore (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c3861cf">commit c3861cf</a>)</li>
<li>Modernize setup.py (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/362e322">commit 362e322</a>)</li>
<li>headers can not handle non-string values (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/94a5c65">commit 94a5c65</a>)</li>
<li>fix ftp test cases (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/a274a7f">commit a274a7f</a>)</li>
<li>The sum up of travis-ci builds are taking like 50min to complete (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/ae1e2cc">commit ae1e2cc</a>)</li>
<li>Update shell.rst typo (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/e49c96a">commit e49c96a</a>)</li>
<li>removes weird indentation in the shell results (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1ca489d">commit 1ca489d</a>)</li>
<li>improved explanations, clarified blog post as source, added link for XPath string functions in the spec (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/65c8f05">commit 65c8f05</a>)</li>
<li>renamed UserTimeoutError and ServerTimeouterror #583 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/037f6ab">commit 037f6ab</a>)</li>
<li>adding some xpath tips to selectors docs (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/2d103e0">commit 2d103e0</a>)</li>
<li>fix tests to account for <a class="reference external" href="https://github.com/scrapy/w3lib/pull/23">https://github.com/scrapy/w3lib/pull/23</a> (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/f8d366a">commit f8d366a</a>)</li>
<li>get_func_args maximum recursion fix #728 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/81344ea">commit 81344ea</a>)</li>
<li>Updated input/ouput processor example according to #560. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/f7c4ea8">commit f7c4ea8</a>)</li>
<li>Fixed Python syntax in tutorial. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/db59ed9">commit db59ed9</a>)</li>
<li>Add test case for tunneling proxy (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/f090260">commit f090260</a>)</li>
<li>Bugfix for leaking Proxy-Authorization header to remote host when using tunneling (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/d8793af">commit d8793af</a>)</li>
<li>Extract links from XHTML documents with MIME-Type “application/xml” (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/ed1f376">commit ed1f376</a>)</li>
<li>Merge pull request #793 from roysc/patch-1 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/91a1106">commit 91a1106</a>)</li>
<li>Fix typo in commands.rst (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/743e1e2">commit 743e1e2</a>)</li>
<li>better testcase for settings.overrides.setdefault (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/e22daaf">commit e22daaf</a>)</li>
<li>Using CRLF as line marker according to http 1.1 definition (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5ec430b">commit 5ec430b</a>)</li>
</ul>
</div>
<div class="section" id="scrapy-0-24-2-2014-07-08">
<h4>Scrapy 0.24.2 (2014-07-08)<a class="headerlink" href="#scrapy-0-24-2-2014-07-08" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Use a mutable mapping to proxy deprecated settings.overrides and settings.defaults attribute (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/e5e8133">commit e5e8133</a>)</li>
<li>there is not support for python3 yet (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/3cd6146">commit 3cd6146</a>)</li>
<li>Update python compatible version set to debian packages (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/fa5d76b">commit fa5d76b</a>)</li>
<li>DOC fix formatting in release notes (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c6a9e20">commit c6a9e20</a>)</li>
</ul>
</div>
<div class="section" id="scrapy-0-24-1-2014-06-27">
<h4>Scrapy 0.24.1 (2014-06-27)<a class="headerlink" href="#scrapy-0-24-1-2014-06-27" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Fix deprecated CrawlerSettings and increase backward compatibility with
.defaults attribute (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8e3f20a">commit 8e3f20a</a>)</li>
</ul>
</div>
<div class="section" id="scrapy-0-24-0-2014-06-26">
<h4>Scrapy 0.24.0 (2014-06-26)<a class="headerlink" href="#scrapy-0-24-0-2014-06-26" title="Permalink to this headline">¶</a></h4>
<div class="section" id="enhancements">
<h5>Enhancements<a class="headerlink" href="#enhancements" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Improve Scrapy top-level namespace (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/494">issue 494</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/684">issue 684</a>)</li>
<li>Add selector shortcuts to responses (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/554">issue 554</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/690">issue 690</a>)</li>
<li>Add new lxml based LinkExtractor to replace unmantained SgmlLinkExtractor
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/559">issue 559</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/761">issue 761</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/763">issue 763</a>)</li>
<li>Cleanup settings API - part of per-spider settings <strong>GSoC project</strong> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/737">issue 737</a>)</li>
<li>Add UTF8 encoding header to templates (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/688">issue 688</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/762">issue 762</a>)</li>
<li>Telnet console now binds to 127.0.0.1 by default (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/699">issue 699</a>)</li>
<li>Update debian/ubuntu install instructions (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/509">issue 509</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/549">issue 549</a>)</li>
<li>Disable smart strings in lxml XPath evaluations (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/535">issue 535</a>)</li>
<li>Restore filesystem based cache as default for http
cache middleware (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/541">issue 541</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/500">issue 500</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/571">issue 571</a>)</li>
<li>Expose current crawler in Scrapy shell (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/557">issue 557</a>)</li>
<li>Improve testsuite comparing CSV and XML exporters (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/570">issue 570</a>)</li>
<li>New <code class="docutils literal notranslate"><span class="pre">offsite/filtered</span></code> and <code class="docutils literal notranslate"><span class="pre">offsite/domains</span></code> stats (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/566">issue 566</a>)</li>
<li>Support process_links as generator in CrawlSpider (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/555">issue 555</a>)</li>
<li>Verbose logging and new stats counters for DupeFilter (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/553">issue 553</a>)</li>
<li>Add a mimetype parameter to <code class="docutils literal notranslate"><span class="pre">MailSender.send()</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/602">issue 602</a>)</li>
<li>Generalize file pipeline log messages (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/622">issue 622</a>)</li>
<li>Replace unencodeable codepoints with html entities in SGMLLinkExtractor (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/565">issue 565</a>)</li>
<li>Converted SEP documents to rst format (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/629">issue 629</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/630">issue 630</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/638">issue 638</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/632">issue 632</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/636">issue 636</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/640">issue 640</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/635">issue 635</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/634">issue 634</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/639">issue 639</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/637">issue 637</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/631">issue 631</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/633">issue 633</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/641">issue 641</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/642">issue 642</a>)</li>
<li>Tests and docs for clickdata’s nr index in FormRequest (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/646">issue 646</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/645">issue 645</a>)</li>
<li>Allow to disable a downloader handler just like any other component (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/650">issue 650</a>)</li>
<li>Log when a request is discarded after too many redirections (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/654">issue 654</a>)</li>
<li>Log error responses if they are not handled by spider callbacks
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/612">issue 612</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/656">issue 656</a>)</li>
<li>Add content-type check to http compression mw (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/193">issue 193</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/660">issue 660</a>)</li>
<li>Run pypy tests using latest pypi from ppa (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/674">issue 674</a>)</li>
<li>Run test suite using pytest instead of trial (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/679">issue 679</a>)</li>
<li>Build docs and check for dead links in tox environment (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/687">issue 687</a>)</li>
<li>Make scrapy.version_info a tuple of integers (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/681">issue 681</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/692">issue 692</a>)</li>
<li>Infer exporter’s output format from filename extensions
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/546">issue 546</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/659">issue 659</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/760">issue 760</a>)</li>
<li>Support case-insensitive domains in <code class="docutils literal notranslate"><span class="pre">url_is_from_any_domain()</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/693">issue 693</a>)</li>
<li>Remove pep8 warnings in project and spider templates (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/698">issue 698</a>)</li>
<li>Tests and docs for <code class="docutils literal notranslate"><span class="pre">request_fingerprint</span></code> function (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/597">issue 597</a>)</li>
<li>Update SEP-19 for GSoC project <code class="docutils literal notranslate"><span class="pre">per-spider</span> <span class="pre">settings</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/705">issue 705</a>)</li>
<li>Set exit code to non-zero when contracts fails (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/727">issue 727</a>)</li>
<li>Add a setting to control what class is instanciated as Downloader component
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/738">issue 738</a>)</li>
<li>Pass response in <code class="docutils literal notranslate"><span class="pre">item_dropped</span></code> signal (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/724">issue 724</a>)</li>
<li>Improve <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">check</span></code> contracts command (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/733">issue 733</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/752">issue 752</a>)</li>
<li>Document <code class="docutils literal notranslate"><span class="pre">spider.closed()</span></code> shortcut (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/719">issue 719</a>)</li>
<li>Document <code class="docutils literal notranslate"><span class="pre">request_scheduled</span></code> signal (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/746">issue 746</a>)</li>
<li>Add a note about reporting security issues (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/697">issue 697</a>)</li>
<li>Add LevelDB http cache storage backend (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/626">issue 626</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/500">issue 500</a>)</li>
<li>Sort spider list output of <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">list</span></code> command (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/742">issue 742</a>)</li>
<li>Multiple documentation enhancemens and fixes
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/575">issue 575</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/587">issue 587</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/590">issue 590</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/596">issue 596</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/610">issue 610</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/617">issue 617</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/618">issue 618</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/627">issue 627</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/613">issue 613</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/643">issue 643</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/654">issue 654</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/675">issue 675</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/663">issue 663</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/711">issue 711</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/714">issue 714</a>)</li>
</ul>
</div>
<div class="section" id="id32">
<h5>Bugfixes<a class="headerlink" href="#id32" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Encode unicode URL value when creating Links in RegexLinkExtractor (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/561">issue 561</a>)</li>
<li>Ignore None values in ItemLoader processors (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/556">issue 556</a>)</li>
<li>Fix link text when there is an inner tag in SGMLLinkExtractor and
HtmlParserLinkExtractor (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/485">issue 485</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/574">issue 574</a>)</li>
<li>Fix wrong checks on subclassing of deprecated classes
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/581">issue 581</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/584">issue 584</a>)</li>
<li>Handle errors caused by inspect.stack() failures (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/582">issue 582</a>)</li>
<li>Fix a reference to unexistent engine attribute (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/593">issue 593</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/594">issue 594</a>)</li>
<li>Fix dynamic itemclass example usage of type() (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/603">issue 603</a>)</li>
<li>Use lucasdemarchi/codespell to fix typos (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/628">issue 628</a>)</li>
<li>Fix default value of attrs argument in SgmlLinkExtractor to be tuple (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/661">issue 661</a>)</li>
<li>Fix XXE flaw in sitemap reader (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/676">issue 676</a>)</li>
<li>Fix engine to support filtered start requests (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/707">issue 707</a>)</li>
<li>Fix offsite middleware case on urls with no hostnames (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/745">issue 745</a>)</li>
<li>Testsuite doesn’t require PIL anymore (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/585">issue 585</a>)</li>
</ul>
</div>
</div>
<div class="section" id="scrapy-0-22-2-released-2014-02-14">
<h4>Scrapy 0.22.2 (released 2014-02-14)<a class="headerlink" href="#scrapy-0-22-2-released-2014-02-14" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>fix a reference to unexistent engine.slots. closes #593 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/13c099a">commit 13c099a</a>)</li>
<li>downloaderMW doc typo (spiderMW doc copy remnant) (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8ae11bf">commit 8ae11bf</a>)</li>
<li>Correct typos (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1346037">commit 1346037</a>)</li>
</ul>
</div>
<div class="section" id="scrapy-0-22-1-released-2014-02-08">
<h4>Scrapy 0.22.1 (released 2014-02-08)<a class="headerlink" href="#scrapy-0-22-1-released-2014-02-08" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>localhost666 can resolve under certain circumstances (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/2ec2279">commit 2ec2279</a>)</li>
<li>test inspect.stack failure (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/cc3eda3">commit cc3eda3</a>)</li>
<li>Handle cases when inspect.stack() fails (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8cb44f9">commit 8cb44f9</a>)</li>
<li>Fix wrong checks on subclassing of deprecated classes. closes #581 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/46d98d6">commit 46d98d6</a>)</li>
<li>Docs: 4-space indent for final spider example (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/13846de">commit 13846de</a>)</li>
<li>Fix HtmlParserLinkExtractor and tests after #485 merge (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/368a946">commit 368a946</a>)</li>
<li>BaseSgmlLinkExtractor: Fixed the missing space when the link has an inner tag (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b566388">commit b566388</a>)</li>
<li>BaseSgmlLinkExtractor: Added unit test of a link with an inner tag (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c1cb418">commit c1cb418</a>)</li>
<li>BaseSgmlLinkExtractor: Fixed unknown_endtag() so that it only set current_link=None when the end tag match the opening tag (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/7e4d627">commit 7e4d627</a>)</li>
<li>Fix tests for Travis-CI build (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/76c7e20">commit 76c7e20</a>)</li>
<li>replace unencodeable codepoints with html entities. fixes #562 and #285 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5f87b17">commit 5f87b17</a>)</li>
<li>RegexLinkExtractor: encode URL unicode value when creating Links (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/d0ee545">commit d0ee545</a>)</li>
<li>Updated the tutorial crawl output with latest output. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8da65de">commit 8da65de</a>)</li>
<li>Updated shell docs with the crawler reference and fixed the actual shell output. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/875b9ab">commit 875b9ab</a>)</li>
<li>PEP8 minor edits. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/f89efaf">commit f89efaf</a>)</li>
<li>Expose current crawler in the scrapy shell. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5349cec">commit 5349cec</a>)</li>
<li>Unused re import and PEP8 minor edits. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/387f414">commit 387f414</a>)</li>
<li>Ignore None’s values when using the ItemLoader. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/0632546">commit 0632546</a>)</li>
<li>DOC Fixed HTTPCACHE_STORAGE typo in the default value which is now Filesystem instead Dbm. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/cde9a8c">commit cde9a8c</a>)</li>
<li>show ubuntu setup instructions as literal code (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/fb5c9c5">commit fb5c9c5</a>)</li>
<li>Update Ubuntu installation instructions (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/70fb105">commit 70fb105</a>)</li>
<li>Merge pull request #550 from stray-leone/patch-1 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/6f70b6a">commit 6f70b6a</a>)</li>
<li>modify the version of scrapy ubuntu package (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/725900d">commit 725900d</a>)</li>
<li>fix 0.22.0 release date (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/af0219a">commit af0219a</a>)</li>
<li>fix typos in news.rst and remove (not released yet) header (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b7f58f4">commit b7f58f4</a>)</li>
</ul>
</div>
<div class="section" id="scrapy-0-22-0-released-2014-01-17">
<h4>Scrapy 0.22.0 (released 2014-01-17)<a class="headerlink" href="#scrapy-0-22-0-released-2014-01-17" title="Permalink to this headline">¶</a></h4>
<div class="section" id="id33">
<h5>Enhancements<a class="headerlink" href="#id33" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>[<strong>Backward incompatible</strong>] Switched HTTPCacheMiddleware backend to filesystem (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/541">issue 541</a>)
To restore old backend set <code class="docutils literal notranslate"><span class="pre">HTTPCACHE_STORAGE</span></code> to <code class="docutils literal notranslate"><span class="pre">scrapy.contrib.httpcache.DbmCacheStorage</span></code></li>
<li>Proxy https:// urls using CONNECT method (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/392">issue 392</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/397">issue 397</a>)</li>
<li>Add a middleware to crawl ajax crawleable pages as defined by google (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/343">issue 343</a>)</li>
<li>Rename scrapy.spider.BaseSpider to scrapy.spider.Spider (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/510">issue 510</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/519">issue 519</a>)</li>
<li>Selectors register EXSLT namespaces by default (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/472">issue 472</a>)</li>
<li>Unify item loaders similar to selectors renaming (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/461">issue 461</a>)</li>
<li>Make <code class="docutils literal notranslate"><span class="pre">RFPDupeFilter</span></code> class easily subclassable (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/533">issue 533</a>)</li>
<li>Improve test coverage and forthcoming Python 3 support (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/525">issue 525</a>)</li>
<li>Promote startup info on settings and middleware to INFO level (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/520">issue 520</a>)</li>
<li>Support partials in <code class="docutils literal notranslate"><span class="pre">get_func_args</span></code> util (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/506">issue 506</a>, issue:<cite>504</cite>)</li>
<li>Allow running indiviual tests via tox (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/503">issue 503</a>)</li>
<li>Update extensions ignored by link extractors (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/498">issue 498</a>)</li>
<li>Add middleware methods to get files/images/thumbs paths (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/490">issue 490</a>)</li>
<li>Improve offsite middleware tests (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/478">issue 478</a>)</li>
<li>Add a way to skip default Referer header set by RefererMiddleware (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/475">issue 475</a>)</li>
<li>Do not send <code class="docutils literal notranslate"><span class="pre">x-gzip</span></code> in default <code class="docutils literal notranslate"><span class="pre">Accept-Encoding</span></code> header (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/469">issue 469</a>)</li>
<li>Support defining http error handling using settings (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/466">issue 466</a>)</li>
<li>Use modern python idioms wherever you find legacies (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/497">issue 497</a>)</li>
<li>Improve and correct documentation
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/527">issue 527</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/524">issue 524</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/521">issue 521</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/517">issue 517</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/512">issue 512</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/505">issue 505</a>,
<a class="reference external" href="https://github.com/scrapy/scrapy/issues/502">issue 502</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/489">issue 489</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/465">issue 465</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/460">issue 460</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/425">issue 425</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/536">issue 536</a>)</li>
</ul>
</div>
<div class="section" id="fixes">
<h5>Fixes<a class="headerlink" href="#fixes" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Update Selector class imports in CrawlSpider template (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/484">issue 484</a>)</li>
<li>Fix unexistent reference to <code class="docutils literal notranslate"><span class="pre">engine.slots</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/464">issue 464</a>)</li>
<li>Do not try to call <code class="docutils literal notranslate"><span class="pre">body_as_unicode()</span></code> on a non-TextResponse instance (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/462">issue 462</a>)</li>
<li>Warn when subclassing XPathItemLoader, previously it only warned on
instantiation. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/523">issue 523</a>)</li>
<li>Warn when subclassing XPathSelector, previously it only warned on
instantiation. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/537">issue 537</a>)</li>
<li>Multiple fixes to memory stats (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/531">issue 531</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/530">issue 530</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/529">issue 529</a>)</li>
<li>Fix overriding url in <code class="docutils literal notranslate"><span class="pre">FormRequest.from_response()</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/507">issue 507</a>)</li>
<li>Fix tests runner under pip 1.5 (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/513">issue 513</a>)</li>
<li>Fix logging error when spider name is unicode (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/479">issue 479</a>)</li>
</ul>
</div>
</div>
<div class="section" id="scrapy-0-20-2-released-2013-12-09">
<h4>Scrapy 0.20.2 (released 2013-12-09)<a class="headerlink" href="#scrapy-0-20-2-released-2013-12-09" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Update CrawlSpider Template with Selector changes (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/6d1457d">commit 6d1457d</a>)</li>
<li>fix method name in tutorial. closes GH-480 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b4fc359">commit b4fc359</a></li>
</ul>
</div>
<div class="section" id="scrapy-0-20-1-released-2013-11-28">
<h4>Scrapy 0.20.1 (released 2013-11-28)<a class="headerlink" href="#scrapy-0-20-1-released-2013-11-28" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>include_package_data is required to build wheels from published sources (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5ba1ad5">commit 5ba1ad5</a>)</li>
<li>process_parallel was leaking the failures on its internal deferreds.  closes #458 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/419a780">commit 419a780</a>)</li>
</ul>
</div>
<div class="section" id="scrapy-0-20-0-released-2013-11-08">
<h4>Scrapy 0.20.0 (released 2013-11-08)<a class="headerlink" href="#scrapy-0-20-0-released-2013-11-08" title="Permalink to this headline">¶</a></h4>
<div class="section" id="id34">
<h5>Enhancements<a class="headerlink" href="#id34" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>New Selector’s API including CSS selectors (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/395">issue 395</a> and <a class="reference external" href="https://github.com/scrapy/scrapy/issues/426">issue 426</a>),</li>
<li>Request/Response url/body attributes are now immutable
(modifying them had been deprecated for a long time)</li>
<li><a class="reference internal" href="index.html#std:setting-ITEM_PIPELINES"><code class="xref std std-setting docutils literal notranslate"><span class="pre">ITEM_PIPELINES</span></code></a> is now defined as a dict (instead of a list)</li>
<li>Sitemap spider can fetch alternate URLs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/360">issue 360</a>)</li>
<li><code class="docutils literal notranslate"><span class="pre">Selector.remove_namespaces()</span></code> now remove namespaces from element’s attributes. (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/416">issue 416</a>)</li>
<li>Paved the road for Python 3.3+ (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/435">issue 435</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/436">issue 436</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/431">issue 431</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/452">issue 452</a>)</li>
<li>New item exporter using native python types with nesting support (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/366">issue 366</a>)</li>
<li>Tune HTTP1.1 pool size so it matches concurrency defined by settings (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b43b5f575">commit b43b5f575</a>)</li>
<li>scrapy.mail.MailSender now can connect over TLS or upgrade using STARTTLS (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/327">issue 327</a>)</li>
<li>New FilesPipeline with functionality factored out from ImagesPipeline (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/370">issue 370</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/409">issue 409</a>)</li>
<li>Recommend Pillow instead of PIL for image handling (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/317">issue 317</a>)</li>
<li>Added debian packages for Ubuntu quantal and raring (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/86230c0">commit 86230c0</a>)</li>
<li>Mock server (used for tests) can listen for HTTPS requests (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/410">issue 410</a>)</li>
<li>Remove multi spider support from multiple core components
(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/422">issue 422</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/421">issue 421</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/420">issue 420</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/419">issue 419</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/423">issue 423</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/418">issue 418</a>)</li>
<li>Travis-CI now tests Scrapy changes against development versions of <code class="docutils literal notranslate"><span class="pre">w3lib</span></code> and <code class="docutils literal notranslate"><span class="pre">queuelib</span></code> python packages.</li>
<li>Add pypy 2.1 to continuous integration tests (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/ecfa7431">commit ecfa7431</a>)</li>
<li>Pylinted, pep8 and removed old-style exceptions from source (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/430">issue 430</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/432">issue 432</a>)</li>
<li>Use importlib for parametric imports (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/445">issue 445</a>)</li>
<li>Handle a regression introduced in Python 2.7.5 that affects XmlItemExporter (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/372">issue 372</a>)</li>
<li>Bugfix crawling shutdown on SIGINT (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/450">issue 450</a>)</li>
<li>Do not submit <code class="docutils literal notranslate"><span class="pre">reset</span></code> type inputs in FormRequest.from_response (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b326b87">commit b326b87</a>)</li>
<li>Do not silence download errors when request errback raises an exception (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/684cfc0">commit 684cfc0</a>)</li>
</ul>
</div>
<div class="section" id="id35">
<h5>Bugfixes<a class="headerlink" href="#id35" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Fix tests under Django 1.6 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b6bed44c">commit b6bed44c</a>)</li>
<li>Lot of bugfixes to retry middleware under disconnections using HTTP 1.1 download handler</li>
<li>Fix inconsistencies among Twisted releases (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/406">issue 406</a>)</li>
<li>Fix scrapy shell bugs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/418">issue 418</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/407">issue 407</a>)</li>
<li>Fix invalid variable name in setup.py (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/429">issue 429</a>)</li>
<li>Fix tutorial references (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/387">issue 387</a>)</li>
<li>Improve request-response docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/391">issue 391</a>)</li>
<li>Improve best practices docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/399">issue 399</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/400">issue 400</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/401">issue 401</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/402">issue 402</a>)</li>
<li>Improve django integration docs (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/404">issue 404</a>)</li>
<li>Document <code class="docutils literal notranslate"><span class="pre">bindaddress</span></code> request meta (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/37c24e01d7">commit 37c24e01d7</a>)</li>
<li>Improve <code class="docutils literal notranslate"><span class="pre">Request</span></code> class documentation (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/226">issue 226</a>)</li>
</ul>
</div>
<div class="section" id="other">
<h5>Other<a class="headerlink" href="#other" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Dropped Python 2.6 support (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/448">issue 448</a>)</li>
<li>Add <a class="reference external" href="https://github.com/scrapy/cssselect/">cssselect</a> python package as install dependency</li>
<li>Drop libxml2 and multi selector’s backend support, <a class="reference external" href="http://lxml.de/">lxml</a> is required from now on.</li>
<li>Minimum Twisted version increased to 10.0.0, dropped Twisted 8.0 support.</li>
<li>Running test suite now requires <code class="docutils literal notranslate"><span class="pre">mock</span></code> python library (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/390">issue 390</a>)</li>
</ul>
</div>
<div class="section" id="thanks">
<h5>Thanks<a class="headerlink" href="#thanks" title="Permalink to this headline">¶</a></h5>
<p>Thanks to everyone who contribute to this release!</p>
<p>List of contributors sorted by number of commits:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">69</span> <span class="n">Daniel</span> <span class="n">Graña</span> <span class="o">&lt;</span><span class="n">dangra</span><span class="o">@...&gt;</span>
<span class="mi">37</span> <span class="n">Pablo</span> <span class="n">Hoffman</span> <span class="o">&lt;</span><span class="n">pablo</span><span class="o">@...&gt;</span>
<span class="mi">13</span> <span class="n">Mikhail</span> <span class="n">Korobov</span> <span class="o">&lt;</span><span class="n">kmike84</span><span class="o">@...&gt;</span>
 <span class="mi">9</span> <span class="n">Alex</span> <span class="n">Cepoi</span> <span class="o">&lt;</span><span class="n">alex</span><span class="o">.</span><span class="n">cepoi</span><span class="o">@...&gt;</span>
 <span class="mi">9</span> <span class="n">alexanderlukanin13</span> <span class="o">&lt;</span><span class="n">alexander</span><span class="o">.</span><span class="n">lukanin</span><span class="o">.</span><span class="mi">13</span><span class="o">@...&gt;</span>
 <span class="mi">8</span> <span class="n">Rolando</span> <span class="n">Espinoza</span> <span class="n">La</span> <span class="n">fuente</span> <span class="o">&lt;</span><span class="n">darkrho</span><span class="o">@...&gt;</span>
 <span class="mi">8</span> <span class="n">Lukasz</span> <span class="n">Biedrycki</span> <span class="o">&lt;</span><span class="n">lukasz</span><span class="o">.</span><span class="n">biedrycki</span><span class="o">@...&gt;</span>
 <span class="mi">6</span> <span class="n">Nicolas</span> <span class="n">Ramirez</span> <span class="o">&lt;</span><span class="n">nramirez</span><span class="o">.</span><span class="n">uy</span><span class="o">@...&gt;</span>
 <span class="mi">3</span> <span class="n">Paul</span> <span class="n">Tremberth</span> <span class="o">&lt;</span><span class="n">paul</span><span class="o">.</span><span class="n">tremberth</span><span class="o">@...&gt;</span>
 <span class="mi">2</span> <span class="n">Martin</span> <span class="n">Olveyra</span> <span class="o">&lt;</span><span class="n">molveyra</span><span class="o">@...&gt;</span>
 <span class="mi">2</span> <span class="n">Stefan</span> <span class="o">&lt;</span><span class="n">misc</span><span class="o">@...&gt;</span>
 <span class="mi">2</span> <span class="n">Rolando</span> <span class="n">Espinoza</span> <span class="o">&lt;</span><span class="n">darkrho</span><span class="o">@...&gt;</span>
 <span class="mi">2</span> <span class="n">Loren</span> <span class="n">Davie</span> <span class="o">&lt;</span><span class="n">loren</span><span class="o">@...&gt;</span>
 <span class="mi">2</span> <span class="n">irgmedeiros</span> <span class="o">&lt;</span><span class="n">irgmedeiros</span><span class="o">@...&gt;</span>
 <span class="mi">1</span> <span class="n">Stefan</span> <span class="n">Koch</span> <span class="o">&lt;</span><span class="n">taikano</span><span class="o">@...&gt;</span>
 <span class="mi">1</span> <span class="n">Stefan</span> <span class="o">&lt;</span><span class="n">cct</span><span class="o">@...&gt;</span>
 <span class="mi">1</span> <span class="n">scraperdragon</span> <span class="o">&lt;</span><span class="n">dragon</span><span class="o">@...&gt;</span>
 <span class="mi">1</span> <span class="n">Kumara</span> <span class="n">Tharmalingam</span> <span class="o">&lt;</span><span class="n">ktharmal</span><span class="o">@...&gt;</span>
 <span class="mi">1</span> <span class="n">Francesco</span> <span class="n">Piccinno</span> <span class="o">&lt;</span><span class="n">stack</span><span class="o">.</span><span class="n">box</span><span class="o">@...&gt;</span>
 <span class="mi">1</span> <span class="n">Marcos</span> <span class="n">Campal</span> <span class="o">&lt;</span><span class="n">duendex</span><span class="o">@...&gt;</span>
 <span class="mi">1</span> <span class="n">Dragon</span> <span class="n">Dave</span> <span class="o">&lt;</span><span class="n">dragon</span><span class="o">@...&gt;</span>
 <span class="mi">1</span> <span class="n">Capi</span> <span class="n">Etheriel</span> <span class="o">&lt;</span><span class="n">barraponto</span><span class="o">@...&gt;</span>
 <span class="mi">1</span> <span class="n">cacovsky</span> <span class="o">&lt;</span><span class="n">amarquesferraz</span><span class="o">@...&gt;</span>
 <span class="mi">1</span> <span class="n">Berend</span> <span class="n">Iwema</span> <span class="o">&lt;</span><span class="n">berend</span><span class="o">@...&gt;</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="scrapy-0-18-4-released-2013-10-10">
<h4>Scrapy 0.18.4 (released 2013-10-10)<a class="headerlink" href="#scrapy-0-18-4-released-2013-10-10" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>IPython refuses to update the namespace. fix #396 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/3d32c4f">commit 3d32c4f</a>)</li>
<li>Fix AlreadyCalledError replacing a request in shell command. closes #407 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b1d8919">commit b1d8919</a>)</li>
<li>Fix start_requests laziness and early hangs (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/89faf52">commit 89faf52</a>)</li>
</ul>
</div>
<div class="section" id="scrapy-0-18-3-released-2013-10-03">
<h4>Scrapy 0.18.3 (released 2013-10-03)<a class="headerlink" href="#scrapy-0-18-3-released-2013-10-03" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>fix regression on lazy evaluation of start requests (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/12693a5">commit 12693a5</a>)</li>
<li>forms: do not submit reset inputs (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/e429f63">commit e429f63</a>)</li>
<li>increase unittest timeouts to decrease travis false positive failures (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/912202e">commit 912202e</a>)</li>
<li>backport master fixes to json exporter (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/cfc2d46">commit cfc2d46</a>)</li>
<li>Fix permission and set umask before generating sdist tarball (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/06149e0">commit 06149e0</a>)</li>
</ul>
</div>
<div class="section" id="scrapy-0-18-2-released-2013-09-03">
<h4>Scrapy 0.18.2 (released 2013-09-03)<a class="headerlink" href="#scrapy-0-18-2-released-2013-09-03" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Backport <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">check</span></code> command fixes and backward compatible multi
crawler process(<a class="reference external" href="https://github.com/scrapy/scrapy/issues/339">issue 339</a>)</li>
</ul>
</div>
<div class="section" id="scrapy-0-18-1-released-2013-08-27">
<h4>Scrapy 0.18.1 (released 2013-08-27)<a class="headerlink" href="#scrapy-0-18-1-released-2013-08-27" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>remove extra import added by cherry picked changes (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/d20304e">commit d20304e</a>)</li>
<li>fix crawling tests under twisted pre 11.0.0 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1994f38">commit 1994f38</a>)</li>
<li>py26 can not format zero length fields {} (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/abf756f">commit abf756f</a>)</li>
<li>test PotentiaDataLoss errors on unbound responses (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b15470d">commit b15470d</a>)</li>
<li>Treat responses without content-length or Transfer-Encoding as good responses (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c4bf324">commit c4bf324</a>)</li>
<li>do no include ResponseFailed if http11 handler is not enabled (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/6cbe684">commit 6cbe684</a>)</li>
<li>New HTTP client wraps connection losts in ResponseFailed exception. fix #373 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1a20bba">commit 1a20bba</a>)</li>
<li>limit travis-ci build matrix (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/3b01bb8">commit 3b01bb8</a>)</li>
<li>Merge pull request #375 from peterarenot/patch-1 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/fa766d7">commit fa766d7</a>)</li>
<li>Fixed so it refers to the correct folder (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/3283809">commit 3283809</a>)</li>
<li>added quantal &amp; raring to support ubuntu releases (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1411923">commit 1411923</a>)</li>
<li>fix retry middleware which didn’t retry certain connection errors after the upgrade to http1 client, closes GH-373 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/bb35ed0">commit bb35ed0</a>)</li>
<li>fix XmlItemExporter in Python 2.7.4 and 2.7.5 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/de3e451">commit de3e451</a>)</li>
<li>minor updates to 0.18 release notes (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c45e5f1">commit c45e5f1</a>)</li>
<li>fix contributters list format (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/0b60031">commit 0b60031</a>)</li>
</ul>
</div>
<div class="section" id="scrapy-0-18-0-released-2013-08-09">
<h4>Scrapy 0.18.0 (released 2013-08-09)<a class="headerlink" href="#scrapy-0-18-0-released-2013-08-09" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Lot of improvements to testsuite run using Tox, including a way to test on pypi</li>
<li>Handle GET parameters for AJAX crawleable urls (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/3fe2a32">commit 3fe2a32</a>)</li>
<li>Use lxml recover option to parse sitemaps (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/347">issue 347</a>)</li>
<li>Bugfix cookie merging by hostname and not by netloc (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/352">issue 352</a>)</li>
<li>Support disabling <code class="docutils literal notranslate"><span class="pre">HttpCompressionMiddleware</span></code> using a flag setting (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/359">issue 359</a>)</li>
<li>Support xml namespaces using <code class="docutils literal notranslate"><span class="pre">iternodes</span></code> parser in <code class="docutils literal notranslate"><span class="pre">XMLFeedSpider</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/12">issue 12</a>)</li>
<li>Support <code class="docutils literal notranslate"><span class="pre">dont_cache</span></code> request meta flag (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/19">issue 19</a>)</li>
<li>Bugfix <code class="docutils literal notranslate"><span class="pre">scrapy.utils.gz.gunzip</span></code> broken by changes in python 2.7.4 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/4dc76e">commit 4dc76e</a>)</li>
<li>Bugfix url encoding on <code class="docutils literal notranslate"><span class="pre">SgmlLinkExtractor</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/24">issue 24</a>)</li>
<li>Bugfix <code class="docutils literal notranslate"><span class="pre">TakeFirst</span></code> processor shouldn’t discard zero (0) value (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/59">issue 59</a>)</li>
<li>Support nested items in xml exporter (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/66">issue 66</a>)</li>
<li>Improve cookies handling performance (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/77">issue 77</a>)</li>
<li>Log dupe filtered requests once (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/105">issue 105</a>)</li>
<li>Split redirection middleware into status and meta based middlewares (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/78">issue 78</a>)</li>
<li>Use HTTP1.1 as default downloader handler (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/109">issue 109</a> and <a class="reference external" href="https://github.com/scrapy/scrapy/issues/318">issue 318</a>)</li>
<li>Support xpath form selection on <code class="docutils literal notranslate"><span class="pre">FormRequest.from_response</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/185">issue 185</a>)</li>
<li>Bugfix unicode decoding error on <code class="docutils literal notranslate"><span class="pre">SgmlLinkExtractor</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/199">issue 199</a>)</li>
<li>Bugfix signal dispatching on pypi interpreter (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/205">issue 205</a>)</li>
<li>Improve request delay and concurrency handling (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/206">issue 206</a>)</li>
<li>Add RFC2616 cache policy to <code class="docutils literal notranslate"><span class="pre">HttpCacheMiddleware</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/212">issue 212</a>)</li>
<li>Allow customization of messages logged by engine (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/214">issue 214</a>)</li>
<li>Multiples improvements to <code class="docutils literal notranslate"><span class="pre">DjangoItem</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/217">issue 217</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/218">issue 218</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/issues/221">issue 221</a>)</li>
<li>Extend Scrapy commands using setuptools entry points (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/260">issue 260</a>)</li>
<li>Allow spider <code class="docutils literal notranslate"><span class="pre">allowed_domains</span></code> value to be set/tuple (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/261">issue 261</a>)</li>
<li>Support <code class="docutils literal notranslate"><span class="pre">settings.getdict</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/269">issue 269</a>)</li>
<li>Simplify internal <code class="docutils literal notranslate"><span class="pre">scrapy.core.scraper</span></code> slot handling (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/271">issue 271</a>)</li>
<li>Added <code class="docutils literal notranslate"><span class="pre">Item.copy</span></code> (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/290">issue 290</a>)</li>
<li>Collect idle downloader slots (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/297">issue 297</a>)</li>
<li>Add <code class="docutils literal notranslate"><span class="pre">ftp://</span></code> scheme downloader handler (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/329">issue 329</a>)</li>
<li>Added downloader benchmark webserver and spider tools <a class="reference internal" href="index.html#benchmarking"><span class="std std-ref">Benchmarking</span></a></li>
<li>Moved persistent (on disk) queues to a separate project (<a class="reference external" href="https://github.com/scrapy/queuelib">queuelib</a>) which scrapy now depends on</li>
<li>Add scrapy commands using external libraries (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/260">issue 260</a>)</li>
<li>Added <code class="docutils literal notranslate"><span class="pre">--pdb</span></code> option to <code class="docutils literal notranslate"><span class="pre">scrapy</span></code> command line tool</li>
<li>Added <code class="xref py py-meth docutils literal notranslate"><span class="pre">XPathSelector.remove_namespaces()</span></code> which allows to remove all namespaces from XML documents for convenience (to work with namespace-less XPaths). Documented in <a class="reference internal" href="index.html#topics-selectors"><span class="std std-ref">Selectors</span></a>.</li>
<li>Several improvements to spider contracts</li>
<li>New default middleware named MetaRefreshMiddldeware that handles meta-refresh html tag redirections,</li>
<li>MetaRefreshMiddldeware and RedirectMiddleware have different priorities to address #62</li>
<li>added from_crawler method to spiders</li>
<li>added system tests with mock server</li>
<li>more improvements to Mac OS compatibility (thanks Alex Cepoi)</li>
<li>several more cleanups to singletons and multi-spider support (thanks Nicolas Ramirez)</li>
<li>support custom download slots</li>
<li>added –spider option to “shell” command.</li>
<li>log overridden settings when scrapy starts</li>
</ul>
<p>Thanks to everyone who contribute to this release. Here is a list of
contributors sorted by number of commits:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">130</span> <span class="n">Pablo</span> <span class="n">Hoffman</span> <span class="o">&lt;</span><span class="n">pablo</span><span class="o">@...&gt;</span>
 <span class="mi">97</span> <span class="n">Daniel</span> <span class="n">Graña</span> <span class="o">&lt;</span><span class="n">dangra</span><span class="o">@...&gt;</span>
 <span class="mi">20</span> <span class="n">Nicolás</span> <span class="n">Ramírez</span> <span class="o">&lt;</span><span class="n">nramirez</span><span class="o">.</span><span class="n">uy</span><span class="o">@...&gt;</span>
 <span class="mi">13</span> <span class="n">Mikhail</span> <span class="n">Korobov</span> <span class="o">&lt;</span><span class="n">kmike84</span><span class="o">@...&gt;</span>
 <span class="mi">12</span> <span class="n">Pedro</span> <span class="n">Faustino</span> <span class="o">&lt;</span><span class="n">pedrobandim</span><span class="o">@...&gt;</span>
 <span class="mi">11</span> <span class="n">Steven</span> <span class="n">Almeroth</span> <span class="o">&lt;</span><span class="n">sroth77</span><span class="o">@...&gt;</span>
  <span class="mi">5</span> <span class="n">Rolando</span> <span class="n">Espinoza</span> <span class="n">La</span> <span class="n">fuente</span> <span class="o">&lt;</span><span class="n">darkrho</span><span class="o">@...&gt;</span>
  <span class="mi">4</span> <span class="n">Michal</span> <span class="n">Danilak</span> <span class="o">&lt;</span><span class="n">mimino</span><span class="o">.</span><span class="n">coder</span><span class="o">@...&gt;</span>
  <span class="mi">4</span> <span class="n">Alex</span> <span class="n">Cepoi</span> <span class="o">&lt;</span><span class="n">alex</span><span class="o">.</span><span class="n">cepoi</span><span class="o">@...&gt;</span>
  <span class="mi">4</span> <span class="n">Alexandr</span> <span class="n">N</span> <span class="n">Zamaraev</span> <span class="p">(</span><span class="n">aka</span> <span class="n">tonal</span><span class="p">)</span> <span class="o">&lt;</span><span class="n">tonal</span><span class="o">@...&gt;</span>
  <span class="mi">3</span> <span class="n">paul</span> <span class="o">&lt;</span><span class="n">paul</span><span class="o">.</span><span class="n">tremberth</span><span class="o">@...&gt;</span>
  <span class="mi">3</span> <span class="n">Martin</span> <span class="n">Olveyra</span> <span class="o">&lt;</span><span class="n">molveyra</span><span class="o">@...&gt;</span>
  <span class="mi">3</span> <span class="n">Jordi</span> <span class="n">Llonch</span> <span class="o">&lt;</span><span class="n">llonchj</span><span class="o">@...&gt;</span>
  <span class="mi">3</span> <span class="n">arijitchakraborty</span> <span class="o">&lt;</span><span class="n">myself</span><span class="o">.</span><span class="n">arijit</span><span class="o">@...&gt;</span>
  <span class="mi">2</span> <span class="n">Shane</span> <span class="n">Evans</span> <span class="o">&lt;</span><span class="n">shane</span><span class="o">.</span><span class="n">evans</span><span class="o">@...&gt;</span>
  <span class="mi">2</span> <span class="n">joehillen</span> <span class="o">&lt;</span><span class="n">joehillen</span><span class="o">@...&gt;</span>
  <span class="mi">2</span> <span class="n">Hart</span> <span class="o">&lt;</span><span class="n">HartSimha</span><span class="o">@...&gt;</span>
  <span class="mi">2</span> <span class="n">Dan</span> <span class="o">&lt;</span><span class="n">ellisd23</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">Zuhao</span> <span class="n">Wan</span> <span class="o">&lt;</span><span class="n">wanzuhao</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">whodatninja</span> <span class="o">&lt;</span><span class="n">blake</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">vkrest</span> <span class="o">&lt;</span><span class="n">v</span><span class="o">.</span><span class="n">krestiannykov</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">tpeng</span> <span class="o">&lt;</span><span class="n">pengtaoo</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">Tom</span> <span class="n">Mortimer</span><span class="o">-</span><span class="n">Jones</span> <span class="o">&lt;</span><span class="n">tom</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">Rocio</span> <span class="n">Aramberri</span> <span class="o">&lt;</span><span class="n">roschegel</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">Pedro</span> <span class="o">&lt;</span><span class="n">pedro</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">notsobad</span> <span class="o">&lt;</span><span class="n">wangxiaohugg</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">Natan</span> <span class="n">L</span> <span class="o">&lt;</span><span class="n">kuyanatan</span><span class="o">.</span><span class="n">nlao</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">Mark</span> <span class="n">Grey</span> <span class="o">&lt;</span><span class="n">mark</span><span class="o">.</span><span class="n">grey</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">Luan</span> <span class="o">&lt;</span><span class="n">luanpab</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">Libor</span> <span class="n">Nenadál</span> <span class="o">&lt;</span><span class="n">libor</span><span class="o">.</span><span class="n">nenadal</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">Juan</span> <span class="n">M</span> <span class="n">Uys</span> <span class="o">&lt;</span><span class="n">opyate</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">Jonas</span> <span class="n">Brunsgaard</span> <span class="o">&lt;</span><span class="n">jonas</span><span class="o">.</span><span class="n">brunsgaard</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">Ilya</span> <span class="n">Baryshev</span> <span class="o">&lt;</span><span class="n">baryshev</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">Hasnain</span> <span class="n">Lakhani</span> <span class="o">&lt;</span><span class="n">m</span><span class="o">.</span><span class="n">hasnain</span><span class="o">.</span><span class="n">lakhani</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">Emanuel</span> <span class="n">Schorsch</span> <span class="o">&lt;</span><span class="n">emschorsch</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">Chris</span> <span class="n">Tilden</span> <span class="o">&lt;</span><span class="n">chris</span><span class="o">.</span><span class="n">tilden</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">Capi</span> <span class="n">Etheriel</span> <span class="o">&lt;</span><span class="n">barraponto</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">cacovsky</span> <span class="o">&lt;</span><span class="n">amarquesferraz</span><span class="o">@...&gt;</span>
  <span class="mi">1</span> <span class="n">Berend</span> <span class="n">Iwema</span> <span class="o">&lt;</span><span class="n">berend</span><span class="o">@...&gt;</span>
</pre></div>
</div>
</div>
<div class="section" id="scrapy-0-16-5-released-2013-05-30">
<h4>Scrapy 0.16.5 (released 2013-05-30)<a class="headerlink" href="#scrapy-0-16-5-released-2013-05-30" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>obey request method when scrapy deploy is redirected to a new endpoint (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8c4fcee">commit 8c4fcee</a>)</li>
<li>fix inaccurate downloader middleware documentation. refs #280 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/40667cb">commit 40667cb</a>)</li>
<li>doc: remove links to diveintopython.org, which is no longer available. closes #246 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/bd58bfa">commit bd58bfa</a>)</li>
<li>Find form nodes in invalid html5 documents (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/e3d6945">commit e3d6945</a>)</li>
<li>Fix typo labeling attrs type bool instead of list (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/a274276">commit a274276</a>)</li>
</ul>
</div>
<div class="section" id="scrapy-0-16-4-released-2013-01-23">
<h4>Scrapy 0.16.4 (released 2013-01-23)<a class="headerlink" href="#scrapy-0-16-4-released-2013-01-23" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>fixes spelling errors in documentation (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/6d2b3aa">commit 6d2b3aa</a>)</li>
<li>add doc about disabling an extension. refs #132 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c90de33">commit c90de33</a>)</li>
<li>Fixed error message formatting. log.err() doesn’t support cool formatting and when error occurred, the message was:    “ERROR: Error processing %(item)s” (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c16150c">commit c16150c</a>)</li>
<li>lint and improve images pipeline error logging (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/56b45fc">commit 56b45fc</a>)</li>
<li>fixed doc typos (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/243be84">commit 243be84</a>)</li>
<li>add documentation topics: Broad Crawls &amp; Common Practies (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1fbb715">commit 1fbb715</a>)</li>
<li>fix bug in scrapy parse command when spider is not specified explicitly. closes #209 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c72e682">commit c72e682</a>)</li>
<li>Update docs/topics/commands.rst (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/28eac7a">commit 28eac7a</a>)</li>
</ul>
</div>
<div class="section" id="scrapy-0-16-3-released-2012-12-07">
<h4>Scrapy 0.16.3 (released 2012-12-07)<a class="headerlink" href="#scrapy-0-16-3-released-2012-12-07" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>Remove concurrency limitation when using download delays and still ensure inter-request delays are enforced (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/487b9b5">commit 487b9b5</a>)</li>
<li>add error details when image pipeline fails (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8232569">commit 8232569</a>)</li>
<li>improve mac os compatibility (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8dcf8aa">commit 8dcf8aa</a>)</li>
<li>setup.py: use README.rst to populate long_description (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/7b5310d">commit 7b5310d</a>)</li>
<li>doc: removed obsolete references to ClientForm (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/80f9bb6">commit 80f9bb6</a>)</li>
<li>correct docs for default storage backend (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/2aa491b">commit 2aa491b</a>)</li>
<li>doc: removed broken proxyhub link from FAQ (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/bdf61c4">commit bdf61c4</a>)</li>
<li>Fixed docs typo in SpiderOpenCloseLogging example (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/7184094">commit 7184094</a>)</li>
</ul>
</div>
<div class="section" id="scrapy-0-16-2-released-2012-11-09">
<h4>Scrapy 0.16.2 (released 2012-11-09)<a class="headerlink" href="#scrapy-0-16-2-released-2012-11-09" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>scrapy contracts: python2.6 compat (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/a4a9199">commit a4a9199</a>)</li>
<li>scrapy contracts verbose option (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/ec41673">commit ec41673</a>)</li>
<li>proper unittest-like output for scrapy contracts (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/86635e4">commit 86635e4</a>)</li>
<li>added open_in_browser to debugging doc (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c9b690d">commit c9b690d</a>)</li>
<li>removed reference to global scrapy stats from settings doc (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/dd55067">commit dd55067</a>)</li>
<li>Fix SpiderState bug in Windows platforms (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/58998f4">commit 58998f4</a>)</li>
</ul>
</div>
<div class="section" id="scrapy-0-16-1-released-2012-10-26">
<h4>Scrapy 0.16.1 (released 2012-10-26)<a class="headerlink" href="#scrapy-0-16-1-released-2012-10-26" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>fixed LogStats extension, which got broken after a wrong merge before the 0.16 release (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8c780fd">commit 8c780fd</a>)</li>
<li>better backward compatibility for scrapy.conf.settings (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/3403089">commit 3403089</a>)</li>
<li>extended documentation on how to access crawler stats from extensions (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c4da0b5">commit c4da0b5</a>)</li>
<li>removed .hgtags (no longer needed now that scrapy uses git) (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/d52c188">commit d52c188</a>)</li>
<li>fix dashes under rst headers (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/fa4f7f9">commit fa4f7f9</a>)</li>
<li>set release date for 0.16.0 in news (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/e292246">commit e292246</a>)</li>
</ul>
</div>
<div class="section" id="scrapy-0-16-0-released-2012-10-18">
<h4>Scrapy 0.16.0 (released 2012-10-18)<a class="headerlink" href="#scrapy-0-16-0-released-2012-10-18" title="Permalink to this headline">¶</a></h4>
<p>Scrapy changes:</p>
<ul class="simple">
<li>added <a class="reference internal" href="index.html#topics-contracts"><span class="std std-ref">Spiders Contracts</span></a>, a mechanism for testing spiders in a formal/reproducible way</li>
<li>added options <code class="docutils literal notranslate"><span class="pre">-o</span></code> and <code class="docutils literal notranslate"><span class="pre">-t</span></code> to the <a class="reference internal" href="index.html#std:command-runspider"><code class="xref std std-command docutils literal notranslate"><span class="pre">runspider</span></code></a> command</li>
<li>documented <a class="reference internal" href="index.html#document-topics/autothrottle"><span class="doc">AutoThrottle extension</span></a> and added to extensions installed by default. You still need to enable it with <a class="reference internal" href="index.html#std:setting-AUTOTHROTTLE_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">AUTOTHROTTLE_ENABLED</span></code></a></li>
<li>major Stats Collection refactoring: removed separation of global/per-spider stats, removed stats-related signals (<code class="docutils literal notranslate"><span class="pre">stats_spider_opened</span></code>, etc). Stats are much simpler now, backward compatibility is kept on the Stats Collector API and signals.</li>
<li>added <code class="xref py py-meth docutils literal notranslate"><span class="pre">process_start_requests()</span></code> method to spider middlewares</li>
<li>dropped Signals singleton. Signals should now be accesed through the Crawler.signals attribute. See the signals documentation for more info.</li>
<li>dropped Signals singleton. Signals should now be accesed through the Crawler.signals attribute. See the signals documentation for more info.</li>
<li>dropped Stats Collector singleton. Stats can now be accessed through the Crawler.stats attribute. See the stats collection documentation for more info.</li>
<li>documented <a class="reference internal" href="index.html#topics-api"><span class="std std-ref">Core API</span></a></li>
<li><code class="docutils literal notranslate"><span class="pre">lxml</span></code> is now the default selectors backend instead of <code class="docutils literal notranslate"><span class="pre">libxml2</span></code></li>
<li>ported FormRequest.from_response() to use <a class="reference external" href="http://lxml.de/">lxml</a> instead of <a class="reference external" href="http://wwwsearch.sourceforge.net/old/ClientForm/">ClientForm</a></li>
<li>removed modules: <code class="docutils literal notranslate"><span class="pre">scrapy.xlib.BeautifulSoup</span></code> and <code class="docutils literal notranslate"><span class="pre">scrapy.xlib.ClientForm</span></code></li>
<li>SitemapSpider: added support for sitemap urls ending in .xml and .xml.gz, even if they advertise a wrong content type (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/10ed28b">commit 10ed28b</a>)</li>
<li>StackTraceDump extension: also dump trackref live references (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/fe2ce93">commit fe2ce93</a>)</li>
<li>nested items now fully supported in JSON and JSONLines exporters</li>
<li>added <a class="reference internal" href="index.html#std:reqmeta-cookiejar"><code class="xref std std-reqmeta docutils literal notranslate"><span class="pre">cookiejar</span></code></a> Request meta key to support multiple cookie sessions per spider</li>
<li>decoupled encoding detection code to <a class="reference external" href="https://github.com/scrapy/w3lib/blob/master/w3lib/encoding.py">w3lib.encoding</a>, and ported Scrapy code to use that module</li>
<li>dropped support for Python 2.5. See <a class="reference external" href="https://blog.scrapinghub.com/2012/02/27/scrapy-0-15-dropping-support-for-python-2-5/">https://blog.scrapinghub.com/2012/02/27/scrapy-0-15-dropping-support-for-python-2-5/</a></li>
<li>dropped support for Twisted 2.5</li>
<li>added <a class="reference internal" href="index.html#std:setting-REFERER_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">REFERER_ENABLED</span></code></a> setting, to control referer middleware</li>
<li>changed default user agent to: <code class="docutils literal notranslate"><span class="pre">Scrapy/VERSION</span> <span class="pre">(+http://scrapy.org)</span></code></li>
<li>removed (undocumented) <code class="docutils literal notranslate"><span class="pre">HTMLImageLinkExtractor</span></code> class from <code class="docutils literal notranslate"><span class="pre">scrapy.contrib.linkextractors.image</span></code></li>
<li>removed per-spider settings (to be replaced by instantiating multiple crawler objects)</li>
<li><code class="docutils literal notranslate"><span class="pre">USER_AGENT</span></code> spider attribute will no longer work, use <code class="docutils literal notranslate"><span class="pre">user_agent</span></code> attribute instead</li>
<li><code class="docutils literal notranslate"><span class="pre">DOWNLOAD_TIMEOUT</span></code> spider attribute will no longer work, use <code class="docutils literal notranslate"><span class="pre">download_timeout</span></code> attribute instead</li>
<li>removed <code class="docutils literal notranslate"><span class="pre">ENCODING_ALIASES</span></code> setting, as encoding auto-detection has been moved to the <a class="reference external" href="https://github.com/scrapy/w3lib">w3lib</a> library</li>
<li>promoted <a class="reference internal" href="index.html#topics-djangoitem"><span class="std std-ref">DjangoItem</span></a> to main contrib</li>
<li>LogFormatter method now return dicts(instead of strings) to support lazy formatting (<a class="reference external" href="https://github.com/scrapy/scrapy/issues/164">issue 164</a>, <a class="reference external" href="https://github.com/scrapy/scrapy/commit/dcef7b0">commit dcef7b0</a>)</li>
<li>downloader handlers (<a class="reference internal" href="index.html#std:setting-DOWNLOAD_HANDLERS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">DOWNLOAD_HANDLERS</span></code></a> setting) now receive settings as the first argument of the constructor</li>
<li>replaced memory usage acounting with (more portable) <a class="reference external" href="https://docs.python.org/2/library/resource.html">resource</a> module, removed <code class="docutils literal notranslate"><span class="pre">scrapy.utils.memory</span></code> module</li>
<li>removed signal: <code class="docutils literal notranslate"><span class="pre">scrapy.mail.mail_sent</span></code></li>
<li>removed <code class="docutils literal notranslate"><span class="pre">TRACK_REFS</span></code> setting, now <a class="reference internal" href="index.html#topics-leaks-trackrefs"><span class="std std-ref">trackrefs</span></a> is always enabled</li>
<li>DBM is now the default storage backend for HTTP cache middleware</li>
<li>number of log messages (per level) are now tracked through Scrapy stats (stat name: <code class="docutils literal notranslate"><span class="pre">log_count/LEVEL</span></code>)</li>
<li>number received responses are now tracked through Scrapy stats (stat name: <code class="docutils literal notranslate"><span class="pre">response_received_count</span></code>)</li>
<li>removed <code class="docutils literal notranslate"><span class="pre">scrapy.log.started</span></code> attribute</li>
</ul>
</div>
<div class="section" id="scrapy-0-14-4">
<h4>Scrapy 0.14.4<a class="headerlink" href="#scrapy-0-14-4" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>added precise to supported ubuntu distros (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b7e46df">commit b7e46df</a>)</li>
<li>fixed bug in json-rpc webservice reported in <a class="reference external" href="https://groups.google.com/forum/#!topic/scrapy-users/qgVBmFybNAQ/discussion">https://groups.google.com/forum/#!topic/scrapy-users/qgVBmFybNAQ/discussion</a>. also removed no longer supported ‘run’ command from extras/scrapy-ws.py (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/340fbdb">commit 340fbdb</a>)</li>
<li>meta tag attributes for content-type http equiv can be in any order. #123 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/0cb68af">commit 0cb68af</a>)</li>
<li>replace “import Image” by more standard “from PIL import Image”. closes #88 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/4d17048">commit 4d17048</a>)</li>
<li>return trial status as bin/runtests.sh exit value. #118 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b7b2e7f">commit b7b2e7f</a>)</li>
</ul>
</div>
<div class="section" id="scrapy-0-14-3">
<h4>Scrapy 0.14.3<a class="headerlink" href="#scrapy-0-14-3" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>forgot to include pydispatch license. #118 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/fd85f9c">commit fd85f9c</a>)</li>
<li>include egg files used by testsuite in source distribution. #118 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/c897793">commit c897793</a>)</li>
<li>update docstring in project template to avoid confusion with genspider command, which may be considered as an advanced feature. refs #107 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/2548dcc">commit 2548dcc</a>)</li>
<li>added note to docs/topics/firebug.rst about google directory being shut down (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/668e352">commit 668e352</a>)</li>
<li>dont discard slot when empty, just save in another dict in order to recycle if needed again. (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8e9f607">commit 8e9f607</a>)</li>
<li>do not fail handling unicode xpaths in libxml2 backed selectors (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/b830e95">commit b830e95</a>)</li>
<li>fixed minor mistake in Request objects documentation (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/bf3c9ee">commit bf3c9ee</a>)</li>
<li>fixed minor defect in link extractors documentation (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/ba14f38">commit ba14f38</a>)</li>
<li>removed some obsolete remaining code related to sqlite support in scrapy (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/0665175">commit 0665175</a>)</li>
</ul>
</div>
<div class="section" id="scrapy-0-14-2">
<h4>Scrapy 0.14.2<a class="headerlink" href="#scrapy-0-14-2" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>move buffer pointing to start of file before computing checksum. refs #92 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/6a5bef2">commit 6a5bef2</a>)</li>
<li>Compute image checksum before persisting images. closes #92 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/9817df1">commit 9817df1</a>)</li>
<li>remove leaking references in cached failures (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/673a120">commit 673a120</a>)</li>
<li>fixed bug in MemoryUsage extension: get_engine_status() takes exactly 1 argument (0 given) (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/11133e9">commit 11133e9</a>)</li>
<li>fixed struct.error on http compression middleware. closes #87 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1423140">commit 1423140</a>)</li>
<li>ajax crawling wasn’t expanding for unicode urls (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/0de3fb4">commit 0de3fb4</a>)</li>
<li>Catch start_requests iterator errors. refs #83 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/454a21d">commit 454a21d</a>)</li>
<li>Speed-up libxml2 XPathSelector (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/2fbd662">commit 2fbd662</a>)</li>
<li>updated versioning doc according to recent changes (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/0a070f5">commit 0a070f5</a>)</li>
<li>scrapyd: fixed documentation link (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/2b4e4c3">commit 2b4e4c3</a>)</li>
<li>extras/makedeb.py: no longer obtaining version from git (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/caffe0e">commit caffe0e</a>)</li>
</ul>
</div>
<div class="section" id="scrapy-0-14-1">
<h4>Scrapy 0.14.1<a class="headerlink" href="#scrapy-0-14-1" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li>extras/makedeb.py: no longer obtaining version from git (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/caffe0e">commit caffe0e</a>)</li>
<li>bumped version to 0.14.1 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/6cb9e1c">commit 6cb9e1c</a>)</li>
<li>fixed reference to tutorial directory (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/4b86bd6">commit 4b86bd6</a>)</li>
<li>doc: removed duplicated callback argument from Request.replace() (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/1aeccdd">commit 1aeccdd</a>)</li>
<li>fixed formatting of scrapyd doc (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/8bf19e6">commit 8bf19e6</a>)</li>
<li>Dump stacks for all running threads and fix engine status dumped by StackTraceDump extension (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/14a8e6e">commit 14a8e6e</a>)</li>
<li>added comment about why we disable ssl on boto images upload (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/5223575">commit 5223575</a>)</li>
<li>SSL handshaking hangs when doing too many parallel connections to S3 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/63d583d">commit 63d583d</a>)</li>
<li>change tutorial to follow changes on dmoz site (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/bcb3198">commit bcb3198</a>)</li>
<li>Avoid _disconnectedDeferred AttributeError exception in Twisted&gt;=11.1.0 (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/98f3f87">commit 98f3f87</a>)</li>
<li>allow spider to set autothrottle max concurrency (<a class="reference external" href="https://github.com/scrapy/scrapy/commit/175a4b5">commit 175a4b5</a>)</li>
</ul>
</div>
<div class="section" id="scrapy-0-14">
<h4>Scrapy 0.14<a class="headerlink" href="#scrapy-0-14" title="Permalink to this headline">¶</a></h4>
<div class="section" id="new-features-and-settings">
<h5>New features and settings<a class="headerlink" href="#new-features-and-settings" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Support for <a class="reference external" href="https://developers.google.com/webmasters/ajax-crawling/docs/getting-started?csw=1">AJAX crawleable urls</a></li>
<li>New persistent scheduler that stores requests on disk, allowing to suspend and resume crawls (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2737">r2737</a>)</li>
<li>added <code class="docutils literal notranslate"><span class="pre">-o</span></code> option to <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">crawl</span></code>, a shortcut for dumping scraped items into a file (or standard output using <code class="docutils literal notranslate"><span class="pre">-</span></code>)</li>
<li>Added support for passing custom settings to Scrapyd <code class="docutils literal notranslate"><span class="pre">schedule.json</span></code> api (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2779">r2779</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2783">r2783</a>)</li>
<li>New <code class="docutils literal notranslate"><span class="pre">ChunkedTransferMiddleware</span></code> (enabled by default) to support <a class="reference external" href="https://en.wikipedia.org/wiki/Chunked_transfer_encoding">chunked transfer encoding</a> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2769">r2769</a>)</li>
<li>Add boto 2.0 support for S3 downloader handler (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2763">r2763</a>)</li>
<li>Added <a class="reference external" href="https://docs.python.org/2/library/marshal.html">marshal</a> to formats supported by feed exports (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2744">r2744</a>)</li>
<li>In request errbacks, offending requests are now received in <code class="docutils literal notranslate"><span class="pre">failure.request</span></code> attribute (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2738">r2738</a>)</li>
<li><dl class="first docutils">
<dt>Big downloader refactoring to support per domain/ip concurrency limits (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2732">r2732</a>)</dt>
<dd><ul class="first last">
<li><dl class="first docutils">
<dt><code class="docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_SPIDER</span></code> setting has been deprecated and replaced by:</dt>
<dd><ul class="first last">
<li><a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS</span></code></a>, <a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_DOMAIN"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_DOMAIN</span></code></a>, <a class="reference internal" href="index.html#std:setting-CONCURRENT_REQUESTS_PER_IP"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_IP</span></code></a></li>
</ul>
</dd>
</dl>
</li>
<li>check the documentation for more details</li>
</ul>
</dd>
</dl>
</li>
<li>Added builtin caching DNS resolver (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2728">r2728</a>)</li>
<li>Moved Amazon AWS-related components/extensions (SQS spider queue, SimpleDB stats collector) to a separate project: [scaws](<a class="reference external" href="https://github.com/scrapinghub/scaws">https://github.com/scrapinghub/scaws</a>) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2706">r2706</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2714">r2714</a>)</li>
<li>Moved spider queues to scrapyd: <code class="docutils literal notranslate"><span class="pre">scrapy.spiderqueue</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">scrapyd.spiderqueue</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2708">r2708</a>)</li>
<li>Moved sqlite utils to scrapyd: <code class="docutils literal notranslate"><span class="pre">scrapy.utils.sqlite</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">scrapyd.sqlite</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2781">r2781</a>)</li>
<li>Real support for returning iterators on <code class="docutils literal notranslate"><span class="pre">start_requests()</span></code> method. The iterator is now consumed during the crawl when the spider is getting idle (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2704">r2704</a>)</li>
<li>Added <a class="reference internal" href="index.html#std:setting-REDIRECT_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">REDIRECT_ENABLED</span></code></a> setting to quickly enable/disable the redirect middleware (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2697">r2697</a>)</li>
<li>Added <a class="reference internal" href="index.html#std:setting-RETRY_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">RETRY_ENABLED</span></code></a> setting to quickly enable/disable the retry middleware (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2694">r2694</a>)</li>
<li>Added <code class="docutils literal notranslate"><span class="pre">CloseSpider</span></code> exception to manually close spiders (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2691">r2691</a>)</li>
<li>Improved encoding detection by adding support for HTML5 meta charset declaration (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2690">r2690</a>)</li>
<li>Refactored close spider behavior to wait for all downloads to finish and be processed by spiders, before closing the spider (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2688">r2688</a>)</li>
<li>Added <code class="docutils literal notranslate"><span class="pre">SitemapSpider</span></code> (see documentation in Spiders page) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2658">r2658</a>)</li>
<li>Added <code class="docutils literal notranslate"><span class="pre">LogStats</span></code> extension for periodically logging basic stats (like crawled pages and scraped items) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2657">r2657</a>)</li>
<li>Make handling of gzipped responses more robust (#319, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2643">r2643</a>). Now Scrapy will try and decompress as much as possible from a gzipped response, instead of failing with an <code class="docutils literal notranslate"><span class="pre">IOError</span></code>.</li>
<li>Simplified !MemoryDebugger extension to use stats for dumping memory debugging info (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2639">r2639</a>)</li>
<li>Added new command to edit spiders: <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">edit</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2636">r2636</a>) and <code class="docutils literal notranslate"><span class="pre">-e</span></code> flag to <code class="docutils literal notranslate"><span class="pre">genspider</span></code> command that uses it (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2653">r2653</a>)</li>
<li>Changed default representation of items to pretty-printed dicts. (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2631">r2631</a>). This improves default logging by making log more readable in the default case, for both Scraped and Dropped lines.</li>
<li>Added <a class="reference internal" href="index.html#std:signal-spider_error"><code class="xref std std-signal docutils literal notranslate"><span class="pre">spider_error</span></code></a> signal (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2628">r2628</a>)</li>
<li>Added <a class="reference internal" href="index.html#std:setting-COOKIES_ENABLED"><code class="xref std std-setting docutils literal notranslate"><span class="pre">COOKIES_ENABLED</span></code></a> setting (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2625">r2625</a>)</li>
<li>Stats are now dumped to Scrapy log (default value of <a class="reference internal" href="index.html#std:setting-STATS_DUMP"><code class="xref std std-setting docutils literal notranslate"><span class="pre">STATS_DUMP</span></code></a> setting has been changed to <code class="docutils literal notranslate"><span class="pre">True</span></code>). This is to make Scrapy users more aware of Scrapy stats and the data that is collected there.</li>
<li>Added support for dynamically adjusting download delay and maximum concurrent requests (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2599">r2599</a>)</li>
<li>Added new DBM HTTP cache storage backend (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2576">r2576</a>)</li>
<li>Added <code class="docutils literal notranslate"><span class="pre">listjobs.json</span></code> API to Scrapyd (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2571">r2571</a>)</li>
<li><code class="docutils literal notranslate"><span class="pre">CsvItemExporter</span></code>: added <code class="docutils literal notranslate"><span class="pre">join_multivalued</span></code> parameter (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2578">r2578</a>)</li>
<li>Added namespace support to <code class="docutils literal notranslate"><span class="pre">xmliter_lxml</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2552">r2552</a>)</li>
<li>Improved cookies middleware by making <code class="docutils literal notranslate"><span class="pre">COOKIES_DEBUG</span></code> nicer and documenting it (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2579">r2579</a>)</li>
<li>Several improvements to Scrapyd and Link extractors</li>
</ul>
</div>
<div class="section" id="code-rearranged-and-removed">
<h5>Code rearranged and removed<a class="headerlink" href="#code-rearranged-and-removed" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li><dl class="first docutils">
<dt>Merged item passed and item scraped concepts, as they have often proved confusing in the past. This means: (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2630">r2630</a>)</dt>
<dd><ul class="first last">
<li>original item_scraped signal was removed</li>
<li>original item_passed signal was renamed to item_scraped</li>
<li>old log lines <code class="docutils literal notranslate"><span class="pre">Scraped</span> <span class="pre">Item...</span></code> were removed</li>
<li>old log lines <code class="docutils literal notranslate"><span class="pre">Passed</span> <span class="pre">Item...</span></code> were renamed to <code class="docutils literal notranslate"><span class="pre">Scraped</span> <span class="pre">Item...</span></code> lines and downgraded to <code class="docutils literal notranslate"><span class="pre">DEBUG</span></code> level</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Reduced Scrapy codebase by striping part of Scrapy code into two new libraries:</dt>
<dd><ul class="first last">
<li><a class="reference external" href="https://github.com/scrapy/w3lib">w3lib</a> (several functions from <code class="docutils literal notranslate"><span class="pre">scrapy.utils.{http,markup,multipart,response,url}</span></code>, done in <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2584">r2584</a>)</li>
<li><a class="reference external" href="https://github.com/scrapy/scrapely">scrapely</a> (was <code class="docutils literal notranslate"><span class="pre">scrapy.contrib.ibl</span></code>, done in <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2586">r2586</a>)</li>
</ul>
</dd>
</dl>
</li>
<li>Removed unused function: <code class="docutils literal notranslate"><span class="pre">scrapy.utils.request.request_info()</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2577">r2577</a>)</li>
<li>Removed googledir project from <code class="docutils literal notranslate"><span class="pre">examples/googledir</span></code>. There’s now a new example project called <code class="docutils literal notranslate"><span class="pre">dirbot</span></code> available on github: <a class="reference external" href="https://github.com/scrapy/dirbot">https://github.com/scrapy/dirbot</a></li>
<li>Removed support for default field values in Scrapy items (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2616">r2616</a>)</li>
<li>Removed experimental crawlspider v2 (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2632">r2632</a>)</li>
<li>Removed scheduler middleware to simplify architecture. Duplicates filter is now done in the scheduler itself, using the same dupe fltering class as before (<code class="docutils literal notranslate"><span class="pre">DUPEFILTER_CLASS</span></code> setting) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2640">r2640</a>)</li>
<li>Removed support for passing urls to <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">crawl</span></code> command (use <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">parse</span></code> instead) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2704">r2704</a>)</li>
<li>Removed deprecated Execution Queue (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2704">r2704</a>)</li>
<li>Removed (undocumented) spider context extension (from scrapy.contrib.spidercontext) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2780">r2780</a>)</li>
<li>removed <code class="docutils literal notranslate"><span class="pre">CONCURRENT_SPIDERS</span></code> setting (use scrapyd maxproc instead) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2789">r2789</a>)</li>
<li>Renamed attributes of core components: downloader.sites -&gt; downloader.slots, scraper.sites -&gt; scraper.slots (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2717">r2717</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2718">r2718</a>)</li>
<li>Renamed setting <code class="docutils literal notranslate"><span class="pre">CLOSESPIDER_ITEMPASSED</span></code> to <a class="reference internal" href="index.html#std:setting-CLOSESPIDER_ITEMCOUNT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CLOSESPIDER_ITEMCOUNT</span></code></a> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2655">r2655</a>). Backward compatibility kept.</li>
</ul>
</div>
</div>
<div class="section" id="scrapy-0-12">
<h4>Scrapy 0.12<a class="headerlink" href="#scrapy-0-12" title="Permalink to this headline">¶</a></h4>
<p>The numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.</p>
<div class="section" id="new-features-and-improvements">
<h5>New features and improvements<a class="headerlink" href="#new-features-and-improvements" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Passed item is now sent in the <code class="docutils literal notranslate"><span class="pre">item</span></code> argument of the <code class="xref std std-signal docutils literal notranslate"><span class="pre">item_passed</span></code> (#273)</li>
<li>Added verbose option to <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">version</span></code> command, useful for bug reports (#298)</li>
<li>HTTP cache now stored by default in the project data dir (#279)</li>
<li>Added project data storage directory (#276, #277)</li>
<li>Documented file structure of Scrapy projects (see command-line tool doc)</li>
<li>New lxml backend for XPath selectors (#147)</li>
<li>Per-spider settings (#245)</li>
<li>Support exit codes to signal errors in Scrapy commands (#248)</li>
<li>Added <code class="docutils literal notranslate"><span class="pre">-c</span></code> argument to <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">shell</span></code> command</li>
<li>Made <code class="docutils literal notranslate"><span class="pre">libxml2</span></code> optional (#260)</li>
<li>New <code class="docutils literal notranslate"><span class="pre">deploy</span></code> command (#261)</li>
<li>Added <a class="reference internal" href="index.html#std:setting-CLOSESPIDER_PAGECOUNT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CLOSESPIDER_PAGECOUNT</span></code></a> setting (#253)</li>
<li>Added <a class="reference internal" href="index.html#std:setting-CLOSESPIDER_ERRORCOUNT"><code class="xref std std-setting docutils literal notranslate"><span class="pre">CLOSESPIDER_ERRORCOUNT</span></code></a> setting (#254)</li>
</ul>
</div>
<div class="section" id="scrapyd-changes">
<h5>Scrapyd changes<a class="headerlink" href="#scrapyd-changes" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Scrapyd now uses one process per spider</li>
<li>It stores one log file per spider run, and rotate them keeping the lastest 5 logs per spider (by default)</li>
<li>A minimal web ui was added, available at <a class="reference external" href="http://localhost:6800">http://localhost:6800</a> by default</li>
<li>There is now a <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">server</span></code> command to start a Scrapyd server of the current project</li>
</ul>
</div>
<div class="section" id="changes-to-settings">
<h5>Changes to settings<a class="headerlink" href="#changes-to-settings" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>added <code class="docutils literal notranslate"><span class="pre">HTTPCACHE_ENABLED</span></code> setting (False by default) to enable HTTP cache middleware</li>
<li>changed <code class="docutils literal notranslate"><span class="pre">HTTPCACHE_EXPIRATION_SECS</span></code> semantics: now zero means “never expire”.</li>
</ul>
</div>
<div class="section" id="deprecated-obsoleted-functionality">
<h5>Deprecated/obsoleted functionality<a class="headerlink" href="#deprecated-obsoleted-functionality" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Deprecated <code class="docutils literal notranslate"><span class="pre">runserver</span></code> command in favor of <code class="docutils literal notranslate"><span class="pre">server</span></code> command which starts a Scrapyd server. See also: Scrapyd changes</li>
<li>Deprecated <code class="docutils literal notranslate"><span class="pre">queue</span></code> command in favor of using Scrapyd <code class="docutils literal notranslate"><span class="pre">schedule.json</span></code> API. See also: Scrapyd changes</li>
<li>Removed the !LxmlItemLoader (experimental contrib which never graduated to main contrib)</li>
</ul>
</div>
</div>
<div class="section" id="scrapy-0-10">
<h4>Scrapy 0.10<a class="headerlink" href="#scrapy-0-10" title="Permalink to this headline">¶</a></h4>
<p>The numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.</p>
<div class="section" id="id36">
<h5>New features and improvements<a class="headerlink" href="#id36" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>New Scrapy service called <code class="docutils literal notranslate"><span class="pre">scrapyd</span></code> for deploying Scrapy crawlers in production (#218) (documentation available)</li>
<li>Simplified Images pipeline usage which doesn’t require subclassing your own images pipeline now (#217)</li>
<li>Scrapy shell now shows the Scrapy log by default (#206)</li>
<li>Refactored execution queue in a common base code and pluggable backends called “spider queues” (#220)</li>
<li>New persistent spider queue (based on SQLite) (#198), available by default, which allows to start Scrapy in server mode and then schedule spiders to run.</li>
<li>Added documentation for Scrapy command-line tool and all its available sub-commands. (documentation available)</li>
<li>Feed exporters with pluggable backends (#197) (documentation available)</li>
<li>Deferred signals (#193)</li>
<li>Added two new methods to item pipeline open_spider(), close_spider() with deferred support (#195)</li>
<li>Support for overriding default request headers per spider (#181)</li>
<li>Replaced default Spider Manager with one with similar functionality but not depending on Twisted Plugins (#186)</li>
<li>Splitted Debian package into two packages - the library and the service (#187)</li>
<li>Scrapy log refactoring (#188)</li>
<li>New extension for keeping persistent spider contexts among different runs (#203)</li>
<li>Added <code class="docutils literal notranslate"><span class="pre">dont_redirect</span></code> request.meta key for avoiding redirects (#233)</li>
<li>Added <code class="docutils literal notranslate"><span class="pre">dont_retry</span></code> request.meta key for avoiding retries (#234)</li>
</ul>
</div>
<div class="section" id="command-line-tool-changes">
<h5>Command-line tool changes<a class="headerlink" href="#command-line-tool-changes" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>New <code class="docutils literal notranslate"><span class="pre">scrapy</span></code> command which replaces the old <code class="docutils literal notranslate"><span class="pre">scrapy-ctl.py</span></code> (#199)
- there is only one global <code class="docutils literal notranslate"><span class="pre">scrapy</span></code> command now, instead of one <code class="docutils literal notranslate"><span class="pre">scrapy-ctl.py</span></code> per project
- Added <code class="docutils literal notranslate"><span class="pre">scrapy.bat</span></code> script for running more conveniently from Windows</li>
<li>Added bash completion to command-line tool (#210)</li>
<li>Renamed command <code class="docutils literal notranslate"><span class="pre">start</span></code> to <code class="docutils literal notranslate"><span class="pre">runserver</span></code> (#209)</li>
</ul>
</div>
<div class="section" id="api-changes">
<h5>API changes<a class="headerlink" href="#api-changes" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">url</span></code> and <code class="docutils literal notranslate"><span class="pre">body</span></code> attributes of Request objects are now read-only (#230)</li>
<li><code class="docutils literal notranslate"><span class="pre">Request.copy()</span></code> and <code class="docutils literal notranslate"><span class="pre">Request.replace()</span></code> now also copies their <code class="docutils literal notranslate"><span class="pre">callback</span></code> and <code class="docutils literal notranslate"><span class="pre">errback</span></code> attributes (#231)</li>
<li>Removed <code class="docutils literal notranslate"><span class="pre">UrlFilterMiddleware</span></code> from <code class="docutils literal notranslate"><span class="pre">scrapy.contrib</span></code> (already disabled by default)</li>
<li>Offsite middelware doesn’t filter out any request coming from a spider that doesn’t have a allowed_domains attribute (#225)</li>
<li>Removed Spider Manager <code class="docutils literal notranslate"><span class="pre">load()</span></code> method. Now spiders are loaded in the constructor itself.</li>
<li><dl class="first docutils">
<dt>Changes to Scrapy Manager (now called “Crawler”):</dt>
<dd><ul class="first last">
<li><code class="docutils literal notranslate"><span class="pre">scrapy.core.manager.ScrapyManager</span></code> class renamed to <code class="docutils literal notranslate"><span class="pre">scrapy.crawler.Crawler</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">scrapy.core.manager.scrapymanager</span></code> singleton moved to <code class="docutils literal notranslate"><span class="pre">scrapy.project.crawler</span></code></li>
</ul>
</dd>
</dl>
</li>
<li>Moved module: <code class="docutils literal notranslate"><span class="pre">scrapy.contrib.spidermanager</span></code> to <code class="docutils literal notranslate"><span class="pre">scrapy.spidermanager</span></code></li>
<li>Spider Manager singleton moved from <code class="docutils literal notranslate"><span class="pre">scrapy.spider.spiders</span></code> to the <code class="docutils literal notranslate"><span class="pre">spiders`</span> <span class="pre">attribute</span> <span class="pre">of</span> <span class="pre">``scrapy.project.crawler</span></code> singleton.</li>
<li><dl class="first docutils">
<dt>moved Stats Collector classes: (#204)</dt>
<dd><ul class="first last">
<li><code class="docutils literal notranslate"><span class="pre">scrapy.stats.collector.StatsCollector</span></code> to <code class="docutils literal notranslate"><span class="pre">scrapy.statscol.StatsCollector</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">scrapy.stats.collector.SimpledbStatsCollector</span></code> to <code class="docutils literal notranslate"><span class="pre">scrapy.contrib.statscol.SimpledbStatsCollector</span></code></li>
</ul>
</dd>
</dl>
</li>
<li>default per-command settings are now specified in the <code class="docutils literal notranslate"><span class="pre">default_settings</span></code> attribute of command object class (#201)</li>
<li><dl class="first docutils">
<dt>changed arguments of Item pipeline <code class="docutils literal notranslate"><span class="pre">process_item()</span></code> method from <code class="docutils literal notranslate"><span class="pre">(spider,</span> <span class="pre">item)</span></code> to <code class="docutils literal notranslate"><span class="pre">(item,</span> <span class="pre">spider)</span></code></dt>
<dd><ul class="first last">
<li>backward compatibility kept (with deprecation warning)</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>moved <code class="docutils literal notranslate"><span class="pre">scrapy.core.signals</span></code> module to <code class="docutils literal notranslate"><span class="pre">scrapy.signals</span></code></dt>
<dd><ul class="first last">
<li>backward compatibility kept (with deprecation warning)</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>moved <code class="docutils literal notranslate"><span class="pre">scrapy.core.exceptions</span></code> module to <code class="docutils literal notranslate"><span class="pre">scrapy.exceptions</span></code></dt>
<dd><ul class="first last">
<li>backward compatibility kept (with deprecation warning)</li>
</ul>
</dd>
</dl>
</li>
<li>added <code class="docutils literal notranslate"><span class="pre">handles_request()</span></code> class method to <code class="docutils literal notranslate"><span class="pre">BaseSpider</span></code></li>
<li>dropped <code class="docutils literal notranslate"><span class="pre">scrapy.log.exc()</span></code> function (use <code class="docutils literal notranslate"><span class="pre">scrapy.log.err()</span></code> instead)</li>
<li>dropped <code class="docutils literal notranslate"><span class="pre">component</span></code> argument of <code class="docutils literal notranslate"><span class="pre">scrapy.log.msg()</span></code> function</li>
<li>dropped <code class="docutils literal notranslate"><span class="pre">scrapy.log.log_level</span></code> attribute</li>
<li>Added <code class="docutils literal notranslate"><span class="pre">from_settings()</span></code> class methods to Spider Manager, and Item Pipeline Manager</li>
</ul>
</div>
<div class="section" id="id37">
<h5>Changes to settings<a class="headerlink" href="#id37" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Added <code class="docutils literal notranslate"><span class="pre">HTTPCACHE_IGNORE_SCHEMES</span></code> setting to ignore certain schemes on !HttpCacheMiddleware (#225)</li>
<li>Added <code class="docutils literal notranslate"><span class="pre">SPIDER_QUEUE_CLASS</span></code> setting which defines the spider queue to use (#220)</li>
<li>Added <code class="docutils literal notranslate"><span class="pre">KEEP_ALIVE</span></code> setting (#220)</li>
<li>Removed <code class="docutils literal notranslate"><span class="pre">SERVICE_QUEUE</span></code> setting (#220)</li>
<li>Removed <code class="docutils literal notranslate"><span class="pre">COMMANDS_SETTINGS_MODULE</span></code> setting (#201)</li>
<li>Renamed <code class="docutils literal notranslate"><span class="pre">REQUEST_HANDLERS</span></code> to <code class="docutils literal notranslate"><span class="pre">DOWNLOAD_HANDLERS</span></code> and make download handlers classes (instead of functions)</li>
</ul>
</div>
</div>
<div class="section" id="scrapy-0-9">
<h4>Scrapy 0.9<a class="headerlink" href="#scrapy-0-9" title="Permalink to this headline">¶</a></h4>
<p>The numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.</p>
<div class="section" id="id38">
<h5>New features and improvements<a class="headerlink" href="#id38" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Added SMTP-AUTH support to scrapy.mail</li>
<li>New settings added: <code class="docutils literal notranslate"><span class="pre">MAIL_USER</span></code>, <code class="docutils literal notranslate"><span class="pre">MAIL_PASS</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2065">r2065</a> | #149)</li>
<li>Added new scrapy-ctl view command - To view URL in the browser, as seen by Scrapy (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2039">r2039</a>)</li>
<li>Added web service for controlling Scrapy process (this also deprecates the web console. (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2053">r2053</a> | #167)</li>
<li>Support for running Scrapy as a service, for production systems (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1988">r1988</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2054">r2054</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2055">r2055</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2056">r2056</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2057">r2057</a> | #168)</li>
<li>Added wrapper induction library (documentation only available in source code for now). (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2011">r2011</a>)</li>
<li>Simplified and improved response encoding support (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1961">r1961</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1969">r1969</a>)</li>
<li>Added <code class="docutils literal notranslate"><span class="pre">LOG_ENCODING</span></code> setting (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1956">r1956</a>, documentation available)</li>
<li>Added <code class="docutils literal notranslate"><span class="pre">RANDOMIZE_DOWNLOAD_DELAY</span></code> setting (enabled by default) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1923">r1923</a>, doc available)</li>
<li><code class="docutils literal notranslate"><span class="pre">MailSender</span></code> is no longer IO-blocking (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1955">r1955</a> | #146)</li>
<li>Linkextractors and new Crawlspider now handle relative base tag urls (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1960">r1960</a> | #148)</li>
<li>Several improvements to Item Loaders and processors (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2022">r2022</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2023">r2023</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2024">r2024</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2025">r2025</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2026">r2026</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2027">r2027</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2028">r2028</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2029">r2029</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2030">r2030</a>)</li>
<li>Added support for adding variables to telnet console (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2047">r2047</a> | #165)</li>
<li>Support for requests without callbacks (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2050">r2050</a> | #166)</li>
</ul>
</div>
<div class="section" id="id39">
<h5>API changes<a class="headerlink" href="#id39" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Change <code class="docutils literal notranslate"><span class="pre">Spider.domain_name</span></code> to <code class="docutils literal notranslate"><span class="pre">Spider.name</span></code> (SEP-012, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1975">r1975</a>)</li>
<li><code class="docutils literal notranslate"><span class="pre">Response.encoding</span></code> is now the detected encoding (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1961">r1961</a>)</li>
<li><code class="docutils literal notranslate"><span class="pre">HttpErrorMiddleware</span></code> now returns None or raises an exception (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2006">r2006</a> | #157)</li>
<li><code class="docutils literal notranslate"><span class="pre">scrapy.command</span></code> modules relocation (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2035">r2035</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2036">r2036</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2037">r2037</a>)</li>
<li>Added <code class="docutils literal notranslate"><span class="pre">ExecutionQueue</span></code> for feeding spiders to scrape (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2034">r2034</a>)</li>
<li>Removed <code class="docutils literal notranslate"><span class="pre">ExecutionEngine</span></code> singleton (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2039">r2039</a>)</li>
<li>Ported <code class="docutils literal notranslate"><span class="pre">S3ImagesStore</span></code> (images pipeline) to use boto and threads (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2033">r2033</a>)</li>
<li>Moved module: <code class="docutils literal notranslate"><span class="pre">scrapy.management.telnet</span></code> to <code class="docutils literal notranslate"><span class="pre">scrapy.telnet</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/2047">r2047</a>)</li>
</ul>
</div>
<div class="section" id="changes-to-default-settings">
<h5>Changes to default settings<a class="headerlink" href="#changes-to-default-settings" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Changed default <code class="docutils literal notranslate"><span class="pre">SCHEDULER_ORDER</span></code> to <code class="docutils literal notranslate"><span class="pre">DFO</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1939">r1939</a>)</li>
</ul>
</div>
</div>
<div class="section" id="scrapy-0-8">
<h4>Scrapy 0.8<a class="headerlink" href="#scrapy-0-8" title="Permalink to this headline">¶</a></h4>
<p>The numbers like #NNN reference tickets in the old issue tracker (Trac) which is no longer available.</p>
<div class="section" id="id40">
<h5>New features<a class="headerlink" href="#id40" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Added DEFAULT_RESPONSE_ENCODING setting (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1809">r1809</a>)</li>
<li>Added <code class="docutils literal notranslate"><span class="pre">dont_click</span></code> argument to <code class="docutils literal notranslate"><span class="pre">FormRequest.from_response()</span></code> method (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1813">r1813</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1816">r1816</a>)</li>
<li>Added <code class="docutils literal notranslate"><span class="pre">clickdata</span></code> argument to <code class="docutils literal notranslate"><span class="pre">FormRequest.from_response()</span></code> method (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1802">r1802</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1803">r1803</a>)</li>
<li>Added support for HTTP proxies (<code class="docutils literal notranslate"><span class="pre">HttpProxyMiddleware</span></code>) (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1781">r1781</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1785">r1785</a>)</li>
<li>Offsite spider middleware now logs messages when filtering out requests (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1841">r1841</a>)</li>
</ul>
</div>
<div class="section" id="id41">
<h5>Backward-incompatible changes<a class="headerlink" href="#id41" title="Permalink to this headline">¶</a></h5>
<ul class="simple">
<li>Changed <code class="docutils literal notranslate"><span class="pre">scrapy.utils.response.get_meta_refresh()</span></code> signature (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1804">r1804</a>)</li>
<li>Removed deprecated <code class="docutils literal notranslate"><span class="pre">scrapy.item.ScrapedItem</span></code> class - use <code class="docutils literal notranslate"><span class="pre">scrapy.item.Item</span> <span class="pre">instead</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1838">r1838</a>)</li>
<li>Removed deprecated <code class="docutils literal notranslate"><span class="pre">scrapy.xpath</span></code> module - use <code class="docutils literal notranslate"><span class="pre">scrapy.selector</span></code> instead. (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1836">r1836</a>)</li>
<li>Removed deprecated <code class="docutils literal notranslate"><span class="pre">core.signals.domain_open</span></code> signal - use <code class="docutils literal notranslate"><span class="pre">core.signals.domain_opened</span></code> instead (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1822">r1822</a>)</li>
<li><dl class="first docutils">
<dt><code class="docutils literal notranslate"><span class="pre">log.msg()</span></code> now receives a <code class="docutils literal notranslate"><span class="pre">spider</span></code> argument (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1822">r1822</a>)</dt>
<dd><ul class="first last">
<li>Old domain argument has been deprecated and will be removed in 0.9. For spiders, you should always use the <code class="docutils literal notranslate"><span class="pre">spider</span></code> argument and pass spider references. If you really want to pass a string, use the <code class="docutils literal notranslate"><span class="pre">component</span></code> argument instead.</li>
</ul>
</dd>
</dl>
</li>
<li>Changed core signals <code class="docutils literal notranslate"><span class="pre">domain_opened</span></code>, <code class="docutils literal notranslate"><span class="pre">domain_closed</span></code>, <code class="docutils literal notranslate"><span class="pre">domain_idle</span></code></li>
<li><dl class="first docutils">
<dt>Changed Item pipeline to use spiders instead of domains</dt>
<dd><ul class="first last">
<li>The <code class="docutils literal notranslate"><span class="pre">domain</span></code> argument of  <code class="docutils literal notranslate"><span class="pre">process_item()</span></code> item pipeline method was changed to  <code class="docutils literal notranslate"><span class="pre">spider</span></code>, the new signature is: <code class="docutils literal notranslate"><span class="pre">process_item(spider,</span> <span class="pre">item)</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1827">r1827</a> | #105)</li>
<li>To quickly port your code (to work with Scrapy 0.8) just use <code class="docutils literal notranslate"><span class="pre">spider.domain_name</span></code> where you previously used <code class="docutils literal notranslate"><span class="pre">domain</span></code>.</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Changed Stats API to use spiders instead of domains (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1849">r1849</a> | #113)</dt>
<dd><ul class="first last">
<li><code class="docutils literal notranslate"><span class="pre">StatsCollector</span></code> was changed to receive spider references (instead of domains) in its methods (<code class="docutils literal notranslate"><span class="pre">set_value</span></code>, <code class="docutils literal notranslate"><span class="pre">inc_value</span></code>, etc).</li>
<li>added <code class="docutils literal notranslate"><span class="pre">StatsCollector.iter_spider_stats()</span></code> method</li>
<li>removed <code class="docutils literal notranslate"><span class="pre">StatsCollector.list_domains()</span></code> method</li>
<li>Also, Stats signals were renamed and now pass around spider references (instead of domains). Here’s a summary of the changes:</li>
<li>To quickly port your code (to work with Scrapy 0.8) just use <code class="docutils literal notranslate"><span class="pre">spider.domain_name</span></code> where you previously used <code class="docutils literal notranslate"><span class="pre">domain</span></code>. <code class="docutils literal notranslate"><span class="pre">spider_stats</span></code> contains exactly the same data as <code class="docutils literal notranslate"><span class="pre">domain_stats</span></code>.</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><code class="docutils literal notranslate"><span class="pre">CloseDomain</span></code> extension moved to <code class="docutils literal notranslate"><span class="pre">scrapy.contrib.closespider.CloseSpider</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1833">r1833</a>)</dt>
<dd><ul class="first last">
<li><dl class="first docutils">
<dt>Its settings were also renamed:</dt>
<dd><ul class="first last">
<li><code class="docutils literal notranslate"><span class="pre">CLOSEDOMAIN_TIMEOUT</span></code> to <code class="docutils literal notranslate"><span class="pre">CLOSESPIDER_TIMEOUT</span></code></li>
<li><code class="docutils literal notranslate"><span class="pre">CLOSEDOMAIN_ITEMCOUNT</span></code> to <code class="docutils literal notranslate"><span class="pre">CLOSESPIDER_ITEMCOUNT</span></code></li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li>Removed deprecated <code class="docutils literal notranslate"><span class="pre">SCRAPYSETTINGS_MODULE</span></code> environment variable - use <code class="docutils literal notranslate"><span class="pre">SCRAPY_SETTINGS_MODULE</span></code> instead (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1840">r1840</a>)</li>
<li>Renamed setting: <code class="docutils literal notranslate"><span class="pre">REQUESTS_PER_DOMAIN</span></code> to <code class="docutils literal notranslate"><span class="pre">CONCURRENT_REQUESTS_PER_SPIDER</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1830">r1830</a>, <a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1844">r1844</a>)</li>
<li>Renamed setting: <code class="docutils literal notranslate"><span class="pre">CONCURRENT_DOMAINS</span></code> to <code class="docutils literal notranslate"><span class="pre">CONCURRENT_SPIDERS</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1830">r1830</a>)</li>
<li>Refactored HTTP Cache middleware</li>
<li>HTTP Cache middleware has been heavilty refactored, retaining the same functionality except for the domain sectorization which was removed. (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1843">r1843</a> )</li>
<li>Renamed exception: <code class="docutils literal notranslate"><span class="pre">DontCloseDomain</span></code> to <code class="docutils literal notranslate"><span class="pre">DontCloseSpider</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1859">r1859</a> | #120)</li>
<li>Renamed extension: <code class="docutils literal notranslate"><span class="pre">DelayedCloseDomain</span></code> to <code class="docutils literal notranslate"><span class="pre">SpiderCloseDelay</span></code> (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1861">r1861</a> | #121)</li>
<li>Removed obsolete <code class="docutils literal notranslate"><span class="pre">scrapy.utils.markup.remove_escape_chars</span></code> function - use <code class="docutils literal notranslate"><span class="pre">scrapy.utils.markup.replace_escape_chars</span></code> instead (<a class="reference external" href="http://hg.scrapy.org/scrapy/changeset/1865">r1865</a>)</li>
</ul>
</div>
</div>
<div class="section" id="scrapy-0-7">
<h4>Scrapy 0.7<a class="headerlink" href="#scrapy-0-7" title="Permalink to this headline">¶</a></h4>
<p>First release of Scrapy.</p>
</div>
</div>
<span id="document-contributing"></span><div class="section" id="contributing-to-scrapy">
<span id="topics-contributing"></span><h3>Contributing to Scrapy<a class="headerlink" href="#contributing-to-scrapy" title="Permalink to this headline">¶</a></h3>
<div class="admonition important">
<p class="first admonition-title">Important</p>
<p class="last">Double check that you are reading the most recent version of this document at
<a class="reference external" href="https://docs.scrapy.org/en/master/contributing.html">https://docs.scrapy.org/en/master/contributing.html</a></p>
</div>
<p>There are many ways to contribute to Scrapy. Here are some of them:</p>
<ul class="simple">
<li>Blog about Scrapy. Tell the world how you’re using Scrapy. This will help
newcomers with more examples and will help the Scrapy project to increase its
visibility.</li>
<li>Report bugs and request features in the <a class="reference external" href="https://github.com/scrapy/scrapy/issues">issue tracker</a>, trying to follow
the guidelines detailed in <a class="reference internal" href="#reporting-bugs">Reporting bugs</a> below.</li>
<li>Submit patches for new functionalities and/or bug fixes. Please read
<a class="reference internal" href="#writing-patches"><span class="std std-ref">Writing patches</span></a> and <a class="reference internal" href="#id2">Submitting patches</a> below for details on how to
write and submit a patch.</li>
<li>Join the <a class="reference external" href="https://reddit.com/r/scrapy">Scrapy subreddit</a> and share your ideas on how to
improve Scrapy. We’re always open to suggestions.</li>
<li>Answer Scrapy questions at
<a class="reference external" href="https://stackoverflow.com/questions/tagged/scrapy">Stack Overflow</a>.</li>
</ul>
<div class="section" id="reporting-bugs">
<h4>Reporting bugs<a class="headerlink" href="#reporting-bugs" title="Permalink to this headline">¶</a></h4>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Please report security issues <strong>only</strong> to
<a class="reference external" href="mailto:scrapy-security&#37;&#52;&#48;googlegroups&#46;com">scrapy-security<span>&#64;</span>googlegroups<span>&#46;</span>com</a>. This is a private list only open to
trusted Scrapy developers, and its archives are not public.</p>
</div>
<p>Well-written bug reports are very helpful, so keep in mind the following
guidelines when you’re going to report a new bug.</p>
<ul class="simple">
<li>check the <a class="reference internal" href="index.html#faq"><span class="std std-ref">FAQ</span></a> first to see if your issue is addressed in a
well-known question</li>
<li>if you have a general question about scrapy usage, please ask it at
<a class="reference external" href="https://stackoverflow.com/questions/tagged/scrapy">Stack Overflow</a>
(use “scrapy” tag).</li>
<li>check the <a class="reference external" href="https://github.com/scrapy/scrapy/issues">open issues</a> to see if the issue has already been reported. If it
has, don’t dismiss the report, but check the ticket history and comments. If
you have additional useful information, please leave a comment, or consider
<a class="reference internal" href="#writing-patches"><span class="std std-ref">sending a pull request</span></a> with a fix.</li>
<li>search the <a class="reference external" href="https://groups.google.com/forum/#!forum/scrapy-users">scrapy-users</a> list and <a class="reference external" href="https://reddit.com/r/scrapy">Scrapy subreddit</a> to see if it has
been discussed there, or if you’re not sure if what you’re seeing is a bug.
You can also ask in the <code class="docutils literal notranslate"><span class="pre">#scrapy</span></code> IRC channel.</li>
<li>write <strong>complete, reproducible, specific bug reports</strong>. The smaller the test
case, the better. Remember that other developers won’t have your project to
reproduce the bug, so please include all relevant files required to reproduce
it. See for example StackOverflow’s guide on creating a
<a class="reference external" href="https://stackoverflow.com/help/mcve">Minimal, Complete, and Verifiable example</a> exhibiting the issue.</li>
<li>the most awesome way to provide a complete reproducible example is to
send a pull request which adds a failing test case to the
Scrapy testing suite (see <a class="reference internal" href="#submitting-patches"><span class="std std-ref">Submitting patches</span></a>).
This is helpful even if you don’t have an intention to
fix the issue yourselves.</li>
<li>include the output of <code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">version</span> <span class="pre">-v</span></code> so developers working on your bug
know exactly which version and platform it occurred on, which is often very
helpful for reproducing it, or knowing if it was already fixed.</li>
</ul>
</div>
<div class="section" id="writing-patches">
<span id="id1"></span><h4>Writing patches<a class="headerlink" href="#writing-patches" title="Permalink to this headline">¶</a></h4>
<p>The better a patch is written, the higher the chances that it’ll get accepted and the sooner it will be merged.</p>
<p>Well-written patches should:</p>
<ul>
<li><p class="first">contain the minimum amount of code required for the specific change. Small
patches are easier to review and merge. So, if you’re doing more than one
change (or bug fix), please consider submitting one patch per change. Do not
collapse multiple changes into a single patch. For big changes consider using
a patch queue.</p>
</li>
<li><p class="first">pass all unit-tests. See <a class="reference internal" href="#id4">Running tests</a> below.</p>
</li>
<li><p class="first">include one (or more) test cases that check the bug fixed or the new
functionality added. See <a class="reference internal" href="#writing-tests">Writing tests</a> below.</p>
</li>
<li><p class="first">if you’re adding or changing a public (documented) API, please include
the documentation changes in the same patch.  See <a class="reference internal" href="#id3">Documentation policies</a>
below.</p>
</li>
<li><p class="first">if you’re adding a private API, please add a regular expression to the
<code class="docutils literal notranslate"><span class="pre">coverage_ignore_pyobjects</span></code> variable of <code class="docutils literal notranslate"><span class="pre">docs/conf.py</span></code> to exclude the new
private API from documentation coverage checks.</p>
<p>To see if your private API is skipped properly, generate a documentation
coverage report as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tox</span> <span class="o">-</span><span class="n">e</span> <span class="n">docs</span><span class="o">-</span><span class="n">coverage</span>
</pre></div>
</div>
</li>
</ul>
</div>
<div class="section" id="submitting-patches">
<span id="id2"></span><h4>Submitting patches<a class="headerlink" href="#submitting-patches" title="Permalink to this headline">¶</a></h4>
<p>The best way to submit a patch is to issue a <a class="reference external" href="https://help.github.com/en/articles/creating-a-pull-request">pull request</a> on GitHub,
optionally creating a new issue first.</p>
<p>Remember to explain what was fixed or the new functionality (what it is, why
it’s needed, etc). The more info you include, the easier will be for core
developers to understand and accept your patch.</p>
<p>You can also discuss the new functionality (or bug fix) before creating the
patch, but it’s always good to have a patch ready to illustrate your arguments
and show that you have put some additional thought into the subject. A good
starting point is to send a pull request on GitHub. It can be simple enough to
illustrate your idea, and leave documentation/tests for later, after the idea
has been validated and proven useful. Alternatively, you can start a
conversation in the <a class="reference external" href="https://reddit.com/r/scrapy">Scrapy subreddit</a> to discuss your idea first.</p>
<p>Sometimes there is an existing pull request for the problem you’d like to
solve, which is stalled for some reason. Often the pull request is in a
right direction, but changes are requested by Scrapy maintainers, and the
original pull request author hasn’t had time to address them.
In this case consider picking up this pull request: open
a new pull request with all commits from the original pull request, as well as
additional changes to address the raised issues. Doing so helps a lot; it is
not considered rude as soon as the original author is acknowledged by keeping
his/her commits.</p>
<p>You can pull an existing pull request to a local branch
by running <code class="docutils literal notranslate"><span class="pre">git</span> <span class="pre">fetch</span> <span class="pre">upstream</span> <span class="pre">pull/$PR_NUMBER/head:$BRANCH_NAME_TO_CREATE</span></code>
(replace ‘upstream’ with a remote name for scrapy repository,
<code class="docutils literal notranslate"><span class="pre">$PR_NUMBER</span></code> with an ID of the pull request, and <code class="docutils literal notranslate"><span class="pre">$BRANCH_NAME_TO_CREATE</span></code>
with a name of the branch you want to create locally).
See also: <a class="reference external" href="https://help.github.com/articles/checking-out-pull-requests-locally/#modifying-an-inactive-pull-request-locally">https://help.github.com/articles/checking-out-pull-requests-locally/#modifying-an-inactive-pull-request-locally</a>.</p>
<p>When writing GitHub pull requests, try to keep titles short but descriptive.
E.g. For bug #411: “Scrapy hangs if an exception raises in start_requests”
prefer “Fix hanging when exception occurs in start_requests (#411)”
instead of “Fix for #411”. Complete titles make it easy to skim through
the issue tracker.</p>
<p>Finally, try to keep aesthetic changes (<span class="target" id="index-0"></span><a class="pep reference external" href="https://www.python.org/dev/peps/pep-0008"><strong>PEP 8</strong></a> compliance, unused imports
removal, etc) in separate commits from functional changes. This will make pull
requests easier to review and more likely to get merged.</p>
</div>
<div class="section" id="coding-style">
<h4>Coding style<a class="headerlink" href="#coding-style" title="Permalink to this headline">¶</a></h4>
<p>Please follow these coding conventions when writing code for inclusion in
Scrapy:</p>
<ul class="simple">
<li>Unless otherwise specified, follow <span class="target" id="index-1"></span><a class="pep reference external" href="https://www.python.org/dev/peps/pep-0008"><strong>PEP 8</strong></a>.</li>
<li>It’s OK to use lines longer than 80 chars if it improves the code
readability.</li>
<li>Don’t put your name in the code you contribute; git provides enough
metadata to identify author of the code.
See <a class="reference external" href="https://help.github.com/articles/setting-your-username-in-git/">https://help.github.com/articles/setting-your-username-in-git/</a> for
setup instructions.</li>
</ul>
</div>
<div class="section" id="documentation-policies">
<span id="id3"></span><h4>Documentation policies<a class="headerlink" href="#documentation-policies" title="Permalink to this headline">¶</a></h4>
<p>For reference documentation of API members (classes, methods, etc.) use
docstrings and make sure that the Sphinx documentation uses the <a class="reference external" href="http://www.sphinx-doc.org/en/stable/ext/autodoc.html">autodoc</a>
extension to pull the docstrings. API reference documentation should follow
docstring conventions (<a class="reference external" href="https://www.python.org/dev/peps/pep-0257/">PEP 257</a>) and be IDE-friendly: short, to the point,
and it may provide short examples.</p>
<p>Other types of documentation, such as tutorials or topics, should be covered in
files within the <code class="docutils literal notranslate"><span class="pre">docs/</span></code> directory. This includes documentation that is
specific to an API member, but goes beyond API reference documentation.</p>
<p>In any case, if something is covered in a docstring, use the <a class="reference external" href="http://www.sphinx-doc.org/en/stable/ext/autodoc.html">autodoc</a>
extension to pull the docstring into the documentation instead of duplicating
the docstring in files within the <code class="docutils literal notranslate"><span class="pre">docs/</span></code> directory.</p>
</div>
<div class="section" id="tests">
<h4>Tests<a class="headerlink" href="#tests" title="Permalink to this headline">¶</a></h4>
<p>Tests are implemented using the <a class="reference external" href="https://twistedmatrix.com/documents/current/core/development/policy/test-standard.html">Twisted unit-testing framework</a>, running
tests requires <a class="reference external" href="https://pypi.python.org/pypi/tox">tox</a>.</p>
<div class="section" id="running-tests">
<span id="id4"></span><h5>Running tests<a class="headerlink" href="#running-tests" title="Permalink to this headline">¶</a></h5>
<p>Make sure you have a recent enough <a class="reference external" href="https://pypi.python.org/pypi/tox">tox</a> installation:</p>
<blockquote>
<div><code class="docutils literal notranslate"><span class="pre">tox</span> <span class="pre">--version</span></code></div></blockquote>
<p>If your version is older than 1.7.0, please update it first:</p>
<blockquote>
<div><code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">-U</span> <span class="pre">tox</span></code></div></blockquote>
<p>To run all tests go to the root directory of Scrapy source code and run:</p>
<blockquote>
<div><code class="docutils literal notranslate"><span class="pre">tox</span></code></div></blockquote>
<p>To run a specific test (say <code class="docutils literal notranslate"><span class="pre">tests/test_loader.py</span></code>) use:</p>
<blockquote>
<div><code class="docutils literal notranslate"><span class="pre">tox</span> <span class="pre">--</span> <span class="pre">tests/test_loader.py</span></code></div></blockquote>
<p>To run the tests on a specific <a class="reference external" href="https://pypi.python.org/pypi/tox">tox</a> environment, use <code class="docutils literal notranslate"><span class="pre">-e</span> <span class="pre">&lt;name&gt;</span></code> with an
environment name from <code class="docutils literal notranslate"><span class="pre">tox.ini</span></code>. For example, to run the tests with Python
3.6 use:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tox</span> <span class="o">-</span><span class="n">e</span> <span class="n">py36</span>
</pre></div>
</div>
<p>You can also specify a comma-separated list of environmets, and use <a class="reference external" href="https://tox.readthedocs.io/en/latest/example/basic.html#parallel-mode">tox’s
parallel mode</a> to run the tests on multiple environments in parallel:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tox</span> <span class="o">-</span><span class="n">e</span> <span class="n">py27</span><span class="p">,</span><span class="n">py36</span> <span class="o">-</span><span class="n">p</span> <span class="n">auto</span>
</pre></div>
</div>
<p>To pass command-line options to <a class="reference external" href="https://docs.pytest.org/en/latest/usage.html">pytest</a>, add them after <code class="docutils literal notranslate"><span class="pre">--</span></code> in your call to
<a class="reference external" href="https://pypi.python.org/pypi/tox">tox</a>. Using <code class="docutils literal notranslate"><span class="pre">--</span></code> overrides the default positional arguments defined in
<code class="docutils literal notranslate"><span class="pre">tox.ini</span></code>, so you must include those default positional arguments
(<code class="docutils literal notranslate"><span class="pre">scrapy</span> <span class="pre">tests</span></code>) after <code class="docutils literal notranslate"><span class="pre">--</span></code> as well:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tox</span> <span class="o">--</span> <span class="n">scrapy</span> <span class="n">tests</span> <span class="o">-</span><span class="n">x</span>  <span class="c1"># stop after first failure</span>
</pre></div>
</div>
<p>You can also use the <a class="reference external" href="https://docs.pytest.org/en/3.0.0/xdist.html">pytest-xdist</a> plugin. For example, to run all tests on
the Python 3.6 <a class="reference external" href="https://pypi.python.org/pypi/tox">tox</a> environment using all your CPU cores:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tox</span> <span class="o">-</span><span class="n">e</span> <span class="n">py36</span> <span class="o">--</span> <span class="n">scrapy</span> <span class="n">tests</span> <span class="o">-</span><span class="n">n</span> <span class="n">auto</span>
</pre></div>
</div>
<p>To see coverage report install <a class="reference external" href="https://pypi.python.org/pypi/coverage">coverage</a> (<code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">coverage</span></code>) and run:</p>
<blockquote>
<div><code class="docutils literal notranslate"><span class="pre">coverage</span> <span class="pre">report</span></code></div></blockquote>
<p>see output of <code class="docutils literal notranslate"><span class="pre">coverage</span> <span class="pre">--help</span></code> for more options like html or xml report.</p>
</div>
<div class="section" id="writing-tests">
<h5>Writing tests<a class="headerlink" href="#writing-tests" title="Permalink to this headline">¶</a></h5>
<p>All functionality (including new features and bug fixes) must include a test
case to check that it works as expected, so please include tests for your
patches if you want them to get accepted sooner.</p>
<p>Scrapy uses unit-tests, which are located in the <a class="reference external" href="https://github.com/scrapy/scrapy/tree/master/tests">tests/</a> directory.
Their module name typically resembles the full path of the module they’re
testing. For example, the item loaders code is in:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scrapy</span><span class="o">.</span><span class="n">loader</span>
</pre></div>
</div>
<p>And their unit-tests are in:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tests</span><span class="o">/</span><span class="n">test_loader</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
</div>
</div>
</div>
<span id="document-versioning"></span><div class="section" id="versioning-and-api-stability">
<span id="versioning"></span><h3>Versioning and API Stability<a class="headerlink" href="#versioning-and-api-stability" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id1">
<h4>Versioning<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h4>
<p>There are 3 numbers in a Scrapy version: <em>A.B.C</em></p>
<ul class="simple">
<li><em>A</em> is the major version. This will rarely change and will signify very
large changes.</li>
<li><em>B</em> is the release number. This will include many changes including features
and things that possibly break backward compatibility, although we strive to
keep theses cases at a minimum.</li>
<li><em>C</em> is the bugfix release number.</li>
</ul>
<p>Backward-incompatibilities are explicitly mentioned in the <a class="reference internal" href="index.html#news"><span class="std std-ref">release notes</span></a>,
and may require special attention before upgrading.</p>
<p>Development releases do not follow 3-numbers version and are generally
released as <code class="docutils literal notranslate"><span class="pre">dev</span></code> suffixed versions, e.g. <code class="docutils literal notranslate"><span class="pre">1.3dev</span></code>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>With Scrapy 0.* series, Scrapy used <a class="reference external" href="https://en.wikipedia.org/wiki/Software_versioning#Odd-numbered_versions_for_development_releases">odd-numbered versions for development releases</a>.
This is not the case anymore from Scrapy 1.0 onwards.</p>
<p class="last">Starting with Scrapy 1.0, all releases should be considered production-ready.</p>
</div>
<p>For example:</p>
<ul class="simple">
<li><em>1.1.1</em> is the first bugfix release of the <em>1.1</em> series (safe to use in
production)</li>
</ul>
</div>
<div class="section" id="api-stability">
<h4>API Stability<a class="headerlink" href="#api-stability" title="Permalink to this headline">¶</a></h4>
<p>API stability was one of the major goals for the <em>1.0</em> release.</p>
<p>Methods or functions that start with a single dash (<code class="docutils literal notranslate"><span class="pre">_</span></code>) are private and
should never be relied as stable.</p>
<p>Also, keep in mind that stable doesn’t mean complete: stable APIs could grow
new methods or functionality but the existing methods should keep working the
same way.</p>
</div>
</div>
</div>
<dl class="docutils">
<dt><a class="reference internal" href="index.html#document-news"><span class="doc">Release notes</span></a></dt>
<dd>See what has changed in recent Scrapy versions.</dd>
<dt><a class="reference internal" href="index.html#document-contributing"><span class="doc">Contributing to Scrapy</span></a></dt>
<dd>Learn how to contribute to the Scrapy project.</dd>
<dt><a class="reference internal" href="index.html#document-versioning"><span class="doc">Versioning and API Stability</span></a></dt>
<dd>Understand Scrapy versioning and API stability.</dd>
</dl>
</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2008–2018, Scrapy developers
      
        <span class="commit">
          Revision <code>e22a8c8c</code>.
        </span>
      

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
 
<script type="text/javascript">
!function(){var analytics=window.analytics=window.analytics||[];if(!analytics.initialize)if(analytics.invoked)window.console&&console.error&&console.error("Segment snippet included twice.");else{analytics.invoked=!0;analytics.methods=["trackSubmit","trackClick","trackLink","trackForm","pageview","identify","reset","group","track","ready","alias","page","once","off","on"];analytics.factory=function(t){return function(){var e=Array.prototype.slice.call(arguments);e.unshift(t);analytics.push(e);return analytics}};for(var t=0;t<analytics.methods.length;t++){var e=analytics.methods[t];analytics[e]=analytics.factory(e)}analytics.load=function(t){var e=document.createElement("script");e.type="text/javascript";e.async=!0;e.src=("https:"===document.location.protocol?"https://":"http://")+"cdn.segment.com/analytics.js/v1/"+t+"/analytics.min.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(e,n)};analytics.SNIPPET_VERSION="3.1.0";
analytics.load("8UDQfnf3cyFSTsM4YANnW5sXmgZVILbA");
analytics.page();
}}();

analytics.ready(function () {
    ga('require', 'linker');
    ga('linker:autoLink', ['scrapinghub.com', 'crawlera.com']);
});
</script>


</body>
</html>